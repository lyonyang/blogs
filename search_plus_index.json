{"./":{"url":"./","title":"介绍","keywords":"","body":"Welcome to Lyon's blog! 🍀 *本博客即将重构...精彩内容就快出来啦... 如需转载, 请注明出处 如果我的博客对你有帮助 , 那就帮我点个星星吧 🤣 Star 作者 🍀 个人主页 : Blogs GitHub : https://github.com/lyonyang QQ : 547903993 微信 : Happy547903993 感谢 🍀 感谢以下老哥们提供的宝贵意见与指正, 一起进步 Jesse 博客向导 🍀 本博客由 Travis CI 自动构建 目录 : . ├── Python ├── Go ├── MySQL ├── Front-End │ └── Vue ├── Web-Framework │ ├── Django │ ├── Django-Rest-Framework │ └── Flask ├── Redis ├── DesignPattern ├── Algorithms ├── Linux │ └── Git └── Read 博客搭建指南 : GitHub Pages&Gitbook&Travis CI持续构建博客 "},"SUMMARY.html":{"url":"SUMMARY.html","title":"目录","keywords":"","body":"Summary 介绍 目录 Python 基础篇 语言基础 数字 字符串 元组 列表 字典 集合 字符编码 文件操作 函数篇 函数基础 匿名函数 函数进阶 内置函数 迭代器 生成器 递归 对象篇 面向对象 继承 多态 封装 方法转换 魔术方法 反射 异常处理 模块篇 模块 包 正则表达式 序列化 os模块 random模块 sys模块 wsgiref模块 网络篇 网络编程 Socket Socket实现QQ聊天 Socket实现远程执行命令 粘包 Socketserver实现多并发 并发篇 进程与线程 多线程 多进程 多进程实例及回调函数 协程 IO多路复用 实现线程池 内存篇 对象机制 对象的创建 整数对象 字符串对象 List对象 Dict对象 Tuple对象 垃圾回收 元类 番外篇 Python - 第三方库之PyMySQL Python - 第三方库之MySQLdb Python - 第三方库之SQlAlchemy Go Golang - 语言基础 MySQL MySQL - 库操作 MySQL - 数据类型 MySQL - 存储引擎 MySQL - 表操作 MySQL - 数据操作 MySQL - 索引 MySQL - 视图 MySQL - 存储过程与函数 MySQL - 触发器 MySQL - 事务 MySQL - SQL注入 SQL优化 前端 Web开发 - HTML-head Web开发 - HTML-body Web开发 - CSS Web开发 - JavaScript Web开发 - BOM Web开发 - DOM Web开发 - jQuery Web开发 - Ajax Vue Vue - 介绍 Vue - 实例 Vue - 模板语法 Vue - 计算属性和侦听器 Vue - Class与Style 绑定 Vue - 条件渲染 Vue - 列表渲染 Vue - 事件处理 Vue - 组件基础 Vue - Vue-cli Web框架 HTTP基础 REST ORM简介 Django Web框架简介 Django - Django初识 Django - Settings Django - Urls Django - Views Django - Model Django - Model Fields Django - Model Field Options Django - Model QuerySet API Django - Model Making queries Django - Forms Django - Template Django - Template Language Django - Middleware Django - Sessions Django - Authentication System Django - 源码之startproject Django - 源码之runserver Django - 源码之middleware Django - 源码之url Django - 源码之admin Django - Django命令整理 Django-Rest-Framework Quickstart Tutorial 1 Serialization Tutorial 2 Requests and Responses Tutorial 3 Class-based Views Tutorial 4 Authentication & Permissions Tutorial 5 Relationships & Hyperlinked APIs Tutorial 6 ViewSets & Routers Tutorial 7 Schemas & client libraries Flask Flask - 源码简要说明 Flask - 源码之开始 Flask - 源码之配置 Flask - 源码之路由 Flask - 源码之视图 Flask - 源码之蓝图 Flask - 源码之本地线程 Flask - 源码之上下文 Flask - 源码之信号 Flask - 扩展 DBUtils virtualenv基本使用 Tornado Redis Redis - 简介 Redis - 配置参数说明 Redis - 基础命令 Redis - 数据库 Redis - 事务 Redis - 主从复制 Redis - 集群 Redis - Sentinel Python操作MongoDB Python操作Redis 设计模式 六大原则 单例模式 算法 算法基础 数组 栈 链表 树 队列 哈希表 双指针 并查集 前缀树 KMP&RK 跳表 剪枝 Python时间复杂度 优先队列 力扣题解 双端队列 排序合集 树 链表 Linux Command 常用命令 vim Docker Git Git&GitHub Git基础命令 GitHub Pages&Gitbook&Travis CI持续构建博客 Travis CI RabbitMQ Read "},"01-Python/":{"url":"01-Python/","title":"Python","keywords":"","body":"Attack on Python 🐍 介绍 本目录下为 进击的Python 系列文章 目录如下 Python ├── 基础篇 ├── 函数篇 ├── 对象篇 ├── 模块篇 ├── 网络篇 ├── 并发篇 ├── 内存篇 └── 番外篇 欢迎阅读 , 文章尚未整理完成 , 持续更新中 ... "},"01-Python/01-基础篇/":{"url":"01-Python/01-基础篇/","title":"基础篇","keywords":"","body":"Attack on Python - 基础篇 🐍 前言 基础篇中的内容 , 应对的是 Python 的基础语法 , 以及基础数据类型的文章 在开始之前 , 你可以熟悉一下 Python 的语言参考 : The Python Language Reference 最好的教程就是官方文档 , 所以阅读官方文档是一个好的学习习惯 介绍 Python基础主要包括基础语句 , 基础数据类型 , 字符编码 , 文件操作等 基础语句 Hello World 变量 行和缩进 多行语句 注释 input print 数据运算 条件语句 for while 数据类型 数字 , Number 字符串 , String 元组 , Tuple 列表 , List 字典 , Dictionary 集合 , Set "},"01-Python/01-基础篇/01-语言基础.html":{"url":"01-Python/01-基础篇/01-语言基础.html","title":"语言基础","keywords":"","body":"Attack on Python - 语言基础 🐍 Hello World 学一门语言基本都是从Hello World开始的 , 如下一个最简单的Hello World程序 Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> print(\"Hello World\") Hello World >>> 此为Python 3.5.2版本 , 上述代码为在Windows环境命令行中执行 , 即以管理员身份运行 \"命令提示符\" # 已添加环境变量 C:\\Windows\\system32>python Python 2.7.x 版本的Hello World程序 Python 2.7.13 (v2.7.13:a06454b1afa1, Dec 17 2016, 20:53:40) [MSC v.1500 64 bit (AMD64)] on win32 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> print \"Hello World\" Hello World >>> 当然使用Python shell 仅仅适合处理一些非常简单的小程序 , 对于比较复杂 , 代码量稍微大一点的就不适合了 变量 变量用于存储在计算机程序中被引用和操作的信息 变量可能被明确为是能表示可变状态、具有存储空间的抽象(如在Java和Visual Basic中) , 变量的唯一目的是在内存中标记和存储数据 , 这些数据可以在整个程序中使用 声明变量 # 声明一个变量name,并绑定值\"Lyon\" name = \"Lyon\" # 同时为多个变量赋值 a = b = c = 1 Python变量定义的规则 : 变量名只能是 字母、数字或者下划线的任意组合 变量名的第一个字符不能是数字 以下关键字不能声明为变量名 , 属于Python中的保留字and and exec not assert finally or break for pass class from print continue global raise def if return del import try elif in while else is with except lambda yield 行和缩进 Python 与其他语言最大的区别就是 , Python 的代码块不使用大括号 {} 来控制类 , 函数以及其他逻辑判断 , Python 最具特色的就是用缩进来写模块 缩进的空白数量是可变的 , 但是所有代码块语句必须包含相同的缩进空白数量 , 这个必须严格执行 if True: print \"True\" else: print \"False\" ''' 执行会出现错误提醒: IndentationError: unexpected indent ''' IndentationError: unexpected indent 错误是Python编译器在告诉你 , 你的文件里格式有问题 , 可能是tab和空格没对齐的问题 还有IndentationError: unindent does not match any outer indentation level 错误表明 , 你使用的缩进方式不一致 , 有的是 tab 键缩进 , 有的是空格缩进 , 改为一致即可。 因此 , 在 Python 的代码块中必须使用相同数目的行首缩进空格数 建议你在每个缩进层次使用 单个制表符 或 两个空格 或 四个空格 , 切记不能混用 多行语句 Python语句中一般以新作为为语句的结束符 但是我们可以使用斜杠 \\ 将一行的语句分为多行显示 , 如下 : total = item_one + \\ item_two + \\ item_three 语句中包含 [], {} 或 () 括号就不需要使用多行连接符 , 如下实例 : days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'] 同一行使用多条语句 Python可以在同一行中使用多条语句 , 语句之间使用分号 ; 分割 , 如下 : #!/usr/bin/python import sys; x = 'runoob'; sys.stdout.write(x + '\\n') 字符串 Python 可以使用引号( ' )、双引号( \" )、三引号( ''' 或 \"\"\" ) 来表示字符串 , 引号的开始与结束必须的相同类型的 其中三引号可以由多行组成 , 编写多行文本的快捷语法 , 常用于文档字符串 , 在文件的特定地点 , 被当做注释 word = 'word' sentence = \"This is a sentence\" paragraph = \"\"\"This is a paragraph Contains multiple statements\"\"\" 注释 Python中单行注释采用 # 开头 # 第一个注释 print(\"Hello,Python\") # 第二个注释 Python中多行注释采用三个单引号 ''' 或三个双引号 \"\"\" ''' 这是多行注释，使用单引号。 这是多行注释，使用单引号。 这是多行注释，使用单引号。 ''' \"\"\" 这是多行注释，使用双引号。 这是多行注释，使用双引号。 这是多行注释，使用双引号。 \"\"\" 字符编码 Python解释器在加载 .py 文件中的代码时 , 会对内容进行编码 (默认ASCII) 然而ASCII是无法处理中文的 , 所以如果我们的代码中出现了中文 , 那么需要在代码的顶端加上一句声明 #!/usr/bin/env python # -*- coding:utf-8 -*- ''' 第一行,为脚本语言指定解释器 第二行,告诉Python解释器,用utf-8编码来进行编码 ''' 用户输入 当我们需要用户自己输入信息时 , 就可以使用input 语句 , 如下 : # 让用户进行输入,并用变量name接收用户输入的值 name = input(\"Please input your name:\") 上述代码 , 会一直等待用户输入 , 直到用户按回车键后才会退出 输出 当我们需要让控制台输出一些我们想要的信息时 , 可以使用print 语句 , 在Hello World里我们已经见到了 #!/usr/bin/python # -*- coding: UTF-8 -*- # Author:Lyon x = \"a\" y = \"b\" # 换行输出 print(x) print(y) print('---------') # 不换行输出 print(x,) print(y,) # 不换行输出 print(x, y) ''' 执行结果: a b --------- a b a b ''' 数据类型 我们知道在变量创建时 , 会在内存中开辟一个空间 , 用来存放变量的值 , 而这些变量的值可以是各种各样的类型 , 如 : 数字 , 字符串 , 列表 , 元组 , 字典 , 集合等等 数字类型 int (整型) 整数的大小范围由计算机字长确定 long (长整型) 跟C语言不同 , Python的长整数没有指定位宽 , 即 : Python没有限制长整数数值的大小 , 但实际上由于机器内存有限 , 我们使用的长整数数值不可能无限大 注意 , 自从Python 2.2 起 , 如果整数发生溢出 , Python会自动将整数数据转换为长整数 , 所以如今在长整数数据后面不加字母 L 也不会导致严重后果了 float (浮点型) 浮点数用来处理实数 , 即带有小数的数字 , 类似于C语言中的double类型 , 占8个字节(64位) , 其中52位表示底 , 11位表示指数 , 剩下的一位表示符号 complex (复数) 复数由实数部分和虚数部分组成，一般形式为x+yh，其中的x是复数的实数部分，y是复数的虚数部分，这里的x和y都是实数 注 : Python中存在整数小数字池 : -5~257 , 在此范围的整数数字共享 布尔值 即真或假 , 1或0 更多数据类型 , 后续文章中详细整理 数据运算 算术运算符 运算符 描述 实例 + 加 - 两个对象相加 a + b 输出结果 30 - 减 - 得到负数或是一个数减去另一个数 a - b 输出结果 -10 * 乘 - 两个数相乘或是返回一个被重复若干次的字符串 a * b 输出结果 200 / 除 - x除以y b / a 输出结果 2 % 取模 - 返回除法的余数 b % a 输出结果 0 ** 幂 - 返回x的y次幂 a**b 为10的20次方 , 输出结果 100000000000000000000 // 取整除 - 返回商的整数部分 9//2 输出结果 4 , 9.0//2.0 输出结果 4.0 比较运算符 运算符 描述 实例 == 等于 - 比较对象是否相等 (a == b) 返回 False != 不等于 - 比较两个对象是否不相等 (a != b) 返回 True <> 不等于 - 比较两个对象是否不相等 (a <> b) 返回 True这个运算符类似 != > 大于 - 返回x是否大于y (a > b) 返回 False 小于 - 返回x是否小于y , 所有比较运算符返回1表示真 , 返回0表示假这分别与特殊的变量True和False等价 , 注意 , 这些变量名的大写 (a >= 大于等于 - 返回x是否大于等于y。 (a >= b) 返回 False 小于等于 - 返回x是否小于等于y。 (a 赋值运算符 运算符 描述 实例 = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c += 加法赋值运算符 c += a 等效于 c = c + a -= 减法赋值运算符 c -= a 等效于 c = c - a *= 乘法赋值运算符 c = a 等效于 c = c a /= 除法赋值运算符 c /= a 等效于 c = c / a %= 取模赋值运算符 c %= a 等效于 c = c % a **= 幂赋值运算符 c **= a 等效于 c = c ** a //= 取整除赋值运算符 c //= a 等效于 c = c // a 位运算符 运算符 描述 实例 & 按位与运算符 : 参与运算的两个值 , 如果两个相应位都为1 , 则该位的结果为1 , 否则为0 (a & b) 输出结果 12 , 二进制解释 : 0000 1100 \\ 按位或运算符 : 只要对应的二个二进位有一个为1时 , 结果位就为1 (a 丨 b) 输出结果 61 , 二进制解释 : 0011 1101 ^ 按位异或运算符 : 当两对应的二进位相异时 , 结果为1 (a ^ b) 输出结果 49 , 二进制解释 : 0011 0001 ~ 按位取反运算符 : 对数据的每个二进制位取反 , 即把1变为0 , 把0变为1 , ~x 类似于 -x-1 (~a ) 输出结果 -61 , 二进制解释 : 1100 0011 , 在一个有符号二进制数的补码形式 左移动运算符 : 运算数的各二进位全部左移若干位 , 由 a >> 右移动运算符 : 把\">>\"左边的运算数的各二进位全部右移若干位 , >> 右边的数字指定了移动的位数 a >> 2 输出结果 15 , 二进制解释 : 0000 1111 逻辑运算符 运算符 逻辑表达式 描述 实例 and x and y 布尔\"与\" - 如果 x 为 False , x and y 返回 False , 否则它返回 y 的计算值 (a and b) 返回 20 or x or y 布尔\"或\" - 如果 x 是非 0 , 它返回 x 的值 , 否则它返回 y 的计算值 (a or b) 返回 10 not not x 布尔\"非\" - 如果 x 为 True , 返回 False , 如果 x 为 False , 它返回 True not(a and b) 返回 False 成员运算符 运算符 描述 实例 in 如果在指定的序列中找到值返回 True , 否则返回 False x 在 y 序列中 , 如果 x 在 y 序列中返回 True not in 如果在指定的序列中没有找到值返回 True , 否则返回 False x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True 身份运算符 运算符 描述 实例 is is 是判断两个标识符是不是引用自一个对象 x is y , 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True , 否则返回 False is not is not 是判断两个标识符是不是引用自不同对象 x is not y , 类似 **id(a) != id(b) , 如果引用的不是同一个对象则返回结果 True , 否则返回 False 运算符优先级表 , 从上到下优先级依次增高 Operator Description lambda Lambda expression if – else Conditional expression or Boolean OR and Boolean AND not x Boolean NOT in, not in, is, is not, , , >, >=, !=, == Comparisons, including membership tests and identity tests 丨 Bitwise OR ^ Bitwise XOR & Bitwise AND , >> Shifts +, - Addition and subtraction *, @, /, //, % Multiplication, matrix multiplication, division, floor division, remainder [5] +x, -x, ~x Positive, negative, bitwise NOT ** Exponentiation [6] await x Await expression x[index], x[index:index], x(arguments...), x.attribute Subscription, slicing, call, attribute reference (expressions...), [expressions...], {key: value...}, {expressions...} Binding or tuple display, list display, dictionary display, set display if ... else 场景一 : 用户登录验证 # 导入getpass模块 import getpass # 等待用户输入 name = input(\"请输入用户名：\") # 等待用户输入密码,密码不可见 password = getpass.getpass(\"请输入密码：\") # 如果用户密码正确,执行如下 if name ==\"Lyon\" and password ==\"yang\": print(\"欢迎你!\") # 否则执行如下 else： print(\"用户名或密码错误\") 场景二 : 猜年龄游戏 # 定义一个年龄 age =21 # 用户输入 user_input = int(input(\"input your guess num:\")) if user_input == age: print(\"Congratulations, you got it !\") elif user_input for循环 循环10次 for i in range(10): print(\"loop:\", i ) ''' 执行结果: loop: 0 loop: 1 loop: 2 loop: 3 loop: 4 loop: 5 loop: 6 loop: 7 loop: 8 loop: 9 ''' 小于5就跳入下一次循环 for i in range(10): if i while循环 写一个死循环 count = 0 while True： print(\"你是风儿我是沙，缠缠绵绵走天涯\", count) count += 1 "},"01-Python/01-基础篇/02-数字.html":{"url":"01-Python/01-基础篇/02-数字.html","title":"数字","keywords":"","body":"Attack on Python - 数字 🐍 整型 在 Python 2.7 版本中 , Python 把 int 和 long 是分开的 int 类型的最大值是 2147483647 , 超过了这个值就是 long 类型了(长整数不过是大一些的数) ; 而在3.x中 , int 和 long 整合到一起了 , 以 int 来表示 >>> num = 123 >>> type(num) 浮点型 float有两种表现形式 , 一种是十进制数形式 , 它由数字和小数点组成 , 并且这里的小数点是不可或缺的 ; 另一种是指数形式 , 用e(大写也可以)来表示之后可以有正负号 , 来表示指数的符号 , e就是10的幂 , 指数必须是整数 >>> a = 10E2 >>> a 1000.0 >>> b = 10e2 >>> b 1000.0 >>> c = 1.1 >>> type(c) 小 Tips : 在我们工作中很多时候会需要一个无穷大 , 或者无穷小的预设值 , 就可以使用 float 来实现 , 无穷小和无穷大分别是 , float('-inf') 和 float('inf') 空值 表示该值是一个空对象 , 空值是python里一个特殊的值 , 用None表示 None不能理解为0 , 因为0是有意义的 , 而None是一个特殊的空值 ; None有自己的数据类型NoneType , 它与其他的数据类型比较永远返回False , 你可以将None复制给任何变量 , 但是你不能创建其他NoneType对象 >>> type(None) >>> None == 0 False >>> None == True False >>> None == False False 布尔值 bool就是用来表征真假的一种方式 True为真 , False为假 ; Python中的值是自带bool值的 , 非0即真 , 为0即假 >>> False + False 0 >>> True + True 2 >>> True + False 1 复数 复数有实数和虚数部分组成 , 一般形式为 x + yj , 其中的 x 是复数的实数部分 , y是复数的虚数部分 , 这里x和y都是实数 注意 , 虚数部分不区分大小写 >>> -.6545 + 0J (-0.6545+0j) >>> 4.53e1 - 7j (45.3-7j) >>> 45.j 45j >>> 3.14j 3.14j 类型转换 int(x [,base]) 将x转换为一个整数 float(x ) 将x转换到一个浮点数 complex(x) 将x转换为复数 str(x) 将对象x转换为字符串 ，通常无法用eval()求值 repr(x) 将对象x转换为表达式字符串 ，可以用eval()求值 eval(str) 用来计算在字符串中的有效Python表达式,并返回一个对象 tuple(s) 将序列s转换为一个元组 list(s) 将序列s转换为一个列表 chr(x) 将一个整数转换为一个字符 unichr(x) 将一个整数转换为Unicode字符 ord(x) 将一个字符转换为它的整数值 hex(x) 将一个整数转换为一个十六进制字符串 oct(x) 将一个整数转换为一个八进制字符串 数学函数 abs(x) 返回数字的绝对值，如abs(-10) 返回 10 ceil(x) 返回数字的上入整数，如math.ceil(4.1) 返回 5 cmp(x, y) 如果 x y 返回 1 exp(x) 返回e的x次幂(ex),如math.exp(1) 返回2.718281828459045 fabs(x) 返回数字的绝对值，如math.fabs(-10) 返回10.0 floor(x) 返回数字的下舍整数，如math.floor(4.9)返回 4 log(x) 如math.log(math.e)返回1.0,math.log(100,10)返回2.0 log10(x) 返回以10为基数的x的对数，如math.log10(100)返回 2.0 max(x1, x2,...) 返回给定参数的最大值，参数可以为序列 min(x1, x2,...) 返回给定参数的最小值，参数可以为序列 modf(x) 返回x的整数部分与小数部分，两部分的数值符号与x相同，整数部分以浮点型表示 pow(x, y) x**y 运算后的值。 round(x [,n]) 返回浮点数x的四舍五入值，如给出n值，则代表舍入到小数点后的位数 sqrt(x) 返回数字x的平方根，数字可以为负数，返回类型为实数，如math.sqrt(4)返回 2+0j "},"01-Python/01-基础篇/03-字符串.html":{"url":"01-Python/01-基础篇/03-字符串.html","title":"字符串","keywords":"","body":"Attack on Python - 字符串 🐍 介绍 字符串是 Python 中最基本的数据类型之一 , 它是一个定长对象 , 这意味着它的一旦创建 , 再也无法改变长度 所以关于字符串的操作 , 都会返回一个新的字符串 , 而无法在原来的字符串上直接操作 字符串的使用需要用引号括起来 , 例如 : name = \"Lyon\" ; 这里name就是一个变量名 , 而引号里面的Lyon 则就是该变量绑定的值 , 该值的类型为 \" str\" 类型 , 我们可以利用type() 函数进行查看 : >>> name = \"Lyon\" >>> type(name) >>> 这就是字符串类型 , 当然如上使用的是双引号 , 这里其实还可以使用单引号'Lyon'以及三引号'''Lyon'''(或者是\"\"\"Lyon\"\"\" , 单引号双引号都可以) , 不过对于三引号 , 我们通常是表示多行字符串 , 这样我们就不需要利用 \" \\n \" （换行符）来进行每一行的换行了 对于嵌套引号的时候要注意 , 需要用不同的引号来避免歧义 , 比如 : 'I am \"Lyon\"' , 也可以 \"I am 'Lyon'\" 对于所有的基本数据类型 , 我们都应该熟悉其特性以及操作 字符串操作主要有 拷贝、拼接、查找、比较、统计、切片、测试、大小写等 拷贝 >>> a = \"Lyon\" >>> b = a >>> print(a,b) Lyon Lyon 拼接 >>> a = \"Hello\" >>> b = \"Lyon\" >>> print(a + b) HelloLyon 小 Tips : 由于字符串是定长对象 , 这就导致我们如果做 + 运算 , 两两相加都会生成一个新的字符串 , 于是如果你这样操作 a + a + a + a + a 除了最后的结果 , 在内存中还会创建 3 个在运算过程中需要的字符串 , 所以如果拼接操作过多 , 我们正确的方式应该是使用 ''.join(list()) , 也就是通过 join 方法 >>> a = \"Lyon\" >>> b = \"Hello\" >>> print(a.join(b)) HLyoneLyonlLyonlLyono #HLyon eLyon lLyon lLyon o 查找 >>> name = \"Lyon\" # 返回L字符所在的下标,下标是从0开始的整数 >>> name.index('L') 0 # 如果不存在就会报错 >>> name.index('N') Traceback (most recent call last): File \"\", line 1, in ValueError: substring not found # 也可以用in,not in来进行判断 >>>'L' in name >>> 比较 本来 Python 2 中有个 str.cmp() 方法来比较两个对象 , 并根据结果返回一个整数 , 整数的正负就是数值的大小了 , 但是在 Python 3 中就没有这个方法了 , 官方文档如下 : ```The cmp() function should be treated as gone, and the cmp() special method is no longer supported. Use lt() for sorting, eq() with hash(), and other rich comparisons as needed. (If you really need the cmp() functionality, you could use the expression (a > b) - (a cmp() special method is no longer supported. Use lt() for sorting, eq() with hash(), and other rich comparisons as needed. (If you really need the cmp() functionality, you could use the expression (a > b) - (a 大致的意思就是cmp()函数已经走了 , 如果你真的需要cmp函数 , 你可以用表达式`(a>b)-(a>> a = \"100\" >>> b = \"50\" >>> cmp(a,b) # a>b 负数 -1 >>> cmp(b,a) # b统计 >>> name = \"Lyon\" # name中\"L\"的个数 >>> name.count(\"L\") 1 切片 >>> name = \"i like Lyon\" # 切取第7个到第9个字符,注意空格也是一个字符 >>> name[7:10] 'Lyo' >>> name = \"i like Lyon\" # 第7到第10各,顾头不顾尾 >>> name[7:11] 'Lyon' 检测 >>> name = \"Lyon\" # 检测\"L\"是否在name中,返回bool值 >>> \"L\" in name True >>> num = \"3412313\" # 检测num里面是否全都是整数 >>> num.isdigit() True >>> name = \"Lyon\" # 检测name是否可以被当作标标志符,即是否符合变量命名规则 >>> name.isidentifier() True　 # 检测name里面有没有\"L\",有就返回下标 >>> name.find('L') 0 # 检测name里面有没有\"N\",没有就返回-1 >>> name.find('N') -1 检测相关 str.startswith(prefix[,start[,end]]) # 是否以prefix开头 str.endswith(suffix[,start[,end]]) # 以suffix结尾 str.isalnum() # 是否全是字母和数字,并至少有一个字符 str.isalpha() # 是否全是字母,并至少有一个字符 str.isdigit() # 是否全是数字,并至少有一个字符 str.isspace() # 是否全是空白字符,并至少有一个字符 str.islower() # 是否全是小写 str.isupper() # 是否便是大写 str.istitle() # 是否是首字母大写的 注 : 返回值全为 bool 值 大小写 >>> name = \"I am Lyon\" # 大小写互换 >>> name.swapcase() 'i AM lYON' # 首字母大写,其它都小写 >>> name.capitalize() 'I am lyon' # 转换为大写 >>> name.upper() 'I AM LYON' # 转换为小写 >>> name.lower() 'i am lyon' 更多 | capitalize(...) | S.capitalize() -> str | | Return a capitalized version of S, i.e. make the first character | have upper case and the rest lower case. | | casefold(...) | S.casefold() -> str | | Return a version of S suitable for caseless comparisons. | | center(...) | S.center(width[, fillchar]) -> str | | Return S centered in a string of length width. Padding is | done using the specified fill character (default is a space) | | count(...) | S.count(sub[, start[, end]]) -> int | | Return the number of non-overlapping occurrences of substring sub in | string S[start:end]. Optional arguments start and end are | interpreted as in slice notation. | | encode(...) | S.encode(encoding='utf-8', errors='strict') -> bytes | | Encode S using the codec registered for encoding. Default encoding | is 'utf-8'. errors may be given to set a different error | handling scheme. Default is 'strict' meaning that encoding errors raise | a UnicodeEncodeError. Other possible values are 'ignore', 'replace' and | 'xmlcharrefreplace' as well as any other name registered with | codecs.register_error that can handle UnicodeEncodeErrors. | | endswith(...) | S.endswith(suffix[, start[, end]]) -> bool | | Return True if S ends with the specified suffix, False otherwise. | With optional start, test S beginning at that position. | With optional end, stop comparing S at that position. | suffix can also be a tuple of strings to try. | | expandtabs(...) | S.expandtabs(tabsize=8) -> str | | Return a copy of S where all tab characters are expanded using spaces. | If tabsize is not given, a tab size of 8 characters is assumed. | | find(...) | S.find(sub[, start[, end]]) -> int | | Return the lowest index in S where substring sub is found, | such that sub is contained within S[start:end]. Optional | arguments start and end are interpreted as in slice notation. | | Return -1 on failure. | | format(...) | S.format(*args, **kwargs) -> str | | Return a formatted version of S, using substitutions from args and kwargs. | The substitutions are identified by braces ('{' and '}'). | | format_map(...) | S.format_map(mapping) -> str | | Return a formatted version of S, using substitutions from mapping. | The substitutions are identified by braces ('{' and '}'). | | index(...) | S.index(sub[, start[, end]]) -> int | | Like S.find() but raise ValueError when the substring is not found. | | isalnum(...) | S.isalnum() -> bool | | Return True if all characters in S are alphanumeric | and there is at least one character in S, False otherwise. | | isalpha(...) | S.isalpha() -> bool | | Return True if all characters in S are alphabetic | and there is at least one character in S, False otherwise. | | isdecimal(...) | S.isdecimal() -> bool | | Return True if there are only decimal characters in S, | False otherwise. | | isdigit(...) | S.isdigit() -> bool | | Return True if all characters in S are digits | and there is at least one character in S, False otherwise. | | isidentifier(...) | S.isidentifier() -> bool | | Return True if S is a valid identifier according | to the language definition. | | Use keyword.iskeyword() to test for reserved identifiers | such as \"def\" and \"class\". | | islower(...) | S.islower() -> bool | | Return True if all cased characters in S are lowercase and there is | at least one cased character in S, False otherwise. | | isnumeric(...) | S.isnumeric() -> bool | | Return True if there are only numeric characters in S, | False otherwise. | | isprintable(...) | S.isprintable() -> bool | | Return True if all characters in S are considered | printable in repr() or S is empty, False otherwise. | | isspace(...) | S.isspace() -> bool | | Return True if all characters in S are whitespace | and there is at least one character in S, False otherwise. | | istitle(...) | S.istitle() -> bool | | Return True if S is a titlecased string and there is at least one | character in S, i.e. upper- and titlecase characters may only | follow uncased characters and lowercase characters only cased ones. | Return False otherwise. | | isupper(...) | S.isupper() -> bool | | Return True if all cased characters in S are uppercase and there is | at least one cased character in S, False otherwise. | | join(...) | S.join(iterable) -> str | | Return a string which is the concatenation of the strings in the | iterable. The separator between elements is S. | | ljust(...) | S.ljust(width[, fillchar]) -> str | | Return S left-justified in a Unicode string of length width. Padding is | done using the specified fill character (default is a space). | | lower(...) | S.lower() -> str | | Return a copy of the string S converted to lowercase. | | lstrip(...) | S.lstrip([chars]) -> str | | Return a copy of the string S with leading whitespace removed. | If chars is given and not None, remove characters in chars instead. | | partition(...) | S.partition(sep) -> (head, sep, tail) | | Search for the separator sep in S, and return the part before it, | the separator itself, and the part after it. If the separator is not | found, return S and two empty strings. | | replace(...) | S.replace(old, new[, count]) -> str | | Return a copy of S with all occurrences of substring | old replaced by new. If the optional argument count is | given, only the first count occurrences are replaced. | | rfind(...) | S.rfind(sub[, start[, end]]) -> int | | Return the highest index in S where substring sub is found, | such that sub is contained within S[start:end]. Optional | arguments start and end are interpreted as in slice notation. | | Return -1 on failure. | | rindex(...) | S.rindex(sub[, start[, end]]) -> int | | Like S.rfind() but raise ValueError when the substring is not found. | | rjust(...) | S.rjust(width[, fillchar]) -> str | | Return S right-justified in a string of length width. Padding is | done using the specified fill character (default is a space). | | rpartition(...) | S.rpartition(sep) -> (head, sep, tail) | | Search for the separator sep in S, starting at the end of S, and return | the part before it, the separator itself, and the part after it. If the | separator is not found, return two empty strings and S. | | rsplit(...) | S.rsplit(sep=None, maxsplit=-1) -> list of strings | | Return a list of the words in S, using sep as the | delimiter string, starting at the end of the string and | working to the front. If maxsplit is given, at most maxsplit | splits are done. If sep is not specified, any whitespace string | is a separator. | | rstrip(...) | S.rstrip([chars]) -> str | | Return a copy of the string S with trailing whitespace removed. | If chars is given and not None, remove characters in chars instead. | | split(...) | S.split(sep=None, maxsplit=-1) -> list of strings | | Return a list of the words in S, using sep as the | delimiter string. If maxsplit is given, at most maxsplit | splits are done. If sep is not specified or is None, any | whitespace string is a separator and empty strings are | removed from the result. | | splitlines(...) | S.splitlines([keepends]) -> list of strings | | Return a list of the lines in S, breaking at line boundaries. | Line breaks are not included in the resulting list unless keepends | is given and true. | | startswith(...) | S.startswith(prefix[, start[, end]]) -> bool | | Return True if S starts with the specified prefix, False otherwise. | With optional start, test S beginning at that position. | With optional end, stop comparing S at that position. | prefix can also be a tuple of strings to try. | | strip(...) | S.strip([chars]) -> str | | Return a copy of the string S with leading and trailing | whitespace removed. | If chars is given and not None, remove characters in chars instead. | | swapcase(...) | S.swapcase() -> str | | Return a copy of S with uppercase characters converted to lowercase | and vice versa. | | title(...) | S.title() -> str | | Return a titlecased version of S, i.e. words start with title case | characters, all remaining cased characters have lower case. | | translate(...) | S.translate(table) -> str | | Return a copy of the string S in which each character has been mapped | through the given translation table. The table must implement | lookup/indexing via __getitem__, for instance a dictionary or list, | mapping Unicode ordinals to Unicode ordinals, strings, or None. If | this operation raises LookupError, the character is left untouched. | Characters mapped to None are deleted. | | upper(...) | S.upper() -> str | | Return a copy of S converted to uppercase. | | zfill(...) | S.zfill(width) -> str | | Pad a numeric string S with zeros on the left, to fill a field | of the specified width. The string S is never truncated. | | ---------------------------------------------------------------------- "},"01-Python/01-基础篇/04-元组.html":{"url":"01-Python/01-基础篇/04-元组.html","title":"元组","keywords":"","body":"Attack on Python - 元组 🐍 介绍 元组和字符串一样 , 也是定长对象 元组的创建很简单 , 只需要在括号中添加元素 , 并使用逗号隔开即可 创建 # 创建一个带有元素的元组 mytuple = (\"Lyon\", \"Alex\", \"Leon\", 1, 2, 3) # 也可以不加括号,但是一定要加引号 mytuple = \"Lyon\", \"Alex\", \"Leon\", 1, 2, 3 # 创建一个空元组 mytuple = () # 当元组中只有一个元素,加逗号来消除歧义哟,这是一个好习惯,因为()既可以表示tuple又可以表示数学公式中的小括号 only_one = (\"Lyon\",) 访问 # 创建一个元组 names = (\"Lyon\", \"Alex\", \"Leon\", 1, 2, 3) # 访问元组中的第一个元素并打印结果,下标索也是从0开始 print(names[0]) # 访问元组中第一和第二个元素并打印结果 print(names[0:2]) ''' 打印结果: Lyon ('Lyon', 'Alex') ''' 修改 # 创建一个元组 tuple_name = (\"Lyon\", \"Alex\", \"Leon\", 1, 2, 3) # 创建另一个元组 tuple_num = (1, 2, 3, 4, 5) # 生成一个新的元组 tuple_total = tuple_name + tuple_num # 打印tuple_total print(tuple_total) # 复制元组内元素一次 tuple_total = tuple_name * 2 # 打印tuple_total看结果 print(tuple_total) # 在列表中可以通过索引取值后进行修改,但是元组里面是非法的哦 tuple_name[0] = \"lyon\" # 这里直接就报错 ''' 打印结果: ('Lyon', 'Alex', 'Leon', 1, 2, 3, 1, 2, 3, 4, 5) ('Lyon', 'Alex', 'Leon', 1, 2, 3, 'Lyon', 'Alex', 'Leon', 1, 2, 3) ''' 注意 : 元组是不可变的 , 所以对于所有的修改操作 , 都是在根据原元组生成了一个新的元组 删除 #创建一个元组 names = (\"Lyon\", \"Alex\", \"Leon\", 1, 2, 3) # 删除元组names del names # TypeError: 'tuple' object doesn't support item deletion del names[0] 切片 names = (\"Lyon\", \"Kenneth\", \"Leon\", \"Charlie\") # 打印子集,第二个至第三个 print(names[1:2]) # 打印子集,倒数第三个(即第二个)至第三个 print(names[-3:3]) # 打印子集,第一个至第三个,隔一个取一个 print(names[0:2:1]) ''' 打印结果: ('Kenneth', 'Leon') ('Kenneth', 'Leon') ('Leon',) ''' 检测 # 创建一个元组 tuple_name = (\"Lyon\",\"Alex\",\"Leon\",1,2,3) # \"Lyon\"是否在tuple_name中，打印结果 print(\"Lyon\" in tuple_name) # 打印结果:True 更多 实例 # 创建一个元组 tuple_name = (\"Lyon\",\"Alex\",\"Leon\",1,2,3) # 计算元组长度 tuple_len = len(tuple_name) # 打印结果 print(tuple_len) # 创建一个元素全为数字的元组 tuple_num = (1,2,3,4,5) # 返回元组中的最大值 print(max(tuple_num)) # 返回元组中的最小值 print(min(tuple_num)) # 创建一个列表 list_name = [\"Lyon\",\"Alex\",\"Leon\"] # 将列表转换为元组 tuple_names = tuple(list_name) # 打印tuple_names print(tuple_names) ''' 打印结果: 6 5 1 ('Lyon', 'Alex', 'Leon') ''' 方法 | count(...) | T.count(value) -> integer -- return number of occurrences of value | | index(...) | T.index(value, [start, [stop]]) -> integer -- return first index of value. | Raises ValueError if the value is not present. "},"01-Python/01-基础篇/05-列表.html":{"url":"01-Python/01-基础篇/05-列表.html","title":"列表","keywords":"","body":"Attack on Python - 列表 🐍 介绍 列表是我们以后最常用的数据类型之一 , 通过列表可以对数据实现最方便的存储、修改等操作 列表是变长对象 , 且列表是有序的 列表相当于其他语言中的数组 创建 # 创建一个列表 names = [\"Alex\",\"Lyon\",\"Leon\"] # 创建一个空列表 names = [] # 也可通过list方法 names = list() 访问 # 创建一个列表 names = [\"Alex\",\"Lyon\",\"Leon\"] # 与字符串的索引一样,列表索引从0开始,访问列表中的第一个元素 fristname = names[0] # 打印结果 print(fristname) # 访问列表中第三个元素 threename = names[2] # 打印结果 print(threename) # 访问列表中最后一个元素 endname = names[-1] # 打印结果 print(endname) # 访问倒数第二个元素 penultimate = names[-2] # 打印结果 print(penultimate) ''' 执行结果: Alex Leon Leon Lyon ''' 获取下标 # 创建一个列表 names = ['Alex', 'Lyon', 'Leon', 'CTO','Lyon'] # 获取下标并打印 print(names.index('Lyon')) # 注:只返回找到的第一个下标 ''' 执行结果: 1 ''' 统计 # 创建一个列表 names = ['Alex', 'Lyon', 'Leon', 'CTO', 'BeiJing',\"IT\",21,\"man\"] # 统计 \"Lyon\" 的个数,并打印 print(names.count(\"Lyon\")) ''' 执行结果: 1 ''' 切片 # 创建一个列表 names = [\"Alex\",\"Lyon\",\"Leon\",\"CTO\",\"WuHan\"] # 取下标为1至下标3之间的值,包括1,不包括4 cutnames1 = names[1:3] # 打印cutnames1 print(cutnames1) # 取下标为1至-1的值,不包括-1（-1就是最后一个） cutnames2 = names[1:-1] # 打印cutnames2 print(cutnames2) # 从第一个到第三个 cutnames3 = names[0:3] # 从头开始取,0可以省略,跟上面的效果一样 cutnames4 = names[:3] # 打印cutnames3,cutnames4 print(cutnames3,cutnames4) # 想取最后一个,只能这样写,切片是不包含后一个参数的 cutnames5 = names[3:] # 后面的2是代表,每隔一个元素,就取一个 cutnames6 = names[0::2] # 或者这样 cutnames7 = names[::2] # 打印cutnames6,cutnames7 print(cutnames6,cutnames7) ''' 执行结果: ['Lyon', 'Leon'] ['Lyon', 'Leon', 'CTO'] ['Alex', 'Lyon', 'Leon'] ['Alex', 'Lyon', 'Leon'] ['Alex', 'Leon', 'WuHan'] ['Alex', 'Leon', 'WuHan'] ''' 追加 # 创建一个列表 names = [\"Alex\",\"Lyon\",\"Leon\",\"CTO\",\"WuHan\"] # 追加一个元素 names.append(\"New\") # 打印names print(names) # 注：append 方法只能追加到列表的最后一位 ''' 执行结果: ['Alex', 'Lyon', 'Leon', 'CTO', 'WuHan', 'New'] ''' 插入 # 创建一个列表 names = [\"Alex\",\"Lyon\",\"Leon\",\"CTO\",\"WuHan\",\"New\"] # 插入到下标1前面 names.insert(1,\"Insert\") # 打印names print(names) # 如果下标不存在就会插入到最后一个 names.insert(7,\"NoIndex\") # 打印names print(names) ''' 执行结果: ['Alex', 'Insert', 'Lyon', 'Leon', 'CTO', 'WuHan', 'New'] ['Alex', 'Insert', 'Lyon', 'Leon', 'CTO', 'WuHan', 'New', 'NoIndex'] ''' 修改 # 创建一个列表 names = ['Alex', 'Insert', 'Lyon', 'Leon', 'CTO', 'WuHan', 'New', 'NoIndex'] # 把 'WuHan' 改成 'BeiJing' names[5] = 'BeiJing' # 打印names print(names) # 注：就是通过下标直接改变list本身 ''' 执行结果: ['Alex', 'Insert', 'Lyon', 'Leon', 'CTO', 'BeiJing', 'New', 'NoIndex'] ''' 删除 # 创建一个列表 names = ['Alex', 'Insert', 'Lyon', 'Leon', 'CTO', 'BeiJing', 'New', 'NoIndex'] # 删除下标为7的元素 del names[7] #打印names print(names) # 删除 'Insert',remove删除指定元素 names.remove(\"Insert\") # 打印names print(names) # 删除最后一个元素 names.pop() # 打印names print(names) ''' 执行结果: ['Alex', 'Insert', 'Lyon', 'Leon', 'CTO', 'BeiJing', 'New'] ['Alex', 'Lyon', 'Leon', 'CTO', 'BeiJing', 'New'] ['Alex', 'Lyon', 'Leon', 'CTO', 'BeiJing'] ''' 扩展 # 创建一个列表 names = ['Alex', 'Lyon', 'Leon', 'CTO', 'BeiJing'] # 创建另一个列表 name = [\"IT\",21,\"man\"] # 将name扩展到names names.extend(name) # 打印names print(names) # 这里还有一个\"万恶的'+' \"也是可以的 print(names + name) ''' 执行结果: ['Alex', 'Lyon', 'Leon', 'CTO', 'BeiJing', 'IT', 21, 'man'] ['Alex', 'Lyon', 'Leon', 'CTO', 'BeiJing', 'IT', 21, 'man'] ''' 拷贝 # 创建一个列表 names = ['Alex', 'Lyon', 'Leon', 'CTO', 'BeiJing',\"IT\",21,\"man\"] # 拷贝names,这只是浅copy names_copy = names.copy() # 打印names_copy print(names_copy) ''' 执行结果: ['Alex', 'Lyon', 'Leon', 'CTO', 'BeiJing', 'IT', 21, 'man'] ''' 注意 : 在 Python 2.7 中列表的内置方法是没有 copy 这个方法的 , 这是在 Python 3 后加的 , 并且 Python 3也只有有 copy (浅copy) 这一个方法 , 用深 copy 需要我们导入 copy 模块 , 即 import copy 排序&翻转 # 创建一个列表 names = ['Alex', 'Lyon', 'Leon', 'CTO', 'BeiJing',\"IT\",21,\"man\"] # 在python3中不同的数据类型不能一起排序,换成str names[-2] = \"21\" # 排序,顺序为数字>大写>小写 names.sort() # 打印names print(names) # 翻转 names.reverse() # 打印names print(names) ''' 执行结果: ['21', 'Alex', 'BeiJing', 'CTO', 'IT', 'Leon', 'Lyon', 'man'] ['man', 'Lyon', 'Leon', 'IT', 'CTO', 'BeiJing', 'Alex', '21'] ''' 所有方法如下 : | append(...) | L.append(object) -> None -- append object to end | | clear(...) | L.clear() -> None -- remove all items from L | | copy(...) | L.copy() -> list -- a shallow copy of L | | count(...) | L.count(value) -> integer -- return number of occurrences of value | | extend(...) | L.extend(iterable) -> None -- extend list by appending elements from the iterable | | index(...) | L.index(value, [start, [stop]]) -> integer -- return first index of value. | Raises ValueError if the value is not present. | | insert(...) | L.insert(index, object) -- insert object before index | | pop(...) | L.pop([index]) -> item -- remove and return item at index (default last). | Raises IndexError if list is empty or index is out of range. | | remove(...) | L.remove(value) -> None -- remove first occurrence of value. | Raises ValueError if the value is not present. | | reverse(...) | L.reverse() -- reverse *IN PLACE* | | sort(...) | L.sort(key=None, reverse=False) -> None -- stable sort *IN PLACE* "},"01-Python/01-基础篇/06-字典.html":{"url":"01-Python/01-基础篇/06-字典.html","title":"字典","keywords":"","body":"Attack on Python - 字典 🐍 介绍 字典是一种 key - value 的数据类型 , 用 冒号 \" : \" 来关联键值对 , 每个对象之间用逗号 \" , \" 分割 , 整个字典包括在花括号 \"{ }\" 中 字典中的键(key)是唯一的 , 但值(value)则不必 字典是变长对象 , 在 Python 3.5 之前字典是无序的 , 但是在 3.6 之后官方就已经改成有序的了 , 所以在使用时需要注意一下 注意 : key 是不可变的 , 所以可变对象无法作为字典的 key , 如 : list , 对于不可变的数据类型则可以 , 如 : str、int、tuple 创建 # 创建一个空字典 empty_info = {} # 创建一个字典 info = {\"name\":\"Lyon\",\"age\":21} # 也可调用dict()方法 info = dict() 增加 # 创建一个字典 info = {\"name\":\"Lyon\",\"age\":21} # 增加新的键/值对 info[\"school\"] = \"university\" # 打印info print(info) # 注:字典是无序的,所以打印结果也是随机打印 ''' 执行结果: {'school': 'university', 'age': 21, 'name': 'Lyon'} ''' 修改 # 创建一个字典 info = {\"name\":\"Lyon\",\"age\":21,\"school\":\"university\"} # 修改age info[\"age\"] = 18 # 打印info print(info) ''' 执行结果: {'age': 18, 'school': 'university', 'name': 'Lyon'} ''' 删除 # 创建一个字典 info = {\"name\":\"Lyon\",\"age\":21,\"school\":\"university\"} # 标准删除姿势 info.pop(\"school\") # 打印info print(info) # 换个姿势 del info[\"age\"] # 打印info print(info) # 随机删除 info.popitem() # 打印info print(info) ''' 执行结果: {'name': 'Lyon', 'age': 21} {'name': 'Lyon'} {} ''' 查找 # 创建一个字典 info = {\"name\":\"Lyon\",\"age\":21,\"school\":\"university\"} # 标准查找,判断name是否在字典info中 print(\"name\" in info) #打印：True # 获取值 print(info.get(\"name\")) #打印：Lyon # 换换姿势 print(info[\"name\"]) #打印：Lyon # 这种方式要注意如果key不存在就会报错,而get仅仅返回None print(info[\"home\"]) # 报错：KeyError: 'home' ''' 执行结果: True Lyon Lyon KeyError:'home' ''' 遍历 # 创建一个字典 info = {\"name\":\"Lyon\",\"age\":21,\"school\":\"university\"} # 方法1,推荐 for key in info: print(key,info[key]) # 方法2 for k,v in info.items(): print(k,v) ''' 执行结果: school university name Lyon age 21 school university name Lyon age 21 ''' 嵌套 # 创建一个多级嵌套字典 datas ={ '湖北省':{ \"武汉市\":{ \"武昌区\":[\"Hello\"], \"洪山区\":[\"Sorry\"], \"江夏区\":[\"Welcome\"], }, }, '湖南省':{ \"长沙市\":{ \"岳麓区\":{}, \"天心区\":{}, \"芙蓉区\":{}, }, }, '广东省':{ \"佛山市\":{ \"三水区\":{}, \"顺德区\":{}, \"男海区\":{}, }, }, } # 修改最里层的value datas[\"湖北省\"][\"武汉市\"][\"武昌区\"].append(\"Lyon\") # 打印结果 print(datas[\"湖北省\"][\"武汉市\"]) ''' 执行结果: {'洪山区': ['Sorry'], '武昌区': ['Hello', 'Lyon'], '江夏区': ['Welcome']} ''' 更多 len(dict) # 计算字典元素个数 dict.clear() # 清空词典所有条目 dict.fromkeys(seq, val)) # 创建一个新字典,以列表 seq 中元素做字典的键,val 为字典所有键对应的初始值 dict.has_key(key) # 如果键在字典dict里返回true,否则返回false dict.items() # 以列表返回可遍历的(键, 值) 元组数组 dict.keys() # 以列表返回一个字典所有的键 dict.values() # 以列表返回字典中的所有值 dict.setdefault(key, default=None) # 和get()类似, 但如果键不存在于字典中,将会添加键并将值设为default dict.update(dict2) # 把字典dict2的键/值对更新到dict里 方法合集 | clear(...) | D.clear() -> None. Remove all items from D. | | copy(...) | D.copy() -> a shallow copy of D | | fromkeys(iterable, value=None, /) from builtins.type | Returns a new dict with keys from iterable and values equal to value. | | get(...) | D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. | | items(...) | D.items() -> a set-like object providing a view on D's items | | keys(...) | D.keys() -> a set-like object providing a view on D's keys | | pop(...) | D.pop(k[,d]) -> v, remove specified key and return the corresponding value. | If key is not found, d is returned if given, otherwise KeyError is raised | | popitem(...) | D.popitem() -> (k, v), remove and return some (key, value) pair as a | 2-tuple; but raise KeyError if D is empty. | | setdefault(...) | D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D | | update(...) | D.update([E, ]**F) -> None. Update D from dict/iterable E and F. | If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] | If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v | In either case, this is followed by: for k in F: D[k] = F[k] | | values(...) | D.values() -> an object providing a view on D's values "},"01-Python/01-基础篇/07-集合.html":{"url":"01-Python/01-基础篇/07-集合.html","title":"集合","keywords":"","body":"Attack on Python - 集合 🐍 介绍 集合是变长对象 , 集合是无序且不重复的数据组合 , 因此我们可以用来做 : 去重 , 把一个列表变成集合 , 就自动去重了 集合运算 , 求两个集合的并集 , 交集 , 差集 , 对称差集 在 Python 2.7 中集合表示如下 : set([1,2,3]) 在 Python 3.x 中则是如下 : {1,2,3} 我们可以通过 set() 方法 , 将 list 和 tuple 转换为集合 : set(list()) , set(tuple()) 创建 与字符串等数据类型一样 , 集合支持如下方式创建 # 创建空集合只能用这种方式,参数为一个可迭代对象 s = set() # 注意集合是单个元素,字典是键值对 s = {1,2,3} 添加 为集合添加元素 # 定义集合 s = {'lyon','kenneth'} # 添加一项 s.add('geek') 注意 : 集合不支持 \"+\" 更新 # 定义集合 s = {'lyon','kenneth'} # 添加多项,参数为可迭代对象 s.update(['1','2','3']) 删除 # 定义集合 s = {'lyon','kenneth'} # 删除一项 s.remove('kenneth') # 清空集合 s.clear() 关系运算 a = {1,2,3,4,5} b = {1,2,3} # 测试是否b中的每一个元素都在a中,即 b=a ,返回bool值 b.issuperset(a) 集合操作 >>>a = {1,2,3} >>>b = {4,5,6} # 求并集 >>>a.union(b) # 同上,求并集 >>>a | b # 求交集 >>>a.intersection(b) # 同上,求交集 >>>a & b # 求差集 >>>a.difference(b) # 同上,求差集 >>>a - b # 求对称差集 >>>a.symmetric_difference(b) # 同上,求对称差集 >>>a ^ b 集合对象所有方法 | add(...) | Add an element to a set. | | This has no effect if the element is already present. | | clear(...) | Remove all elements from this set. | | copy(...) | Return a shallow copy of a set. | | difference(...) | Return the difference of two or more sets as a new set. | | (i.e. all elements that are in this set but not the others.) | | difference_update(...) | Remove all elements of another set from this set. | | discard(...) | Remove an element from a set if it is a member. | | If the element is not a member, do nothing. | | intersection(...) | Return the intersection of two sets as a new set. | | (i.e. all elements that are in both sets.) | | intersection_update(...) | Update a set with the intersection of itself and another. | | isdisjoint(...) | Return True if two sets have a null intersection. | | issubset(...) | Report whether another set contains this set. | | issuperset(...) | Report whether this set contains another set. | | pop(...) | Remove and return an arbitrary set element. | Raises KeyError if the set is empty. | | remove(...) | Remove an element from a set; it must be a member. | | If the element is not a member, raise a KeyError. | | symmetric_difference(...) | Return the symmetric difference of two sets as a new set. | | (i.e. all elements that are in exactly one of the sets.) | | symmetric_difference_update(...) | Update a set with the symmetric difference of itself and another. | | union(...) | Return the union of sets as a new set. | | (i.e. all elements that are in either set.) | | update(...) | Update a set with the union of itself and others. "},"01-Python/01-基础篇/08-字符编码.html":{"url":"01-Python/01-基础篇/08-字符编码.html","title":"字符编码","keywords":"","body":"Attack on Python - 字符编码 🐍 介绍 字符编码 字符编码 (Character encoding) 也称字集码 , 它是一套法则 , 使用该法则能够对自然语言的字符的一个集合 (如字母表或音节表) , 与其他东西的一个集合 (如号码或电脉冲) 进行配对 , 即在符号集合与数字系统之间建立对应关系 再简单一点说其实就是一张具有对应关系的表格 , 如下 +----+-----------+ | id | character | +----+-----------+ | 65 | A | | 66 | B | | 67 | C | +----+-----------+ 如上表所示 , 这就是一套法则 , 使我们用数字成功的表示了字符 为什么要一套这样的法则 ? 众所周知 , 计算机只认识机器码 , 也就是一堆0101之类的二进制数字 , 计算机并不认识我们的 \"A\" , \"B\" ,\"C\" , 我们为了使其友好的显示 , 就需要一套这样的法则 , 来完成这些转换 , 于是两个名词诞生了 编码 通俗的说 , 就是按照何种规则将字符存储在计算机中 . 比如 \"A\" 用65表示 , 也就是把字符\"A\"以二进制的方式存储在计算机中 解码 反之 , 将存储在计算机中的二进制数解析显示出来 , 这就是解码 在Python中 '''既然是对于字符,那么自然对应着Python中的字符串了''' '''Python中提供了两个函数来完成编码和解码''' # 编码函数encode() encode() character → byte # 解码函数decode() byte → character PS : 必须采用相对应的法则 , 否则就会出错 , 也就是我们常说的乱码 最后还有一个名词 , 字符集 字符集 是一个系统支持的所有抽象字符的集合 , 字符是各种文字和符号的总称 , 包括各国家文字、标点符号、图形符号、数字等 字符编码就是在字符集与数字系统之间建立的对应关系 ASCII ASCII (American Standard Code for Information Interchange , 美国信息交换标准码) , 是基于拉丁字母的一套电脑编码系统 , 主要用于显示现代英语 ASCII字符集 : 主要包括控制字符 (回车键 , 退格 , 换行键等) , 可显示字符 (英文大小写字符 , 阿拉伯数字和西文符号) ASCII编码 : 将ASCII字符集转换为计算机可以接收的数字系统的数的规则 , 使用7位(Bit)表示一个字符 , 1 Byte = 8 Bit , 一共可以表示128(2的7次方)个字符 具体ASCII字符集映射到数字编码规则可以自行查询 ANSI ANSI编码为在ASCII编码(7位)的基础上 , 将其最后一位也使用上 , 即使用8位 ANSI使使计算机支持更多语言 , 通常对于没超过128的即用ASCII编码 , 超过的即用扩展的ASCII编码ANSI 当然不同的国家和地区指定了不同的标准 , 由此产生了GB2312、GBK、GB18030、Big5、Shift_JIS 等各自的编码标准 在简体中文Windows操作系统中 , ANSI 编码代表 GBK 编码 ; 在繁体中文Windows操作系统中 , ANSI编码代表Big5 ; 在日文Windows操作系统中 , ANSI 编码代表 Shift_JIS 编码 GBXXX GB2312编码 计算机发明之初及后面很长一段时间 , 只应用于美国及西方一些发达国家 , 于是到中国时 , 一个字节8位 , 256个字符是远远不能满足的 , 要想想中国有多少汉字 , 于是聪明的中国人这样规定 : 一个小于127的字符的意义与原来相同 , 但是两个大于127的字符连在一起时 , 就表示一个汉字 , 前面的一个字节称为高字节 , 后面的为低字节 , 这样就组合出了大约7000多个简体汉字了 , 这就是GB2312 ,全称 信息交换用汉字编码字符集 ▪ 基本集 GB18030 由于7000多个汉字还是不够用 , 于是继续改进 , 每个汉字可以由1个 , 2个或4个字节组成 , 于是庞大的编码空间形成了 , 最多可以定义161万个字符 , 这就是GB18030 , 全称 信息技术中文编码字符集 Unicode 各种各样的字符编码都出来了 , 大家各用各的 , 那么问题就来了 , 一旦出现在网络上 , 由于不兼容 , 互相访问就出现了乱码现象 , 为了解决这个问题 , Unicode编码系统应运而生 Unicode编码系统为表达任意语言的任意字符而设计 , 它使用2字节的数字来表达每个字母 , 符号 , 或者表意文字 , 每个数字代表唯一的至少在某种语言中使用的符号 (并不是所有的数字都用上了 , 但是总数已经超过了65535 所以2个字节的数字是不够用的) 总而言之 , Unicode是业界的一种标准 , 也叫做统一码 , 万国码 , 单一码 , 标准万国码 所以Unicode编码也成为了一个编码转换的基础 , 因为大家都支持他 , 从一种编码到另一中编码 , 只需要Unicode在中间搭桥就能简单的实现了 UTF - 8 对于Unicode来讲 , 任何字符都使用2个字节来存储 , 这明显是很浪费内存的 , 因为我们编写代码时 , 用到中文毕竟极少 , 所以为了节省内存 , 就有了UTF-8 , UTF - 8规定 , 英文只使用1个字节 , 中文使用3个字节 虽然说UTF - 8具有良好的国际兼容性 , 但中文需要比GBK/BIG5版本多占用50%的数据库存储空间 , 因此并不推荐使用 Python编码处理 在Python3中 , 源代码读取进行语法校验时 , 会将源代码中的字符串从声明的编码转换成Unicode类型 , 等到语法校验通过后 , 再将这些字符换回初始的编码 , 这也就是说 , Python3中 , 字符串默认编码就是Unicode 查看默认编码 >>> import sys >>> sys.getdefaultencoding() PS : Windows下命令行的字符编码默认是GBK ; 并且Python2中 , 字符串是有两种类型的 , 这里不多说明 "},"01-Python/01-基础篇/09-文件操作.html":{"url":"01-Python/01-基础篇/09-文件操作.html","title":"文件操作","keywords":"","body":"Attack on Python - 文件操作 🐍 介绍 在磁盘上读写文件的功能都是由操作系统提供的 , 现代操作系统不允许普通的程序直接操作磁盘 , 所以 , 读写文件就是请求操作系统打开一个文件对象 (通常称为文件描述符) ; 然后 , 通过操作系统提供的接口从这个文件对象中读取数据 (读文件) , 或者把数据写入这个文件对象 (写文件) 在Python中我们进行文件操作需要首先利用open() 函数获取一个文件流来操作文件 这个流就是我们所使用的文件描述符 , 是一个I/O通道 open() open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None): \"\"\" file:文件名 mode:模式 buffering:设置缓冲策略 encoding:指定使用编码 errors:指定处理编码和解码错误的方式 newline:控制通用换行模式的工作方式(只适用文本模式) closefd:如果为False并且给出了文件描述符而不是文件名,则文件关闭时,文件描述符将保持打开;如果给定文件名,则closefd必须为True,否则将抛出异常 opener:自定义开启器 \"\"\" 对于上述参数中 , 我们主要需要了解的就是file , mode , encoding 这三个 对于mode , 有以下模式 : Character Meaning 'r' open for reading (default) 'w' open for writing, truncating the file first 'x' open for exclusive creation, failing if the file already exists 'a' open for writing, appending to the end of the file if it exists 'b' binary mode 't' text mode (default) '+' open a disk file for updating (reading and writing) 'U' universal newlines mode (deprecated) 常使用的就是'r' , 'w' , 'a' , '+' , 'b' , 当然还可以组合使用 , 下面进行详细介绍 : r , 只读模式 , 文件必须已经存在 r+ , 可读可写模式 , 文件必须已经存在 w , 只写模式 , 会重新创建 , 意味着文件如果已存在会被空文件覆盖 w+ , 可写可读模式 , 同样会创建文件 a , 追写模式 , 文件不存在参考'w' a+ , 追写并可读模式 , 文件不存在参考'w' b , 以二进制的模式进行处理 (Linux可忽略 , Windows处理二进制文件时需标注) , 可以用该模式来读取图片 , 视频等等 rb , 同r wb , 同w ab , 同a 简单实例 file.txt A man is not old until his regrets take place of his dreams. Nothing can help us endure dark times better than our faith. No one but ourselves can degrade us. 实例 f = open('file.txt','r') contents = f.read print(contents) ''' 执行结果: A man is not old until his regrets take place of his dreams. Nothing can help us endure dark times better than our faith. No one but ourselves can degrade us. ''' file-like object 以下内容可以学习完模块篇之后再继续学习 io 模块提供了Python处理各种类型I/O的主要工具 , 有三种主要类型 , 即文本I/O , 二进 制I/O和原始I/O , 这些是通用类别 , 并且可以为它们中的每一个使用各种后备存储 三种主要类型详细见 : TextIOBase , BufferedIOBase , RawIOBase 属于这些类别中的任何一个的具体对象称为file-like object 创建这些类别的具体对象最简单的方法就是使用内置的open() 函数 , 其也被定义在io模块中 , 下面仅介绍一些这些类别对象常用的方法 : detach() ''' Separate the underlying binary buffer from the TextIOBase and return it. After the underlying buffer has been detached, the TextIOBase is in an unusable state. Some TextIOBase implementations, like StringIO, may not have the concept of an underlying buffer and calling this method will raise UnsupportedOperation. New in version 3.1. ''' read(size) ''' Read and return at most size characters from the stream as a single str. If size is negative or None, reads until EOF. ''' readline(size=-1) ''' Read until newline or EOF and return a single str. If the stream is already at EOF, an empty string is returned. If size is specified, at most size characters will be read. ''' readlines(hint=-1) ''' Read and return a list of lines from the stream. hint can be specified to control the number of lines read: no more lines will be read if the total size (in bytes/characters) of all lines so far exceeds hint. Note that it’s already possible to iterate on file objects using for line in file: ... without calling file.readlines(). ''' readable() ''' Return True if the stream can be read from. If False, read() will raise OSError. ''' write(s) ''' Write the string s to the stream and return the number of characters written. ''' writable() ''' Return True if the stream supports writing. If False, write() and truncate() will raise OSError. ''' writelines(lines) ''' Write a list of lines to the stream. Line separators are not added, so it is usual for each of the lines provided to have a line separator at the end. ''' seek(offset[, whence]) ''' Change the stream position to the given offset. Behaviour depends on the whence parameter. The default value for whence is SEEK_SET. SEEK_SET or 0: seek from the start of the stream (the default); offset must either be a number returned by TextIOBase.tell(), or zero. Any other offset value produces undefined behaviour. SEEK_CUR or 1: “seek” to the current position; offset must be zero, which is a no-operation (all other values are unsupported). SEEK_END or 2: seek to the end of the stream; offset must be zero (all other values are unsupported). Return the new absolute position as an opaque number. New in version 3.1: The SEEK_* constants. ''' tell() ''' Return the current stream position as an opaque number. The number does not usually represent a number of bytes in the underlying binary storage. ''' close() ''' Flush and close this stream. This method has no effect if the file is already closed. Once the file is closed, any operation on the file (e.g. reading or writing) will raise a ValueError. As a convenience, it is allowed to call this method more than once; only the first call, however, will have an effect. ''' fileno() ''' Return the underlying file descriptor (an integer) of the stream if it exists. An OSError is raised if the IO object does not use a file descriptor. ''' flush() ''' Flush the write buffers of the stream if applicable. This does nothing for read-only and non-blocking streams. ''' isatty() ''' Return True if the stream is interactive (i.e., connected to a terminal/tty device). ''' seek(offset[, whence]) ''' Change the stream position to the given byte offset. offset is interpreted relative to the position indicated by whence. The default value for whence is SEEK_SET. Values for whence are: SEEK_SET or 0 – start of the stream (the default); offset should be zero or positive SEEK_CUR or 1 – current stream position; offset may be negative SEEK_END or 2 – end of the stream; offset is usually negative Return the new absolute position. New in version 3.1: The SEEK_* constants. New in version 3.3: Some operating systems could support additional values, like os.SEEK_HOLE or os.SEEK_DATA. The valid values for a file could depend on it being open in text or binary mode. ''' seekable() ''' Return True if the stream supports random access. If False, seek(), tell() and truncate() will raise OSError. ''' truncate(size=None) ''' Resize the stream to the given size in bytes (or the current position if size is not specified). The current stream position isn’t changed. This resizing can extend or reduce the current file size. In case of extension, the contents of the new file area depend on the platform (on most systems, additional bytes are zero-filled). The new file size is returned. Changed in version 3.5: Windows will now zero-fill files when extending. ''' 注意 : 当使用完文件后一定要记得使用close() 方法将其关闭 ; 其次在进行文件操作时要注意文件描述符所在的位置 with 为了避免打开文件后忘记手动关闭 , 可以通过管理上下文 , 即使用with语句 , 如下 : with open('filepath','mode') as f: pass 在Python 2.7以上的版本 , 支持同时对多个文件同时进行上下文管理 , 如下 : with open('filepath1','mode') as f1,open('filepath2','mode') as f2: pass 更多文档资料 : https://docs.python.org/3.5/library/io.html?highlight=io#module-io "},"01-Python/02-函数篇/":{"url":"01-Python/02-函数篇/","title":"函数篇","keywords":"","body":"Attack on Python - 函数篇 🐍 介绍 该目录下为Python函数篇 , 主要为面向函数编程 , 内容概述如下 函数 函数基础语法 嵌套函数 高阶函数 闭包 装饰器 递归 匿名函数 内置函数 迭代器、生成器 "},"01-Python/02-函数篇/01-函数基础.html":{"url":"01-Python/02-函数篇/01-函数基础.html","title":"函数基础","keywords":"","body":"Attack on Python - 函数基础 🐍 介绍 函数是组织好的 , 可重复使用的 , 用来实现单一 , 或相关联功能的代码段 函数能提高应用的模块性 , 和代码的重复利用率 , 比如我们一直使用的print() , input() 等等 , 都是函数 如下我们写了一个用户认证程序 ; 而现在我们又需要写一个用户管理系统 , 管理系统中有很多的功能 , 比如添加用户 , 删除用户 , 查询用户 , 修改用户 ; 但是这些功能必须先通过用户认证程序才能使用 , 明显我们不可能在每一个功能前加上一段用户认证代码 , 因为这将大大增加我们的重复代码 那么为了解决这个问题我们就可以将用户认证功能封装到一个函数之中 , 而后续我们如果需要使用这个函数仅需调用即可 , 着就是函数的魅力所在 , 当然更多的还是通过下面进一步了解函数 定义函数 # 自定义函数,function_name为函数名 def function_name(): \"\"\"注释\"\"\" ''' 功能代码块 ''' # 返回值,一般都具有返回值,当然也不可以不设定 return result 简单实例 def hello(): print(\"Hello Lyon!\") return None 注意 : 上述仅为定义函数 , 函数并不会执行 , 只有当函数被调用时 , 函数内部代码才会执行 函数调用 函数调用通过函数名后加() 进行调用 , 如下 : # 定义函数 def hello(): print(\"Hello Lyon!\") return None # 调用函数 hello() 既然函数调用是通过函数名后加括号 , 在这个固定语法之中前者函数名有是什么? 如下 : # 定义函数 def hello(): print(\"Hello Lyon!\") return None # 打印函数名 print(hello) ''' 执行结果: ''' 我们可以发现 , 函数名打印出来的是一个内存地址 , 由此不难理解 : 函数名相当于一个变量 , 而变量所绑定的对象就是函数对象本身 ; 参数说明 形参 : 变量只有在被调用时才分配内存单元 , 在调用结束时 , 即刻释放所分配的内存单元 ; 因此 , 形参只在函数内部有效 , 函数调用结束返回主调用函数后则不能再使用该形参变量 实参 : 可以是常量、变量、表达式、函数等 , 无论实参是何种类型的量 , 在进行函数调用时 , 它们都必须有确定的值 , 以便把这些值传送给形参 ; 因此应预先用赋值 , 输入等办法使参数获得确定值 # 定义函数func def func(argument1,argument2): # argument1与argument2都为形参,形式参数 print(argument1,argument2) # 调用函数func func(\"Hello\", \"Lyon\") # Hello和Lyon都是实参,实际参数 ''' 执行结果: Hello Lyon ''' 位置参数 : 即参数必须以正确的顺序传入函数 , 传入的数量必须和声明的一样 , 不一样就报错 # 用户登录验证 def login(username,password): if username == \"Lyon\" and password == \"123456\": print(\"Login successfully!\") else: print(\"Login failed!\") # 进行调用 login(\"Lyon\",\"123456\") # 进行调用 login(\"Lyon\",\"78910JkQ\") ''' 执行结果: Login successfully! Login failed! ''' 默认参数 调用时不指定就以默认值传入 , 指定则按指定值传入 # 同时定义位置参数和默认参数 def add_userinfo(name,age,province=\"北京\"): return name,province # 位置参数必填,默认参数可选 add_userinfo(\"Lyon\",18) ''' 执行结果: ('Lyon', '北京') ''' 注：通过默认参数，我们就算不传参数也不会报错 , 即province 默认为\"北京\" 关键字参数 正常情况下 , 给函数传参数的时候要按照顺序传 , 如果不想按照顺序就可以使用关键参数 def add_userinfo(name,age,province=\"北京\"): return name,province add_userinfo(\"Lyon\",province=\"湖北\",age=18) # 注意关键参数不用按照顺序传入,但是关键参数必须写在位置参数后面 非固定参数 当我们想要传入多个参数 , 但是我们又不确定的时候就可以使用非固定参数 ; 非固定参数有两个 , 一个 *args (元组形式) 以及 **kwargs (字典形式) # 设定两个非固定参数 def main(*args,**kwargs): # 打印args,以及args的类型 print(args,type(args)) # 打印kwargs,以及kwargs的类型 print(kwargs,type(kwargs)) # 调用 main((1,2,3,4),{1:2,3:4}) 对于非固定参数 , 其主要在于* 号 , * 号的作用是进行打包与解包 : 一个* 号 , 则表示打包成元组或者将元组进行解包 , 过程如下 : def main(n,*args): return args # 传递参数,第一个参数被认为是位置参数n,余后参数*号将会对其进行打包成元组,但参数形式必须符合元组规范 result = main(1,2,3,4,5) print(result) ''' 执行结果: (2, 3, 4, 5) ''' ''' 额外说明: 传递参数时,*号将参数封装成一个元组,即元组args ''' 两个** 号 , 则表示打包成字典或者将字典进行解包 , 过程如下 : def main(**kwargs): return kwargs # 传递参数,**号将会对其进行打包成字典,但参数形式必须符合字典规范,即必须key-value result = main(n2=2,n3=3,n4=4) print(result) ''' 执行结果: {'n4': 4, 'n2': 2, 'n3': 3} ''' ''' 额外说明: 传递参数时,**号将参数封装成一个字典,即字典kwargs ''' 两者的解包如下 : # 进行打包 def main(*args,**kwargs): # 参数状态:(1,2,3,4,5){'n1':1,'n2':2,'n3'=3} # 进行解包 return (*args),{**kwargs} # 参数状态:1,2,3,4,5,n1=1,n2=2,n3=3 result = main(1,2,3,4,5,n1=1,n2=2,n3=3) print(result) ''' 执行结果: (1, 2, 3, 4, 5, {'n2': 2, 'n3': 3, 'n1': 1}) ''' # 解包补充 '''只要是可迭代对象我们都可以对其进行解包,如下''' mytuple = (1,2,3,4,5,6,7) # _为占位符,*c打包成列表 a,_,b,*c,d = mytuple print(a) print(b) print(c) print(d) ''' 执行结果: 1 3 [4, 5, 6] 7 ''' 参数顺序与传递 参数顺序 在函数头部 (定义参数) : 一般参数 → 默认参数 → 非固定参数*args → 非固定参数**kwargs 在函数调用中 (传递参数) : 位置参数 → 关键字参数 → 默认参数 → 非固定参数*args → 非固定参数**kwargs 参数传递 在我们使用过程中 , 如果没有非固定参数 , 那么我们的关键参数或者默认参数可以用关键字进行传递 ; 如果有非固定参数 , 必须按照位置参数的方式进行传递 默认参数和非固定参数*args位置可以进行调换 , 调换后默认参数传递需要加上关键字 全局与局部变量 局部变量：只在函数内部起作用的变量 全局变量：在整个程序中都起作用 # 全局变量name name = \"Lyon\" def func(name): print(name) # 局部变量name name = \"Kenneth\" print(name) # 调用函数 func(name) print(name) ''' 执行结果: Lyon Kenneth Lyon ''' 总结 : 全局变量作用域是整个程序 , 局部变量作用域是定义该变量的子程序 ; 当全局变量与局部变量同名时 : 在定义局部变量的子程序内 , 局部变量起作用 ; 在其他地方全局变量起作用 global语句 : 可以将局部变量变成全局变量 , 在函数内部变量前加上 global 即可如 : global name return语句 return 语句用于返回函数的执行结果 , 比如操作类函数一般都不需要返回值 , 当然可由我们的需要自己进行设定 不使用return 即返回None , 没有返回值 我们函数在执行过程中如果遇到return语句 , 就会结束并返回结果 def sum( arg1, arg2 ): # 返回2个参数的和 total = arg1 + arg2 print(\"两数之和:\",total) return total # 上一步函数就已经结束,不会往下执行 print(\"已经返回!\") # 调用sum函数 total = sum( 10, 20 ) ''' 执行结果: 两数之和: 30 ''' 如果我们返回函数名 def func(): print(\"I am Lyon\") # 返回func,函数名 → 内存地址 return func # result1接收返回值func函数名 result1 = func() # 返回一个函数对象 print(result1) # 可以继续调用 result2 = result1() print(result2) result2() ''' 执行结果: I am Lyon I am Lyon I am Lyon ''' "},"01-Python/02-函数篇/02-匿名函数.html":{"url":"01-Python/02-函数篇/02-匿名函数.html","title":"匿名函数","keywords":"","body":"Attack on Python - 匿名函数 🐍 介绍 匿名函数顾名思义就是一个没有名字的函数 , 我们可以通过 lambda 关键字来定义 lambda 是一个表达式 , 而并非语句 , 所以可以出现在def语句所不能出现的位置 , 并且不需要指定函数名; lambda 表达式还可以提高代码的可读性 , 简化代码 lambda 表达式主要用于写一些简单的方法 , 对于复杂的还是用函数写的好 示例: # 普通函数 def func(x): return x * x print(func(5)) ----------------------- # 匿名函数,自带return功能 func = lambda x : x * x print(func(5)) --------------------------------------------------- func = lambda arguments : expression using argument 使用匿名函数可以减少命名空间使用内存 , 因为没有函数名 可直接后面传递参数 >>> (lambda x,y : x if x > y else y)(1,2) 2 非固定参数 >>> (lambda *args : args)(1,2,3,4) (1, 2, 3, 4) PS : 匿名函数主要是与其他函数搭配使用 运用 结合使用 map , 计算平方 # map后返回的对象为map对象,所以利用list方法进行强转 >>> list(map(lambda x : x * x, [1,2,3,4])) [1,4,9,16] filter , 筛选偶数 >>> list(filter(lambda x : x % 2 == 0,[1,2,3,4])) [2,4] reduce , 求和 # python3中已经没有reduce方法了,调用需要导入 >>> from functools import reduce # reduce(function, sequence, initial=None) >>> reduce(lambda x , y : x + y, [1,2,3,4,5],100) 115 嵌套使用 版本一 def func(x): return lambda x : x + y f = func(2) print(f(2)) # output: 4 版本二 func = lambda x : (lambda y: x + y) y = func(1) y(2) # output: 3 "},"01-Python/02-函数篇/03-函数进阶.html":{"url":"01-Python/02-函数篇/03-函数进阶.html","title":"函数进阶","keywords":"","body":"Attack on Python - 函数进阶 🐍 介绍 接下来我们会介绍一些函数更高级的用法 嵌套函数 嵌套函数即函数里面再套一个函数 , 如下 : # 全局变量name name = \"Lyon_1\" def func(): # 第一层局部变量name name = \"Lyon_2\" print(\"第1层打印\",name) #嵌套 def func2(): # 第二层局部变量name name = \"Lyon_3\" print(\"第2层打印\", name) # 嵌套 def func3(): # 第三层局部变量 name = \"Lyon_4\" print(\"第3层打印\", name) # 调用内层函数 func3() # 调用内层函数 func2() func() print(\"最外层打印\", name) ''' 执行结果: 第1层打印 Lyon_2 第2层打印 Lyon_3 第3层打印 Lyon_4 最外层打印 Lyon_1 ''' 嵌套函数不能越级调用 , 也就是说我们不能在func2 的外部去调用func3 , 当然反过来我们的代码就进入无限递归了 当然我们有时需要的就是在嵌套函数中 , 使用上一层的变量 , 那么我们可以使用nonlocal 语句 nonlocal 的作用就是改变变量的作用域 , 但是不会扩展到全局变量 , 即只能在函数内部改变 ; nonlocal声明之后 , 会从上层开始找并返回第一个变量 , 没找到则会报错 def func(arg): n = arg def func1(): n = 2 def func2(): nonlocal n # n = 2 n += 1 func2() print(n) # n = 3 func1() print(n) func(10) ''' 执行结果: 3 10 ''' 高阶函数 高阶函数就是将一个函数以参数的形式传入另一个函数 # 定义一个主函数,并设置一个参数func def main_func(func): # 返回func的值 return func # 定义一个函数作为参数传入主函数 def func(): # 返回\"Lyon\"给func() return \"Lyon\" # res接收main_func的返回值,将func()的返回值作为参数传入main_func函数 res = main_func(func()) print(res) ''' 执行结果: Lyon ''' 闭包 闭包是一个结构体 , 闭包必须是内部定义的函数 (嵌套函数) , 该函数包含对外部作用域而不是全局作用域名字 (命名空间) 的引用 def foo(): # 局部变量name name = 'Lyon' # 内部定义的函数 def bar(): # 引用了外部定义的变量name,即内部函数使用外部函数变量,这一行为就叫闭包 print(\"I am\",name) return \"In the bar\" # 调用bar并打印结果 print(bar()) return \"In the foo\" # 调用foo并打印结果 print(foo()) ''' 执行结果: I am Lyon In the bar In the foo ''' 我们可以通过查看函数对象的 __closure__ 属性来显示的查看是否有闭包 def foo(): # 局部变量name name = 'Lyon' def bar(): print(\"I am\",name) return \"In the bar\" print(bar.__closure__) foo() ''' 执行结果: (,) ''' 闭包的这种引用方式 , 我们完全可以把闭包当做一个局部的 \"全局命名空间\" , 也就是说它只是在闭包的作用域中是可见的 , 对外并不可见 , 且闭包只有调用时才会创建 , 所以每个闭包都是完全独立的 , 拥有自己的环境 而且在闭包中被引用的变量的生命周期将会得到提升 , 只要有一个闭包引用了这个变量 , 它就会一直存在 我们来用两个例子加深一下印象 我们可以利用上面第一条所说的来实现一个累加器 # 利用闭包实现一个累加器 def add(): count = [0] def inner(): count[-1] += 1 return count[-1] return inner adder1 = add() # 实例化第一个累加器 adder2 = add() # 实例化第二个累加器 print(adder1()) print(adder1()) print(adder1()) print(adder2()) ''' 执行结果: 1 2 3 1 ''' 可以看到两个累加器互不干扰 , 这就像对象的实例化 , 所以你应该知道了 , 闭包可以用来实现对象系统 我们再看看生命周期提升的好处 # 方式一, 利用闭包 def func(): name = \"Lyon\" def inner(): hello_name = 'Hello' + name [inner() for _ in range(10)] # 方式二, 不利用闭包 def func(): def inner(): name = \"Lyon\" hello_name = 'Hello' + name [inner() for _ in range(10)] func() \"\"\" 首先我们不讨论这段代码是否有实际意义, 只讨论它们的执行方式 我们对比一下方式1和方式2, 它们两者的区别在于 name = \"Lyon\" 一个在inner外部, 一个在内部 当func执行时 方式1: 创建name变量, 然后执行10次inner函数 方式2: 执行10次inner函数, 每次执行inner函数中, 创建name变量 \"\"\" 通过代码 , 很明显 , 方式1只需要创建1次 name , 而方式2会创建10次 , 原因就在于当一个函数执行完毕 , Python 的垃圾回收机制会将无用的对象进行销毁 虽然从这里可以看出 , 闭包的使用可以提升某些时候的性能 , 但是同时 , 由于生命周期的提升 , 它将永远都不会被销毁 , 这不见得是一件好事 , 所以使用闭包还是需要注意不要滥用 我们再留一个思考 , 思考一下下面这道面试题的结果会是什么呢 s = [lambda x: x + i for i in range(10)] print(s[0](10)) print(s[1](10)) print(s[2](10)) print(s[3](10)) 装饰器 装饰器即给原来的函数进行装饰的工具 装饰器由函数去生成 , 用于装饰某个函数或者方法 (类中的说法) , 它可以让这个函数在执行之前或者执行之后做某些操作 装饰器其实就是上一节闭包中的应用 , 而 Python 为了方便我们使用就创造出一个语法糖来方便我们使用 语法糖 : 指那些没有给计算机语言添加新功能 , 而只是对人类来说更\"甜蜜\"的语法 , 语法糖主要是为程序员提供更实用的编码方式 , 提高代码的可读性 , 并有益于更好的编码风格 语法糖如下 : # 装饰器函数 def decorator(func): def inner(): # 我们可以在func执行前, 干一些别的事情 # 引用外部传入的func, 一般是一个函数对象 func() # 当然也可以在func执行后, 干一些别的事情 return inner # 语法糖版本,@ 函数名 @decorator def func(): pass # 闭包调用版本 func = decorator(func) 该语法糖只是将我们闭包中最后自己处理的部分进行处理了 , 如下 : @decorator ↓ 等价 func = decorator(func) 实例 def decorator(func): def inner(): print(\"I am decorator\") func() return inner @decorator # → func = decorator(func) def func(): print(\"I am func\") return func func() ''' 执行结果: I am decorator I am func ''' 多个装饰器装饰同一个函数 def decorator1(func): def inner(): return func() return inner def decorator2(func): def inner(): return func() return inner @decorator1 @decorator2 def func(): print(\"I am func\") func() 被装饰函数带有参数 def decorator(func): def inner(*args,**kwargs): return func(*args,**kwargs) return inner @decorator def func(name): print(\"my name is %s\" % name) func(\"Lyon\") 带参数的装饰器 F = False def outer(flag): def decorator(func): def inner(*args,**kwargs): if flag: print('before') ret = func(*args,**kwargs) print('after') else: ret = func(*args, **kwargs) return ret return inner return decorator @outer(F) # outer(F) = decorator(func) def func(): print('I am func') 我们利用装饰器虽然功能达到了 , 但是注意原函数的元信息却没有赋值到装饰器函数内部 , 比如函数的注释信息 , 如果我们需要将元信息也赋值到装饰器函数内部 , 可以使用 functools 模块中的wraps()方法 , 如下 : import functools def outer(func): @functools.wraps(func) def inner(*args, **kwargs): print(inner.__doc__) return func() return inner @outer def func(): \"\"\" I am func \"\"\" return None func() 我们也可以自己手动修改 , 比如 inner.__qualname__ = func.__qualname__ , inner.__doc__ = func.__doc__ "},"01-Python/02-函数篇/04-内置函数.html":{"url":"01-Python/02-函数篇/04-内置函数.html","title":"内置函数","keywords":"","body":"Attack on Python - 内置函数 🐍 str类型代码的执行(3个) exec(object[, globals[, locals]]) 👈 将字符串当做表达式去执行，没有返回值 # 流程语句用exec >>> exec(\"print('123')\") 123 >>> exec('1+2+3+4') 10 >>> res = exec('1+2+3+4') None eval(expression, globals=None, locals=None) 👈 将字符串当做表达式去执行，并返回执行结果 # 简单求值表达式用eval >>> res = eval('1+2+3+4') >>> res 10 compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1) 👈 把字符传编译成python可执行的代码，但是不会执行 filename : 默认sys.stout，即默认打印在控制台，打印到指定文件 mode : 指定compile后的对象的执行模式，注意有个single模式，当source带有变量赋值时，eval模式是解释不了的，所以需要用single模式或者exec模式 # 交互语句用single >>> code3 = 'name = input(\"please input your name:\")' >>> compile3 = compile(code3,'','single') # 执行前name变量不存在 >>> name # 报错说'name'变量没有定义 Traceback (most recent call last): File \"\", line 1, in name NameError: name 'name' is not defined >>> exec(compile3) # 执行时显示交互命令，提示输入 please input your name:'pythoner' # 执行后name变量有值 >>> name \"'pythoner'\" 数据类型相关(38) 数字相关 数据类型 bool([x]) 👈 查看一个元素的布尔值 int(x=0) / int(x, base=10) 👈 获取一个数的十进制或者进行进制转换 >>> int('1') 1 # 二进制转十进制 >>> int('0b11',base=2) 3 float([x]) 👈 将整数和字符串转换成浮点数 complex([real[, imag]]) 👈 创建一个值为real + imag * j的复数或者转化一个字符串或数为复数。如果第一个参数为字符串，则不需要指定第二个参数 >>> complex(1, 2) (1+2j) # 数字 >>> complex(1) (1+0j) # 当做字符串处理 >>> complex(\"1\") (1+0j) # 注意：这个地方在“+”号两边不能有空格，也就是不能写成\"1 + 2j\"，应该是\"1+2j\"，否则会报错 >>> complex(\"1+2j\") (1+2j) 进制转换 bin(x) 👈 将整数x转换为二进制字符串，如果x不为Python中int类型，x必须包含方法__index__()并且返回值为整数 # 返回一个整数的二进制 >>> bin(999) '0b1111100111' # 非整型的情况，必须包含__index__()方法且返回值为integer的类型 >>> class myType: ... def __index__(self): ... return 35 ... >>> myvar = myType() >>> bin(myvar) '0b100011' oct(x) 👈 转换为八进制 >>> oct(8) '0o10' hex(x) 👈 转换为十六进制 >>> oct(13) '0o15' 数学运算 abs(x) 👈 返回一个数的绝对值 >>> num = -1 >>> abs(num) 1 divmod(a, b) 👈 返回两个数的除,余 >>> divmod(5,2) # 第一个数为整除,第二个为取余 (2, 1) min(iterable, *[, key, default]) 👈 min(arg1, arg2, *args[, key]) 👈 返回最小值,如果多个参数最小值一样,则返回第一个 >>> min([1,2,3,4]) 1 # 返回第一个 >>> min([1,2,3],[4,5],[1,2]) [1,2,3] max(iterable, *[, key, default]) 👈 max(arg1 , arg2, *args[, key]) 👈 返回最大值,如果多个参数最大值,则返回第一个 >>> max([1,2,3,4]) 4 >>> max([2,3],[1,2,3]) [2, 3] sum(iterable[, start]) 👈 求和,参数为可迭代对象 >>> sum((1,2,3,4)) 10 round(number[, ndigits]) 👈 小数精确 # 保留两位小数,四舍五入 >>> round(1.235,2) 1.24 pow(x, y[, z]) 👈 幂运算 >>> pow(2,2) 4 # 参数z相当余 x**y % z >>> pow(2,2,2) 0 数据类型相关 序列 列表和元组 list([iterable]) 👈 将可迭代对象转换成list对象,实际上我们创建一个空list时,python解释器自动为我们调用了该方法 tuple([iterable]) 👈 将可迭代对象转换成tuple对象,与list类似 相关内置函数 reversed(seq) 👈 顺序翻转,与list中reverse的区别在于,该翻转为新生成了一个对象,而不是在原对象上操作 slice(stop) 👈 slice(start, stop[, step]) 👈 返回切片操作的三个参数 # 相当于[0:2:],注意最后一个参数不能为0而是None >>> op = slice(0,2,None) >>> l = [1,2,3,4] >>> l[op] [1,2,3] 字符串 str(object='') 👈 str(object=b'', encoding='utf-8', errors='strict') 👈 返回一个字符串对象,创建字符串时python解释器为我们调用了该方法进行创建 repr(object) 👈 返回一个可打印的字符串对象 >>> repr(123) format(value[, format_spec]) 👈 格式化字符串 bytes([source[, encoding[, errors]]]) 👈 将字符串转成bytes类型 >>> bytes('lyon',encoding='utf-8') b'lyon' bytearray([source[, encoding[, errors]]]) 👈 返回一个byte数组,Bytearray类型是一个可变的序列,并且序列中的元素的取值范围为[0,255] source : 如果source为整数,则返回一个长度为source的初始化数组; 如果source为字符串,则按照指定的encoding将字符串转换为字节序列; 如果source为可迭代类型,则元素必须为[0,255]中的整数; 如果source为与buffer接口一致的对象,则此对象也可以被用于初始化bytearray memoryview(obj) 👈 函数返回给定参数的内存查看对象(Momory view) 所谓内存查看对象，是指对支持缓冲区协议的数据进行包装，在不需要复制对象基础上允许Python代码访问 ord(c) 👈 把一个字符转换成ASCII表中对应的数字 >>> ord('a') 97 chr(i) 👈 返回一个数字在ASCII编码中对应的字符 >>> chr(66) 'B' ascii(object) 👈 在对象的类中寻找__repr__方法,获取返回值 >>> class Foo: ... def __repr_(self): ... return \"hello\" ... >>> obj = Foo() >>> r = ascii(obj) >>> print(r) # 返回的是一个可迭代的对象 数据集合 字典 dict(*\\kwarg*) dict(mapping, *\\kwarg*) dict(iterable, *\\kwarg*) 转换成字典类型,创建一个字典时python解释器会自动帮我们调用该方法 集合 set([iterable]) 👈 转换成集合类型,创建集合时,事实上就是通过该方法进行创建的 frozenset([iterable]) 👈 定义冻结集合,即不可变集合,存在hash值 好处是它可以作为字典的key，也可以作为其它集合的元素。缺点是一旦创建便不能更改，没有add，remove方法 相关内置函数 len(s) 👈 返回一个对象的长度 enumerate(iterable, start=0) 👈 为元素生成序号,可以定义序号的初始值,默认从0开始 >>> l = ['a','b','c'] >>> for i,k in enumerate(l,0): ... print(i,k) ... 0 a 1 b 2 c all(iterable) 👈 判断一个可迭代对象中的元素是否都为空,返回bool值 any(iterable) 👈 判断一个可迭代对象中是否有真元素,返回bool值 zip(*iterables) 👈 将两个长度相同的序列整合成键值对,返回一个zip对象可以用dict方法转换查看 >>> l1 = ['k1','k2','k3'] >>> l2 = ['v1','v2','v3'] >>> ret = zip(l1,l2) >>> dict(ret) {'k1':'v1','k2':'v2','k3':'v3'} filter(function, iterable) 👈 筛选过滤,把可迭代对象中的元素一一传入function中进行过滤 # 筛选出偶数 >>> def func(x): ... return x % 2 == 0 >>> f = filter(func,[1,2,3,4,5]) >>> ret = list(f) [2,4] map(function, iterable, ...) 👈 将可迭代对象中的元素一一传入function中执行并返回结果 >>> def func(s): ... return s + ' hello' >>> m = map(func,['alex','egon','lyon']) >>> m >>> ret = list(m) >>> ret ['alex hello', 'egon hello', 'lyon hello'] sorted(iterable, **, key=None, reverse=False*) 👈 为一个对象进行排序,在list中有个sort方法;区别:sort会改变原list,而sorted则不会改变原list >>> l = [3,4,5,1,2,9,8,7,6] >>> sorted(l) [1,2,3,4,5,6,7,8,9] >>> l 迭代器/生成器相关(3个) range(stop) 👈 range(start, stop[, step]) 👈 返回一个序列,为一个可迭代对象,并可用下标取值 >>> from collections import Iterable >>> r = range(10) >>> r[0] 0 >>> isinstance(r,Iterable) True >>> list(r) [0,1,2,3,4,5,6,7,8,9] next(iterator[, default]) 👈 拿取迭代器中的元素,一次只拿一个 >>> Iter = iter([1,2,3,4]) >>> next(Iter) 1 >>> next(Iter) 2 >>> next(Iter) 3 >>> next(Iter) 4 # 没有元素就会进行报错 >>> next(Iter) Traceback (most recent call last): File \"\", line 1, in StopIteration iter(object[, sentinel]) 👈 创建一个迭代器 >>> obj = iter([1,2,3,4]) >>> obj 作用域相关(2个) locals() 👈 打印函数局部命名空间 globals() 👈 打印函数的全局命名空间 面向对象相关(8个) 定义类方法 classmethod(function) 👈 返回一个函数的类方法 staticmethod(function) 👈 返回一个函数的属性方法 property(fget=None, fset=None, fdel=None, doc=None) 👈 返回一个静态属性 判断类之间的关系 isinstance(object, classinfo) 👈 判断对象的类型,返回bool值,主要用于判断类之间的关心,因为type无法判断类之间的关心 issubclass(class, classinfo) 👈 判断一个类是否为另一个类的子类,返回bool值 所有类的基类 classobject 👈 返回一个基类,不接收任何参数 继承 super([type[, object-or-type]]) 👈 用于继承父类 封装 vars([object]) 👈 返回一个对象中包含的属性 反射相关(4个) hasattr(object, name) > vars([object]) 👈 参数是一个对象和一个字符串。如果字符串是对象的一个属性的名称，则结果为True,否则为False getattr(object, name[, default]) > vars([object]) 👈 返回对象的命名属性的值,name必须是字符串,如果字符串是对象属性之一的名称,则返回该属性的值 setattr(object, name, value) > vars([object]) 👈 为某个对象设置一个属性 delattr(object, name) > vars([object]) 👈 删除对象中的属性值 其他(10个) input([prompt]) > vars([object]) 👈 交互式输入 print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False) > vars([object]) 👈 交互式输出 open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None) > vars([object]) 👈 打开文件 help([object]) > vars([object]) 👈 查找官方说明 hash(object) > vars([object]) 👈 返回一个hash地址 callable(object) > vars([object]) 👈 判断一个对象是否可以被调用执行 dir([object]) 👈 返回一个对象中的所有方法 id(object) 👈 返回一个对象的内存地址 type(object) type(name, bases, dict) 👈 查看一个对象的数据类型 __import__(name, globals=None, locals=None, fromlist=(), level=0) 👈 该函数是由import进行调用的,我们一般不用 "},"01-Python/02-函数篇/05-迭代器.html":{"url":"01-Python/02-函数篇/05-迭代器.html","title":"迭代器","keywords":"","body":"Attack on Python - 迭代器 🐍 介绍 迭代器一般用于对容器对象进行遍历访问 , 例如我们对 Python 中的 str , list , tuple , dict , set 等对象的遍历都可以通过迭代器进行遍历访问 在介绍迭代器之前 , 我们需要介绍一下可迭代对象 可迭代对象 迭代是重复反馈过程的活动 , 其目的通常是为了逼近所需目标或结果 可迭代对象 , 即可以按照迭代的方式进行存取数据的对象 , 在 Python 中我们可以理解为可以用 for 循环遍历的对象就是可迭代对象 可迭代对象的标志是 , 它具有__iter__()方法 如何判断一个对象为可迭代对象 # 导入模块 >>> from collections import Iterable >>> l = ['lyon','oldboy'] # 判断是否为Iterable , 即可迭代对象 >>> isinstance(l,Iterable) # 返回bool值 True 迭代器 for循环做的那些事 : for循环是我们用来遍历一个数据集合的方法 , 其实就是根据一定的要求 (这个要求叫做'协议' ) 进行一次次的迭代的效果 . 当我们用 for 循环去遍历时 , 它做的第一件事就是判断对象是否是可迭代对象 , 如果是 , 那么它就会通过 __iter__ 方法返回一个迭代器 , 最后利用__next__()方法将迭代器中的内容一个接一个的取出来 也就是说在 Python 中 , 迭代器已经内置在语言中了 , 我们可以称这种为隐式迭代器 所以迭代器其实就是遍历访问容器对象的一种工具 , 设计人员不需要关心容器对象的内存分配的实现细节 特点: 不依赖索引取值 , 访问者不需要关心迭代器内部的结构 , 仅需通过 __next__() 方法去访问 不能随机访问集合中的某个值 , 只能从头到尾依次访问 , 不可返回访问 惰性计算 , 只有在需要访问时才会生成值 , 节省内存 在 Python 中有一个iter()方法 , 作用就是将可迭代对象变成一个迭代器 , 实质上 iter() 是去调用了__iter__()方法 , 看代码: >>> l = ['lyon'] >>> l.__iter__() # iterator即迭代器 可迭代对象与迭代器的区别: # 用dir方法查看对象中的所有方法 >>> dir_list = dir([1,2]) >>> dir_iter = dir([1,2].__iter__()) # 筛选出不同点 >>> set(dir_iter) - set(dir_list) {'__length_hint__', '__setstate__', '__next__'} 我们可以看出迭代器比可迭代对象多出了三个方法 , 所以我们可以根据这一点来判断一个对象到底是可迭代对象还是一个迭代器 # 创建一个迭代器 >>> i = iter([1,2,3,4]) # 查看迭代器中元素的长度 >>> i.__length_hint__() 4 # 根据索引指定迭代开始位置 >>> i.__setstate__(3) # 进行取值 >>> i.__next__() 4 判断方法 # 导入Iterable类 >>> from collections import Iterable # 导入Iterator类 >>> from collections import Iterator # 是否为可迭代对象 >>> isinstance(obj,Iterable) # 是否为迭代器 >>> isinstance(obj,Iterator) # 注意:迭代器也是可迭代对象 在迭代时 , 我们需要注意迭代器中是否有值的问题 , 即当我们一直调用__next__ 方法取值时 , 如果值都取完了 , 而此时我们再执行 __next__ 方法 , 解释器就会抛出 StopIteration , 因为已经没有值可以取了 迭代器的实现 , 一种常见的方式是使用受限的协程 , 就是生成器 , 另外生成器也可以叫做 \"半协程\" 关于协程的文章 , 你可以在 并发篇 中找到它 , 下一章我们会介绍 Python 的生成器 "},"01-Python/02-函数篇/06-生成器.html":{"url":"01-Python/02-函数篇/06-生成器.html","title":"生成器","keywords":"","body":"Attack on Python - 生成器 🐍 介绍 生成器 , 又称为 \"半协程\" , 它与迭代器 , 协程都有着亲密的关系 生成器 生成器非常类似于返回数组的函数 , 都是具有参数、可被调用、产生一系列的值 , 但是生成器不是构造出数组包含所有的值并一次性返回 , 而是每次产生一个值 , 因此生成器看起来像函数 , 但行为像迭代器 因此我们可以利用生成器进行惰性求值 , 不提前存储 , 每次都是通过计算 在 Python 中 , 生成器是用来实现迭代器的 , 所以生成器实际上是迭代器的构造器 虽然迭代器是由生成器构造 , 但是生成器同样是可迭代对象 , 自然生成器也可以算是迭代器 , 至少在 Python 中我们可以这样来判断 from collections import Iterator print(isinstance((i for i in range(10)), Iterator)) 生成器函数 一个函数调用时返回一个迭代器 , 那么这个函数就叫做生成器函数 利用生成器做一个range( 2.x中的xrange ) 的功能 # 定义生成器 >>> def range(n): ... start = 0 ... while start >> obj = range(5) >>> obj.__next__() >>> obj.__next__() >>> obj.__next__() >>> obj.__next__() >>> obj.__next__() # 也可以使用()定义生成器 range = (i for i in range(5)) yield 的作用 : yield 的作用是中断函数的执行并记录中断的位置 , 等下次重新调用这个函数时 , 就会接着上次继续执行 PS : 调用生成器函数时 , 仅仅会返回一个生成器 , 并不会执行函数的内容 , 生成器只能由 next() 进行调用执行 , 实质上next() 方法就是调用的__next__() 方法 yield from def func1(): for i in 'AB': yield i for j in range(3): yield j print(list(func())) def func2(): yield from 'AB' yield from range(3) print(list(func2())) 除了通过 yield 和 yield from 语句 , 我们还可以通过生成器表达式来定义 , 也就是 (i for i in range(10)) 这种方式 , 而其他的推导式则是使用 () 之外的定义 # 列表推导式 l = [i for i in range(10)] # 字典推导式 d = {i:i for i in range(10)} # 集合推导式 s = {i for i in range(10)} 应用 监听文件 import time def tail(filename): # 打开文件 f = open(filename,encoding='utf-8') # 从文件末尾算起 f.seek(0, 2) while True: # 读取文件中新的文本行 line = f.readline() if not line: time.sleep(0.1) continue yield line tail_g = tail('tmp') # 生成器也是可迭代对象 for line in tail_g: print(line) 计算动态平均值 def averager(): total = 0 count = 0 average = None while True: term = yield average total += term count += 1 average = total/count # 生成生成器 g_avg = averager() # 激活生成器,不激活无法send next(g_avg) # send相当于先传参,后调用next() print(g_avg.send(10)) print(g_avg.send(30)) print(g_avg.send(50)) 当然在我们工作中更多的是利用生成器来实现惰性计算 "},"01-Python/02-函数篇/07-递归.html":{"url":"01-Python/02-函数篇/07-递归.html","title":"递归","keywords":"","body":"Attack on Python - 递归 🐍 递归算法 递归算法是一种直接或者间接地调用自身算法的过程（递归函数就是一个体现）。在计算机编写程序中，递归算法对解决一大类问题是十分有效的，它往往使算法的描述简介而且易于理解。 特点：👈 递归就是再过程或函数里调用自身 再使用递归策略时，必须有一个明确的递归结束条件，称为递归出口。递归算法解题通常显得很简洁，但递归算法解题的运行效率低 在递归调用的过程当中系统为每一层的返回点、局部量等开辟了栈来存储。递归次数过多容易造成栈溢出等。所以一般不提倡用递归算法设计程序 要求：👈 每次调用在问题规模上都有所减少（通常是减半） 相邻两次重复之间有紧密的联系，前一次要为后一次做准备（通常前一次的输出就作为后一次的输入） 再问题的规模极小时必须要直接给出解答而不再进行递归调用，因而每次递归调用都是有条件的（以规模未达到直接解答的大小为条件），无条件递归条用将会称为死循环而不能正常结束 递归函数 面向函数编程中，利用递归思想来解决一些简单的问题是非常简单便洁的 递归函数就是函数内部通过调用自己本身来实现功能的函数。既然是调用自身,那么每次调用，需要解决的问题就应该有所减少，不然这个函数就没有尽头的执行下去。 打印10-0 def counter(num): # 打印num print(num) # 如果num小于等于0 if num 递归应用 用递归实现斐波那契数列 l = [] def fibonacci(n1,n2): # 大于1000后结束递归 if n1 > 2000: # 终止函数，并返回 \"不搞了\" return \"不搞了！\" # 追加进列表 l.append(n1) # 前两个数之和 n3 = n1 + n2 # 进行递归 fibonacci(n2, n3) # 从0开始 fibonacci(0, 1) print(l) 用递归实现三级菜单 menu = { '北京': { '海淀': { '五道口': { 'soho': {}, '网易': {}, 'google': {} }, '中关村': { '爱奇艺': {}, '汽车之家': {}, 'youku': {}, }, '上地': { '百度': {}, }, }, '昌平': { '沙河': { '老男孩': {}, '北航': {}, }, '天通苑': {}, '回龙观': {}, }, '朝阳': {}, '东城': {}, }, '上海': { '闵行': { \"人民广场\": { '炸鸡店': {} } }, '闸北': { '火车战': { '携程': {} } }, '浦东': {}, }, '山东': {}, } def threeLM(menu): while True: # 打印本级菜单内容 for key in menu: # 打印字典的key print(key) # 用户输入内容 chooice = input(\"请输入菜单>>\") if chooice == 'back': return elif chooice == 'quit': return 'q' if chooice in menu.keys(): # 将新字典作为参数进行递归调用 ret = threeLM(menu[chooice]) if ret == 'q':return 'q' threeLM(menu) "},"01-Python/03-对象篇/":{"url":"01-Python/03-对象篇/","title":"对象篇","keywords":"","body":"Attack on Python - 对象篇 🐍 介绍 一切皆对象 "},"01-Python/03-对象篇/01-面向对象.html":{"url":"01-Python/03-对象篇/01-面向对象.html","title":"面向对象","keywords":"","body":"Attack on Python - 面向对象 🐍 介绍 编程范式 编程是程序员用 特定的语法 + 数据结构 + 算法组成的代码来告诉计算机如何执行任务的过程 , 而实现一个任务的方式有很多种不同的方式 , 对这些不同的编程方式的特点进行归纳总结得出来的编程方式类别，即为编程范式 面向过程编程 Procedural Programming 面向过程编程就是程序从上到下一步步执行 , 基本设计思路就是程序一开始是要着手解决一个大的问题 , 然后把一个大问题分解成很多个小问题或子过程 , 这写子过程再执行的过程再继续分解直到小问题足够简单到可以在一个小步骤范围内解决 在Python中 , 我们通过把大段代码拆成函数 , 通过一层一层的函数调用 , 就可以把复杂任务分解成简单的任务 , 这种分解可以称之为面向过程的程序设计 . 函数就是面向过程的程序设计的基本单元 函数式编程 Functional Programming 函数式编程就是一种抽象程度很高的编程范式 , 纯粹的函数式编程语言编写的函数没有变量 , 函数式编程的一个特点就是 , 允许把函数本身作为参数传入另一个函数 , 还允许返回一个函数 , Python对函数式编程提供部分支持 . 由于Python允许使用变量 , 因此 , Python不是纯函数式编程语言 面向对象编程 Object Oriented Programming 面向对象编程是利用\"类\"和\"对象\"来创建各种模型来实现对真实世界的描述 , 使用面向对象编程的原因一方面是因为它可以使程序的维护和扩展变得更简单 , 并且可以大大提高程序开发效率 , 另外 , 基于面向对象的程序可以使它人更加容易理解你的代码逻辑 , 从而使团队开发变得更从容 类与实例 类的语法 class 类名: pass 一个栗子🌰 # 创建一个人的'类',首字母要大写 class Person(object): # 构造函数,初始化属性 def __init__(self,name): self.name = name # 人可以吃饭 def eat(self): print(\"I am eatting\") # 创造了一个叫做'Lyon'的人 p = Person('Lyon') # 执行吃饭功能 p.eat() # 执行结果: I am eatting 类 (class) 类就是 对现实生活中一类具有共同特征事物的抽象 类起到一个模板的作用 , 当我们创建一个类时 , 就相当于创建了一个初始的'模型' , 我们可以通过这个'模型' 来创建出一个个具有相同特征或功能的事物 , 来帮助我们更好的处理问题 在上述栗子中类名Person 后有一个(object) , 这是新式类的写法 , 而在python3.x 以上的版本中 , 默认为新式类 , 所以也可直接 class Person: 我们创建类时 , 都默认继承了object类 , object详解见后期文章 实例 (instance) 我们知道类是一个抽象 , 既然是抽象那就是不可操作的 , 所以我们如果进行操作 , 就需要将这一抽象的概念变成具体的事物 , 这个过程我们称为实例化 实例化: 由抽象的类转换成实际存在的对象的过程 实例: 由类进行实例化所得到的对象 , 上述栗子中的 p 就是一个实例 属性与方法 属性是实体的描述性质或特征 , 比如人有名字 , 年龄 , 性别等 . 当然还有人所能做的事情也是一种属性 , 比如吃饭 , 睡觉 , 喝水等 . 对于这两种属性 , 一种是表示特征的 , 叫做静态属性 , 另一种则是表示功能的 , 叫做动态属性 在Python中 , 我们将静态属性 就称为属性 , 将动态属性 就称为方法 , 并且以变量来表示属性 , 以函数表示方法 , PS:类中的函数已经不叫函数了 , 而叫做方法 调用方式: 类名.属性名 class Person: # 类变量 role = 'student' # 构造函数 def __init__(self,name): # 实例变量 self.name = name 调用方式: 类名 . 方法名( ) class Person: # 普通方法 def eat(self): pass 特殊的类属性 属性名 说明 __dict__ 查看类或对象成员 , 返回一个字典 __name__ 查看类的名字 __doc__ 查看类的描述信息 , 即注释部分 __base__ 查看第一个父类 __bases__ 查看所有父类 , 返回一个元组 __module__ 查看类当前所在模块 __class__ 查看对象通过什么类实例化而来 PS:对于属性和方法 , 在网上分类各种各样的都有 , 比如字段 , 还有菜鸟教程中的一些 , 其实本质上都是一个东西 构造函数 在上述例子中 , 可以看到有一个__init__ 方法 , 这个方法叫做构造方法 , 用于初始化属性 , 所以如果我们要设置属性 , 那么构造方法是必须要的 self 我们直接通过实例来说明 class Foo: def __init__(self,name): self.name = name def func(self): print(id(self)) a = Foo('Lyon') # 打印实例a的内存地址 print(id(a)) # 调用类中的func方法,即打印self的内存地址 a.func() ''' 执行结果: 1703689404544 1703689404544 结果分析: 我们发现a的内存地址和self的内存地址是一样的,也就是说self其实就是实例本身 那么在我们进行实例化的时候,self.name = name 就是给实例添加一个name属性,该属性的值就是我们在实例化时传入的'Lyon' 所以如果我们需要给对象添加属性的话,可以直接通过 对象.属性名 = 属性值 的方式进行添加 ''' 将上栗子中的构造函数再换个姿势看看 a = Foo('Lyon') # 等价于如下,用类名调用类中的方法 Foo.__init__(a,'Lyon') 命名空间 在函数中 , Python解释器在执行时 , 会将函数名称依次加载到命名空间 , 类当然也一样 我们创建一个类时 , Python解释器一执行就会创建一个类的命名空间 , 用来存储类中定义的所有名称( 属性和方法 ) , 而我们进行实例化时 , Python解释器又会为我们创建一个实例命名空间 , 用来存放实例中的名称 当我们利用 对象. 名称 来访问对象属性 ( 静态与动态 ) 时 , Python解释器会先到该对象的命名空间中去找该名称 , 找不到就再到类 ( 该对象实例化之前的类 ) 的命名空间中去找 , 最后如果都没找到 , 那么就抛出异常了 命名空间的本质是一个字典 , 我们可以访问对象的 __dict__ 属性得到命名空间 访问属性实例 class A(object): \"\"\" 这是一个类 \"\"\" pass a = A() # 访问实例a的__doc__属性 print(a.__doc__) ''' 执行结果: 这是一个类 ''' 嵌套组合 对象交互 class Person: def __init__(self, name): self.name = name def attack(self,per): print(\"{} attacked {}\".format(self.name, per.name)) lyon = Person(\"Lyon\") kenneth = Person(\"kenneth\") lyon.attack(kenneth) # 执行结果: Lyon attacked kenneth 类的组合 传参时组合 class BirthDate: def __init__(self, year, month, day): self.year = year self.month = month self.day = day class Person: def __init__(self, name, birthdate): self.name = name self.birthdate = birthdate p = Person('Lyon', BirthDate(2000, 1, 1)) 定义时组合 class BirthDate: def __init__(self, year, month, day): self.year = year self.month = month self.day = day class Person: def __init__(self, name, year, month, day): self.name = name self.birthdate = BirthDate(year, month, day) p = Person('Lyon', 2000, 1, 1) "},"01-Python/03-对象篇/02-继承.html":{"url":"01-Python/03-对象篇/02-继承.html","title":"继承","keywords":"","body":"Attack on Python - 继承 🐍 抽象与继承 抽象 抽象是从众多的事物中抽取出共同的、本质性的特征，而舍弃其非本质的特征 比如 🍎 , 🍌 , 🍇 , 等 , 它们共同的特性就是水果 , 我们得出水果这个概念的过程就是一个抽象的过程 , 抽象能使复杂度降低 , 好让人们能够以纵观的角度来了解许多特定的事态 有抽象就会有具体 , 我们会用抽象的对象来表示一类事物 , 而用具体的对象表示某个事物 , 比如苹果 , 香蕉 , 葡萄都是具体的对象 , 水果则是抽象的对象 继承 继承是基于抽象的结果 抽象可以让我们来以纵观的角度了解一类事物事物 , 并且这类事物都拥有该抽象中所有的特征 , 相当于继承了该抽象中的特征 , 这样我们就可以只将这类事物不同的特征放到具体中 , 而不需要再次关心共同特征 , 所以先有抽象后才能有继承 介绍抽象的概念时利用了水果来进行说明 , 为了更好的理解 , 继承就用动物为例子 '-----------抽象出动物类-----------' # 从狗和猫中抽取共同的特征,它们都能吃,喝,睡,玩 class Animal(object): # 吃 def eat(self): pass # 喝 def drink(self): pass # 睡 def sleep(self): pass # 玩 def play(self): pass '------------具体动物类------------' # 所有的类默认是继承了object类的,让'猫'类继承动物类 class Cat(Animal): # 抓老鼠 def catch_mouse(self): pass # 让'狗'类继承动物类 class Dog(Animal): # 跳墙 def jump_wall(self): pass 我们把🌰栗子中的Animal类叫做父类 , 基类或超类 , Cat和Dog类叫做子类或派生类 简单的继承方式就是在类名后面加入要继承的类 使用继承可以减少我们代码重用 , 简化代码 新式类与经典类 在说新式类与经典类之前 , 先说一说单继承和多继承 单继承与多继承 单继承就是只以一个类作为父类进行继承 # 定义基类 class Parent: pass # 继承基类 class Subclass(Parent): pass 多继承就是同时以多个类做为基类进行继承 # 定义第一个基类 class Parent1: pass # 定义第二个基类 class Parent2: pass # 定义第三个基类 class Parent3: pass # 继承三个基类 class Subclass1(Parent1,Parent2,Parent3): pass 在多继承中我们需要考虑一个继承优先的问题 , 就像上面的例子 , 如果我们所定义的三个父类中 , 都拥有一个同样的方法那么Python解释器会怎么去继承父类的方法? 三个同名的方法明显只能选择其中一个进行继承 , 这就关系到经典类和新式类了 经典类和新式类 经典的东西都是比较旧的 , so , 在Python 2.x 中默认都是经典类 , 只有显示继承了object才是新式类 ; 而Python 3.x 中默认都是新式类 , 不必显示的继承object 经典类与新式类在声明时的区别在于 , 新式类需要加上object关键字 # python 2.x 环境下 # 经典类 class A(): pass # 新式类 class A(object): pass # python 3.x 环境下 class A: pass 经典类与新式类多继承顺序的区别在于 , 经典类会按照深度优先 (纵向)的方式查找 , 新式类会按照广度优先 (横向)的方式查找 实例环境Python2 经典类 # 经典类 class A(): def __init__(self): pass def display(self): print \"This is from A\" class B(A): def __init__(self): pass class C(A): def __init__(self): pass def display(self): print \"This is from C\" class D(B,C): def __init__(self): pass obj = D() obj.display() ''' 执行结果: This is from A 说明:经典类深度优先,我们通过实例调用display方法时,Python解释器会先找B类,如果B类中没有就会去B类的父类(即A类)中查找,如果在所有的父类中都没有找到需要的方法,才会开始继续找下一个继承的类(即C类) ''' 新式类 # 新式类 class A(object): def __init__(self): pass def display(self): print \"This is from A\" class B(A): def __init__(self): pass class C(A): def __init__(self): pass def display(self): print \"This is from C\" class D(B,C): def __init__(self): pass obj = D() obj.display() ''' 执行结果: This is from C 说明:新式类广度优先,Python解释器首先到B类进行查找,B类中没有就直接去C类中找,并不会去B类的父类(A类)中去查找,如果C类中没有才会再去B类的父类(A类)中查找,最后如果没找到就会报错 ''' 派生 利用继承机制 , 新的类可以从已有的类中派生 子类继承了父类 , 父类派生了子类 , 继承是站在子类的角度 , 派生是站在父类的角度 , 我们在子类中可以添加新的属性或方法 . 但是要注意父类属性名与子类属性名相同 , 以及父类与子类中方法名的情况 , 说的有点绕了 , 通过实例进一步描述 属性名 , 方法名不发生冲突 # 创建一个基类 class Person: # 基类属性 country = 'China' # 构造方法 def __init__(self, name, age): self.name = name self.age = age # 工作方法 def work(self): print(\"I am working ...\") # 派生一个子类,继承基类中的属性和方法 class Man(Person): # 子类属性 male = 'man' # 新增睡觉方法 def sleep(self): print(\"I am sleepiing ...\") # 实例化子类 man = Man('Lyon', 18) # 调用从基类继承过来的工作方法 man.work() # 访问从基类继承过来的国家属性 print(man.country) # 调用子类中的睡觉方法 man.sleep() # 访问子类中的male属性 print(man.male) ''' 执行结果: I am working ... China I am sleepiing ... man ''' 属性或方法冲突 , 会按照加载顺序进行覆盖 , 定义过程就已完成 # Python解释器开始执行,将Person类的名字以及类中包含的属性名方法名加载到Person类的命名空间 class Person: country = 'China' # 注意构造方法也是方法,Python解释器加载时仅仅会将__init__这个名字加载到命名空间,并不会执行内部代码 def __init__(self, name, age): self.name = name self.age = age # 加载方法名 def work(self): print(\"I am working ...\") # Python解释器将Man类的名字加载到Man的命名空间,随后由于Person类在这步之前已经完成加载,此时就会通过Person类名从Person的命名空间中取出属性和方法名加载到Man类的命名空间 class Man(Person): # 由于上一已完成Person类中的同名__init__的加载,此时会将其覆盖 def __init__(self, male, country): self.male = male self.country = country # 同__init__,将同名work覆盖 def work(self): print(\"I don't like working ...\") # 加载到Man类的命名空间 def sleep(self): print(\"I am sleepiing ...\") # 实例化Man类 man = Man('male', 'America') # 此work为覆盖后的work即子类自己的work man.work() # country为父类的类属性,在实例化时被实例属性覆盖 print(man.country) # 调用子类中的sleep方法 man.sleep() # 打印实例属性male print(man.male) ''' 执行结果: I don't like working ... America I am sleepiing ... male ''' 当然我们在使用时仅需注意一下几点: 重名时 , 会以子类的方法或属性为主 , 因为父类的会被覆盖 构造方法里是实例属性 , 子类如果也有构造方法 , 以子类的构造方法为主 通俗的讲 : 我有就用我的 , 没有就拿你的 但是上述派生中有两个问题: 当子类父类都有构造方法时 , 如果子类需要父类构造方法中的实例属性怎么办 ? 当子类父类都有同名方法时 , 如果子类需要用父类中的方法怎么办? 这两个问题放到下节 super 中解决 super 先解决上节中的两个问题 , 既然父类中的方法被覆盖掉了 , 那么我们不妨再加载一次父类中的方法 , 将子类中的再次覆盖 解决问题1 : 子类父类构造方法中实例属性集合 class Person: def __init__(self, name, age): self.name = name self.age = age class Man(Person): # 实例属性集合也还是要传参的,只是传入后各拿各的 def __init__(self, name, age, male): self.male = male # 通过类名.方法调用Person类中的__init__方法,即将__init__中的代码拿过来用了一遍 Person.__init__(self, name, age) # 实例化Man类 man = Man('Lyon', 18, 'male') # 访问man中的name实例属性 print(man.name) # 访问man中的age实例属性 print(man.age) # 访问man中的male print(man.male) ''' 执行结果: Lyon 18 male ''' 解决问题2 : 使用父类中的重名方法 对于第二个问题明显不能利用问题1同样的方式了 , 因为调用就意味着执行 , 虽然我们可以以问题1中的方式执行父类的方法 , 但是子类的方法也还是会照常执行 , so , 我们得换个方式 class Person: def work(self): print(\"I am working ...\") class Man(Person): def work(self): print(\"I don't like working ...\") man = Man() # 将实例man作为self传入Person类中的work方法 # Person().work() Person.work(man) ''' 执行结果: I am working ... ''' 两个问题解决了 , 但是我们发现通过这两种方式来解决会对后期修改造成非常大的麻烦 , 只要类名一变 , 那么我们就得一个个修改 , 开发中来个100个就够你改半小时了 ... 所以就有了super super super只能用在新式类中 , 在经典类中则只能按照上面的方式进行处理了 截取官方文档中的一部分 # 相当于super(type, obj),first argument一般是self实例本身 super() -> same as super(__class__, ) # 返回非绑定父类对象 super(type) -> unbound super object # 返回父类的实例 super(type, obj) -> bound super object; requires isinstance(obj, type) # 返回父类的实例 super(type, type2) -> bound super object; requires issubclass(type2, type) # type参数为子类 Python中一切皆对象 , 所以其实super是一个类 , 在我们使用super时事实上调用了super类的初始化函数 , 产生了一个super对象 首先用super的方式解决上面的问题吧 问题1 class Person: def __init__(self, name, age): self.name = name self.age = age class Man(Person): def __init__(self, name, age, male): self.male = male super().__init__(name, age) 问题2 class Person: def work(self): print(\"I am working ...\") class Man(Person): def work(self): print(\"I don't like working ...\") man = Man() # super的第一个参数是要找父类的那个类 super(Man,man).work() 但是在我们使用多继承时 , 这两者的区别就能显现出来了 使用__init__ class A(object): def __init__(self): print(\"This is from A\") class B(A): def __init__(self): print(\"This is from B\") A.__init__(self) print(\"This is from B\") class C(A): def __init__(self): print(\"This is from C\") A.__init__(self) print(\"This is from C\") class D(B,C): def __init__(self): print(\"This is from D\") B.__init__(self) C.__init__(self) print(\"This is from D\") d = D() ''' 执行结果: This is from D This is from B This is from A This is from B This is from C This is from A This is from C This is from D ''' 使用super class A(object): def __init__(self): print(\"This is from A\") class B(A): def __init__(self): print(\"This is from B\") super().__init__() print(\"This is from B\") class C(A): def __init__(self): print(\"This is from C\") super().__init__() print(\"This is from C\") class D(B,C): def __init__(self): print(\"This is from D\") super().__init__() print(\"This is from D\") d = D() ''' 执行结果: This is from D This is from B This is from C This is from A This is from C This is from B This is from D ''' 用__init__ 和 super我们得到的结果是不一样的 , 因为super是一个类名 , super( ) 事实上调用了super类的初始化函数 , 产生了一个super对象 , 所以使用super可以避免父类被重复调用 PS : super的查找方式遵循MRO表中的顺序 , MRO表后续文章中在研究 抽象类与接口 Python本身不提供抽象类和接口机制 抽象类 在Java中抽象类的定义是这样的 : 由abstract 修饰的类叫抽象类 , 该类不能被实例化 , 并且仅支持单继承 在Python中如果要实现抽象类 , 需要借助abc模块 . ABC是Abstract Base Class的缩写 在abc模块中有一个用来生成抽象类的元类 ABCMeta 生成抽象类 # 导入抽象元类和抽象方法 from abc import ABCMeta,abstractmethod class Abstract_class(metaclass=ABCMeta): # 使用抽象方法进行约束 @abstractmethod # 父类可以简单实现,子类必须实现 def func(self): print('hello func') 抽象类提供了继承的概念 , 它的出发点就是为了继承 , 否则它没有存在的任何意义 , 所以说定义的抽象类一定是用来继承的 接口 在Java中接口是一个抽象类型 , 是抽象方法的集合 , 接口通常以interface来声明 . 一个类通过继承接口的方式 , 从而来继承接口的抽象方法 , 达到约束的目的 在Python中默认是没有的 , 所以我们如果要使用接口 , 有两种方法 , 第一种就是我们在抽象类的基础上进行定义 , 第二种则是借助第三方模块 zope.interface 这里我们只说第一中方法 # 导入抽象元类和抽象方法 from abc import ABCMeta,abstractmethod class Abstract_class(metaclass=ABCMeta): # 使用抽象方法进行约束 @abstractmethod # 父类不能实现,子类必须实现 def func(self): pass 与抽象类中的例子比较 , 因为在Python中抽象类与接口类这两者区分并不清晰 , 我们在对于方法是否实现上 , 修改之后基本就实现了一个接口 什么时候使用抽象类与接口 当几个子类的父类,有相同的功能需要被实现的时候,就使用抽象类 当几个子类,有相同的功能,但是实现各不相同的时候,就使用接口 (接口归一) 接口归一实例 from abc import ABCMeta, abstractmethod # 定义接口 class Payment(metaclass = ABCMeta): @abstractmethod def pay(self, money): pass # 继承接口 class Applepay(Payment): def pay(self, money): print('The payment method is Applepay , {}'.format(money)) # 继承接口 class Zhifubao(Payment): def pay(self, money): print('The payment method is Zhiwubaopay , {}'.format(money)) # 继承接口 class Wexin(Payment): # 没有接口中的pay方法,实例化时就报错 def fuqian(self, money): print('The payment method is Wexinpay , {}'.format(money)) # 接口归一 def payment(obj,money): obj.pay(money) # 实例化就报错,没有pay方法 # wexin = Wexin() zhifubao = Zhifubao() apple = Applepay() payment(zhifubao,100) payment(apple,1000) 总结 抽象类与接口都不能被实例化 (抽象方法约束) , 所以必须被继承才能使用 抽象类中的方法能够被实现 , 接口中的方法不能被实现 抽象类中可以有构造方法 , 接口中不可有 抽象类最好不要用多继承 , 而接口类可以 isinstance 和 issubclass isinstance(obj, cls) 检查obj是否是类cls的对象 class Foo: pass obj = Foo() print(isinstance(obj, Foo)) print(isinstance(obj, object)) print(isinstance(obj, type)) ''' 执行结果: True #obj是类Foo的对象 True #obj是object的对象,Foo类继承了object类 False #object类是有type类的实例 ''' issubclass(sub, super) 检查sub类是否是super类的派生类 class A: pass class B(A): pass print(issubclass(B, B)) print(issubclass(B, A)) print(issubclass(B, object)) print(issubclass(B, type)) ''' 执行结果: True #B类是自己的派生类 True #B类是A类的派生类 True #B类是object类的派生类,因为A类继承了object类 False #B类不是type类的派生类,type类实例化产生了object类 ''' "},"01-Python/03-对象篇/03-多态.html":{"url":"01-Python/03-对象篇/03-多态.html","title":"多态","keywords":"","body":"Attack on Python - 多态 🐍 介绍 上一篇中已经得知 , 继承可以扩展已存在的代码模块(类) , 其目的是为了解决代码重用 问题 多态则是为了实现另一个目的 : 接口重用 多态 多态 (Polymorphism) 按字面的意思就是\"多种状态\" , 比如动物有多种形态 , 人 , 猫 , 狗 ; 文件也有多种格式 exe , txt , md(MarkDown格式) , 这就是多态 在面向对象语言中 , 接口的多种不同的实现方式即为多态 多态性是允许你将父对象设置成为一个或多个他的子对象相等的技术 , 赋值之后 , 父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作 静态多态性 必须在编译时就能确定其处理方式 n1 = 12 n2 = 34 # int类型相加 print(n1 + n2) s1 = 'hello ' s2 = 'word' # str类型相加 print(s1 + s2) ''' 执行结果: 46 hello word ''' 如上述例子我们利用运算符 \"+\", 完成了两种情况下的运算 , 并且Python解释器在执行前就已确定处理方式 , 即编译过程中就已经知道了调用哪个函数 动态多态性 编译时无法立即确定其处理方式 , 只有在执行时才确定处理方式 , 注意一定要同名 from abc import ABCMeta,abstractclassmethod # 接口继承 class Animal(metaclass=ABCMeta): @abstractclassmethod # 约束派生类必须有talk方法 def talk(self): pass class Cat(Animal): def talk(self): print(\"喵喵喵\") class Dog(Animal): def talk(self): print(\"汪汪汪\") c = Cat() d = Dog() # 因为接口的缘故,我们无需考虑实例化后的对象具体是什么类型,因为动物都有talk方法,所以我们可以直接使用 c.talk() d.talk() # 我们进行接口统一 def talk(obj): obj.talk() talk(c) talk(d) ''' 执行结果: 喵喵喵 汪汪汪 喵喵喵 汪汪汪 ''' 上栗中, Python解释器在解释时是无法确定处理方式的 , 因为存在几个同名的方法 , 编译时并不能确定是哪一个 , 只有在执行时 , 才能确定使用哪个类中的talk() 方法 , 这就是动态多态性 小结: 静态多态性与动态多态性的区别在于 , 编译时是否能确定其处理方式 通过多态可以实现用一个函数名调用不同内容的函数 多态性的好处 多态性是面向对象的三大特性之一 , 有很多人说Python不支持多态 , 事实上Python处处是多态 , 比如内置函数len() 就是多态的一种体现 多态的作用: 增加了程序的灵活性 以不变应万变 , 不论对象有多少中形态 , 使用者都是同一种形式去调用 , 如 talk(obj) 增加了程序的可扩展性 通过继承Animal类派生了一个新的类 , 使用者无需更改自己的代码 , 依旧利用 talk(obj) 进行调用 对于多态 , 可能会觉得比较模糊 , 这是因为 , 我们在写程序时不知不觉就用上了 , 哈哈所以还是说处处是多态 鸭子类型 Python崇尚鸭子类型 以下是维基百科中对鸭子类型得论述 : 在程序设计中 , 鸭子类型 (英语 : duck typing) 是动态类型的一种风格。在这种风格中 , 一个对象有效的语义 , 不是由继承自特定的类或实现特定的接口 , 而是由当前方法和属性的集合决定 . 这个概念的名字来源于由James Whitcomb Riley提出的鸭子测试 , \" 鸭子测试 \"可以这样表述： \" 如果走起来像鸭子 , 游泳起来像鸭子 , 叫起来也像鸭子 , 那么它就是鸭子 \" 在鸭子类型中 , 关注的不是对象的类型本身 , 而是它是如何使用的 . 例如 , 在不使用鸭子类型的语言中 , 我们可以编写一个函数 , 它接受一个类型为鸭的对象 , 并调用它的走和叫方法 . 在使用鸭子类型的语言中 , 这样的一个函数可以接受一个任意类型的对象 , 并调用它的走和叫方法 . 如果这些需要被调用的方法不存在 , 那么将引发一个运行时错误 . 任何拥有这样的正确的走和叫方法的对象都可被函数接受的这种行为引出了以上表述 , 这种决定类型的方式因此得名。 鸭子类型通常得益于不测试方法和函数中参数的类型 , 而是依赖文档 , 清晰的代码和测试来确保正确使用 . 从静态类型语言转向动态类型语言的用户通常试图添加一些静态的 ( 在运行之前的 ) 类型检查 , 从而影响了鸭子类型的益处和可伸缩性 , 并约束了语言的动态特性 例1 : 利用标准库中定义的各种 ' 与文件类似的对象 ' , 尽管这些对象的工作方式像文件 , 但他们没有继承内置对象的方法 # 文本文件 class TxtFile: def read(self): pass def write(self): pass # 磁盘文件 class DiskFile: def read(self): pass def write(self): pass 二者都像鸭子, 二者看起来都像文件,因而就可以当文件一样去用 例2 : 序列类型有多种形态 : 字符串 , 列表 , 元组 , 但他们没有直接的继承关系 # 三者都是序列类型 name = 'Lyon' namel = ['Lyon'] namet = ('Lyon',) # 字符串,列表,元组并没有直接关系,都可以调用len(),并且我们无需考虑类型 print(len(name)) print(len(namel)) print(len(namet)) "},"01-Python/03-对象篇/04-封装.html":{"url":"01-Python/03-对象篇/04-封装.html","title":"封装","keywords":"","body":"Attack on Python - 封装 🐍 介绍 封装就是把客观事物封装成抽象的类 , 并且类可以把自己的数据和方法只让可信的类或者对象操作 , 对不可信的进行信息隐藏 私有问题 当我们类中的一些属性或者方法想要对不可信的类或者对象隐藏时 , 我们就可以将这些属性或者方法 , 定义成私有属性或者私有方法 在Python中用双下划线开头的方式将属性隐藏起来 , 即带双下划线就为私有属性或者私有方法 私有属性 class A: def __init__(self,name): # 定义私有属性 self.__name = name # 实例化 a = A(\"Lyon\") # 访问a中的__name属性 print(a.__name) # 执行结果 : AttributeError: 'A' object has no attribute '__name' ''' 结果报错,意思是对象A中没有__name这个属性 也就是说,外部已经不能直接利用 .__name 来访问这个属性了 因为此时它是一个私有属性 ''' 将属性定义成私有属性其实是一种变形操作 , 即类中所有以双下划线开头的名称都会自动变形成:_类名+名称 如下: class A: def __init__(self, name): # 定义私有属性 self.__name = name # 实例化 a = A(\"Lyon\") # 访问a中的__name属性 print(a._A__name) # 执行结果: Lyon ''' __name自动变形为 _A__name 所以使用a._A__name是可以访问到的 ''' 由上可知变形的特点如下: 类中定义的__name只能在内部使用 , 并且内部使用是引用的变形的结果,即( self._A__name) 这种变形其实是针对外部的变形 , 在外部是无法通过__name访问的 PS : 这种变形机制其实并没有真正意义上限制我们从外部直接访问属性 , 知道了类名和属性名就可以拼出名字 : _类名__属性 , 然后就可以访问了 , 如 a._A__name . 并且变形的过程只在类的定义时发生一次 私有方法 class A: def __func(self): print(\"In the A\") a = A() a.__func() # 执行结果: AttributeError: 'A' object has no attribute '__func' a._A__func() # 执行结果: In the A 当私有遇到继承 当我们在继承中使用私有属性或者方法时 , 因为变形机制 , 我们已经不能将私有属性或者方法 , 来与普通属性或者方法那样看待了 私有属性继承 class A: def __init__(self, ame): self.__name = ame class B(A): def __init__(self, name, ame): self.__name = name # 继承父类中的属性 super().__init__(ame) a = B('a', 'b') print(a._A__name) print(a._B__name) ''' 执行结果: b a ''' 例子说明 : 在上节中已经知道变形操作这回事了 , 当遇到继承时需要注意的就是 , 我们表面上看到的是两个类中都只有一个__name属性 , 但是由于变形 , 使其在定义完成后就分别变成了_A__name 和 _B__name , 所以继承时已经是两个不同的属性了 , 所以两个属性都存在 , 只是我们表面上还是看不到 私有方法继承 与私有属性继承一样 , 需要注意私有方法名变形的问题 我们可以利用这一特点 , 来实现继承时达到子类不会覆盖父类方法的效果 class A: def __func(self): print('from A') def test1(self): self.__func() class B(A): def __func(self): print('from B') def test2(self): self.__func() b=B() b.test1() b.test2() ''' 执行结果: from A from B ''' 封装与扩展性 封装在于明确区分内外 , 使得类实现者可以修改封装内的东西而不影响外部调用者的代码 ; 而外部使用者只知道一个接口(函数) , 只要接口(函数)名 , 参数不变 , 使用者的代码永远无需改变 . 这就提供了一个良好的合作基础 , 相当于只要接口这个基础约定不变 , 则代码改变也不足为虑 原始类 class Room: def __init__(self, name, owner, width, length, high): self.name = name self.owner = owner self.__width = width self.__length = length self.__high = high # 对外提供的求面积接口,隐藏内部实现详解 def tell_area(self): return self.__width * self.__length r1 = Room('卧室','Lyon','0.3','2','2') r1.tell_area() 修改类 class Room: def __init__(self, name, owner, width, length, high): self.name = name self.owner = owner self.__width = width self.__length = length self.__high = high # 对外提供的求体积接口,隐藏内部实现详解 def tell_area(self): return self.__width * self.__length * self.__high r1 = Room('卧室','Lyon','0.3','2','2') r1.tell_area() 我们发现我们将类的功能作出了修改 , 但是对于使用类功能的人来说 , 接口并没有发生变化 , 他们依然可以用原来的接口使用新功能 "},"01-Python/03-对象篇/05-方法转换.html":{"url":"01-Python/03-对象篇/05-方法转换.html","title":"方法转换","keywords":"","body":"Attack on Python - 方法转换 🐍 属性方法 属性方法就是通过使用装饰器 @property , 将一个方法变成一个静态属性 , 于是我们就可以通过访问属性 , 来或得一个方法的返回值 from urllib.request import urlopen class Web_page: def __init__(self, url): self.url = url self.__content = None # 将content方法变成属性 @property def content(self): # 返回私有属性 return self.__content if self.__content else urlopen(self.url).read() con = Web_page('http://www.baidu.com') res = con.content print(res) 在property中为我们实现了三种方法 , get , set , delete class Foo: # 获取属性 @property def AAA(self): print(\"执行了get方法\") # 设定属性值 @AAA.setter def AAA(self, value): print(\"执行了set方法\") # 删除属性 @AAA.deleter def AAA(self): print(\"执行了delete方法\") # 实例化 f = Foo() # 获取属性 f.AAA # 设置属性值,必须设置参数,即使不使用 f.AAA = 'aaa' # 删除属性值 del f.AAA ''' 执行结果: 执行了get方法 执行了set方法 执行了delete方法 ''' 换一种写法看看 class Foo: def get_AAA(self): print('执行了get方法') def set_AAA(self,value): print('执行了set方法') def delete_AAA(self): print('执行了delete方法') # 实例化property类 AAA = property(get_AAA, set_AAA, delete_AAA) # 实例化 f = Foo() # 获取属性直接调用,执行了get_AAA f.AAA # 设置属性值,传入参数执行了set_AAA f.AAA = 'aaa' # 删除属性值,执行了delete_AAA del f.AAA ''' 执行结果: 执行了get方法 执行了set方法 执行了delete方法 ''' 实际应用 class Goods: def __init__(self): # 原价 self.original_price = 100 # 折扣 self.discount = 0.8 @property def price(self): # 实际价格 = 原价 * 折扣 new_price = self.original_price * self.discount return new_price @price.setter def price(self, value): self.original_price = value @price.deleter def price(self): del self.original_price goods = Goods() goods.price goods.price = 200 print(goods.price) del goods.price 类方法 类方法是通过@classmethod装饰器 , 将普通方法变成类方法 , 类方法只能与类属性交互 , 不能访问实例变量 , 并且默认有一个cls参数传进来表示本类 class Person: country = 'China' def __init__(self,name,age): self.name = name self.age = age @classmethod def search(cls): # 在类方法中不能使用实例变量,会抛出AttributeError print(\"I come from {}\".format(cls.country)) # print(\"{} come from {}\".format(self.name,cls.country)) 报错 p = Person('lyon','18') p.search() # 执行结果: I come from China PS:类方法中的默认参数可以改成self , 并不会改变结果 , 同样只能访问类变量 , 不能访问实例变量 静态方法 静态方法是通过@staticmethod装饰器将类中的方法变成一个静态方法 静态方法就像静态属性一样 , 在类中可以通过 self. 的方式进行调用 , 但是静态是不能够访问实例变量或类变量的 , 也就是说静态方法中的self已经跟本类没有关系了 , 它与本类唯一的关联就是需要通过类名来进行调用 class Person: country = 'China' def __init__(self,name,age): self.name = name self.age = age # 已经跟本类没有太大的关系了,所以类中的属性无法调用 @staticmethod def search(): print(\"我是静态方法\") p = Person('lyon','18') p.search() # 执行结果: 我是静态方法 加上self , self只为一个普通参数而已 class Person: country = 'China' def __init__(self,name,age): self.name = name self.age = age @staticmethod def search(self): print(\"{} come from {}\".format(self.name,self.country)) p = Person('lyon','18') # 将实例传入search方法中 p.search(p) # 执行结果: lyon come from China "},"01-Python/03-对象篇/06-魔术方法.html":{"url":"01-Python/03-对象篇/06-魔术方法.html","title":"魔术方法","keywords":"","body":"Attack on Python - 魔术方法 🐍 介绍 在 Python 中 , 我们自定义类都是基于 Object 对象实现的 , 而在 Object 对象中有一些特殊的操作符 (__method__) 控制着整个对象的行为 , 所以 , 如果我们想对对象的行为进行控制 , 我们就需要自己来实现这些方法 下面 , 看看这些方法吧 基本行为 操作符 控制行为 调用说明 __new__ 对象创建 __init__ 只是用处初始化 , __new__ 调用的结果会交给 __init__ 进一步处理 __init__ 对象初始化 构造函数 , 进行属性设置 __del__ 对象删除 析构函数 , 进行对象的销毁 __repr__ 对象显示 , 针对对象 终端显示 , 返回值必须为字符串 , 实例见表下方 __str__ 对象显示 , 针对 print print 显示结果 , 返回值必须为字符串 , 如果未实现该方法 , print 将使用 __repr__ __bytes__ 字节对象转换 返回值必须为一个bytes对象 , bytes(obj) __format__ 格式化字符串 返回值必须为字符串对象 , format(obj) __lt__ 运算 x , 返回布尔值 , 下同 __le__ 运算 x __eq__ = 运算 x == y __ne__ != 运算 x != y __gt__ > 运算 x > y __ge__ >= 运算 x >= y __hash__ 可哈希 返回一个哈希对象 , hash(obj) , 注意 : 定义该方法同时应该定义 __eq__ __bool__ 真假测试 返回布尔值 __call__ 对象调用 在对象被调用时执行 __len__ len() 使用 len(obj) 时被调用 , 为防止值测试抛出 OverflowError , 必须定义 __bool__() __repr__ 与 __str__ 对比实例 : # 类定义 class Foo: def __init__(self, name): self.name = name def __repr__(self): return '' % self.name def __str__(self): return '' % self.name # 终端结果 >>> obj = Foo('Lyon') >>> obj >>> print(obj) 访问行为 操作符 控制行为 调用说明 __getattr__ . 属性访问运算 获取 x.name , __getattribute__ 查询失败后被调用 , 下方实例进一步说明 __getattribute__ . 属性访问运算 获取 x.name , 查询属性时被调用 __setattr__ . 属性赋值运算 self.attr = value → self.__setattr__(\"attr\", value) , 见下方实例进一步说明 __delattr__ . 属性删除运算 del obj.name 时被调用 __dir__ dir 运算 dir() 调用时被调用 , 必须返回一个序列 , dir() 会将序列转换成 list 并排序 __getattr__ 说明实例 : # __getattr__ # 注意在定义__getattr__或者__getattribute__时,不要出现 self. 因为这样会导致递归调用 # 正确的方式是,使用object的__getattr__,__getattribute__,或者直接定义返回值 class Foo: def __init__(self, name): self.name = name def __getattr__(self, item): return 'Attribute fetch failure' % item def __getattribute__(self, item): # return object.__getattribute__(self, item) if item == \"name\": return 'Lyon' else: raise AttributeError(item) x = Foo('Lyon') print(x.name) print(x.age) \"\"\" 执行结果: Lyon Attribute fetch failure \"\"\" __setattr__ 说明实例 : # __setattr__ # 与__getattr__一样,在定义__setattr__时,不要出现 self. 因为这样会导致递归调用 # 正确的方式是,使用object的__setattr__,或者使用self.__dict__[key] class Foo: def __init__(self, name): self.name = name def __setattr__(self, key, value): # object.__setattr__(self, key, value) if key == \"name\": self.__dict__[key] = value else: raise AttributeError(key + ' not allowed') x = Foo('Lyon') x.name = \"Kenneth\" x.age = 18 print(x.__dict__) \"\"\" 执行结果: {'name': 'Kenneth'} Traceback (most recent call last): File \"test.py\", line 19, in x.age = 18 File \"test.py\", line 11, in __setattr__ raise AttributeError(key + ' not allowed') AttributeError: age not allowed \"\"\" 描述器行为 操作符 控制行为 调用说明 __get__ . 对象访问运算 访问对象时被调用 , 对象访问意指 . 后面接的不是一个属性而是一个对象 , 见下方实例说明 __set__ . 对象赋值运算 对象赋值时被调用 __delete__ . 对象删除运算 对象删除时被调用 __set_name__ 所有者创建 在创建所有者时被调用 , Python 3.6 新增 __get__ , __set__ , __delete__ 实例 # 关于对象访问一说,是建立在两个的使用基础上的 # 单纯来讲,就是所有者类中的一个属性,是另一个类的实例 class Dependency: \"\"\" 附属类 \"\"\" def __get__(self, instance, owner): print('%s.%s is called...' % ('Dependency', '__get__')) def __set__(self, instance, value): print('%s.%s is called...' % ('Dependency', '__set__')) def __delete__(self, instance): print('%s.%s is called...' % ('Dependency', '__delete__')) class Owner: \"\"\" 所有者类 \"\"\" dependency = Dependency() o = Owner() o.dependency o.dependency = 'Lyon' del o.dependency \"\"\" 执行结果: Dependency.__get__ is called... Dependency.__set__ is called... Dependency.__delete__ is called... \"\"\" __set_name__ 是在上例 Owner 实例创建时被调用 , Python 3.6 新增 容器行为 操作符 控制行为 调用说明 __getitem__ 序列方式访问 self[key] 时被调用 __missing__ 序列方式访问失败 self[key] 时 key 不在字典中被调用 __setitem__ 序列方式赋值 self[key] = value 时被调用 __delitem__ 序列方式删除 del self[key] 时被调用 __iter__ 迭代环境 通过 iter(obj) 调用 , 如使用for循环进行遍历 __reversed__ reversed() reversed(obj) 时被调用 __contains__ 成员关系 in item in self 时调用 运算行为 # 基本运算行为 object.__add__(self, other) # + object.__sub__(self, other) # - object.__mul__(self, other) # * object.__matmul__(self, other) # @ object.__truediv__(self, other) # / object.__floordiv__(self, other) # // object.__mod__(self, other) # % object.__divmod__(self, other) # divmod() object.__pow__(self, other[, modulo]) # pow() ** object.__lshift__(self, other) # > object.__and__(self, other) # & object.__xor__(self, other) # ^ object.__or__(self, other) # | # 二进制运算行为 object.__radd__(self, other) object.__rsub__(self, other) object.__rmul__(self, other) object.__rmatmul__(self, other) object.__rtruediv__(self, other) object.__rfloordiv__(self, other) object.__rmod__(self, other) object.__rdivmod__(self, other) object.__rpow__(self, other) object.__rlshift__(self, other) object.__rrshift__(self, other) object.__rand__(self, other) object.__rxor__(self, other) object.__ror__(self, other) # 加=运算行为 object.__iadd__(self, other) # += object.__isub__(self, other) # -= object.__imul__(self, other) # *= object.__imatmul__(self, other) object.__itruediv__(self, other) object.__ifloordiv__(self, other) object.__imod__(self, other) object.__ipow__(self, other[, modulo]) object.__ilshift__(self, other) object.__irshift__(self, other) object.__iand__(self, other) object.__ixor__(self, other) object.__ior__(self, other) # 一元算数运算 object.__neg__(self) object.__pos__(self) object.__abs__(self) object.__invert__(self) # complex(),int(),float() object.__complex__(self) object.__int__(self) object.__float__(self) # 整数值hex(X),bin(X),oct(X),o[X],O[X:] object.__index__(self) # round(),trunc(),floor(),ceil() object.__round__(self[, ndigits]) object.__trunc__(self) object.__floor__(self) object.__ceil__(self) 上下文管理行为 操作符 控制行为 调用说明 __enter__ 进入上下文环境 使用with进入上下文环境时被调用 __exit__ 退出上下文环境 退出上下文环境时被调用 __aenter__ 进入上下文环境 , 异步方法 使用with进入上下文环境时被调用 __aexit__ 退出上下文环境 , 异步方法 退出上下文环境时被调用 实例 class Foo: def __init__(self, name): self.name = name def __enter__(self): # 返回值赋值给as指定变量 return self def __exit__(self, exc_type, exc_val, exc_tb): print('exc_type',exc_type) # 异常类型 print('exc_val',exc_val) # 异常值 print('exc_tb',exc_tb) # 追溯信息 return True # 返回值为True,那么异常会被清空,就好像啥都没发生一样, # with后的语句正常执行 # 为False异常会抛出 with Foo('Lyon') as f: raise AttributeError('ignore exception') print('over') __slots__ __slots__ 的作用是阻止在实例化类时为实例分配dict , 默认情况下每个类都会有一个dict,通过__dict__ 访问 , 这个dict维护了这个实例的所有属性 作用 : 减少内存使用 限制对实例添加新的属性 缺点 : 不可被继承 不可动弹添加新属性 实例 class Foo: __slots__ = ['name', 'age'] def __init__(self, name, age): self.name = name self.age = age f = Foo('Lyon', 18) print(f.name) print(f.age) # 报错 f.sex = 'Man' 更多 Data model "},"01-Python/03-对象篇/07-反射.html":{"url":"01-Python/03-对象篇/07-反射.html","title":"反射","keywords":"","body":"Attack on Python - 反射 🐍 介绍 反射主要是指程序可以访问、检测和修改它本身状态或行为的一种能力 Python面向对象中的反射是通过字符串的形式来操作对象相关的属性 , 在Python中一切皆对象 , 并且只要是对象就可以使用反射 hasattr 判断对象中是否具有给定名称的属性 def hasattr(*args, **kwargs): # real signature unknown \"\"\" Return whether the object has an attribute with the given name. This is done by calling getattr(obj, name) and catching AttributeError. \"\"\" pass 实例1 # 定义一个字符串 name = 'lyon' # 查看是否具有给定名称的属性 bool = hasattr(name,'__len__') # 打印bool print(bool) # 执行结果:True ''' 说明:很多初学者可能一直不理解为什么说Python里一切皆对象,因为没有意识到,在Python中str、list、int ...等这些数据类型,其实就是用class写出来的一个模型,那么既然是类就会有属性这一说,就可以利用反射来操作对象了 ''' 实例2 import sys def s1(): pass def s2(): pass this_modules = sys.modules[__name__] print(type(this_modules),hasattr(this_modules,'s1')) # 执行结果: True getattr 从一个对象中获取属性名称 def getattr(object, name, default=None): # known special case of getattr \"\"\" Get a named attribute from an object; getattr(x, 'y') is equivalent to x.y. When a default argument is given, it is returned when the attribute doesn't exist; without it, an exception is raised in that case. \"\"\" pass 实例 class A: def __init__(self,name,age): self.name = name self.age = age def hello(self): print('hello {}'.format(self.name)) # 创建一个实例a a = A('Lyon',18) # 获取静态属性age age = getattr(a,'age') # 打印age print(age) # 获取动态属性hello,即方法 hello = getattr(a,'hello') # 执行hello hello() # 如果不存在就需要设置default参数,否则就报错 birthday = getattr(a,'birthday','today') # 打印birthday,即为default参数 print(birthday) ''' 执行结果: 18 hello Lyon today ''' setattr 定义属性 def setattr(x, y, v): # real signature unknown; restored from __doc__ \"\"\" Sets the named attribute on the given object to the specified value. setattr(x, 'y', v) is equivalent to ``x.y = v'' \"\"\" pass 实例 class B: def __init__(self): pass b = B() # 新增属性,如果存在即为修改 setattr(b, 'age', 18) # 打印age属性 print(b.age) # 新增add方法 setattr(b, 'add', lambda age: age + 1) # 修改age属性 b.age = b.add(b.age) # 打印age属性 print(b.age) ''' 执行结果: 18 19 ''' delattr 删除对象中的属性 def delattr(x, y): # real signature unknown; restored from __doc__ \"\"\" Deletes the named attribute from the given object. delattr(x, 'y') is equivalent to ``del x.y'' \"\"\" pass 实例 class C: def __init__(self,name,age): self.name = name self.age = age def add(self): self.age = self.age + 1 c = C('Lyon',18) # 删除c中的 delattr(c,'name') # print(c.name) 报错 delattr(c,'add') # c.add() 报错 "},"01-Python/03-对象篇/08-异常处理.html":{"url":"01-Python/03-对象篇/08-异常处理.html","title":"异常处理","keywords":"","body":"Attack on Python - 异常处理 🐍 介绍 在我们写程序时难免会出现错误 , 一种为语法错误 , 即为python解释器的语法检测都通不过的错误 , 这种错误只能我们在程序执行前就处理好 . 另一种为逻辑错误 , 这是我们在程序设计时所出现的错误 , 也就是我们通常所说的bug 在编程过程中为了增加友好性 , 在程序出现bug时一般不会将错误信息显示给用户 , 而是显示一个提示错误的页面 基本语法 try: pass except Exception as e: pass # except: 默认就为Exception 实例 try:0 # 用户输入 num = input(\"Please input the number:\") # 遇到无法int的对象就用except进行捕获 int(num) # 利用ValueError来捕获错误,并将捕获的错误返回给e except ValueError as e: # 打印捕获信息 print(e) ''' 执行结果: Please input the number:Lyon invalid literal for int() with base 10: 'Lyon' ''' PS : 在try代码块中只要出现异常 , 那么代码块中异常后面的代码就不会执行了 异常种类 Python中的异常种类非常多 , 上述中说了个ValueError只能处理值错误 , 当我们需要处理其他的错误时 , 就需要对症下药了 , 并且异常其实也是class , 并且所有的异常都继承了BaseException类 常用异常 异常名称 说明 ValueError 传入无效的参数 AttributeError 与对象的属性相关 IOError 输入/输出操作失败 , 基本上是无法打开文件 ImportError 无法引入模块或包 , 基本上是路径问题或名称错误 IndentationError 缩进错误 IndexError 下标索引超出范围 , 即索引不存在 KeyError 字典中不存在该key KeyboardInterrupt 用户中断执行 , 即被Ctrl + C NameError 变量还未声明/初始化 SyntaxError 语法错误 TypeError 传入对象类型与要求的不符合 UnboundLocalError 试图访问一个还未被设置的局部变量，基本上是由于另有一个同名的全局变量，导致你以为正在访问它 ValueError 传入无效的参数 继承关系与其他异常 # 所有异常都继承自BaseException类 BaseException +-- SystemExit +-- KeyboardInterrupt +-- GeneratorExit +-- Exception +-- StopIteration +-- StopAsyncIteration +-- ArithmeticError | +-- FloatingPointError | +-- OverflowError | +-- ZeroDivisionError +-- AssertionError +-- AttributeError +-- BufferError +-- EOFError +-- ImportError +-- ModuleNotFoundError +-- LookupError | +-- IndexError | +-- KeyError +-- MemoryError +-- NameError | +-- UnboundLocalError +-- OSError | +-- BlockingIOError | +-- ChildProcessError | +-- ConnectionError | | +-- BrokenPipeError | | +-- ConnectionAbortedError | | +-- ConnectionRefusedError | | +-- ConnectionResetError | +-- FileExistsError | +-- FileNotFoundError | +-- InterruptedError | +-- IsADirectoryError | +-- NotADirectoryError | +-- PermissionError | +-- ProcessLookupError | +-- TimeoutError +-- ReferenceError +-- RuntimeError | +-- NotImplementedError | +-- RecursionError +-- SyntaxError | +-- IndentationError | +-- TabError +-- SystemError +-- TypeError +-- ValueError | +-- UnicodeError | +-- UnicodeDecodeError | +-- UnicodeEncodeError | +-- UnicodeTranslateError +-- Warning +-- DeprecationWarning +-- PendingDeprecationWarning +-- RuntimeWarning +-- SyntaxWarning +-- UserWarning +-- FutureWarning +-- ImportWarning +-- UnicodeWarning +-- BytesWarning +-- ResourceWarning 为什么要说继承关系 , 因为在使用except是 , 它不但捕获该异常 , 还会把该异常类的子类也全部捕获 所以我们把 Exception 也叫做万能异常 , 因为除了SystemExit , KeyboardInterrupt 和 GeneratorExit 三个异常之外 , 其余所有异常基本都为Exception的子类 异常其他结构 多分支 name = 'Lyon' try: int(name) except IndexError as e: print(e) except KeyError as e: print(e) # ValueError捕获成功 except ValueError as e: print(e) # 执行结果:invalid literal for int() with base 10: 'Lyon' else num = '1' try: int(num) except ValueError as e: print(e) # 与for..else 和 while...else类似,没被打断就执行 else: print('没有异常就执行我') # 执行结果: 没有异常就执行我 finally num = 'Lyon' try: int(num) except ValueError as e: print(e) else: print('没有异常就执行我') finally: print('不管怎么样都执行我') ''' 执行结果: invalid literal for int() with base 10: 'Lyon' 不管怎么样都执行我 ''' 主动触发异常 raise try: raise TypeError('类型错误') except Exception as e: print(e) # 执行结果: 类型错误 自定义异常 通过继承BaseException来实现 class LyonException(BaseException): def __init__(self,msg): self.msg = msg def __str__(self): return self.msg try: # 主动触发异常 raise LyonException('你就是错了,别问为什么') # 捕获LyonException except LyonException as e: print(e) # 执行结果: 你就是错了,别问为什么 断言 断定条件成立 , 不成立就出现AssertionError异常 try: # 断定1等于1 assert 1 == 1 print('第一个断言成功就执行') assert 2 == 1 print(\"第二个断言失败不执行\") # 捕获AssertionError异常 except Exception: print(\"抓到你了\") ''' 执行结果: 第一个断言成功就执行 抓到你了 ''' "},"01-Python/04-模块篇/":{"url":"01-Python/04-模块篇/","title":"模块篇","keywords":"","body":"Attack on Python - 模块篇 🐍 "},"01-Python/04-模块篇/01-模块.html":{"url":"01-Python/04-模块篇/01-模块.html","title":"模块","keywords":"","body":"Attack on Python - 模块 🐍 import 我们知道一个模块就是一个py文件 , 当我们执行py文件时 , python解释器会先加载内置命名空间 , 其次是加载全局命名空间( 学习函数就已知道 ) , 还有个局部命名空间就不说了 当python解释器遇到我们的import语句时 , import会将模块进行初始化 , 即会将模块中的内容执行一遍 , 既然执行 , 那么被import的模块的全局命名空间就创建成功了 , 并且会将这个创建成功的命名空间加载到使用import语句的本地的全局命名空间 . 于是我们就可以在本地使用被导入模块了 自定义模块my_module.py , 文件名my_module.py , 模块名my_module 在模块my_module.py下 ---------------文件内容---------------- | print('from the my_module.py') | | def read(): | | print('in the module.py read') | -------------------------------------- 在当前文件test.py下 ---------------文件内容---------------- | import my_module | | my_module.read() | -------------------------------------- # 执行test.py文件,打印结果 ''' # 执行了my_module.py的print语句 from the my_module.py # 成功调用my_module.py中的read函数 in the module.py read ''' import语句是可以在程序中的任意位置使用的 , 且针对同一个模块import多次时 , 为了防止你重复导入 , python进行了如下优化 : 第一次导入后就将模块名加载到内存了 , 后续的import语句仅是对已经加载大内存中的模块对象增加一次引用 , 不会重新执行模块内的语句 import多次同以模块 在模块my_module.py下 ---------------文件内容---------------- | print('from the my_module.py') | | def read(): | | print('in the module.py read') | -------------------------------------- 在当前test.py文件下 ---------------文件内容---------------- | import my_module | | import my_module | | import my_module | | my_module.read() | -------------------------------------- # 执行test.py文件,打印结果 ''' # 仅执行了一次my_module.py中的print语句 from the my_module.py # 成功调用my_module.py中的read函数 in the module.py read ''' 我们可以从sys.modules中找到当前已经加载的模块 , sys.modules是一个字典 , 内部包含模块名与模块对象的映射 ,该字典决定了导入模块时是否需要重新导入 每个模块的命名空间都是相互独立的 , 这样我们在编写自己的模块时 , 就不用担心我们定义在自己模块中全局变量在被导入时 , 与使用者的同名全局变量冲突 ps:模块中的内容使用 :模块名 .函数或者变量或者类来进行调用 总结 首次导入模块时python会做三件事 为源文件(如my_module模块) 创建新的命名空间 , 在my_module中定义的函数和方法若是使用到了globals() 时访问的就是这个命名空间 在新创建的命名空间执行模块中包含的代码 , 如上例中执行了模块中的print语句 , 并加载了函数 创建名字my_module 来引用该命名空间 , 使用my_module.名字的方式访问my_module.py文件中定义的名字 , 且名字与test.py文件中的名字来自两个完全不同的地方 import ... as ... 为模块取名 根据用户需求选择额不同的sql(数据库)功能 # 在mysql.py中 def sqlparse(): print('from mysql sqlparse') # 在oracle.py中 def sqlparse(): print('from oracle sqlparse') # 在test.py中 db_type = input('Please choice the database >>').strip() if db_type == 'mysql': import mysql as db elif db_type == 'oracle': import oracle as db 一行导入多个模块 import sys,os,re from ... import ... 相当于import , 同样会执行一遍my_module文件 , 同样也会创建命名空间 , 但是from .. . import ... 是将my_module中的名字直接导入到当前的命名空间 , 也就意味着可以直接调用 , 而不用像import那样 , 利用 my_module . 名字 来进行调用 两种方式对比 # import方式 import my_module # 模块名 + '.' + 函数名进行调用 my_module.read() # from...import...方式 from my_module import read # 直接用函数名调用 read() PS : 利用from...import...方式进行导入 , 一般用来指定导入模块中的某一部分 , 或者方便使用 , 还有一个特殊的导入 from ... import * (作用是导入模块中的所有内容 , 但是有弊端) as from my_module import read as r 多行 from my_module import (read1, read2, read3) from ... import * from mymodule import * 会将my_module 中的所有的不是以下划线 ' ' 开头的名字都导入到当前位置 , 在大部分情况下我们python程序不应该使用这种导入方式 , 因为你无法知道 * 导入了什么名字 , 很有可能会覆盖掉你已经定义过的名字 , 而且可读性极其的差 在my_module.py中新增一行 # 这样在另外一个文件中用from my_module import * 就能导入列表中规定的两个名字 __all__ = ['money' , 'read1'] if __name__ == '__main__' 所有的模块都有一个内置属性 __name__ , 可以用来查看模块名 在当前文件执行时会返回' _main_ ', 如果不在当前文件执行那么就会返回所执行的模块名 # my_module.py中 print(__name__) # 执行my_module.py 执行结果: __main__ # test.py中 import my_modlue # 执行 test.py 执行结果: my_module 所以利用_name_ 属性 , 我们就可以实现 , 模块可以自己执行 , 也可以导入到别的模块中执行 , 并且他不会执行 两次 # my_module.py中 def main(): print('we are in %s' % __name__) # 如果在当前文件下就会执行 if __name__ == '__main__': main() # test.py中 , 执行test.py # 解释from语句时 , 并不会执行my_module中的main() from my_module import main # 执行main() main() 执行结果：we are in my_module # 结果显示只执行了一次main() "},"01-Python/04-模块篇/02-包.html":{"url":"01-Python/04-模块篇/02-包.html","title":"包","keywords":"","body":"Attack on Python - 包 🐍 介绍 为了帮助组织模块并提供命名层次结构 , Python有一个概念 : 包 包就相当于一个文件系统的目录 , 模块相当于目录中的文件 , 也就是说所有的包都时模块 , 但不是所有的模块都是包 包只是一种特殊的模块 , 具体来说 , 包含__path__ 属性的任何模块都被视为包 所有模块都有一个名称 , 子包名与他们的父包名由点隔开 , 类似于Python的标准属性访问语法 Python定义了两种类型的包 , 即 regular packages 和 namespace packages , 我们通常使用的就是regular packages , 对于namespace packages可通过上述链接进行学习 常规包 常规包时传统的包 , 因为它们存在于Python 3.2 及更早的版本中 ; 常规包通常实现为包含__init__.py 文件的目录 当我们导入常规包时 , 这个__init__.py文件会被隐式执行 (这意味着我们应该在__init__.py 文件中完成我们的导入 , 即初始化包) , 它定义的对象被绑定到包命名空间中 ; Python会在导入时为模块添加一些其他属性 , 如下 : parent/ __init__.py one/ __init__.py two/ __init__.py three/ __init__.py ''' 导入parent.one将隐式执行parent/__init__.py和parent/one/__init__.py 随后导入parent.two或parent.three将执行parent/two/__init__.py和parent/three/__init__.py ''' 在我们使用import导入文件时 , 产生命名空间的名字来源于文件 , import packages产生的命名空间的名字同样来源于文件 , 即包下的__init__.py , 导入包本质就是在导入该文件 注意 : 在Python 3中 , 即使包下没有__init__.py文件 , import packages仍然不会报错 , 而在Python 2中 , 包下一定要有该文件 , 否则import packages就会抛出异常 导入包 glance包 glance/ ├── __init__.py ├── api │ ├── __init__.py __all__ = ['policy','versions'] │ ├── policy.py │ └── versions.py ├── cmd __all__ = ['manage'] │ ├── __init__.py │ └── manage.py └── db __all__ = ['models'] ├── __init__.py └── models.py import import glance.db.models glance.db.models.register_models('mysql') from ... import ... # import后接的必须是明确的模块或者方法或者类或者变量,否则会抛出异常 from glance.db import models models.register_models('mysql') from glance.db.models import register_models register_models('mysql') 绝对导入与相对导入 我们的glance包时写给别人用的 , 但是在glance包内部也会有彼此之间互相导入的需求 , 那么就有了绝对导入和相对导入两种方式 : 绝对导入 : 以glance作为起始 相对导入 : 用.或者.. 的方式最为起始 , 只能在一个包中使用 , 即包内目录 我们在glance/api/version.py中导入glance/cmd/manage.py glance/api/version.py 下 # 绝对导入 from glance.cmd import manage manage.main() # 相对导入,一个点表示当前目录,两个点表示上一层 from ..cmd import manage manage.main() 绝对导入 glance/ ├── __init__.py from glance import api from glance import cmd from glance import db ├── api │ ├── __init__.py from glance.api import policy from glance.api import versions │ ├── policy.py │ └── versions.py ├── cmd from glance.cmd import manage │ ├── __init__.py │ └── manage.py └── db from glance.db import models ├── __init__.py └── models.py 相对导入 glance/ ├── __init__.py from . import api #.表示当前目录 from . import cmd from . import db ├── api │ ├── __init__.py from . import policy from . import versions │ ├── policy.py │ └── versions.py ├── cmd from . import manage │ ├── __init__.py │ └── manage.py from ..api import policy #..表示上一级目录，想再manage中使用policy中的方法就需要回到上一级glance目录往下找api包，从api导入policy └── db from . import models ├── __init__.py └── models.py 单独导入 单独导入包时不会导入包中所有包含的所有子模块 , 如 : import glance glance.cmd.manage.main() ''' 执行结果: AttributeError: module 'glance' has no attribute 'cmd' ''' 上述导入会隐式执行__init__.py , 所以我们可以让这个文件来初始化 , 如下 : # glance/__init__.py from . import cmd # glance/cmd/__init__.py from . import manage 关于导入系统 : https://docs.python.org/3/reference/import.html "},"01-Python/04-模块篇/03-正则表达式.html":{"url":"01-Python/04-模块篇/03-正则表达式.html","title":"正则表达式","keywords":"","body":"Attack on Python - 正则表达式 🐍 介绍 正则表达式并不是python的一部分，而是在各个编程语言都有的一种用于处理字符串的强大工具。 使用正则处理字符串在效率上可能不如str自带的方法，但是它的功能十分强大。python中的正则封装在re模块中。 匹配方法 首先将匹配方法进行说明，即re模块的内置方法 re.match(pattern, string, flags=0) : 👈 从字符串的开头开始匹配，匹配成功返回一个_sre.SRE_Match类型，可用.group() 取出结果，失败返回None pattern : 匹配格式 string : 要匹配的字符串 flags : 编译标志位，用于修改正则表达式的匹配方式 # 导入re模块，后续方法实例省略这一步 >>> import re >>> res = re.match('lyon','lyon') # 查看类型 >>> type(res) # 用.group()取出结果 >>> res.group() 'lyon' re.search(pattern, string, flags=0) : 👈 扫描整个字符串，匹配成功则返回匹配到的第一个对象（_sre.SRE_Match类型），失败返回None pattern : 匹配格式 string : 要匹配的字符串 flags : 编译标志位，用于修改正则表达式的匹配方式 # 匹配数字 >>> re.search('\\d+','abc123abc').group() '123' re.findall(pattern, string, flags=0) : 👈 匹配字符串所有的内容，把匹配到的字符串以列表的形式返回 pattern : 匹配格式 string : 要匹配的字符串 flags : 编译标志位，用于修改正则表达式的匹配方式 # 匹配数字 >>> re.findall('\\d','abc123abc456') ['1','2','3','4','5','6'] re.split(pattern, string, maxsplit=0, flags=0) : 👈 指定格式进行切分，返回一个列表 pattern : 切分格式 string : 要切分的字符串 maxsplit : 切分次数 flags : 编译标志位，用于修改正则表达式的匹配方式 # 以数字进行切分 >>> re.split('\\d+','abc123abc123+-*/45') ['abc', 'abc', '+-*/', ''] re.sub(pattern, repl, string, count=0, flags=0) : 👈 替换匹配到的字符串并返回替换后的结果 pattern : 匹配格式 repl : 替换格式 string : 要匹配替换的字符串 flags : 编译标志位，用于修改正则表达式的匹配方式 >>> re.sub(\"abc\",\"def\",\"abc123abc\") 'def123def' # 只替换查找到的字符串一次 >>> re.sub(\"abc\",\"def\",\"abc123abc\",count=1) 'def123abc' flags说明（轻轻了解） : 标志 说明 re.I (re.IGNORECASE) 忽略大小写（括号内为全拼写法，效果一样） re.M (MULTILINE) 多行模式，改变 '^' 和 '$' 的行为 （改变？见下节匹配模式） re.S (DOTALL) 任意匹配模式，改变 ' . ' 的行为（同上） re.L (LOCALE) 做本地化识别（locale-aware）匹配，法语等 re.X (VERBOSE) 该标志通过给予更灵活的格式以便将正则表达式写得更易于理解 re.U 根据Unicode字符集解析字符，这个标志影响\\w,\\W,\\b,\\B # 忽略大小写 >>> re.findall('a','aA123aAAA',flags=re.I) ['a', 'A', 'a', 'A', 'A', 'A'] 注意转义的问题：当我们的匹配格式中有我们需要匹配的特殊字符，如 ' \\ '、' '、' + '等，为了让解释器知道我们这是需要匹配的，我们可以在格式前加 'r' 进行转义，或者在每个需要匹配的之前加个 ' \\ '来完成转义。* .group()小知识： 在我们使用.group()方法时，要注意如果我们的正则表达式没有匹配到结果，即返回None时，用.group()时就会报错，因为\"NoneType\"是没有该方法的，只有_sre.SRE_Match类型才能使用该方法。 匹配模式 字符匹配 字符 描述 . 默认匹配除\\n之外的任意一个字符，若指定flag DOTALL,则匹配任意字符，包括换行 \\d \\D 匹配数字0-9/非数字 \\s 匹配空白字符、\\t、\\n、\\r , re.search(\"\\s+\",\"ab\\tc1\\n3\").group() 结果 '\\t' \\S 非空白字符 \\w 匹配[A-Za-z0-9] \\W 匹配非[A-Za-z0-9] \\b 匹配一个单词边界，也就是指单词和空格间的位置。例如， 'er\\b' 可以匹配\"never\" 中的 'er'，但不能匹配 \"verb\" 中的 'er'。 \\B 匹配非单词边界。'er\\B' 能匹配 \"verb\" 中的 'er'，但不能匹配 \"never\" 中的 'er'。 次数匹配 字符 描述 * 匹配*号前的字符0次或多次，re.findall(\"ab*\",\"cabb3abcbbac\") 结果为['abb', 'ab', 'a'] + 匹配前一个字符1次或多次，re.findall(\"ab+\",\"ab+cd+abb+bba\") 结果['ab', 'abb'] ? 匹配前一个字符0次或者1次 {m} 匹配前一个字符m次 {n,m} 匹配前一个字符n到m次，re.findall(\"ab{1,3}\",\"abb abc abbcbbb\") 结果'abb', 'ab', 'abb'] *?/+?/?? 转为非贪婪模式（尽可能少的匹配） [...] 字符集，匹配字符集中任意字符，字符集可给出范围或者逐个列出 边界匹配 字符 描述 ^ 匹配字符串开头，若指定flags MULTILINE，这种也可以匹配上，(r'^a','\\nabc\\neee',flags=re.MULTILINE) $ 匹配字符结尾，或e.search(\"foo$\",\"bfoo\\nsdfsf\",flags=re.MULTILINE).group()也可以 \\A 只从字符开头匹配，re.search(\"\\Aabc\",\"alexabc\") 是匹配不到的 \\Z 匹配字符结尾，同$ 分组匹配 字符 描述 丨 匹配丨左或丨右的字符，re.search(\"abc丨ABC\",\"ABCBabcCD\").group() 结果'ABC' (...) 分组匹配，re.search(\"(abc){2}a(123丨456)c\", \"abcabca456c\").group() 结果 abcabca456c (?P\\) 命名分组匹配 re.search(\"(?P\\[0-9]{4})(?P\\[0-9]{2})(?P\\[0-9]{4})\",\"371481199306143242\").groupdict(\"city\") 结果{'province': '3714', 'city': '81', 'birthday': '1993'} 补充 补充方法 re.subn(pattern, repl, string, count=0, flags=0) : 返回替换后的字符串和替换次数 re.escape(pattern) : 自动进行转义，除了ASCII字母、数字和'_'之外 re.compile(pattern, flags=0) : 生成一个_sre.SRE_Pattern对象，以便多次调用 re.finditer(pattern, string, flags=0) : 返回一个匹配结果的迭代器，可迭代取值 re.fullmatch(pattern, string, flags=0) : 完整匹配，不完整则返回None re.template(pattern, flags=0) : 没人知道是干嘛的，跟compile差不多 re.purge() : 清除正则表达式缓存 ''' 当你在程序中使用 re 模块，无论是先使用 compile 还是直接使用比如 findall 来使用正则表达式操作文本，re 模块都会将正则表达式先编译一下， 并且会将编译过后的正则表达式放到缓存中，这样下次使用同样的正则表达式的时候就不需要再次编译， 因为编译其实是很费时的，这样可以提升效率，而默认缓存的正则表达式的个数是 100, 当你需要频繁使用少量正则表达式的时候，缓存可以提升效率，而使用的正则表达式过多时，缓存带来的优势就不明显了 ''' 实例 连续匹配 # 导入模块 >>> import re # 获取字符串 >>> source ='192.168.0.1 25/Oct/2012:14:46:34 \"GET /api HTTP/1.1\" 200 44 \"http://abc.com/search\" \"Mozilla/5.0\"' # 设置匹配格式 >>> res = re.match('^(?P[^ ]*) (?P[^ ]*) \"(?P[^\"]*)\" (?P[^ ]*) (?P[^ ]*) \"(?P[^\"]*)\" \"(?P[^\"]*)\"',source) # 返回一个字典，groupdict中的key为组名，value为值 >>> source_dic = res.groupdict() # for循环打印 >>> for k in source_dic: #打印key和vaule ... print(k+\": \"+source_dic[k]) ... # 打印结果 date: 25/Oct/2012:14:46:34 remote_ip: 192.168.0.1 referrer: http://abc.com/search status: 200 user_agent: Mozilla/5.0 size: 44 request: GET /api HTTP/1.1 "},"01-Python/04-模块篇/04-序列化.html":{"url":"01-Python/04-模块篇/04-序列化.html","title":"序列化","keywords":"","body":"Attack on Python - 序列化 🐍 介绍 先说个例子 , 当我们将一个字典或者列表再或者变量存入磁盘中 , 而存入磁盘后原本数据类型就得不到保持了 . 这个时候我们就得用序列化和反序列化了 序列化是将对象进行存储时保持当时对象的状态 , 实现其生命周期的延长 反序列化则是将存储的对象读取出来并转成原本的数据类型 序列化的目的 以某种存储形式使自定义对象持久化 将对象从一个地方传递到另一个地方 使程序更具维护性 此时应该想到 eval :那么问题来了 , 序列化所达到的功能我用eval()也能达到啊 , eval()直接就可以把字符串转换成python解释器能解释的代码 , 即可以直接将字符串中的字典 , 列表都转成原来的数据类型 . 但是要注意的是 , eval本来就是将字符串内容转换成python可以执行的代码 , 并执行它 , 这样看来eval就不安全了 , 因为如果在我能读取的内容中含有一些其他的 ' 危险代码 ' 如 ' 删除文件 ' , 于是造成了毁灭性的打击 , 所以eval是存在风险的 Python为我们提供了三个序列化工具 , 分别是 json , pickle , shelve json 用于字符串和python数据类型之间进行转换 , 因为json表示出来就是一个字符串 json模块提供了四个方法 方法 描述 dump 接收一个文件句柄 , 将原数据类型转换成字符串写入文件 load 接收一个文件句柄 , 将文件中的字符串转换成原数据类型返回 dumps 接收一个数据类型 , 将其转换成字符串 loads 接收一个字符串 , 将其转换成原数据类型 dump 和 load 实例 # 导入json模块 import json # 创建一个文件句柄 f = open('json_file','w') # 创建一个字典 dic = {'k1':'v1','k2':'v2'} # 将字典转换成字符串写入文件 json.dump(dic,f) # 关闭文件 f.close() # 创建一个文件句柄 f = open('json_file') # 将文件中的字符串读出并转换成原数据类型 dic2 = json.load(f) # 关闭文件句柄 f.close() # 打印类型和结果 print(type(dic2),dic2) # {'k1': 'v1', 'k2': 'v2'} dumps 和 loads 实例 # 导入json模块 import json # 创建一个新列表 lst = ['1','2','3','4'] # 将列表转换成字符串,用j_d来接收返回值 j_d = json.dumps(lst) # 将字符串转换成原数据类型,用j_s来接收返回值 j_s = json.loads(j_d) # 打印j_d的值以及类型 print(j_d,type(j_d)) # [\"1\", \"2\", \"3\", \"4\"] # 打印j_s的值以及类型 print(j_s,type(j_s)) # ['1', '2', '3', '4'] loads的特殊情况 # 导入json模块 import json # 创建一个字符串,内部为一个字典 dic_s = \"{'k1':'v1','k2':'v2','k3':3}\" # 将字符串转换成字典 json.loads(dic_s) # 解释器出现报错 # json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1) ''' 报错原因,用json的loads功能时,字符串类型的字典中的字符串必须由 \"\" 表示 即上面的dic_s应该改为 '{\"k1\":\"v1\",\"k2\":\"v2\",\"k3\":3}' 结论:用json的loads功能时,字符串类型的字典中的字符串必须由 \"\" 表示 ''' PS : json可用于不同语言之间的数据交换 pickle 用于python特有的类型和python的数据类型间进行转换 pickle模块也提供了四个方法 , 与json一样 dumps , dump , loads , load 由于pickle是对于python特有的类型 , 所以 load 和 loads方法不仅支持字典 , 列表 , 它还能把python中任意的数据类型进行序列化 -------dumps和loads-------- # 导入pickle模块 import pickle # 创建一个字典 dic = {'k1':'v1','k2':'v2'} # 将字典转换成二进制内容 p_d = pickle.dumps(dic) # 将二进制内容转换成字典 p_l = pickle.loads(p_d) # 打印p_d print(p_d) # b'\\x80\\x03}q\\x00(X\\x02\\x00\\x00\\x00k2q\\x01X\\x02\\x00\\x00\\x00v2q\\x02X\\x02\\x00\\x00\\x00k1q\\x03X\\x02\\x00\\x00\\x00v1q\\x04u.' # 打印p_d的类型 print(type(p_d)) # # 打印p_l print(p_l) # {'k2': 'v2', 'k1': 'v1'} # 打印p_l的类型 print(type(p_l)) # ---------dump 和 load--------- # 创建一个文件句柄 f = open('pickle_file','wb') # 写入内容 pickle.dump('lyon',f) # 关闭文件 f.close() # 创建一个文件句柄 f = open('pickle_file','rb') # 读出内容 p_f = pickle.load(f) # 关闭文件 f.close() # 打印 print(p_f) # lyon 但是pickle仅仅只能对python中的数据进行序列化 , 反序列化时其他语言就无法读懂了这是什么了 , 所以我们一般用推荐使用json shelve shelve也是python提供给我们的序列化工具 , 比pickle用起来简单一些 shelve只提供给我们一个open方法 , 是用key来访问的 , 使用起来和字典类似 # 导入shelve模块 import shelve # shelve提供open方法 f = shelve.open('shelve_file') # 直接对文件句柄进行操作,就可以写入文件中 f['key'] = {'int':10, 'float':9.5, 'string':'Sample data'} # 关闭文件 f.close() # 打开文件 f1 = shelve.open('shelve_file') # 直接用key取值,key不存在就报错 existing = f1['key'] # 关闭文件 f1.close() # 打印结果 print(existing) # {'float': 9.5, 'int': 10, 'string': 'Sample data'} shelve不支持多个应用同时往一个数据库进行操作 , 所以当我们知道我们的应用如果只进行操作 , 我们可以设置shelve.open() 方法的参数来进行 shelve.open(filename, flag='c', protocol=None, writeback=False) import shelve # flag参数为设置操作模式,r 设置只读模式 f = shelve.open('shelve_file', flag='r') existing = f['key'] f.close() print(existing) writeback参数 , 可以减少我们出错的概率 , 并且让对象的持久化对用户更加的透明了 ; 但这种方式并不是所有的情况下都需要 , 首先 , 使用writeback以后 , shelf在open()的时候会增加额外的内存消耗 , 并且当数据库在close()的时候会将缓存中的每一个对象都写入到数据库 , 这也会带来额外的等待时间 , 因为shelve没有办法知道缓存中哪些对象修改了 , 哪些对象没有修改 , 因此所有的对象都会被写入 import shelve f1 = shelve.open('shelve_file') print(f1['key']) f1['key']['new_value'] = 'this was not here before' f1.close() # 设置writeback f2 = shelve.open('shelve_file', writeback=True) print(f2['key']) f2['key']['new_value'] = 'this was not here before' f2.close() "},"01-Python/04-模块篇/05-os模块.html":{"url":"01-Python/04-模块篇/05-os模块.html","title":"os模块","keywords":"","body":"Attack on Python - OS模块 🐍 介绍 os模块为我们提供了与操作系统相关的诸多接口 在Python中 , 使用字符串类型来表示文件名 , 命令行参数和环境变量 os模块功能总体分为以下几个部分 : 当前进程和用户操作 文件描述符操作 文件和目录操作 进程管理 调度程序接口 (仅在一些Unix平台上) 系统信息处理 总体概况 DESCRIPTION This exports: - all functions from posix, nt or ce, e.g. unlink, stat, etc. - os.path is either posixpath or ntpath - os.name is either 'posix', 'nt' or 'ce'. - os.curdir is a string representing the current directory ('.' or ':') - os.pardir is a string representing the parent directory ('..' or '::') - os.sep is the (or a most common) pathname separator ('/' or ':' or '\\\\') - os.extsep is the extension separator (always '.') - os.altsep is the alternate pathname separator (None or '/') - os.pathsep is the component separator used in $PATH etc - os.linesep is the line separator in text files ('\\r' or '\\n' or '\\r\\n') - os.defpath is the default search path for executables - os.devnull is the file path of the null device ('/dev/null', etc.) 注意 : 在os模块中有很多方法只有在Unix系统上才能使用 由于os模块提供的方法太多 , 所以本文仅介绍一些在windows下常用的方法 OS os.getcwd() \"\"\" Return a string representing the current working directory. \"\"\" os.chdir(path) \"\"\" Change the current working directory to path. \"\"\" os.curdir \"\"\" The constant string used by the operating system to refer to the current directory. This is '.' for Windows and POSIX. Also available via os.path. \"\"\" os.pardir \"\"\" The constant string used by the operating system to refer to the parent directory. This is '..' for Windows and POSIX. Also available via os.path. \"\"\" os.makedirs(name, mode=0o777, exist_ok=False) \"\"\" Recursive directory creation function. Like mkdir(), but makes all intermediate-level directories needed to contain the leaf directory. \"\"\" os.removedirs(name) \"\"\" Remove directories recursively. Works like rmdir() except that, if the leaf directory is successfully removed, removedirs() tries to successively remove every parent directory mentioned in path until an error is raised \"\"\" os.rmdir(path, *, dir_fd=None) \"\"\" Remove (delete) the directory path. Only works when the directory is empty, otherwise, OSError is raised. In order to remove whole directory trees, shutil.rmtree() can be used. \"\"\" os.listdir(path='.') \"\"\" Return a list containing the names of the entries in the directory given by path. The list is in arbitrary order, and does not include the special entries '.' and '..' even if they are present in the directory. \"\"\" os.remove(path, *, dir_fd=None) \"\"\" Remove (delete) the file path. If path is a directory, OSError is raised. Use rmdir() to remove directories. \"\"\" os.rename(src, dst, *, src_dir_fd=None, dst_dir_fd=None) \"\"\" Rename the file or directory src to dst. \"\"\" os.stat(path, *, dir_fd=None, follow_symlinks=True) \"\"\" Get the status of a file or a file descriptor. \"\"\" os.sep \"\"\" The character used by the operating system to separate pathname components. This is '/' for POSIX and '\\\\' for Windows. \"\"\" os.linesep \"\"\" The string used to separate (or, rather, terminate) lines on the current platform. This may be a single character, such as '\\n' for POSIX, or multiple characters, for example, '\\r\\n' for Windows. \"\"\" os.pathsep \"\"\" The character conventionally used by the operating system to separate search path components (as in PATH), such as ':' for POSIX or ';' for Windows. Also available via os.path. \"\"\" os.name \"\"\" The name of the operating system dependent module imported. The following names have currently been registered: 'posix', 'nt', 'java'. \"\"\" os.system(command) \"\"\" Execute the command (a string) in a subshell. \"\"\" os.popen(cmd, mode='r', buffering=-1) \"\"\" Open a pipe to or from command cmd. The return value is an open file object connected to the pipe, which can be read or written depending on whether mode is 'r' (default) or 'w'. \"\"\" os.environ \"\"\" A mapping object representing the string environment. \"\"\" 更多os模块相关 : os — Miscellaneous operating system interfaces OS.Path os.path.abspath(path) \"\"\" Return a normalized absolutized version of the pathname path. On most platforms, this is equivalent to calling the function normpath() as follows: normpath(join(os.getcwd(), path)). \"\"\" os.path.exists(path) \"\"\" Return True if path refers to an existing path or an open file descriptor. Returns False for broken symbolic links. \"\"\" os.path.isabs(path) \"\"\" Return True if path is an absolute pathname. \"\"\" os.path.isfile(path) \"\"\" Return True if path is an existing regular file. \"\"\" os.path.isdir(path) \"\"\" Return True if path is an existing directory. \"\"\" os.path.join(path, *paths) \"\"\" Join one or more path components intelligently. \"\"\" os.path.getatime(path) \"\"\" Return the time of last access of path. \"\"\" os.path.getmtime(path) \"\"\" Return the time of last modification of path. \"\"\" os.path.getsize(path) \"\"\" Return the size, in bytes, of path. Raise OSError if the file does not exist or is inaccessible. \"\"\" 更多os.path相关 : os.path — Common pathname manipulations 补充 : 如果需要读取命令行上所有文件中的所有行 , 可以查看fileinput 模块 如果需要创建临时文件和目录 , 可以查看tempfile 模块 关于文件和文件集合的高级操作 , 可以查看shutil 模块 "},"01-Python/04-模块篇/06-random模块.html":{"url":"01-Python/04-模块篇/06-random模块.html","title":"random模块","keywords":"","body":"Python - 标准库之random 介绍 🍀 random模块为我们提供了各种分布的伪随机数生成器 random模块功能分为以下几个部分 : Bookkeeping functions Functions for integers Functions for sequences Real-valued distributions Bookkeeping functions 🍀 random.seed(a=None, version=2): \"\"\" Initialize the random number generator. \"\"\" random.getstate(): \"\"\" Return an object capturing the current internal state of the generator. This object can be passed to setstate() to restore the state. \"\"\" random.setstate(state): \"\"\" State should hava been obtained from a previous call to getstate(), and setstate() restores the internal state of the generator to what it was at the time getstate() was called. \"\"\" random.getrandbits(k): \"\"\" Returns a Python integer with k random bits. This method is supplied with the Mersenne Twister generator and some other generators may also provide it as an optional part of the API. When available, getrandbits() enables randrange() to handle arbitrarily large ranges. \"\"\" Functions for integers 🍀 random.randrange(stop) random.randrange(start, stop[, step]): \"\"\" Return a randomly selected element from range(start, stop, step). This is equivalent to choice(range(start, stop, step)), but doesn't actually build a range object. \"\"\" random.randint(a, b): \"\"\" Return a random integer N such that a Functions for sequences 🍀 random.choice(seq): \"\"\" Return a random element from the non-empty sequence seq. If seq is empty, raises IndexError. \"\"\" random.choices(population, weights=None, *, cum_weights=None, k=1): \"\"\" Return a k sized list of elements chosen from the population with replacement. If the population is empty, raises IndexError. \"\"\" random.shuffle(x[, random]): \"\"\" Shuffle the sequence x in place. \"\"\" random.sample(population, k): \"\"\" Return a k length list of unique elements chosen from the population sequence or set. Used for random sampling without replacement. \"\"\" Real-valued distributions 🍀 random.random(): \"\"\" Return the next random floating point number in the range [0.0, 1.0). \"\"\" random.uniform(a, b): \"\"\" Return a random floating point number N such that a 0 and beta > 0. Returned values range between 0 and 1. \"\"\" random.expovariate(lambd): \"\"\" Exponential distribution. lambd is 1.0 divided by the desired mean. \"\"\" random.gammavariate(alpha, beta): \"\"\" Gamma distribution. (Not the gamma function!) Conditions on the parameters are alpha > 0 and beta > 0. \"\"\" random.gauss(mu, sigma): \"\"\" Gaussian distribution. mu is the mean, and sigma is the standard deviation. This is slightly faster than the normalvariate() function defined below. \"\"\" random.lognormvariate(mu, sigma): \"\"\" Log normal distribution. \"\"\" random.normalvariate(mu, sigma): \"\"\" Normal distribution. mu is the mean, and sigma is the standard deviation. \"\"\" random.vonmisesvariate(mu, kappa): \"\"\" mu is the mean angle, expressed in radians between 0 and 2*pi, and kappa is the concentration parameter, which must be greater than or equal to zero. If kappa is equal to zero, this distribution reduces to a uniform random angle over the range 0 to 2*pi. \"\"\" random.paretovariate(alpha): \"\"\" Pareto distribution. alpha is the shape parameter. \"\"\" random.weibullvariate(alpha, beta): \"\"\" Weibull distribution. alpha is the scale parameter and beta is the shape parameter. \"\"\" Examples and Recipes 🍀 Basic examples : >>> import random >>> random.random() # Random float: 0.0 >> random.uniform(2.5, 10.0) # Random float: 2.5 >> random.expovariate(1 / 5) # Interval between arrivals averaging 5 seconds 5.148957571865031 >>> random.randrange(10) # Integer from 0 to 9 inclusive 7 >>> random.randrange(0, 101, 2) # Even integer from 0 to 100 inclusive 26 >>> random.choice(['win', 'lose', 'draw']) # Single random element from a sequence 'draw' >>> deck = 'ace two three four'.split() >>> random.shuffle(deck) # Shuffle a list >>> deck ['four', 'two', 'ace', 'three'] >>> random.sample([10, 20, 30, 40, 50], k=4) # Four samples without replacement [40, 10, 50, 30] Simulations : >>> # Six roulette wheel spins (weighted sampling with replacement) >>> random.choices(['red', 'black', 'green'], [18, 18, 2], k=6) ['red', 'green', 'black', 'black', 'red', 'black'] >>> # Deal 20 cards without replacement from a deck of 52 playing cards >>> # and determine the proportion of cards with a ten-value >>> # (a ten, jack, queen, or king). >>> import collections >>> deck = collections.Counter(tens=16, low_cards=36) >>> seen = random.sample(list(deck.elements()), k=20) >>> seen.count('tens') / 20 0.15 >>> # Estimate the probability of getting 5 or more heads from 7 spins >>> # of a biased coin that settles on heads 60% of the time. >>> trial = lambda: random.choices('HT', cum_weights=(0.60, 1.00), k=7).count('H') >= 5 >>> sum(trial() for i in range(10000)) / 10000 0.4169 >>> # Probability of the median of 5 samples being in middle two quartiles >>> trial = lambda : 2500 >> sum(trial() for i in range(10000)) / 10000 0.7958 更多random相关 : random — Generate pseudo-random numbers "},"01-Python/04-模块篇/07-sys模块.html":{"url":"01-Python/04-模块篇/07-sys模块.html","title":"sys模块","keywords":"","body":"Attack on Python - sys模块 🐍 介绍 sys模块为我们提供了对解释器使用或维护的一些变量的访问 , 以及解释器交互的函数 sys模块总体分为四个部分 : Dynamic objects , 动态对象 Static objects , 静态对象 Functions , 函数 Data , 配置 Dynamic objects argv -- command line arguments; argv[0] is the script pathname if known path -- module search path; path[0] is the script directory, else '' modules -- dictionary of loaded modules displayhook -- called to show results in an interactive session excepthook -- called to handle any uncaught exception other than SystemExit To customize printing in an interactive session or to install a custom top-level exception handler, assign other functions to replace these. stdin -- standard input file object; used by input() stdout -- standard output file object; used by print() stderr -- standard error object; used for error messages By assigning other file objects (or objects that behave like files) to these, it is possible to redirect all of the interpreter's I/O. last_type -- type of last uncaught exception last_value -- value of last uncaught exception last_traceback -- traceback of last uncaught exception These three are only available in an interactive session after a traceback has been printed. Static objects builtin_module_names -- tuple of module names built into this interpreter copyright -- copyright notice pertaining to this interpreter exec_prefix -- prefix used to find the machine-specific Python library executable -- absolute path of the executable binary of the Python interpreter float_info -- a struct sequence with information about the float implementation. float_repr_style -- string indicating the style of repr() output for floats hash_info -- a struct sequence with information about the hash algorithm. hexversion -- version information encoded as a single integer implementation -- Python implementation information. int_info -- a struct sequence with information about the int implementation. maxsize -- the largest supported length of containers. maxunicode -- the value of the largest Unicode code point platform -- platform identifier prefix -- prefix used to find the Python library thread_info -- a struct sequence with information about the thread implementation. version -- the version of this interpreter as a string version_info -- version information as a named tuple dllhandle -- [Windows only] integer handle of the Python DLL winver -- [Windows only] version number of the Python DLL __stdin__ -- the original stdin; don't touch! __stdout__ -- the original stdout; don't touch! __stderr__ -- the original stderr; don't touch! __displayhook__ -- the original displayhook; don't touch! __excepthook__ -- the original excepthook; don't touch! Functions displayhook() -- print an object to the screen, and save it in builtins._ excepthook() -- print an exception and its traceback to sys.stderr exc_info() -- return thread-safe information about the current exception exit() -- exit the interpreter by raising SystemExit getdlopenflags() -- returns flags to be used for dlopen() calls getprofile() -- get the global profiling function getrefcount() -- return the reference count for an object (plus one :-) getrecursionlimit() -- return the max recursion depth for the interpreter getsizeof() -- return the size of an object in bytes gettrace() -- get the global debug tracing function setcheckinterval() -- control how often the interpreter checks for events setdlopenflags() -- set the flags to be used for dlopen() calls setprofile() -- set the global profiling function setrecursionlimit() -- set the max recursion depth for the interpreter settrace() -- set the global debug tracing function Data __stderr__ = ' mode='w' encoding='cp9... __stdin__ = ' mode='r' encoding='cp936... __stdout__ = ' mode='w' encoding='cp9... api_version = 1013 argv = [''] base_exec_prefix = r'C:\\Users\\Lyon\\AppData\\Local\\Programs\\Python\\Pytho... base_prefix = r'C:\\Users\\Lyon\\AppData\\Local\\Programs\\Python\\Python35' builtin_module_names = ('_ast', '_bisect', '_codecs', '_codecs_cn', '_... byteorder = 'little' copyright = 'Copyright (c) 2001-2016 Python Software Foundati...ematis... dllhandle = 1373306880 dont_write_bytecode = False exec_prefix = r'C:\\Users\\Lyon\\AppData\\Local\\Programs\\Python\\Python35' executable = r'C:\\Users\\Lyon\\AppData\\Local\\Programs\\Python\\Python35\\py... flags = sys.flags(debug=0, inspect=0, interactive=0, opt...ing=0, quie... float_info = sys.float_info(max=1.7976931348623157e+308, max_...epsilo.. . float_repr_style = 'short' hash_info = sys.hash_info(width=64, modulus=2305843009213693...iphash2... hexversion = 50660080 implementation = namespace(cache_tag='cpython-35', hexversion=506...in... int_info = sys.int_info(bits_per_digit=30, sizeof_digit=4) maxsize = 9223372036854775807 maxunicode = 1114111 meta_path = [, , '_ast': , >> ' ps2 = '... ' stderr = ' mode='w' encoding='cp936'> stdin = ' mode='r' encoding='cp936'> stdout = ' mode='w' encoding='cp936'> thread_info = sys.thread_info(name='nt', lock=None, version=None) version = '3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1... version_info = sys.version_info(major=3, minor=5, micro=2, releaseleve.. . warnoptions = [] winver = '3.5' 更多见 : sys — System-specific parameters and functions "},"01-Python/04-模块篇/08-wsgiref模块.html":{"url":"01-Python/04-模块篇/08-wsgiref模块.html","title":"wsgiref模块","keywords":"","body":"Attack on Python - wsgiref模块 🐍 介绍 wsgiref 模块是 WSGI 规范的一个参考实现 , 它可以用于将WSGI支持添加到Web服务器或框架中 , 它提供了用于操作WSGI环境变量和响应头的实用工具 、 用于实现WSGI服务器的基类 、 用于服务WSGI应用程序的样本HTTP服务器 、以及检查WSGI服务器和应用程序的验证工具 , 以满足WSGI规范(PEP3333) 包内容 handlers - server/gateway base classes headers - WSGI response header tools simple_server - a simple WSGI HTTP server util - WSGI environment utilities validate - WSGI conformance checker handlers 这个模块提供了用于实现WSGI服务器和网关的基本处理程序类 . 这些基类处理与WSGI应用程序通信的大部分工作 , 只要它们提供了一个CGI-like环境 , 以及输入、输出和错误流 CLASSES builtins.object BaseHandler \"\"\"管理WSGI应用程序的调用\"\"\" SimpleHandler \"\"\"初始化数据流,环境等的处理程序\"\"\" BaseCGIHandler \"\"\"CGI-like系统,使用输入/输出/错误流和环境映射\"\"\" CGIHandler \"\"\"CGI-based调用,通过sys.stdin/stdout/stderr和os.environ\"\"\" IISCGIHandler \"\"\"CGI-based调用与IIS路径错误的解决方法\"\"\" # 由上到下是一个基类到子类的过程 以上类中主要的实现在BaseHandler中 , 其它几个都是在基类基础上做了简单的实现 FUNCTIONS read_environ() \"\"\"读取环境,修改HTTP变量\"\"\" 本文中所有思维导图全部来自这里 , 点我吧 对于各个类中的具体实现 , 可以去阅读源代码https://pypi.python.org/pypi/wsgiref headers 这个模块提供了一个类(Headers) , 可以使用mapping-like的接口来方便地操作WSGI响应头 , 也就是一个类似于dict的数据结构 , 并且其实现了dict操作中的get , keys , values 函数 CLASSES builtins.object Headers class Headers(builtins.object) \"\"\"管理一个HTTP响应头的集合\"\"\" headers思维导图 ! simple_server 这个模块实现了一个WSGI应用程序的简单HTTP服务器 (基于HTTP.server) , 每个服务器实例都在给定的主机和端口上提供一个WSGI应用 CLASSES http.server.BaseHTTPRequestHandler(socketserver.StreamRequestHandler) WSGIRequestHandler # WSGIRequestHandler继承体系 # +--------------------+ # | BaseRequestHandler | # +--------------------+ # ↓ # +-----------------------+ # | StreamRequestHandler | # +-----------------------+ # ↓ # +------------------------+ # | BaseHTTPRequestHandler | # +------------------------+ # ↓ # +--------------------+ # | WSGIRequestHandler | # +--------------------+ http.server.HTTPServer(socketserver.TCPServer) WSGIServer # WSGIServer继承体系 # +------------+ # | BaseServer | # +------------+ # ↓ # +------------+ # | TCPServer | # +------------+ # ↓ # +------------+ # | HTTPServer | # +------------+ # ↓ # +------------+ # | WSGIServer | # +------------+ class WSGIRequestHandler(http.server.BaseHTTPRequestHandler) \"\"\"HTTP请求处理程序基类\"\"\" class WSGIServer(http.server.HTTPServer) \"\"\"实现Python WSGI协议的BaseHTTPServer\"\"\" FUNCTIONS demo_app(environ, start_response) \"\"\"应用程序部分\"\"\" make_server(host, port, app, server_class=, handler_class=) \"\"\"创建一个新的WSGI服务器,监听主机和端口\"\"\" simple_server思维导图 simple_server模块主要有两部分内容 应用程序 函数demo_app是应用程序部分 服务器程序 服务器程序主要分成Server和Handler两部分 , 另外make_server函数用来生成一个服务器实例 图上可知simple_server中还有一个ServerHandler模块 , 它继承于handlers模块中的SimpleHandler , 继承体系如下 # +-------------+ # | BaseHandler | # +-------------+ # ↓ # +----------------+ # | SimpleHandler | # +----------------+ # ↓ # +---------------+ # | ServerHandler | # +---------------+ 该模块主要完成的功能如下 : 启动服务器 模块用户请求 处理用户请求 执行simple_server.py时内容如下 httpd = make_server('', 8000, demo_app) sa = httpd.socket.getsockname() print \"Serving HTTP on\", sa[0], \"port\", sa[1], \"...\" # M: webbrowser provides a high-level interface to allow displaying Web-based documents # to users. Under most circumstances import webbrowser webbrowser.open('http://localhost:8000/xyz?abc') httpd.handle_request() # serve one request, then exit demo_app demo_app(environ, start_response) ''' 参数说明: environ:为一个字典 start_response:为一个可调用函数 return:返回一个可迭代对象 另外demo_app中会调用start_response函数 ''' def demo_app(environ,start_response): from StringIO import StringIO stdout = StringIO() print >> stdout, \"Hello world!\" print >> stdout h = environ.items() h.sort() for k,v in h: print >> stdout, k,'=',`v` start_response(\"200 OK\", [('Content-Type','text/plain')]) return [stdout.getvalue()] make_server def make_server(host, port, app, server_class=WSGIServer, handler_class=WSGIRequestHandler) ''' 参数说明: host:主机名 port:端口号 server_class:生成server实例时所使用的基类,默认为WSGIServer handler_class:用于处理请求的handler类,默认为WSGIRequestHandler ''' def make_server(host, port, app, server_class=WSGIServer, handler_class=WSGIRequestHandler): '''引用no_1y的注释,文尾有详细链接''' # no_1y: -> HTTPServer.__init__ # -> TCPServer.__init__ # -> TCPServer.server_bind # -> TCPServer.socket.bind # -> TCPServer.server_activate # -> TCPServer.socket.listen server = server_class((host, port), handler_class) # no_1y: conresponding to WSGIRequestHandler.handle() # -> handler.run(self.server.get_app()) server.set_app(app) return server \"\"\" server_class为WSGIServer,生成时会沿着继承方向到达最底层的TCPServer,并完成对socket的绑定和监听 set_app设置了app,它会在handler_class的handle函数中被取出来,交给handler的run函数执行 \"\"\" util 这个模块提供了用于处理WSGI环境的各种实用函数 , WSGI环境是一个包含在PEP 3333中描述的HTTP请求变量的字典 CLASSES builtins.object FileWrapper class FileWrapper(builtins.object): \"\"\" 将文件类对象转换为迭代器的包装器 \"\"\" FUNCTIONS application_uri(environ) \"\"\"返回应用程序的基本URI\"\"\" guess_scheme(environ) \"\"\"返回一个猜测wsgi.url_scheme是否是http或https\"\"\" request_uri(environ, include_query=True) \"\"\"返回完整的请求URI,包括任意的查询字符串\"\"\" setup_testing_defaults(environ) \"\"\"用于设置虚拟环境的服务器和应用程序,目的是使WSGI的单元测试更加容易\"\"\" shift_path_info(environ) \"\"\"将一个名称从PATH_INFO转移到SCRIPT_NAME,并返回它,如果在pathinfo中没有其他路径段，则返回None\"\"\" util思维导图 validate 在创建新的WSGI应用程序对象 , 框架 , 服务器或中间件时 , 使用wsgiref.validate验证新代码的一致性是很有用的 这个模块提供了一个函数 , 它创建了WSGI应用程序对象 , 它可以验证WSGI服务器或网关和WSGI应用程序对象之间的通信 , 从而检查双方是否符合协议的一致性 简单的说就是检查你对WSGI的实现是否满足标准 思维导图如下 本文主要参考http://blog.csdn.net/on_1y/article/details/18818081 思维导图来自https://github.com/minixalpha/SourceLearning/tree/master/wsgiref-0.1.2 "},"01-Python/05-网络篇/":{"url":"01-Python/05-网络篇/","title":"网络篇","keywords":"","body":"Attack on Python - 网络篇 🐍 "},"01-Python/05-网络篇/01-网络编程.html":{"url":"01-Python/05-网络篇/01-网络编程.html","title":"网络编程","keywords":"","body":"Attack on Python - 网络编程 🐍 前言 在互联网没有诞生之前 , 我们都是在自己的计算机上自娱自乐 , 那时候的程序也都是单机版的程序 , 随后互联网诞生了 , 用网络把各个计算机连接到了一起 , 让处在网络中的计算机可以互相通信 , 网络编程就是如何在程序中实现两台计算机之间的通信 最基本的例子莫过于我们传输文件了 , 没有网络的情况下我们只能利用U盘或者硬盘 , 先从我的计算机上将要传输的文件写入到我们的U盘或者硬盘 , 然后再用已有文件的U盘或者硬盘写入其他计算机 , 这样的局限性有多大可想而知 ; 利用网络我们可以直接十万八千里进行文件传输 , 比如用我的QQ传文件给你的QQ , 当然这个例子可能不怎么好 , 因为你传文件一般可能不会用QQ来传 网络协议 网络的存在是为了能使计算机之间进行通信 , 既然是通信那么就得有一门大家都会的语言吧 . 就像我跟你说话 , 我只会中文而你只会英文 , 那么我们两个拿什么交流 ? 花钱请个翻译官 ? 不存在的 ...... 那么在网络上的各台计算机之间也需要一种大家都会的语言 , 这就是网络协议 网络协议是网络上所有设备之间通信规则的集合 , 它规定了通信时信息必须采用的格式和这些格式的意义 为了使不同计算机厂家生产的计算机能够相互通信 , 以便在更大的范围内建立计算机网络 , 国际标准化组织( ISO ) 在1987年提出了 \"开放系统互联参考模型\" , 即著名的OSI/RM模型(Open System Interconection/Reference Model) . 它将计算机网络体系结构的通信协议分为七层 , 如下图 在上图中右边协议部分我们可以了解各层中所包含的协议 , 互联网协议包含了上百种协议 , 但是最重要的两个协议是TCP和IP协议 , 所以我们把互联网的协议简称TCP/IP协议 IP协议 IP ( Internet Protocol ) 就是为计算机网络相互连接进行通信而设计的协议 , 翻译过来即\"因特网协议\" , 简称\"网协\" 它定义的地址称为IP地址 , 广泛采用v4版本即IPv4 , 它规定网络地址由32位2进制表示 , 范围为 0.0.0.0 ~ 255.255.255.255 , 一个IP地址通常协程四段十进制数 , 例如 : 127.0.0.1 . 还有IPv6地址 , 规定网络地址由128位2进制表示 , 它是目前使用的IPv4的升级版 , 以字符串表示如 : 2001:0db8:85a3:0042:1000:8a2e:0370:7334 通信的时候 , 双方必须知道对方的标识 , 好比发邮件必须知道对方的邮件地址 . 互联网上每个计算机的唯一标识就是IP地址 , 如果一台计算机同时接入到两个或更多的网络 , 比如路由器 , 它就会有两个或多个IP地址 , 所以 , IP地址对应的实际上是计算机的网络接口 , 通常是网卡 IP协议负责把数据从一台计算机通过网络发送到另一台计算机 . 数据被分割成一小块一小块 , 然后通过IP包发送出去 , 由于互联网链路复杂 , 两台计算机之间经常有多条线路 , 因此 , 路由器就负责决定如何把一个IP包转发出去 ; IP包的特点是按块发送 , 途径多个路由 , 但不保证能到达 , 也不保证顺序到达 一个IP包除了包含要传输的数据外 , 还包含源IP地址和目标IP地址 , 源端口和目标端口 TCP协议 TCP协议则是建立在 IP协议 之上的 , TCP协议负责在两台计算机之间建立可靠连接 , 保证数据包按顺序到达 ; TCP协议会通过握手建立连接 , 然后 , 对每个IP包编号 , 确保对方按顺序收到 , 如果包丢掉了 , 就自动重发 许多常用的更高级的协议都是建立在TCP协议基础上的 , 比如用于浏览器的HTTP协议、发送邮件的SMTP协议等 互联网本质上就是一系列的网络协议 , 互联网协议的功能是定义计算机如何接入internet , 以及接入internet的计算机通信标准 网络编程 互联网已经建立成功了 , 也就是说一大堆协议都准备好了 , 你只是规定好了计算机怎么接入互联网 , 但是却没告诉计算机接入之后怎么收发消息 , 也就是说并没有完全实现通信 , 仅仅是\"通\"了而已 网络编程就是以实现计算机之间通信为目的的编程 , 而实现计算机之间的通信实质上是实现计算机上两个进程的通信 , 比如我在两台计算机上都装有QQ , 我用一台计算机上的QQ给另一台计算机上的QQ发消息 , 明显实现该通信并不是两台计算机直接通信的 , 而是通过QQ这个正在运行的软件即一个进程来实现该通信的 所以我们可以这样说网络编程就是以实现进程间通信为目的的编程 "},"01-Python/05-网络篇/02-Socket.html":{"url":"01-Python/05-网络篇/02-Socket.html","title":"Socket","keywords":"","body":"Attack on Python - socket 🐍 C/S架构 在网络通信中 , 一般是一方求一方应 , 求的一方就是客户端即 Client , 应的一方就是服务端即Server , 这就是C/S架构 , 在互联网中处处是C/S架构 , 比如我们访问百度 , 百度就是一个服务端 , 而我们的浏览器就是一个客户端 Socket Socket是应用层与TCP/IP协议族通信的中间软件抽象层 , 它是一组接口 , 是从顶上三层 (osi七层协议的应用层) 进入传输层的接口 ; 顶上三层通常构成所谓的用户进程 , 底下四层却通常作为操作系统内核的一个部分提供 Socket又叫做套接字 , Python中socket为我们封装好了TCP/UDP协议 , 所以我们无需深入理解 , 只要遵循socket的规定去编程就可以了 创建socket对象 创建socket对象就是一个建立TCP的过程 , 即三次握手 , 断开当然就是四次挥手了 代码实现 # 导入socket模块 import socket # 调用socket模块中的socket类实例化出对象 sock = socket.socket(socket.AF_INET,socket.SOCK_STREAM,0) '''或者可以使用 from module import * ,可以大幅度减少代码,仅仅提一下,毕竟有弊端''' # 导入socket模块中的所有内容 from socket import * # 实例化socket类 sock = socket(AF_INET,SOCK_STREAM,0) socket类参数说明 其构造函数源码 def __init__(self, family=AF_INET, type=SOCK_STREAM, proto=0, fileno=None): # 下面内容就不摘了 pass family : 地址簇 参数 说明 AF_INET IPv4 , 即默认为IPv4 AF_INET6 IPv6 AF_UNIX 针对Unix系统进程间通信 type : 类型 参数 说明 SOCK_STREAM 面向流 , 即TCP SOCK_DGRAM 面向数据报 , 即UDP SOCK_RAW 原始套接字 , 可处理ICMP,IGMP等网络报文 ; 可以处理特殊的IPv4报文 ; 利用原始套接字 , 可以通过IP_HDRINCL套接字选项由用户构造IP头 SOCK_RDM 一种可靠的UDP形式 . SOCK_RAM用来提供对原始协议的低级访问 , 在需要执行某些特殊操作时使用 , 如发送ICMP报文 , SOCK_RAW通常仅限于高级用户或管理员运行的程序使用 SOCK_SEQPACKET 可靠的连续数据包服务 proto : 协议 参数 说明 0 与特定的地址家族相关的协议 , 如果是0 , 则系统就会根据地址格式和套接类别 , 自动选择一个合适的协议 还有一个fileno参数是无需理会的 基于TCP TCP协议是有链接的 , 面向流的 , 数据传输可靠 , 必须先启动服务端 TCP服务端 创建套接字对象 创建socket对象 绑定IP和端口 绑定 bind() 开始监听链接 监听 listen() 阻塞 , 等待客户端成功连接 阻塞 accept() 接收请求数据 接收 recv() 处理并发送请求数据 发送 send() 通信完毕 , 关闭链接 , 关闭套接字 关闭 close() TCP客户端 创建套接字对象 创建socket对象 连接服务端 , 按照IP和端口连接 连接 connet() 发送请求数据 发送 send() 接收请求数据 接收 recv() 通信完毕 , 关闭套接字 关闭 close() 简单实例 tcp_server.py # 导入socket模块 import socket # 创建socket对象,默认参数就不填了 sock = socket.socket() # 绑定IP和端口,参数是一个元组(ip,port) sock.bind(('127.0.0.1', 8080)) # 开始监听,最大监听数为5 sock.listen(5) # 阻塞,等待连接,返回一个链接通道和一个地址 conn,addr = sock.accept() # 接收请求数据,接收大小为1024字节 content = conn.recv(1024) # 打印结果(bytes转成str显示) print(content.decode()) # 发送请求结果,必须以bytes类型 conn.send(b'Hello Lyon') # 关闭链接 conn.close() # 关闭套接字 sock.close() tcp_client.py # 导入socket模块 import socket # 创建socket对象 sock = socket.socket() # 建立链接 sock.connect(('127.0.0.1', 8080)) # 发送请求数据,必须以bytes类型 sock.send(b\"I'm Lyon\") # 接收请求结果 content = sock.recv(1024) # 打印结果 print(content.decode()) # 关闭套接字 sock.close() 基于UDP UDP协议是无链接的 , 面向数据报的 , 数据传输全靠吼 , 不可靠 , 先启动哪一端都不会报错 UDP服务端 创建套接字对象 创建socket对象 绑定IP和端口 绑定 bind() 接收请求数据 接收 recvfrom() 通信完毕 , 关闭套接字 关闭 close() UDP客户端 创建套接字对象 创建socket对象 发送请求数据 发送 sendto() 通信完毕 , 关闭套接字 关闭 close() 简单实例 udp_server.py # 导入socket模块 import socket # 创建socket对象 sock = socket.socket(type=socket.SOCK_DGRAM) # 绑定ip和端口 sock.bind(('127.0.0.1', 8090)) # 接收请求,返回数据和地址 data,addr = sock.recvfrom(1024) # 打印请求 print(data.decode()) # 关闭套接字 sock.close() udp_client.py # 导入socket模块 import socket # 创建socket对象 sock = socket.socket(type=socket.SOCK_DGRAM) # 发送请求到指定地址 sock.sendto(b\"I'm Lyon\", ('127.0.0.1', 8090)) # 关闭套接字 sock.close() Socket对象方法 方法 描述 s.bind() 绑定地址（host,port）到套接字， 在AF_INET下,以元组（host,port）的形式表示地址。 s.listen() 开始TCP监听。backlog指定在拒绝连接之前，操作系统可以挂起的最大连接数量。该值至少为1，大部分应用程序设为5就可以了。 s.accept() 被动接受TCP客户端连接,(阻塞式)等待连接的到来 s.connect() 主动初始化TCP服务器连接，。一般address的格式为元组（hostname,port），如果连接出错，返回socket.error错误。 s.connect_ex() connect()函数的扩展版本,出错时返回出错码,而不是抛出异常 s.recv() 接收TCP数据，数据以字符串形式返回，bufsize指定要接收的最大数据量。flag提供有关消息的其他信息，通常可以忽略。 s.send() 发送TCP数据，将string中的数据发送到连接的套接字。返回值是要发送的字节数量，该数量可能小于string的字节大小。 s.sendall() 完整发送TCP数据，完整发送TCP数据。将string中的数据发送到连接的套接字，但在返回之前会尝试发送所有数据。成功返回None，失败则抛出异常。 s.recvfrom() 接收UDP数据，与recv()类似，但返回值是（data,address）。其中data是包含接收数据的字符串，address是发送数据的套接字地址。 s.sendto() 发送UDP数据，将数据发送到套接字，address是形式为（ipaddr，port）的元组，指定远程地址。返回值是发送的字节数。 s.close() 关闭套接字 s.getpeername() 返回连接套接字的远程地址。返回值通常是元组（ipaddr,port）。 s.getsockname() 返回套接字自己的地址。通常是一个元组(ipaddr,port) s.setsockopt(level,optname,value) 设置给定套接字选项的值。 s.getsockopt(level,optname[.buflen]) 返回套接字选项的值。 s.settimeout(timeout) 设置套接字操作的超时期，timeout是一个浮点数，单位是秒。值为None表示没有超时期。一般，超时期应该在刚创建套接字时设置，因为它们可能用于连接的操作（如connect()） s.gettimeout() 返回当前超时期的值，单位是秒，如果没有设置超时期，则返回None。 s.fileno() 返回套接字的文件描述符。 s.setblocking(flag) 如果flag为0，则将套接字设为非阻塞模式，否则将套接字设为阻塞模式（默认值）。非阻塞模式下，如果调用recv()没有发现任何数据，或send()调用无法立即发送数据，那么将引起socket.error异常。 s.makefile() 创建一个与该套接字相关连的文件 解决OSError: [Errno 48] Address already in use 问题 添加一条socket配置 , 重用ip和端口 import socket sock = socket.socket() # 添加在bind前 sock.setsockopt(socket.SOL_SOCKET,SO_REUSEADDR,1) sock.bind(address) "},"01-Python/05-网络篇/03-Socket实现QQ聊天.html":{"url":"01-Python/05-网络篇/03-Socket实现QQ聊天.html","title":"Socket实现QQ聊天","keywords":"","body":"Attack on Python - socket实现QQ聊天 🐍 介绍 在上一篇中写了最基本版的socket服务端和客户端 , 即仅能通信一次后就自动关闭了 , 显然实际应用中可不是这样的 , 那就来写一个像QQ一样的聊天程序吧 TCP实现 因为TCP是有链接的 , 这就导致只能有一个服务端 , 但是可以有多个客户端 tcpqq_server.py import socket sock = socket.socket() sock.bind(('127.0.0.1', 8080)) sock.listen(5) # 实现链接循环 while True: print(\"Watiting for the link...\") conn, addr = sock.accept() print(\"Your friend {} is online...\".format(addr)) # 实现通信循环 while True: messages = conn.recv(1024) print(\"Messages from [{}]:{}\".format(addr, messages.decode('utf-8'))) if messages == b'q': break else: while True: data = input(\"Please input the messages to be sent:\").strip().encode('utf-8') # 注意发送的内容不能为空,否则接收方就会一直等下去 if not data: print(\"Can't be empty...\") continue conn.send(data) break print(\"Your friend {} is offline...\".format(addr)) conn.close() sock.close() tcpqq_client.py import socket sock = socket.socket() sock.connect(('127.0.0.1', 8080)) # 实现通信循环 while True: messages = input(\"Please input your messages to be sent:\").strip().encode('utf-8') # 注意发送的内容不能为空,否则接收方就会一直等下去 if not messages: print(\"Can't be empty...\") continue elif messages == b'q': break else: sock.send(messages) data = sock.recv(1024) print(\"Messages from [{}]:{}\".format(('127.0.0.1', 8080), data.decode('utf-8'))) sock.close() 当然实际应用中是不会用TCP来完成的 , 而是用UDP , 这里只是模拟 , 并且以上还有有问题没有解决的 , 比如如果发送的消息大于1024字节 , 那么就不能完整接收信息了 , 后续再进行处理 TCP版本的服务端可以允许同时连入5个客户端 , 值得注意的是并不是同时连入 , 按照顺序排队 , 只有前面的人说完了会连入后序的客户端 UDP实现 以为UDP是无链接的 , 所以它可以实现想跟谁说话就跟谁说话 udpqq_server.py import socket sock = socket.socket(type=socket.SOCK_DGRAM) sock.bind(('127.0.0.1', 8080)) # 实现通信循环 while True: data, addr = sock.recvfrom(1024) print(\"Receive a message from {}:{}\".format(addr, data.decode('utf-8'))) if data == b'q': break while True: messages = input(\"Please input the messages to be sent:\").strip().encode('utf-8') if not messages: print(\"Can't be empty...\") continue sock.sendto(messages, addr) break sock.close() udpqq_client.py import socket sock = socket.socket(type=socket.SOCK_DGRAM) # 实现通信循环 while True: messages = input(\"Please input your messages to be sent:\").strip().encode('utf-8') if not messages: print(\"Can't be empty...\") continue elif messages == b'q': break else: sock.sendto(messages, ('127.0.0.1',8080)) data, addr = sock.recvfrom(1024) print(\"Receive a message from {}:{}\".format(addr, data.decode('utf-8'))) sock.close() 利用UDP实现才更接近现实 , 我们只需要知道他的ip和端口 , 我们就可以跟他讲话 , 在他即可以是服务端 , 也可以是客户端 , 不过必须注意接收和发送流程的问题 以上两种实现方式 , 都只是最基础的版本 , 在UDP中我们可以将所有人的ip和端口放到一个字典里或者其他存储里 , 利用ip和端口就可以实现跟所有人进行聊天了 "},"01-Python/05-网络篇/04-Socket实现远程执行命令.html":{"url":"01-Python/05-网络篇/04-Socket实现远程执行命令.html","title":"Socket实现远程执行命令","keywords":"","body":"Attack on Python - socket实现远程执行命令 🐍 os模块实现 osssh_server.py # 导入socket模块 import socket # 导入os模块 import os # 创建套接字对象 sock = socket.socket() # 重置ip和端口 sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) # 绑定ip和端口 sock.bind(('127.0.0.1', 8080)) # 监听 sock.listen(5) # 链接循环 while True: print(\"Waitting for connection...\") # 阻塞 conn, addr = sock.accept() print(\"{}successful connection...\".format(addr)) while True: cmd = conn.recv(1024) # 接收为空说明客户端断开了连接 if not cmd: print(\"Client is disconnected...\") break print(\"The command is {}\".format(cmd.decode())) # 利用os模块进行系统调用,py3中popen参数为str,所以先decode data = os.popen(cmd.decode()).read() # 发送命令执行结果 conn.send(data.encode('utf-8')) # 关闭链接 conn.close() # 关闭套接字 sock.close() osssh_client.py # 导入socket模块 import socket # 创建套接字对象 sock = socket.socket() # 连接服务端 sock.connect(('127.0.0.1', 8080)) while True: cmd = input(\"Please input the command:\").strip() if not cmd: print(\"Can't empty...\") continue elif cmd == 'exit': break # 发送命令 sock.send(cmd.encode('utf-8')) # 接收命令执行结果 data = sock.recv(1024) print(data.decode('utf-8')) # 关闭套接字 sock.close() subprocess模块实现 subprocess_server.py import socket import subprocess sock = socket.socket() sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) sock.bind(('127.0.0.1', 8080)) sock.listen(5) while True: print(\"Waitting for connection...\") conn, addr = sock.accept() print(\"{}successful connection...\".format(addr)) while True: cmd = conn.recv(1024) if not cmd: print(\"Client is disconnected...\") break print(\"The command is {}\".format(cmd.decode())) # 利用subprocess模块进行系统调用 data = subprocess.Popen(cmd.decode(),shell=True, stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE) stdout = data.stdout.read() stderr = data.stderr.read() # 打包执行结果 res = stdout + stderr # 发送执行结果 conn.send(res) conn.close() sock.close() subprocess_client.py import socket sock = socket.socket() sock.connect(('127.0.0.1', 8080)) while True: cmd = input(\"Please input the command:\").strip() if not cmd: print(\"Can't empty...\") continue elif cmd == 'exit': break sock.send(cmd.encode('utf-8')) data = sock.recv(1024) # Windows终端默认编码是gbk,所以得用gbk进行解码 print(data.decode('gbk')) sock.close() 以上两种方法实现了简单的ssh , 即远程执行命令 , 但是这两个都一个问题 , 当我们执行多次命令后 , 结果就不是我们想要得到了 , 它会发生粘包 , 即有可能上条命令的结果粘到这条命令的结果了 , 如何解决粘包问题 ? 下一篇整理 "},"01-Python/05-网络篇/05-粘包.html":{"url":"01-Python/05-网络篇/05-粘包.html","title":"粘包","keywords":"","body":"Attack on Python - 粘包 🐍 粘包 由上一篇 Socket实现远程执行命令 中所出现的问题引出了粘包这个问题 , 粘包到底是什么? 首先 , 粘包现象只出现在TCP中 , 为什么说只有在TCP中才会发生粘包现象 , 先来详细解释一下TCP与UDP吧 TCP TCP (transprot control protocol, 传输控制协议) 是面向连接的 , 面向流的 , 提供高可靠性服务 . 收发两端都有要一一对应的socket(一对一模式) , 因此发送端为了将多个发往接收端的包 , 更有效的发到对方 , 使用了优化方法(Nagle算法) , 将多次间隔较小且数据量小的数据 , 合并成一个大的数据块 , 然后进行封包 . 必须提供科学的拆包机制 , 才能进行合理的分辨 , 所以说面向流的通信是无消息保护边界的 UDP UDP(user datagram protocol, 用户数据报协议) 是无连接的 , 面向消息的 , 提供高效率服务 . 不使用块的合并优化算法 , 由于UDP支持的是一对多的模式 , 所以接收端的skbuff (套接字缓冲区) 采用了链式结构来记录每一个到达的UDP包 , 在每个UDP包中就有了消息头 (消息来源地址 , 端口等信息) , 这样 , 对于接收端来说 , 就容易进行区分处理了 . 即面向的通信是有消息保护边界的 区别 TCP是基于数据流的 , 于是收发的消息不能为空 , 这就需要在客户端和服务端都添加空消息的处理机制 , 防止程序卡住 , 而UDP是基于数据报的 , 就算收发空内容 , 也不是空消息 , UDP协议会自动帮你封装上消息头 粘包现象发生的原因 粘包分为两种 发送方引起的粘包 这种情况下引起的粘包是TCP协议本身造成的 , TCP为了提高传输效率 , 发送方往往要收集到足够多的数据后才发送一个TCP段 (超过时间间隔也会发送,时间间隔是很短的) , 如果连续几次需要发送的数据都很少 , 通常TCP会根据优化算法把这些数据合成一个TCP段后一次发送出去 , 所以几次的数据到接收方时就粘成一包了 如下 : # 发送方第一次发送 send(b\"I'm \") # 立马第二次,不超过时间间隔 send(b\"Lyon\") ------------- # 接收 data = recv(1024) # 收到的是两次粘在一起的数据 print(data.decode()) # 打印结果: I'm Lyon 接收方引起的粘包 这种情况引起的粘包则是因为接收方不及时接收缓冲区的数据包造成的 , 比如发送方一次发送了10字节的数据 , 而接收方只接收了2字节 , 那么剩余的8字节的数据将都在缓冲区等待接收 , 而此时发送方又发送了2字节的数据 , 过了一会接收方接收了20字节(大于剩余10字节) , 接收完毕 , 缓冲区剩余的数据就和第二次发送的数据粘成了一个包 , 产生粘包 如下 : # 发送4字节内容 send(b\"I'm \") # 接收1字节,缓冲区还有3字节 data1 = recv(1) print(\"data1:\",data1) # 发送4字节内容,粘到缓冲区中剩余的3字节后面 send(b\"Lyon\") # 接收7字节,接收完毕 data2 = recv(7) print(\"data2:\",data2) ''' 打印结果: data1:I data2:'m Lyon ''' SO : 所以所谓粘包问题主要还是因为接收方不知道消息之间的界限 , 不知道一次性提取多少字节的数据所造成的 解决方法 既然粘包是因为接收方不知道消息界限 , 那么我们就自己创建界限 low方法 我们只需要对上一篇中subprocess_server.py以及subprocess_client.py 做一点点修改就行了 subprocess_server_development.py import socket import subprocess sock = socket.socket() sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) sock.bind(('127.0.0.1', 8080)) sock.listen(5) while True: print(\"Waitting for connection...\") conn, addr = sock.accept() print(\"{}successful connection...\".format(addr)) while True: # 接收指令 cmd = conn.recv(1024) if not cmd: print(\"Client is disconnected...\") break print(\"The command is {}\".format(cmd.decode())) # 获取执行结果 data = subprocess.Popen(cmd.decode(),shell=True, stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE) # 获取错误句柄 err = data.stderr.read() if err: res = err else: res = data.stdout.read() # 发送数据长度 conn.send(str(len(res)).encode('utf-8')) # 防止与两次发送数据粘在一起 ready = conn.recv(1024) if ready == b'OK': # sendall连续调用send完成发送 conn.sendall(res) conn.close() sock.close() subprocess_client_development.py import socket sock = socket.socket() sock.connect(('127.0.0.1', 8080)) while True: cmd = input(\"Please input the command:\").strip() if not cmd: print(\"Can't empty...\") continue elif cmd == 'exit': break # 发送指令 sock.send(cmd.encode('utf-8')) # 获取数据长度 length = sock.recv(1024).decode('utf-8') # 发送标志 sock.send(b'OK') recvsize = 0 data = b'' # 循环接收 while recvsize 利用这种方式 , 我们需要提前先将数据大小发送过去 , 这无疑会放大网络延迟带来的性能损耗 制作报头 既然需要将大小发送过去 , 那我们是不是可以为字节流加上自定义固定长度报头 , 报头中包换数据大小等信息 , 然后一次直接发送过去 , 对方只要在接收的时候先从取出报头 , 再取数据 所以我们只需要固定好报头的长度可以了 , 我们可以利用struct模块来制作报头 , 只需对上方法稍作修改 subprocess_struct_server.py import socket,struct import subprocess sock = socket.socket() sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) sock.bind(('127.0.0.1', 8080)) sock.listen(5) while True: print(\"Waitting for connection...\") conn, addr = sock.accept() print(\"{}successful connection...\".format(addr)) while True: cmd = conn.recv(1024) if not cmd: print(\"Client is disconnected...\") break print(\"The command is {}\".format(cmd.decode())) data = subprocess.Popen(cmd.decode(),shell=True, stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE) err = data.stderr.read() if err: res = err else: res = data.stdout.read() # 制作4位固定报头并发送 conn.send(struct.pack('i', len(res))) # 直接循环发送 conn.sendall(res) conn.close() sock.close() subprocess_struct_client.py import socket,struct sock = socket.socket() sock.connect(('127.0.0.1', 8080)) while True: cmd = input(\"Please input the command:\").strip() if not cmd: print(\"Can't empty...\") continue elif cmd == 'exit': break sock.send(cmd.encode('utf-8')) res = sock.recv(4) # 解开报头取出数据长度 length = struct.unpack('i', res)[0] recvsize = 0 data = b'' # 循环接收 while recvsize "},"01-Python/05-网络篇/06-Socketserver实现多并发.html":{"url":"01-Python/05-网络篇/06-Socketserver实现多并发.html","title":"Socketserver实现多并发","keywords":"","body":"Attack on Python - Socketserver实现多并发 🐍 介绍 在上面的整理篇章中 , 简单的网络编程基本已经会了 , 一个TCP , 一个UDP , 然后就是粘包问题 但是在上述中有一个问题 , 在现实生活中 , 一个服务端肯定常常需要同时服务好几个客户端 , 而上述篇章中并没有实现一对多同时进行的情况 , TCP中只能等前一个链接断开后续的才能连上 , 没连上就一直等 ; UDP则是接一次发一次 , 并不能同时接两次发两次 . 为了处理这个问题 , 即实现并发 (后续文章详细讲解) , Python中有一个socketserver模块可以满足我们的要求 socketserver Python提供了两个级别访问的网络服务: 低级别的网络服务支持基本的socket , 它提供了标准的BSD Socket API , 可以访问底层操作系统Socket接口的全部方法 高级别的网络服务模块socketserver , 它提供了服务器中心类 , 可以简化网络服务器的开发 socket就不用说了 , now socketserver 我们知道基于TCP的套接字 , 关键就是两个循环 , 一个链接循环(多人) , 一个通信循环(多消息) 在socketserver模块中分为两大类 : server类 (解决链接问题) 和request类 (解决通信问题) 如果想进一步了解 , 可以看看官方文档 , socketserver官方文档 > 实现多并发 multi_socketserver_server.py import socketserver class MyServer(socketserver.BaseRequestHandler): def handle(self): # 创建一个链接,继承于socketserver中的BaseRequestHandler类 conn = self.request # 发送登录提示 conn.sendall(b\"Welcome to login...\") print(\"Client connect...\") while True: print(\"Waitting for recving message...\") # 接收消息 message = conn.recv(1024) print(message.decode('utf-8')) # 收到exit就退出 if message == \"exit\": break # 回复消息 data = input(\"Reply message:\") # 发送消息 conn.sendall(data.encode('utf-8')) if __name__ == \"__main__\": 　# 实例化 server = socketserver.ThreadingTCPServer(('127.0.0.1', 999, ), MyServer) # 调用serve_forever方法 server.serve_forever() ''' def serve_forever(self, poll_interval=0.5): \"\"\" Handle one request at a time until shutdown. Polls for shutdown every poll_interval seconds. Ignores self.timeout. If you need to do periodic tasks, do them in another thread. \"\"\" ''' multi_socketserver_client.py # 就是一个简单的TCP客户端 import socket sock = socket.socket() # 连接服务端 sock.connect(('127.0.0.1', 999, )) login = sock.recv(1024) print(login.decode('utf-8')) while True: message = input(\"Please input the message:\").strip() if message == \"exit\": sock.sendall(b'exit') break else: sock.sendall(message.encode('utf-8')) print(\"Waitting for recving message...\") data = sock.recv(1024) print(data.decode('utf-8')) sock.close() 到这里 , 我们成功实现了多并发 , 多并发是什么? 这就关系到操作系统中的进程和线程了 , 网络编程既然是实现两个进程间的通信 , 那么就逃不过进程 , 线程等了 "},"01-Python/06-并发篇/":{"url":"01-Python/06-并发篇/","title":"并发篇","keywords":"","body":"Attack on Python - 并发篇 🐍 "},"01-Python/06-并发篇/01-进程与线程.html":{"url":"01-Python/06-并发篇/01-进程与线程.html","title":"进程与线程","keywords":"","body":"Attack on Python - 进程与线程 🐍 进程 进程是对正在运行程序的一个抽象 , 即一个进程就是一个正在执行程序的实例 从概念上说 , 每个进程拥有它自己的虚拟CPU . 当然 , 实际上真正的CPU在各进程之间来回切换 . 这种快速切换就是多道程序设计 . 但是某一瞬间 , CPU只能运行一个进程 , 但在1秒钟期间 , 它可能运行多个进程 , 就是CPU在进行快速切换 , 有时人们所说的 伪并行 就是指这种情形 创建进程 操作系统中 , 有4种事件导致进程的创建 系统初始化 , 启动操作系统时 , 通常会创建若干个进程 , 分为前台进程和后台进程 执行了正在运行的进程所调用的进程创建系统调用 用户请求创建一个新的进程 一个批处理作业的初始化 从技术上看 , 在所有这些情况中 , 新进程都是由一个已存在的进程执行了一个用于创建进程的系统调用而创建的 . 这个进程可以是一个运行的用户进程 , 一个由键盘或鼠标启动的系统进程或者一个批处理管理进程 . 这个进程所做的工作是 , 执行一个用来创建新进程的系统调用 . 在Linux/Unix中提供了一个fork() 系统调用就是用来创建进程的 (子进程) , 当然在Windows中也有相对应的系统调用 在Python中的os模块封装了常见的系统调用 , 其中就包括fork , 可以在Python程序中轻松创建子进程 '''因为Windows中没有fork调用,所以下程序只能在Unix/Linux下执行''' import os # os.getpid()获取父进程的ID print(\"Process %s start...\" % os.getpid()) # fock()调用一次会返回两次 pid = os.fork() # 子进程返回0 if pid == 0: print(\"I am child process %s and my parent is %s\" % (os.getpid(), os.getppid())) # 父进程返回子进程的ID else: print(\"I %s just created a child process %s\" % (os.getpid(), pid)) 终止进程 进程不可能永恒的存在 , 迟早都会终止 , 通常由下列条件引起 : 正常退出(自愿的) , 任务完成退出 出错退出(自愿的) , 进程中的错误 严重错误(非自愿) , 由进程引起的错误 被其他进程杀死(非自愿) , 某进程执行一个系统调用通知操作系统杀死某个其他进程 在有些系统中 , 当一个进程终止时 , 不论是自愿的还是其他原因 , 由该进程所创建的所有进程也一律立即被杀死 . 不过Unix和Windows都不是这种工作方式 进程状态 每个进程都有自己的程序计数器和内部状态 , 但进程之间经常需要相互作用 , 一个进程的输出结构可能作为另一个进程的输入 , 所以进程就会出现如下三种状态 : 运行态(该时刻进程实际占用CPU) 就绪态(可运行 , 但因为其他进程正在运行而暂时停止) 阻塞态(除非某中外部事件发生 , 否则进程不能运行) 进程的三种状态之间有四种可能的转换关系 一个进程状态 另一个进程状态 过程 运行态 阻塞态 进程为等待输入而 运行态 就绪态 调度程序选择另一个进程 就绪态 运行态 调度程序选择这个进程 阻塞态 就绪态 出现有效输入 进程中还有一点就是进程实现的问题 , 这就是依靠进程表了 , 具体就不说明了 进程的作用主要是提供了多道编程(多进程) , 并且提高了计算机的利用率 , 但是有两点是进程没有解决的 : 进程在同一时间只能做一件事 , 显然这不够我们的需求 进程在执行过程中一旦阻塞 , 整个进程就挂起了 , 这也是对计算机资源的一种浪费 人们想到的解决办法就是 , 在一个进程里面再有一类进程 , 称为迷你进程 , 也就是下面要说的线程 线程 在传统操作系统中 , 每个进程有一个地址空间和一个控制线程 , 事实上 , 这几乎就是进程的定义 所以我们可以知道 , 线程是操作系统能够进程运算调度的最小单位 , 它被包含在进程之中 , 是进程中的实际运作单位 . 不过 , 经常存在在同一个地址空间中准并行运行多个控制线程的情况 , 这些线程就像分离的进程 一个线程指的是进程中一个单一顺序的控制流 , 一个进程中可以并发多个线程 线程的使用 人们需要使用线程有两个理由 : 在多进程模型中 , 没有并行实体共享同一个地址空间和所有可用数据的能力 线程比进程更轻量级 , 在许多系统中 , 创建一个线程较创建一个进程要快10~100倍 线程与进程的区别 线程是执行的指令集 , 进程是资源的集合 线程的启动速度要比进程的启动速度要快 两个线程的执行速度是一样的 进程与线程的运行速度是没有可比性的 线程共享创建它的进程的内存空间 , 进程的内存是独立的 两个线程共享的数据都是同一份数据 , 两个子进程的数据不是共享的 , 而且数据是独立的 同一个进程的线程之间可以直接交流 , 同一个主进程的多个子进程之间是不可以进行交流 , 如果两个进程之间需要通信 , 就必须要通过一个中间代理来实现 一个新的线程很容易被创建 , 一个新的进程创建需要对父进程进行一次克隆 一个线程可以控制和操作同一个进程里的其他线程 , 线程与线程之间没有隶属关系 , 但是进程只能操作子进程 改变主线程 , 有可能会影响到其他线程的行为 , 但是对于父进程的修改是不会影响子进程 并发与并行 并发 在早期操作系统只有一个处理器 , 所以想达到同时运行多个程序 , 显然是不可能的 , 唯一的办法就是骗自己 , 告诉自己这几个是\"同时\"在运行 , 怎么骗 ? 如下 🌰一 现在你女朋友要你同时做三件事 1.洗衣服 2.洗碗 3.拖地 明显你要同时完成是不可能的,那现在我赋予你超能力,你获得了光速加成,你可以在一瞬间到达洗衣房(厕所吧),厨房,客厅.然后你女朋友就发现了惊悚的一幕 1.你女朋友看向客厅,你正在客厅拖地 2.接着转头看向厨房,你正在洗碗 3.而后转头看向洗衣房,你正在洗衣服 你女朋友就会告诉你:亲爱的,你是不是有分身呀,怎么可以同时做三件事情?我不管你得再分一个分身出来陪我玩,最后你成功的骗了你女朋友 这就是操作系统中 , 单个CPU + 多道技术实现的并发 CPU就是你本人 , 多道技术就是我赋予你的用速度\"同时\"干多件事的能力 🌰二 现在你女朋友已经知道你有超能力了,原来你一下只能干一件事情,她不高兴了,说道:我不管你得同时陪我还得做事情 于是你又想出了一个办法 1.陪女朋友0.25秒 2.洗衣服0.25秒 3.洗碗0.25秒 4.拖地0.25秒 以你女朋友的眼力绝对不可能看出你不在,就这样把1秒钟的时间平摊下来,然后一直循环下去,完美,再一次骗到了你女朋友 这就是分时系统的并发 , 按时间进行分配 并发 , 就是伪并行的 并行 真正的同时运行 , 只有具备多个CPU才能实现 并发事实上就是串行 , 还是一个人在做多个任务 , 而并行则是多个人在做多个任务 . 明显一个人 , 即只有一个执行者同时不可能做两件事的 , 但是并行 , 多个执行者就能够同时做多件事 所以并发与并行 , 就是一瞬间是否能存在多个进程 同步与异步 同步 所谓同步 , 就是在发出一个功能调用时 , 在没有得到结果之前，该调用就不会返回 . 按照这个定义，其实绝大多数函数都是同步调用 . 但是一般而言 , 我们在说同步、异步的时候 , 特指那些需要其他部件协作或者需要一定时间完成的任务 异步 异步的概念和同步相对 , 当一个异步功能调用发出后 , 调用者不能立刻得到结果 . 当该异步功能完成后 , 通过状态、通知或回调来通知调用者 , 如果异步功能用状态来通知 , 那么调用者就需要每隔一定时间检查一次 , 效率就很低(有些初学多线程编程的人 , 总喜欢用一个循环去检查某个变量的值 , 这其实是一 种很严重的错误) . 如果是使用通知的方式 , 效率则很高 , 因为异步功能几乎不需要做额外的操作 . 至于回调函数 , 其实和通知没太多区别 阻塞与非阻塞 阻塞 阻塞调用是指调用结果返回之前，当前线程会被挂起（如遇到io操作）。函数只有在得到结果之后才会将阻塞的线程激活。有人也许会把阻塞调用和同步调用等同起来，实际上他是不同的。对于同步调用来说，很多时候当前线程还是激活的，只是从逻辑上当前函数没有返回而已 非阻塞 非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前也会立刻返回，同时该函数不会阻塞当前线程 小结 对于进程和线程 , 直接阅读《现代操作系统》 一书再好不过了 并发与并行要注意执行顺序的问题 同步与异步针对的是函数/任务的调用方式 , 是否等待结果 阻塞与非阻塞针对的是进程或线程 , 阻塞进程则挂起 , 非阻塞即不挂起 这一篇基本属于纯理论 , 罗哩罗嗦了半天 "},"01-Python/06-并发篇/02-多线程.html":{"url":"01-Python/06-并发篇/02-多线程.html","title":"多线程","keywords":"","body":"Attack on Python - 多线程 🐍 介绍 在上一篇中说了一大堆理论 , 那么现在就开始实践了 先说线程再说进程 , 为什么 ? 因为在Python中有一个Python GIL全局解释器锁 , 这是个什么东西? 最后来说 总之线程和进程都是与操作系统有关的知识 , 所以操作系统基础 , 对于这两节内容的理解会有很大的帮助 Threading Python通过两个标准库_thread (built-in) 和threading提供对线程的支持 , threading对_thread进行了封装 _thread.py ''' This module provides primitive operations to write multi-threaded programs. The 'threading' module provides a more convenient interface. ''' So , 明显我们一般直接使用threading threading模块中提供了Thread , Lock , RLock , Semaphore , Event , Condition , Timer等组件 Thread 参数说明 参数 说明 group 未使用 , 值始终 target 表示调用对象 , 即子线程要执行的任务 name 子线程的名称 args 传入target函数中的位置参数 , 是一个元组 , 参数后必须加逗号 kwargs 表示调用对象的字典 方法说明 方法 说明 Thread.run (self) 进程启动时运行的方法 , 由该方法调用target参数所指定的函数 , 在子类中可以进行重构 , 与线程中一样 Thread.start (self) 启动进程 , start方法就是去帮你调用run方法 Thread.terminate (self) 强制终止线程 , 不会进行任何清理操作 , 使用时需小心其子进程与锁的问题 Thread.join (self, timeout=None) 阻塞调用 , 主线程进行等待 , timeout为超时时间 Thread.is_alive (self) 这个方法在run()方法开始之前返回True , 在run()方法结束之后 , 返回所有活动线程的列表 Thread.isDaemon(self) 判断是否为守护线程 , 返回bool值 Thread.setDaemon(self,daemonic) 将子线程设置为守护线程 , daemonic = daemon Thread.getName(self,name) 获取线程名称 Thread.setName(self,name) 设置线程名称 实例属性说明 属性 说明 Thread.daemon 默认值为False , True则为守护线程 Thread.name 线程的名称 Thread.isAlive 即为is_alive的返回值 Thread.ident 线程标识符 , 没启动则为None 创建线程 Python中使用线程有两种方式 : 函数或者用类来包装线程对象 函数调用 import threading import time # 定义线程要运行的函数 def func(name): print(\"I am %s\" % name) # 为了便于观察,让它睡上2秒 time.sleep(2) # 防止被导入执行两次 if __name__ == '__main__': # 创建一个线程实例,args参数是一个元组,必须加逗号 t1 = threading.Thread(target=func, args=(\"Lyon\",)) # 再创建一个线程实例 t2 = threading.Thread(target=func, args=(\"Kenneth\",)) # 启动线程 t1.start() # 启动另一个线程 t2.start() # 打印线程名 print(t1.getName()) # 打印线程名 print(t2.getName()) ''' 执行结果: I am Lyon I am Kenneth Thread-1 Thread-2 ''' 类继承调用 import threading import time # 继承threading中的Thread类 class MyThread(threading.Thread): # 线程中所需要的参数 def __init__(self, name): # threading.Thread.__init__(self) super().__init__() self.name = name # 重构run方法,注意这个是表示线程活动的方法,必须有 def run(self): print(\"I am %s\" % self.name) time.sleep(2) # 防止被导入执行两次 if __name__ == '__main__': # 创建一个线程实例 t1 = MyThread('Lyon') # 创建另一个线程实例 t2 = MyThread('Kenneth') # 启动线程,调用了类中的run方法 t1.start() # 启动另一个线程 t2.start() # 获取线程名 print(t1.getName()) # 获取线程名 print(t2.getName()) ''' 执行结果: I am Lyon I am Kenneth Lyon Kenneth ''' Thread实例对象的方法 # isAlive(): 返回线程是否活动的。 # getName(): 返回线程名。 # setName(): 设置线程名。 threading模块提供的一些方法： # threading.currentThread(): 返回当前的线程变量。 # threading.enumerate(): 返回一个包含正在运行的线程的list。正在运行指线程启动后、结束前，不包括启动前和终止后的线程。 # threading.activeCount(): 返回正在运行的线程数量，与len(threading.enumerate())有相同的结果。 Join & setDaemon 在说这两个方法之前 , 需要知道主线程与子线程的概念 主线程 : 当一个程序启动时 , 就有一个进程被操作系统创建 , 与此同时一个线程也立刻运行 , 该线程通常叫做程序的主线程 子线程 : 因为程序是开始时就执行的 , 如果你需要再创建线程 , 那么创建的线程就是这个主线程的子线程 主线程的重要性体现在两方面 : 1. 是产生其他子线程的线程 ; 2. 通常它必须最后完成执行比如执行各种关闭作 在Python中线程的一些机制与C/C++不同 , 在C/C++中 , 主线程结束后 , 其子线程会默认被主线程kill掉 . 而在Python中 , 主线程结束后 , 会默认等待子线程结束后 , 主线程才退出 Join 在上面的线程的创建时 , 获取线程名并不是在最后执行的 , 而是遇到sleep阻塞自动切换执行的 , 而sleep(2)则是在最后执行的 , 如果还不明白在看下面一个例子 遇到阻塞自动切换 import threading import time # 定义线程要执行的函数 def run(name): # 打印内容 print(\"I am %s\" % name) # 睡两秒 time.sleep(2) # 睡完继续起来干活 print(\"When I'm done, I'm going to keep talking...\") if __name__ == '__main__': # 创建一个线程实例 lyon = threading.Thread(target=run, args=('Lyon',)) # 创建另一个线程实例 kenneth = threading.Thread(target=run, args=('Kenneth',)) # 启动线程 lyon.start() # 启动另一个线程 kenneth.start() # 我是主线程,我应该最后执行的 print(\"I was the main thread, and I ended up executing\") ''' 执行结果: I am Lyon I am Kenneth I was the main thread, and I ended up executing When I'm done, I'm going to keep talking... When I'm done, I'm going to keep talking... 结果分析: 第一行打印了 I am Lyon,这没问题第一个线程启动了 第二行打印了 I am Kenneth,这就有问题了,这明明是第二个线程中的事情,我擦我的第一个线程都没执行完 第三行打印了 I was the main thread, and I ended up executing,你牛逼把我主线程的事都打印了 睡了两秒,看来是遇到阻塞自动切换了 最后打印了两个线程中的 When I'm done, I'm going to keep talking... ''' 在很多情况下 , 我们需要的是让各个线程执行完毕后 , 才接着往下执行 , 也就是不跳过阻塞 , 就让它等下去 , 这个时候就需要用join了 join : 阻塞调用程序 , 知道join () 方法的线程调用终止 , 才会继续往下执行 上面加上join后 import threading import time def run(name): print(\"I am %s\" % name) time.sleep(2) print(\"When I'm done, I'm going to keep talking...\") if __name__ == '__main__': lyon = threading.Thread(target=run, args=('Lyon',)) kenneth = threading.Thread(target=run, args=('Kenneth',)) lyon.start() lyon.join() kenneth.start() kenneth.join() print(\"I was the main thread, and I ended up executing\") ''' 执行结果: I am Lyon # sleep 2 seconds When I'm done, I'm going to keep talking... I am Kenneth # sleep 2 seconds When I'm done, I'm going to keep talking... I was the main thread, and I ended up executing ''' 程序按照我们的意愿按顺序执行了 setDaemon 无论进程还是线程 , 都遵循 : 守护进程 (线程) 会等待主进程 (线程) 运行完毕后被销毁 对于主进程来说 , 运行完毕指的是主进程代码运行完毕 对于主线程来说 , 运行完毕指的是主线程所在的进程内所有非守护线程统统运行完毕 setDaemon() 与 join() 基本上是相对的 , join会等子线程执行完毕 ; 而setDaemon则不会等 , 只要主线程执行完了 , 我才不管你子线程执没执行完毕 , 统统给我回收 , 这样才能保证进程能正常结束 setDaemon设置守护线程 import threading import time def run(name): print(\"I am %s\" % name) time.sleep(2) print(\"When I'm done, I'm going to keep talking...\") if __name__ == '__main__': lyon = threading.Thread(target=run, args=('Lyon',)) kenneth = threading.Thread(target=run, args=('Kenneth',)) # 设置守护线程,必须在启动前设置 lyon.setDaemon(True) # 启动线程 lyon.start() # 设置守护线程 kenneth.setDaemon(True) kenneth.start() print(\"I was the main thread, and I ended up executing\") ''' 执行结果: I am Lyon I am Kenneth I was the main thread, and I ended up executing 结果说明: 主线程一旦执行完毕,那么守护线程就一并退出,不管被守护线程是否执行完毕 所以lyon和kenneth两个子线程并没有执行完毕,如果在主线程中在加上sleep(5), 即超过子线程阻塞,那么这两个子线程就能执行完毕了 ''' 将主线程设置为守护线程 import threading import time def run(num): print(\"I like num %d\" % num) time.sleep(2) print(\"When I'm done, I'm going to keep talking...\") def main(): for i in range(1, 6): # 创建线程实例 t = threading.Thread(target=run, args=(i,)) # 启动线程 t.start() # 阻塞调用 t.join() if __name__ == '__main__': # 创建一个主线程 m = threading.Thread(target=main, args=[]) # 设置为守护线程 m.setDaemon(True) # 启动线程 m.start() # 等待其子线程执行完毕后,再8秒退出 m.join(timeout=8) ''' 执行结果: I like num 1 When I'm done, I'm going to keep talking... I like num 2 When I'm done, I'm going to keep talking... I like num 3 When I'm done, I'm going to keep talking... I like num 4 结果说明: 子线程并没有执行完毕,主线程退出,守护线程一并退出 ''' Python GIL ''' In CPython, the global interpreter lock, or GIL, is a mutex that prevents multiple native threads from executing Python bytecodes at once. This lock is necessary mainly because CPython’s memory management is not thread-safe. (However, since the GIL exists, other features have grown to depend on the guarantees that it enforces.) ''' 基本意思是说 , 在CPython解释器中 , 同一个进程下开启的多线程 , 同一时刻只能有一个线程执行 , 无法利用多核优势 GIL并不是Python的一种特性 , 它是在实现Python解释器(CPhthon)时引入的一个概念 , 就比如同一段代码可以通过CPython , PyPy , Psyco等不同的Python执行环境来执行 , 像JPython中就没有GIL . 由于CPython是大部分环境下默认的Python执行环境 , 所以在很多人的概念里CPython就是Python , 但是要记住 , GIL并不是Python的特性 , Python完全可以不依赖GIL GIL GIL本质就是一把互斥锁 , 即会将并发运行变成串行 , 以此来控制同一时间内共享数据只能被一个任务进行修改 , 从而保证数据的安全性 保护不同的数据时 , 应该加不同的锁 , GIL是解释器级别的锁 , 又叫做全局解释器锁 CPython加入GIL主要的原因是为了降低程序的开发复杂度 , 让你不需要关心内存回收的问题 , 你可以理解为Python解释器里有一个独立的线程 , 每过一段时间它起wake up做一次全局轮询看看哪些内存数据是可以被清空的 , 此时你自己的程序 里的线程和Python解释器自己的线程是并发运行的 , 假设你的线程删除了一个变量 , py解释器的垃圾回收线程在清空这个变量的过程中的clearing时刻 , 可能一个其它线程正好又重新给这个还没来及得清空的内存空间赋值了 , 结果就有可能新赋值的数据被删除了 , 为了解决类似的问题 , Python解释器简单粗暴的加了锁 , 即当一个线程运行时 , 其它人都不能动 , 这样就解决了上述的问题 , 这可以说是Python早期版本的遗留问题 . 毕竟Python出来的时候 , 多核处理还没出来呢 , 所以并没有考虑多核问题 以上就可以说明 , Python多线程不适合CPU密集型应用 , 但适用于IO密集型应用 Lock 🍀 多线程与多进程最大的不同在于 , 多进程中 , 同一个变量 , 各自有一份拷贝存在于每个进程中 , 互不影响 , 但是在多线程中 , 所有变量对于所有线程都是共享的 , 因此 , 线程之间共享数据最大的危险在于多个线程同时修改一个变量 , 那就乱套了 , 所以我们需要GIL一样 , 来锁住数据 上面说了 , 保护不同的数据 , 要加不同的锁 , GIL是为了保护解释器的数据 , 明显我们还需要保护用户数据的锁 所以为了保证用户数据的安全 , 我们需要另一个锁 , 互斥锁(Mutex) 无锁版本 # 线程的调度是由操作系统决定的,一旦线程交替执行,并且次数足够多,那么就可能出问题了 # 直接用廖大大的例子,地址:www.liaoxuefeng.com import threading # 假定这是你的银行存款: balance = 0 def change_it(n): # 先存后取，结果应该为0: global balance balance = balance + n balance = balance - n def run_thread(n): for i in range(100000): change_it(n) for j in range(10000): t1 = threading.Thread(target=run_thread, args=(5,)) t2 = threading.Thread(target=run_thread, args=(8,)) # 这里跟join的位置有关系,因为join也是可以实现锁的功能的,下面说 t1.start() t2.start() t1.join() t2.join() print(balance,end=\"\") ''' 执行结果: 0 0 5 5 5 # 这里我就只给出5次的结果,因为5次就已经出现错误了 # 正常情况下数据不混乱,结果应该一直为0 ''' 加锁版本 import threading # 假定这是你的银行存款: balance = 0 def change_it(n): # 先存后取，结果应该为0: global balance balance = balance + n balance = balance - n # 创建一把锁 lock = threading.Lock() def run_thread(n): for i in range(100000): # 先要获取锁: lock.acquire() try: # 放心地改吧: change_it(n) finally: # 改完了一定要释放锁: lock.release() for j in range(10000): t1 = threading.Thread(target=run_thread, args=(5,)) t2 = threading.Thread(target=run_thread, args=(8,)) t1.start() t2.start() t1.join() t2.join() print(balance) ''' 执行结果: 0 # 这里的结果一直都是0,So我就只写出一个结果了 ''' join vs lock 上面第一个无锁版本的例子中 , 其实join()就可以实现我们想要的功能 , 只需要各个线程后面不加多余的东西直接接join()就行 , 因为我们知道join()的功能是进行阻塞 , 一加join() , 肯定其他就没有线程能动了 , 上面例子中故意将t1.join() 加在了t2.start()的后面 , 就是为了能让t2\"有机可趁\" , 既然join() 就可以实现 , 那我们还要锁干嘛? 我们应该想想 , join实现的原理 , join会使线程进行阻塞 , 也就是说会让真个线程变成完全串行的 , 既然只有一个线程在进行操作 , 那么它肯定就不会乱 , 但是使用join影响了执行效率 , 所以我们想能不能只让线程中的一部分串行? 答案是能的 , 就是利用互斥锁 , 想让哪里串行就让哪里串行 PS : Python3.x好像会自动加锁 , 但是Python2.x是不会的 , 写的时候还是都加上把 , 保证安全性 RLock RLock叫做递归锁 , 在说之前先说一个死锁问题 进程也有死锁和递归锁 , 所谓死锁 : 是指两个或两个以上的进程或线程在执行过程中 , 因争夺资源而造成的一种互相等待的现象 , 若无外力作用 , 他们都将无法推进下去 . 此时称系统处于死锁状态或系统产生了死锁 , 这些永远在互相等待的进程称为死锁进程 , 如下 import threading import time # 创建两个锁 mutexA = threading.Lock() mutexB = threading.Lock() class MyThread(threading.Thread): # 重构run方法 def run(self): self.func1() self.func2() def func1(self): # 获取锁A mutexA.acquire() print(\"\\033[31m%s get mutexA...\\033[0m\" % self.name) # 获取锁B mutexB.acquire() print(\"\\033[33m%s get mutexB...\\033[0m\" % self.name) # 释放锁B mutexB.release() # 释放锁A mutexA.release() def func2(self): mutexB.acquire() print(\"\\033[35m%s get mutexB...\\033[0m\" % self.name) # 睡1秒 time.sleep(1) mutexA.acquire() print(\"\\033[37m%s get mutexA...\\033[0m\" % self.name) mutexA.release() mutexB.release() if __name__ == '__main__': for i in range(10): t = MyThread() t.start() ''' 执行结果: Thread-1 get mutexA... Thread-1 get mutexB... Thread-1 get mutexB... Thread-2 get mutexA... # 到这里整个程序就永远等着了 结果说明: 首先执行了func1,没有阻塞,顺利执行完毕 然后执行func2,获取了锁B后就开始睡1一秒,也就是阻塞开始 于是系统自动切换,再次执行了func1,而B的锁在阻塞前没释放 最后func1中的mutexB.acquire()就一直等前面一个线程把锁给释放了 等到天荒地老,海枯石烂,也等不到了 ''' 为了解决这样的问题 , 于是就有了递归锁 , 在Python中为了支持在同一线程中多次请求同一资源 , Python提供了可重入锁RLock 这个RLock内部维护着一个Lock和一个counter变量 , counter记录了acquire的次数 , 从而使得资源可以被多次require . 直到一个线程所有的acquire都被release , 其他的线程才能获得资源 RLock版本 # 仅仅只需如下修改 mutexA = threading.Lock() mutexB = threading.Lock() # 以上两行修改为 mutexA = mutexB = threading.RLock() # 注意如果仅仅修改后部分,即将Lock() -> RLock()是不行的,那样等于创建了两把递归锁 queue 我们可以使用队列处理线程编程中多个线程之间交换的安全问题 在queue中有三种模式 , Queue (先进先出 , FIFO) , LifoQueue (后进先出 , LIFO) , 还有一个可以设置优先级的队列PriorityQueue Queue import Queue q = Queue.Queue() q.put('First') q.put('Second') q.put('Third') print(q.get()) print(q.get()) print(q.get()) ''' 执行结果: First Second Third ''' LifoQueue import Queue q = Queue.LifoQueue() q.put('First') q.put('Second') q.put('Third') print(q.get()) print(q.get()) print(q.get()) ''' 执行结果: Third Second First ''' PriorityQueue import Queue q = Queue.PriorityQueue() # put进入一个元组,元组的第一个元素是优先级,越小优先级越高 q.put((20, 'A')) q.put((10, 'B')) q.put((30, 'C')) print(q.get()) print(q.get()) print(q.get()) ''' 执行结果: (10, 'B') (20, 'A') (30, 'C') ''' 更多请阅读Python标准库目录下的queue模块内容 Producer-Consumer 生产者 - 消费者问题 又称有界缓冲区问题 , 在进程中 , 两个进程共享一个公共的固定大小的缓冲区 , 其中一个是生产者 , 将信息放入缓冲区 ; 另一个是消费者 , 从缓冲区取出信息 . 问题在于当缓冲区满时 , 而此时生产者还想向其中放入一个新的数据项的情况 ; 相反 , 当缓冲区为空时 , 消费者视图从缓冲区中取数据 , 该如何去解决? 为了解决这个问题于是引入了生产者和消费者模式 , 基本思路也是如进程中睡眠和唤醒 生产者消费模式 通过一个容器来解决生产者和消费者的强耦合问题 . 生产者与消费者彼此之间不直接通讯 , 而通过阻塞队列来进行通讯 , 所以生产者生产完数据之后不用等待消费者处理 , 直接扔给阻塞队列 , 消费者不找生产者要数据 , 而是直接从阻塞队列里取 , 阻塞队列就相当于一个缓冲区 , 平衡了生产者和消费者的处理能力 在并发编程中使用生产者和消费者模式能解决绝大多数并发问题 , 在线程世界里 , 生产者就是生产数据的线程 , 消费者就是消费数据的线程 . 以下有两个生产者消费者问题的例子 基础版本 import threading import queue def producer(): for i in range(10): # 进行生产,放入队列 q.put(\"%d bottle of milk\" % i) print(\"Start waiting for all the milk to be taken...\") q.join() print(\"All the milk was taken out...\") def consumer(name): # 队列中有就取 while q.qsize() > 0: print(\"%s got %s\" % (name, q.get())) q.task_done() # 创建一个队列对象 q = queue.Queue() p = threading.Thread(target=producer,) p.start() c1 = consumer(\"Lyon\") 生产与消费同时进行 import time import random import queue import threading q = queue.Queue() def Producer(name): count = 1 while count Semaphore 信号量(Semaphore) , 引入一个整型变量来累计线程的唤醒次数 , threading模块中 , 有一个Semaphore类管理一个内置的计数器 , 每当调用acquire()时内置计数器 -1 ;调用release()时内置计数器 +1;计数器不能小于0 ; 当计数器等于0时 , acquire()将阻塞线程知道其他线程调用release() 一次最多连接5个线程 import threading import time def func(): # 内置计数器 -1 sm.acquire() print('%s get semaphores' % threading.current_thread().getName()) time.sleep(2) # 内置计数器 +1 sm.release() if __name__ == '__main__': # 一次最多只能有5个线程获取信号量 sm = threading.Semaphore(5) for i in range(10): t = threading.Thread(target=func) t.start() 利用信号量可以解决生产者与消费者问题 , 《现代操作系统中》一书中进行了简单的实现 Event 在多线程中 , 每个线程都是互相独立的 , 互不影响 , 如果我们需要通过某个线程的状态来控制程序的执行过程 , 是非常难的 . 为了解决这些问题 , 我们就可以使用threading中的Event对象来实现我们的目的 Event对象中包含一个可由线程设置的信号标志 , 它允许线程等待某些事件的发生 . 在初始情况下 , Event对象中的信号标志被设置为假 ; 如果有线程等待一个Event对象 , 而这个Event对象的标志为假 , 那么这个线程将会被一直阻塞直至该标志为真 . 一个线程如果将一个Event对象的信号标志设置为真 , 它将唤醒所有等待这个Event对象的线程 . 如果一个线程等待一个已经被设置为真的Event对象 , 那么它将忽略这个事件 , 继续执行 方法 描述 Event.isSet() 返回Event的状态 , isSet == is_set Event.wait() 如果Event.isSet() == False将阻塞线程 Event.set() 设置Event的状态值为True , 所有阻塞池中的线程激活进入就绪状态 , 等待操作系统调度 Event.clear() 回复Event的状态值为False 解决重复连接问题 import threading import time import random def conn_mysql(): count = 1 while not event.is_set(): # 大于3次主动触发TimeoutError if count > 3: raise TimeoutError('Connection timeout...') print('%s %sth attempt to connect' % (threading.current_thread().getName(), count)) # 阻塞0.5秒 event.wait(0.5) count += 1 print('%s connect successfully' % threading.current_thread().getName()) def check_mysql(): print('%s is checking mysql' % threading.current_thread().getName()) time.sleep(random.randint(2, 4)) # 激活线程 event.set() if __name__ == '__main__': event = threading.Event() conn1 = threading.Thread(target=conn_mysql) conn2 = threading.Thread(target=conn_mysql) check = threading.Thread(target=check_mysql) conn1.start() conn2.start() check.start() Condition 使线程等待 , 只有满足条件时 , 才释放线程 import threading def condition_func(): ret = False inp = input('>>>') # 只有当inp等于1时才会执行 if inp == '1': ret = True return ret def run(n): con.acquire() con.wait_for(condition_func) print(\"run the thread: %s\" %n) con.release() if __name__ == '__main__': con = threading.Condition() for i in range(10): t = threading.Thread(target=run, args=(i,)) t.start() Timer threading模块中还有一个Timer类 , 可以指定时间后执行某操作 import threading def hello1(): print(\"I am Lyon\") def hello2(): print(\"Hello, future\") # 1秒后执行 t1 = threading.Timer(1, hello1) # 两秒后执行 t2 = threading.Timer(2,hello2) t1.start() t2.start() "},"01-Python/06-并发篇/03-多进程.html":{"url":"01-Python/06-并发篇/03-多进程.html","title":"多进程","keywords":"","body":"Attack on Python - 多进程 🐍 介绍 上一篇 多线程 中已经对Python中多线程部分进行了整理 , 进程中有很多也是相似的 概念在并发编程第一篇中就已经介绍了 , So直接开始操作 multiprocessing 从上一篇我们也已经知道了 , Python中的多线程无法利用多核优势 , 所以如果我们想要充分地使用多核CPU的资源 , 那么就只能靠多进程了 , 因为进程是系统调度的 , Python提供了multiprocessing模块了对多进程的支持 multiprocessing模块中提供了Process , Queue , Pipe , Lock , RLock , Event , Condition等组件 , 与threading模块有很多相似之处 Process 用于创建进程的类 , 与threading模块中的_Thread类类似 ''' Process类的构造函数 def __init__(self, group=None, target=None, name=None, args=(), kwargs={}): ''' 参数说明 参数 说明 group 未使用 , 值始终 target 与threading.Tread中的target参数一样 , 表示调用对象 , 即子进程要执行的任务 name 子进程的名称 args 传入target函数中的位置参数 , 是一个元组 , 与线程一样 , 参数后必须加逗号 kwargs 表示调用对象的字典 方法说明 方法 说明 Process.run (self) 进程启动时运行的方法 , 由该方法调用target参数所指定的函数 , 在子类中可以进行重构 , 与线程中一样 Process.start (self) 启动进程 , start方法就是去帮你调用run方法 Process.terminate (self) 强制终止进程 , 不会进行任何清理操作 , 使用时需小心其子进程与锁的问题 Process.join (self, timeout=None) 与线程中一样 , 阻塞调用 , 主进程进行等待 , timeout为超时时间 Process.is_alive (self) 判断进程是否正在运行 , 返回bool值 实例属性说明 属性 说明 Process.daemon 默认值为False , True则为守护进程 Process.name 进程的名称 Process.pid 进程的pid Process.exitcode 进程运行时为None , 如果为-N , 表示被信号N结束 Process.authkey 进程的身份验证键 , 默认是由os.urandom()随机生成的32字符的字符串 . 这个键的用途是为涉及网络连接的底层进程间通信提供安全性 , 这类连接只有在具有相同的身份验证键时才能成功 创建进程 与创建线程的方式一样 , 有两种 函数调用 import multiprocessing import time def hello(name): print(\"I am %s\" % name) time.sleep(1) print(\"Hello future...\") if __name__ == '__main__': # 创建一个进程实例 p = multiprocessing.Process(target=hello, args=(\"Lyon\",)) # 启动进程,实质调用run() p.start() print(\"End of main process...\") ''' 执行结果: End of main process... I am Lyon Hello future... ''' 类继承调用 import multiprocessing import time # 自定义进程类,继承multiprocessing中的Process类 class MyProcess(multiprocessing.Process): def __init__(self, name): super().__init__() self.name = name # 重构父类中的run方法 def run(self): print(\"I am %s\" % self.name) time.sleep(1) print(\"Hello future...\") if __name__ == '__main__': # 创建一个进程实例 p = MyProcess('Lyon') # 启动进程 p.start() print(\"End of main process...\") ''' 执行结果: End of main process... I am Lyon Hello future... ''' 在上栗创建进程中有一个问题 , 就是如果我们在Windows下 , 使用start()方法 , 就必须加上if __name__ == '__main__': , 进程是通过fork系统调用 , 而Windows中并没有fork , 所以多处理模块启动了一个新的Python进程 , 并导入了调用模块 . 如果进程在导入的时候被调用 , 那么这就会引发无限的新进程 , 后果不言而喻 . 当然还是可以直接使用run()的 Join & Daemon join 进程中join与线程中的join是一样的 , 就进行阻塞调用 , 让主进程进行等待 , 整体串行 实例 # 多线程中的例子,换汤不换药 import multiprocessing import time def run(name): print(\"I am %s\" % name) time.sleep(2) print(\"When I'm done, I'm going to keep talking...\") if __name__ == '__main__': lyon = multiprocessing.Process(target=run, args=('Lyon',)) kenneth = multiprocessing.Process(target=run, args=('Kenneth',)) lyon.start() lyon.join() kenneth.start() kenneth.join() print(\"I was the main thread, and I ended up executing\") ''' 执行结果: I am Lyon When I'm done, I'm going to keep talking... I am Kenneth When I'm done, I'm going to keep talking... I was the main thread, and I ended up executing ''' Daemon 守护进程会在主进程代码执行结束后就终止 # 还是多线程中的例子 import multiprocessing import time def run(num): print(\"I like num %d\" % num) time.sleep(2) print(\"When I'm done, I'm going to keep talking...\") def main(): for i in range(1, 6): t = multiprocessing.Process(target=run, args=(i,)) t.daemon = True t.start() t.join() if __name__ == '__main__': m = multiprocessing.Process(target=main, args=[]) m.start() m.join(timeout=8) ''' 执行结果: I like num 1 When I'm done, I'm going to keep talking... I like num 2 When I'm done, I'm going to keep talking... I like num 3 When I'm done, I'm going to keep talking... I like num 4 When I'm done, I'm going to keep talking... I like num 5 When I'm done, I'm going to keep talking... ''' PS : 与线程不同的是 , 守护进程内无法再开启子进程 , 否则就抛出异常 Lock 进程之间的数据是不共享的 , 因为每个进程之间是相互独立的 , 但是进程共享一套文件系统 , 所以访问同一个文件 , 是没有问题的 , 但是如果有多个进程对同一文件进行修改 , 就会造成错乱 , 所以我们为了保护文件数据的安全 , 就需要给其进行加锁 同样的 , join为整体串行 , lock为局部串行 廖大大实例 , Lock import multiprocessing # 假定这是你的银行存款: balance = 0 def change_it(n): # 先存后取，结果应该为0: global balance balance = balance + n balance = balance - n # 创建一把锁 lock = multiprocessing.Lock() def run_thread(n): for i in range(100000): # 先要获取锁: lock.acquire() try: # 放心地改吧: change_it(n) finally: # 改完了一定要释放锁: lock.release() # 在多线程例子中并没有写这句,但是多进程中使用start()必须加 if __name__ == '__main__': for j in range(10000): t1 = multiprocessing.Process(target=run_thread, args=(5,)) t2 = multiprocessing.Process(target=run_thread, args=(8,)) t1.start() t2.start() t1.join() t2.join() print(balance) ''' 执行结果: 0 . # 数据安全得到了保障,所以全为0 ... RLock import multiprocessing import time mutexA = mutexB = multiprocessing.RLock() class MyThread(multiprocessing.Process): def run(self): self.func1() self.func2() def func1(self): mutexA.acquire() print(\"\\033[31m%s get mutexA...\\033[0m\" % self.name) mutexB.acquire() print(\"\\033[33m%s get mutexB...\\033[0m\" % self.name) mutexB.release() mutexA.release() def func2(self): mutexB.acquire() print(\"\\033[35m%s get mutexB...\\033[0m\" % self.name) time.sleep(1) mutexA.acquire() print(\"\\033[37m%s get mutexA...\\033[0m\" % self.name) mutexA.release() mutexB.release() if __name__ == '__main__': for i in range(10): t = MyThread() t.start() Producer-consumer 生产者消费者模式 , 在多线程中已经有过说明了 , 目的是为了解决并发问题 实例 # 可与多线程篇中进行对照 import time import random import multiprocessing q = multiprocessing.Queue() def Producer(name, q): count = 1 while count Queue multiprocessing模块支持进程间通信有两种主要形式 , 队列和管道 在多线程中有queue模块 , 供我们实现队列接口 , 在多进程中则是Queue类为我们提供队列接口 Queue为单向通道 , 先进先出(FIFO) class Queue(object): def __init__(self, maxsize=-1): self._maxsize = maxsize # 返回队列中目前项目数量,使用时防止竞争,最好令其串行 def qsize(self): return 0 # 队列是否为空,返回True,使用时防止竞争,最好令其串行 def empty(self): return False # 队列是否已满,返回True,使用时防止竞争,最好令其串行 def full(self): return False # 将数据放入队列 def put(self, obj, block=True, timeout=None): pass # 同上put def put_nowait(self, obj): pass # 从队列中取出项 def get(self, block=True, timeout=None): pass # 同上get def get_nowait(self): pass # 关闭队列,垃圾回收会调用此方法 def close(self): pass # 连接队列的后台线程,用于等待所有队列项消耗 def join_thread(self): pass # 不会在在进程退出时自动连接后台线程,可防止join_thread()方法阻塞 def cancel_join_thread(self): pass 实例 import multiprocessing q = multiprocessing.Queue(3) q.put(\"First\") q.put(\"Second\") q.put(\"Third\") print(q.full()) print(q.get()) print(q.get()) print(q.get()) print(q.empty()) ''' 执行结果: True First Second Third True ''' Pipe 介绍 # Pipe在进程之间创建一条管道,并返回元组(connection(),connection()) def Pipe(duplex=True): return Connection(), Connection() # 管道端的连接对象 class Connection(object): # 发送对象 def send(self, obj): pass # 接收另一端发送的对象 def recv(self): pass # 返回连接使用的整数文件描述符 def fileno(self): return 0 # 关闭链接 def close(self): pass # 如果链接上的数据可用,返回True def poll(self, timeout=None): pass # 发送字节到数据缓冲区,buffer是支持缓冲区接口的任意对象,offset为偏移量,size为字节数 def send_bytes(self, buffer, offset=-1, size=-1): pass # 接收一条完整字节消息 def recv_bytes(self, maxlength=-1): pass # 接收一条完整的字节消息,并把它保存在buffer对象中,该对象支持可写入的缓冲区接口 def recv_bytes_into(self, buffer, offset=-1): pass ''' Connection类与我们网络编程中所使用的socket(TCP)类似,socket(TCP)对象之间通信也是双向的 ... 基于管道实现进程间通信 import multiprocessing def producer(seq, p): left,right = p # 关闭不使用的一端 right.close() for i in seq: # 发送进管道中 left.send(i) else: # 关闭管道 left.close() def consumer(p, name): left,right = p # 关闭不使用的一端 left.close() while True: # 如果消费者不使用的一端忘记关闭,消费者中的recv()就一直等下去 try: bun = right.recv() print('%s got %s buns...' % (name, bun)) # 触发EOFError except EOFError: right.close() break if __name__ == '__main__': # 创建管道实例 left, right = multiprocessing.Pipe() c1 = multiprocessing.Process(target=consumer, args=((left, right), 'c1')) c1.start() seq = (i for i in range(10)) producer(seq, (left, right)) right.close() left.close() c1.join() print('End of main process...') Manager 进程之间是相互独立的 , 在multiprocessing模块中的Manager可以实现进程间数据共享 , 并且Manager还支持进程中的很多操作 , 比如Condition , Lock , Namespace , Queue , RLock , Semaphore等 由于基于消息传递(Queue , Pipe)的并发编程才是未来的主流 , 所以对于Manager应该尽量避免使用 Manager实例 import multiprocessing # 既然数据共享了,就需要像多线程那样,防止竞争 def run(d,lock): # 演示没加锁的实例 # lock.acquire() d['count'] -= 1 # lock.release() if __name__ == '__main__': # lock = multiprocessing.Lock() with multiprocessing.Manager() as m: dic = m.dict({'count' : 100}) process_list = [] for i in range(100): p = multiprocessing.Process(target=run, args=(dic, lock,)) process_list.append(p) p.start() for p in process_list: p.join() print(dic) ''' 执行结果: # 该结果看缘分了,没加锁数据共享,导致混乱,与线程中一样 {'count': 1} ''' 更多详细内容multiprocessing.Manager > Semaphore 与线程中一样 class Semaphore(object): def __init__(self, value=1): pass def acquire(self, blocking=True, timeout=None): pass def release(self): pass 实例 import multiprocessing import time def func(sem, num): sem.acquire() print('%s get semaphores' % num) time.sleep(2) sem.release() if __name__ == '__main__': sem = multiprocessing.Semaphore(5) for i in range(1,11): t = multiprocessing.Process(target=func, args=(sem, i,)) t.start() Event 与线程中一样 class Event(object): def is_set(self): return False def set(self): pass def clear(self): pass def wait(self, timeout=None): pass 实例 import multiprocessing import time import random def conn_mysql(conn, event): count = 1 while not event.is_set(): if count > 3: # 主动触发超时异常 raise TimeoutError('Connection timeout...') print('%s %sth attempt to connect' % (conn, count)) event.wait(0.5) count += 1 print('%s connect successfully' % conn) def check_mysql(conn, event): print('%s is checking mysql' % conn) time.sleep(random.randint(2, 4)) event.set() if __name__ == '__main__': event = multiprocessing.Event() for i in range(10): conn = multiprocessing.Process(target=conn_mysql, args=('conn'+str(i), event)) conn.start() Pool multiprocessing中的Process实现了我们对多进程的需求 , 但是当我们进行并发编程时 , 一旦需要开启的进程数量非常大时 , 使用Process已经不能满足我们的要求了 . 因为进程是需要占用系统资源的 , 操作系统不可能去无限的开启进程 ; 并且使用Process动态生成多个进程 , 我们还需要手动的去限制进程的数量 , 所以这个时候我们就应该用进程池(Pool)来实现了 multiprocessing.Pool 参数说明 参数 说明 numprocess 要创建的进程数 , 如果省略 将默认使用cpu_count() initializer 每个进程启动时要执行的可调用对象 initargs 传给initializer的参数组 方法说明 方法 说明 Pool.apply(self, func, args=(), kwds={}) 在一个进程池中执行func(args , *kwargs) , 并返回结果 Pool.apply_async(self, func, args=(), kwds={}, callback=None, 与apply()方法一样 , 该方法为异步版本应用的方法 , 返回结果是AsyncResult类的实例 , callback指定回调的函数 . callback禁止执行任何阻塞操作 , 否则将接收其他异步操作中的结果 Pool.close(self) 关闭进程池 , 如果所有操作持续挂起 , 它们将在工作进程终止前完成 Pool.join(self) 等待所有工作进程退出 Pool.get(self, timeout=None) 获取结果 , timeout可选 Pool.ready(self) 完成调用就返回True Pool.successful(self) 完成调用并且没有引发异常返回True , 在结果就绪之前调用此方法会引发异常 Pool.wait(self, timeout=None) 等待结果变为可用 Pool.terminate(self) 立即终止所有工作进程 , 垃圾回收会自动调用此方法 同步调用apply from multiprocessing import Pool import os import time def run(n): print(\"%s run...\" % os.getpid()) # 不令其阻塞,结果会同时打印 time.sleep(2) return n**2 if __name__ == '__main__': # 进程池没满就新创建进程执行请求,否则就等待 # 注意,这里指定进程池数量为3,会一直是这三个进程在执行,只不过执行的请求可能改变 pool = Pool(3) res_list = [] for i in range(10): # 获取执行结果,同步运行,会阻塞等待拿到结果,等待过程中无论是否阻塞都会在原地等 # 注意等待过程中由于阻塞,其cpu权限会被夺走 res = pool.apply(run, args=(i,)) res_list.append(res) print(res_list) 异步调用apply_async from multiprocessing import Pool import os import time def run(n): print(\"%s run...\" % os.getpid()) time.sleep(2) return n**2 if __name__ == '__main__': # 进程池没满就新创建进程执行请求,否则就等待 # 注意,这里指定进程池数量为3,会一直是这三个进程在执行,只不过执行的请求可能改变 pool = Pool(3) res_list = [] for i in range(10): res = pool.apply_async(run, args=(i,)) res_list.append(res) pool.close() pool.join() for res in res_list: print(res.get()) "},"01-Python/06-并发篇/04-多进程实例及回调函数.html":{"url":"01-Python/06-并发篇/04-多进程实例及回调函数.html","title":"多进程实例及回调函数","keywords":"","body":"Attack on Python - 多进程实例及回调函数 🐍 进程池实例 使用进程池维护固定数目的进程 server.py import socket import os import multiprocessing server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server.bind(('127.0.0.1', 8080)) server.listen(5) def talk(conn, client_addr): print(\"Process pid : %s\" % os.getpid()) while True: try: msg = conn.recv(1024) if not msg:break conn.send(msg.upper()) except Exception: break if __name__ == '__main__': pool = multiprocessing.Pool() while True: conn, client_addr = server.accept() # 同步则一时间只有一个客户端能访问,所以使用异步 pool.apply_async(talk,args=(conn, client_addr,)) client.py import socket client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client.connect(('127.0.0.1', 8080)) while True: msg = input(\"Please input message:\").strip() if not msg: continue client.send(msg.encode('utf-8')) data = client.recv(1024) print(data.decode('utf-8')) 回调函数 回调函数就是一个通过函数指针调用的函数 , 如果你把函数的指针(地址)作为参数传递给另一个函数 , 当这个指针被用来调用其所指向的函数时 , 我们就说这是回调函数 回调函数不是由该函数的实现方直接调用 , 而是在特定的事件或条件发生时由另外的一方调用的 , 用于对该事件或条件进程响应 进程池中使用回调函数 apply_async(func[, args[, kwds[, callback[, error_callback]]]]) If callback is specified then it should be a callable which accepts a single argument. When the result becomes ready callback is applied to it, that is unless the call failed, in which case the error_callback is applied instead. ''' 意思是如果指定了回调,那么它应该是可调用的,调用失败则会应用error_callback ''' 实例 import multiprocessing import requests import os def get_page(url): print('Process %s get %s...' % (os.getpid(), url)) respone = requests.get(url) if respone.status_code == 200: return {'url': url, 'text': respone.text} # 进行回调的函数,处理结果 def pasrse_page(res): print('Process %s parse %s...' % (os.getpid(), res['url'])) parse_res = 'url : %s\\nsize : %s\\n' % (res['url'], len(res['text'])) with open('db.txt', 'a') as f: f.write(parse_res) if __name__ == '__main__': urls = [ 'https://www.baidu.com', 'https://www.python.org', 'https://www.openstack.org', 'https://help.github.com/', 'http://www.sina.com.cn/' ] p = multiprocessing.Pool(3) res_list = [] for url in urls: # 执行并返回结果,异步, res = p.apply_async(get_page, args=(url,), callback=pasrse_page) res_list.append(res) p.close() p.join() # 拿到的是get_page的结果,其实完全没必要拿该结果,该结果已经传给回调函数处理了 print([res.get() for res in res_list]) 处理结果db.txt url : https://www.openstack.org size : 60191 url : https://www.python.org size : 49081 url : https://www.baidu.com size : 2443 url : https://help.github.com/ size : 118622 url : http://www.sina.com.cn/ size : 601426 爬虫案例 from multiprocessing import Pool import requests import re def get_page(url, pattern): response = requests.get(url) if response.status_code == 200: print(response.text) return (response.text,pattern) def parse_page(info): page_content, pattern = info res=re.findall(pattern, page_content) for item in res: dic={ 'index' : item[0], 'title' : item[1], 'actor' : item[2].strip()[3:], 'time' : item[3][5:], 'score' : item[4]+item[5] } print(dic) if __name__ == '__main__': pattern1=re.compile(r'.*?board-index.*?>(\\d+)(.*?)(.*?)(.*?)(.*?) "},"01-Python/06-并发篇/05-协程.html":{"url":"01-Python/06-并发篇/05-协程.html","title":"协程","keywords":"","body":"Attack on Python - 协程 🐍 介绍 协程 (Coroutine) , 就是一组可以协调工作 (协作式) 的子程序 (函数) 协程的本质就是一组函数 , 一组协同工作的函数 线程和协程的区别 相对于线程而言 , 线程是抢占式的 , 它的调度方案是由操作系统控制的 , 而协程是协作式或者说非抢占式的 , 由程序自己主动让出处理器 , 所以协程会更加的灵活 ; 并且线程是昂贵的 , 线程上下文切换的成本要高于协程上下文切换 , 而且协程与 CPU 和操作系统通常没有关系 , 所以没有理论上限 , 也就是说 , 协程要比线程轻得多 , 这也是为什么协程又被叫做 \"微线程\" \"微线程\" 只是用来说明协程比线程要轻量级 , 但是协程和线程完全是两个概念 , 说白了协程只是一组函数 , 而线程是操作系统的内核对象 在历史上是先有的协程 , 后有的线程 , 协程出现的目的是为了实现并发 , 而线程则是为了并行 , 也就是线程可以利用多核优势 (当然在 Python 中由于 GIL 锁线程同样无法利用多核优势) , 而协程无法利用多核优势 , 并且非抢占式调度的公平性是一个很大的问题 , 所以相对而言协程都没参与多核 CPU 并行处理 , 而线程可以利用多核达到真正的并行计算 , 这两者的差距就不言而喻了 , 这也是为什么后来线程比协程要更加广泛 ; 而到了现代 , 协程更多的是用来做成一组执行队列 , 比如迭代器 , 事件循环等等 那为什么协程现在被网络上神化了呢 ? 接着往下看 要实现协程 , 需要实现中断 , 恢复 , 切换上下文这三项功能 , 在实现之前 , 我们先说说生成器 生成器 生成器是一次生成一个值的特殊类型函数 , 它可以进行惰性求值 , 因为生成器每次调用都只生成一个值 , 所以他不需要提前将数据加载到内存 在 Python 中我们可以通过 yield 来定义一个生成器 , yield 语句会把你需要的值返回给调用生成器的地方 , 后退出函数 下一次调用生成器函数的时候又从上次中断的地方开始执行 , 而生成器内的所有变量参数都会被保存下来供下一次使用 , 这就是生成器实现的原理 生成器和协程的关系 生成器和协程的区别就是它们都可以挂起自身的执行 , 或者说拥有中断能力 , 但是协程除了中断能力 , 还拥有控制能力 , 协程可以控制在它让位之后哪个协程立即续它来执行 , 而生成器不能 , 生成器只能把控制权转交给调用生成器的调用者 , 所以生成器 , 也叫做 \"半协程\" , 是协程的子集 实现协程 在 Python 的早期版本里 , 我们可以通过 yield 以及 send 方法来实现协程 import time from queue import Queue q = Queue() def produce(consumer): count = 0 while True: while not q.qsize(): count += 1 q.put(count) time.sleep(1) print('[PRODUCER] Producing %s...' % count) # 恢复 consumer.send(count) def consumer(): while True: while q.qsize(): count = q.get() time.sleep(1) print('[CONSUMER] Consuming %s...' % count) # 中断 yield consumer = consumer() # 初始化生成器 next(consumer) produce(consumer) 一个协程作为生产者 , 一个协程作为消费者 , 这样我们就实现了一个简单的多任务生产者消费者模型 , 生产者消费者协同工作自动切换 , yield 来中断执行 , send 来恢复执行 , 而代码逻辑控制了上下文的切换 , 这个例子只是为了证明协程的实用性 你可能会发现 , 把上面的代码按照 yield 拆分成几个函数功能上是一样的 , 我们把拆分的函数叫做子例程 , 实际上 , 子例程可以看做是特定状态的协程 , 任何的子例程都可以转写成不使用 yield 的协程 子例程和协程的区别 相对于子例程而言 , 协程更加灵活 , 协程更加适合用来实现彼此比较熟悉的程序组件 , 或者说耦合度高一点的组件 , 比如 : 协作式多任务、异常处理、事件循环、迭代器、无限列表和管道) 协程的切换概念是 \"让步\" , 而子例程的切换概念是 \"出产\" , 一个主动 , 一个被动 , 以下摘自 Wiki : 子例程可以调用其他子例程 , 调用者等待被调用者结束后继续执行 , 故而子例程的生命期遵循后进先出 , 即最后一个被调用的子例程最先结束返回 , 协程的生命期完全由对它们的使用需要来决定 子例程的起始处是惟一的入口点 , 每当子例程被调用时，执行都从被调用子例程的起始处开始 , 协程可以有多个入口点 , 协程的起始处是第一个入口点 , 每个 yield 返回出口点都是再次被调用执行时的入口点 子例程只在结束时一次性的返回全部结果值 , 协程可以在 yield 时不调用其他协程 , 而是每次返回一部分的结果值 , 这种协程常称为生成器)或迭代器 所以到这里 , 协程的应用并没有线程那么广泛 , 可能也并没有想象中那么强大 , 而且协程是在单线程下的 , 只要一处阻塞那么整个协程全部都得阻塞 , 并且 IO 是系统调用 , 这个不是用户态能处理的 , 协程无法绕开 以上就是 \"纯协程\" 了 , 所以综上 , 协程的性能是比不过线程的 , 所以遇到 IO 正确的操作应该是使用多线程 , 不会一堵全堵 , 但是线程的调度算法是比较僵硬的 , 时间片的算法无法准确地识别线程是否正在等待 IO , 从而造成了很多空等的 CPU 资源 , 所以我们应该使用像 epoll 这种异步回调的方式 , 让我们来看看异步回调的代码是怎么写的 : 有3个 IO 操作按顺序执行 , 先执行 select_data , 耗时 1 秒 , 随后执行 update_data , 耗时 0.5 秒 , 最后再执行 delete_data , 耗时 0.3 秒 import time # 3个 IO 操作顺序执行, 顺序如下: select_data, update_data, delete_data # 功能函数 def select_data(callback): def callback_for_select(): time.sleep(1) result = 'select_data_result\\n' return callback(result) # 模拟IO回调 return callback_for_select() def update_data(select_result, callback): def callback_for_update(): time.sleep(0.5) result = select_result + 'update_data_result\\n' return callback(result) # 模拟IO回调 return callback_for_update() def delete_data(update_result, callback): def callback_for_delete(): time.sleep(0.3) result = update_result + 'delete_data_result' return callback(result) # 模拟IO回调 return callback_for_delete() # 我们的调用代码 def select_callback(select_result): def update_callback(update_result): def delete_callback(delete_result): result = delete_result return result return delete_data(update_result, delete_callback) return update_data(select_result, update_callback) result = select_data(select_callback) print(result) # 运行结果 \"\"\" select_data_result update_data_result delete_data_result \"\"\" 不难发现 , 异步回调实际上就是一组子例程协同工作的过程 , 只不过它的切换由我们注册的回调函数来控制 , 上面这段代码中 , 通过闭包来保存上下文 , 为了能让这段代码跑起来 , 我们这里就通过调用回调函数来模拟 IO 事件的回调 上面这段代码 , 如果我们使用同步的方式 , 会是这样的 : # 功能函数 def select_data(): return 'select_data_result\\n' def update_data(select_data_result): return select_data_result + 'update_data_result\\n' def delete_data(update_data_result): return update_data_result + 'delete_data_result' # 我们的调用函数 select_result = select_data() update_result = update_data(select_result) delete_result = delete_data(update_result) 异步和同步在代码的可读性上差别还是相当的大的 , 异步回调的代码实现相当的复杂 , 而且很容易遇到 callback hell , 而在上面我们已经知道了 , 子例程可以看作是特定的协程 , 任何子例程都可以转写为不调用 yield 的协程 , 如下 : def select_data(): # 模拟IO回调 yield 'select_data_result\\n' def update_data(select_result): # 模拟IO回调 yield select_result + 'update_data_result\\n' def delete_data(update_result): # 模拟IO回调 yield update_result + 'delete_data_result' def main(): select_result = next(select_data()) update_result = next(update_data(select_result)) delete_result = next(delete_data(update_result)) return delete_result main() 这里简单的解释一下 , 因为代码无法体现出 IO 的异步回调 , 所以在异步回调的版本中通过 callback_for_xx() 进行模拟 , 而这个 yield 的版本中就是通过 yield 进行模拟 , 另外不管是操作系统的切换(线程切换) , 还是我们自己控制的切换(协程切换) , 都是切换出当前的执行线让 CPU 去做别的事情 到这里我们直接对比 , 明显协程的方式的实现代码要比异步回调方式的实现代码可读性要高得多 , 没有了回调噩梦 这也是我们为什么要使用协程的原因 , 可以更好的和 异步IO 结合 , 如果用一句话概括的话 : 让原来要使用异步回调方式写的非人类代码 ,可以用看似同步的方式写出来 还有一点要说明的是 , 现在网络上的 \"协程\" 其实不只是 \"协程\" , 你在上面可以看到我有写过 \"纯协程\" ; 网络上的协程实际上是协程和一些组件的结合体 , 因为协程本质就是一组协同工作的程序 , 举个典型例子 , IO 阻塞就不是协程能处理的 , 而是协程 + epoll 的结果 , 而且有的协程库还融合了多线程来实现 使用协程 到这里 , 协程的概念已经讲完了 , 那么协程要怎么去使用呢 ? 在 Python 3.4 引入了 asyncio 对异步 IO 的支持 , 而在 Python 3.5 引入了 async/await 两个关键字提供了对无栈协程(见后文)的支持 协程的使用有个很大的问题 , 那就是我们要如何去控制调度 , 有一个好的想法就是我们可以弄一个任务队列 , 然后再跑一个死循环 , 切换就把当前任务追到队列的尾部 , 再从头部取一个任务 , 直到所有任务完成 , 当然它还要你应该具备遇到时钟阻塞 , IO 切换的功能 , 它就是事件循环 我们先看看已有的 asyncio 怎么去编写异步代码 : import asyncio # 功能函数 async def select_data(): await asyncio.sleep(1) return 'select_data_result\\n' async def update_data(select_data_result): await asyncio.sleep(0.5) return select_data_result + 'update_data_result\\n' async def delete_data(update_data_result): await asyncio.sleep(0.3) return update_data_result + 'delete_data_result' # 调用函数 async def main(): select_result = await select_data() update_result = await update_data(select_result) delete_result = await delete_data(update_result) return delete_result print(asyncio.get_event_loop().run_until_complete(main())) async 用来定义一个协程 , await 则是用来切换上下文 , 最后利用 asyncio.get_event_loop 获取事件循环来完成我们的任务 事件循环 有栈协程与无栈协程 "},"01-Python/06-并发篇/06-IO多路复用.html":{"url":"01-Python/06-并发篇/06-IO多路复用.html","title":"IO多路复用","keywords":"","body":"Attack on Python - IO多路复用 🐍 前言 在网络编程中 , 如果服务端需要面临同时接收上千甚至上万次的客户端请求 , 利用 \"进程池\" 或 \"线程池\" 或许可以缓解部分压力 , 但是并不是一个好的选择 , 因为超过数量还是得等 ; 又或者线程一旦进行堵塞 ; 以及任务之间的高度独立 , 并不需要互相通信或者等待时 , 我们就需要用到I/O多路复用(IO Multiplexing) 了 , 又叫做事件驱动IO (Event driven IO) I/O多路复用 I/O多路复用是指单个线程中 , 通过记录跟踪每个I/O流(sock)的状态 , 来同时管理多个I/O流 在I/O多路复用中只要一遇到IO就注册一个事件 , 然后主程序就可以继续干其他的事情了 , 直到IO处理完毕 , 继续恢复之前中断的任务 , 也就是说一个线程可以同时处理多个请求 举🌰 在UI编程中 , 常常要对鼠标点击进行响应 , 还要同时对键盘敲击也进行响应 多进程多线程方式 : 创建一个进程 , 进程中由两个线程 , 一个循环检测鼠标点击 , 一个循环检测键盘敲击 , 一旦检测到有情况就再开一个线程去处理 , 然后一直开下去......基本上是由创建进程/线程 , 维护进程/线程来解决的 , 这样对于CPU的资源是很浪费的 IO多路复用(事件驱动) : 创建一个事件(消息)队列 , 鼠标点击就往队列中增加一个鼠标点击事件 , 键盘敲击就往队列中增加一个键盘敲击事件 , 创建一个线程(IO线程)负责不断从队列中取出事件 , 根据不同的事件 , 调用不同的函数 , 如onClick() , onKeyDown()等 , 即一个线程解决了所有事件的问题 , 这就是复用 比较 : 与多进程多线程技术相比 , I/O多路复用最大的优势是系统开销小 , 系统不必创建进程/线程 , 也不必维护这些进程/线程 , 从而大大减小了系统的开销 目前常见支持I/O多路复用的系统调用select , poll , epoll ,I/O多路复用就是通过一种机制 , 一个进程可以监视多个描述符 , 一旦某个描述符就绪(一般是读就绪或者写就绪) , 能够通知程序进行相应的读写操作 而I/O多路复用的具体实现就是 , select , poll , epoll Select select 监视的文件描述符(FD)分3类 , 分别是writefds、readfds和exceptfds , 程序启动后select函数会阻塞 , 直到有描述符就绪(有数据 可读、可写、或者有except) , 或者超时(timeout指定等待时间 , 如果立即返回设为null即可) , 函数返回 , 当select函数返回后 , 可以通过遍历fdset , 来找到就绪的描述符 I/O多路复用概念被提出来后 , select是第一个实现的 , select虽然实现了I/O多路复用 , 但是暴露出了很多问题 : select 会修改传入的参数数组 , 这对于一个需要调用很多次的函数 , 是非常不友好的 select 如果任何一个sokc(I/O stream) 出现了数据 , select仅仅会返回 , 但是并不会告诉你是哪个sock上有数据 , 于是你只能自己一个一个的找 , 十几个sock还好 , 但是数量一旦多了 , 这无谓的开销可就大了 select 只能监视1024个链接 select对socket进行扫描时是线性扫描 , 即采用轮询的方法 , 效率较低 select 不是线程安全的 , 如果你把一个sock(I/O stream) 加入到select , 然后突然另外一个线程发现这个sock不用 , 需要收回 , 那么对不起 , select不支持 , 并且如果你想关掉这个sock , 那么select的标准行为是不可预测的 If a file descriptor being monitored by select() is closed in another thread , the result is unspecified Python实现select模型代码 import select import socket sk1 = socket.socket() sk1.bind(('127.0.0.1', 8002, )) sk1.listen() demo_li = [sk1] outputs = [] message_dict = {} while True: r_list, w_list, e_list = select.select(sk1, outputs, [], 1) print(len(demo_li),r_list) for sk1_or_conn in r_list: if sk1_or_conn == sk1: conn, address = sk1_or_conn.accept() demo_li.append(conn) message_dict[conn] = [] else: try: data_bytes = sk1_or_conn.recv(1024) # data_str = str(data_bytes, encoding=\"utf-8\") # print(data_str) # sk1_or_conn.sendall(bytes(data_str+\"good\", encoding=\"utf-8\")) except Exception as e: demo_li.remove(sk1_or_conn) else: data_str = str(data_bytes, encoding=\"utf-8\") message_dict[sk1_or_conn].append(data_str) outputs.append(sk1_or_conn) for conn in w_list: recv_str = message_dict[conn][0] del message_dict[conn][0] conn.sendall(bytes(recv_str+\"Good\", encoding=\"utf-8\")) outputs.remove(conn) Poll poll本质上和select没有区别 , 它将用户传入的数组拷贝到内核空间 , 然后查询每个fd对应的设备状态 , 如果设备就绪则在设备等待队列中加入一项并继续遍历 , 如果遍历完所有fd后没有发现就绪设备 , 则挂起当前进程 , 直到设备就绪或者主动超时 , 被唤醒后它又要再次遍历fd , 这个过程经历了多次无谓的遍历 它没有最大连接数的限制 , 原因是它是基于链表来存储的 , 但是同样有缺点 : 大量的fd的数组被整体复制于用户态和内核地址空间之间 , 而不管这样的复制是不是有意义 poll还有一个特点是\"水平触发\" , 如果报告了fd后 , 没有被处理 , 那么下次poll时会再次报告该fd 同样不是线程安全的 Epoll 🍀 poll是在2.6内核中提出的 , 是之前的select和poll的增强版本 , 相对于select和poll来说 , epoll更加灵活 , 没有描述符限制 ; epoll使用一个文件描述符管理多个描述符 , 将用户关系的文件描述符的事件存放到内核的一个事件表中 , 这样在用户空间和内核空间的copy只需一次 基本原理 : epoll支持水平触发和边缘触发 , 最大的特点在于边缘触发 , 它只告诉进程哪些fd刚刚变为就绪态 , 并且只会通知一次 ; 还有一个特点是 , epoll使用\"事件\"的就绪通知方式 , 通过epoll_ctl注册fd , 一旦该fd就绪 , 内核就会采用类似callback的回调机制来激活该fd , epoll_wait便可以收到通知 epoll的优点 : 没有最大并发连接的限制 , 能打开的FD的上限远大于1024(1G的内存上能监听约10万个端口) 效率提升 , 不是轮询的方式 , 不会随着FD数目的增加效率下降 , 只有活跃可用的FD才会调用callback函数 ; 即Epoll最大的优点就在于它只管你\"活跃\"的连接 , 而跟连接总数无关 , 因此在实际的网络环境中 , Epoll的效率就会远远高于select和poll 内存拷贝 , 利用mmap()文件映射内存加速与内核空间的消息传递 ; 即epoll使用mmap减少复制开销 是线程安全的 epoll对文件描述符的操作有两种模式 : LT(level trigger)和ET(edge trigger) , LT模式是默认模式 , LT模式与ET模式的区别如下 : LT模式 : 当epoll_wait检测到描述符事件发生并将此事件通知应用程序 , 应用程序可以不立即处理该事件 , 下次调用epoll_wait时 , 会再次响应应用程序并通知此事件 ET模式 : 当epoll_wait检测到描述符事件发生并将此事件通知应用程序 , 应用程序必须立即处理该事件 , 如果不处理 , 下次调用epoll_wait时 , 不会再次响应应用程序并通知此事件 LT模式 LT(level triggered)是缺省的工作方式 , 并且同时支持block和no-block socket , 在这种做法中 , 内核告诉你一个文件描述符是否就绪了 , 然后你可以对这个就绪的fd进行IO操作 , 如果你不作任何操作 , 内核还是会继续通知你的 ET模式 ET(edge-triggered)是高速工作方式 , 只支持no-block socket , 在这种模式下 , 当描述符从未就绪变为就绪时 , 内核通过epoll告诉你 , 然后它会假设你知道文件描述符已经就绪 , 并且不会再为那个文件描述符发送更多的就绪通知 , 直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如 , 你在发送 , 接收或者接收请求 , 或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误) , 但是请注意 , 如果一直不对这个fd作IO操作(从而导致它再次变成未就绪) , 内核不会发送更多的通知(only once) , ET模式在很大程度上减少了epoll事件被重复触发的次数 , 因此效率要比LT模式高 , epoll工作在ET模式的时候 , 必须使用非阻塞套接口 , 以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死 在select/poll中 , 进程只有在调用一定的方法后 , 内核才对所有监视的文件描述符进行扫描 , 而epoll事先通过epoll_ctl()来注册一个文件描述符 , 一旦基于某个文件描述符就绪时 , 内核会采用类似callback的回调机制 , 迅速激活这个文件描述符 , 当进程调用epoll_wait()时便得到通知 (此处去掉了遍历文件描述符 , 而是通过监听回调的的机制 , 这正是epoll的魅力所在) "},"01-Python/06-并发篇/07-实现线程池.html":{"url":"01-Python/06-并发篇/07-实现线程池.html","title":"实现线程池","keywords":"","body":"Attack on Python - 实现线程池 🐍 方式一 import Queue import threading class ThreadPool(object): def __init__(self, max_num=20): self.queue = Queue.Queue(max_num) for i in xrange(max_num): self.queue.put(threading.Thread) def get_thread(self): return self.queue.get() def add_thread(self): self.queue.put(threading.Thread) \"\"\" 使用: pool = ThreadPool(10) def func(arg, p): import time time.sleep(2) p.add_thread() for i in range(30): thread = pool.get_thread() t = thread(target=func, args=(i, pool)) t.start() \"\"\" 方式二 import queue import threading import contextlib import time StopEvent = object() class ThreadPool(object): def __init__(self, max_num, max_task_num = None): if max_task_num: self.q = queue.Queue(max_task_num) else: self.q = queue.Queue() self.max_num = max_num self.cancel = False self.terminal = False self.generate_list = [] self.free_list = [] def run(self, func, args, callback=None): \"\"\" 线程池执行一个任务 :param func: 任务函数 :param args: 任务函数所需参数 :param callback: 任务执行失败或成功后执行的回调函数，回调函数有两个参数1、任务函数执行状态；2、任务函数返回值（默认为None，即：不执行回调函数） :return: 如果线程池已经终止，则返回True否则None \"\"\" if self.cancel: return if len(self.free_list) == 0 and len(self.generate_list) "},"01-Python/07-内存篇/":{"url":"01-Python/07-内存篇/","title":"内存篇","keywords":"","body":"Attack on Python - 内存篇 🐍 Python总体架构 Python总体分为三个部分 , 即文件组 , Python核心 (解释器) , 运行环境 , 如下 : File Groups Python Core Runtime Environment INTERPRETER +---------------+ +----------------+ | Core Modules | | Scanner | ↓ +---------------+ +----------------+ +--------------------------+ | Library | | Parser | ↓ | Object/Type Structures | +---------------+ +----------------+ +--------------------------+ | User-defined | | Compiler | ↓ | Memory Allocator | | Modules | +----------------+ +--------------------------+ +---------------+ | Code Evauator | ↓ | Current State of Python | +----------------+ +--------------------------+ 源码组织 我们可以在Python官网中获取源码 , 即http://www.python.org 本目录下深入整理主要参考Python 2.7 与Python 3.5.4源码 参考书籍 : Python源码剖析——深度探索动态语言核心技术 Python 源码目录结构如下 : Python ├── Doc ├── Grammar ├── Include ├── Lib ├── Mac ├── Misc ├── Modules ├── Objects ├── Parser ├── PC ├── PCbuild ├── Programs ├── Python └── Tools 主要说明 , 其中加粗部分为主要分析对象 : Include : 该目录下包含了Python提供的所有头文件 , 如果用户需要自己用C或C++来编写自定义模块扩展Python , 那么就需要用到这里提供的头文件 Lib : 该目录包含了Python自带的所有标准库 , Lib中的库都是用Python语言编写的 Modules : 该目录中包含了所有用C语言编写的模块 , 比如random , cStringIO等 ; Modules中的模块时那些对速度要求非常严格的模块 , 而有一些对速度没有太严格要求的模块 , 比如os , 就是用Python编写的 , 并且放在Lib目录下 Parser : 该目录中包含了Python解释器中的Scanner和Parser部分 , 即对Python源代码进行词法分析和语法分析的部分 ; 除了这些 , Parser目录下还包含了一些有用的工具 , 这些工具能够根据Python语言的语法自动生成Python语言的词法和语法分析器 , 与YACC非常类似 Objects : 该目录中包含了所有Python的内建对象 , 包括整数 , list , dict等 , 同时 , 该目录还包括了Python在运行时需要的所有的内部使用对象的实现 Python : 该目录下包含了Pyton解释器中的Compiler和执行引擎部分 , 是Python运行的核心所在 PCBuild : 包含了VS使用的工程文件 "},"01-Python/07-内存篇/01-对象机制.html":{"url":"01-Python/07-内存篇/01-对象机制.html","title":"对象机制","keywords":"","body":"Attack on Python - 对象机制 🐍 介绍 在Python中一切皆对象 我们知道Python是用C语言设计出来的 , 而在Python中 , 对象就是C中的结构体在堆上申请的一块内存 对象是不能被静态初始化的 , 并且也不能在栈空间上生存 ; 唯一列外的就是类型对象 , Python中所有的内建类型对象 (如整数类型对象 , 字符串类型对象) 都是被静态初始化的 在Python中 , 一个对象一旦被创建 , 那么它在内存中的大小就固定不变了 , 这就意味着对于那些可变长度的数据对象 (如列表) , 只能在对象内维护一个指向一块可变大小的内存区域的指针 利用这种对象机制可以使由指针维护对象的工作变得非常的简单 对象机制的基石 Python中一切皆对象 , 而所有的对象都拥有一些相同的内容 , 其被定义在PyObject中 我们先对比源码 , 从源码目录Python-2.7\\Include\\object.h中 , 截取如下片段 : 106:typedef struct _object { 107: PyObject_HEAD /*这个宏如下*/ 108:} PyObject; 77:/* PyObject_HEAD defines the initial segment of every PyObject. */ 78:#define PyObject_HEAD \\ 79: _PyObject_HEAD_EXTRA \\ /* Py_ssize_t 是一个所占字节数与 size_t 相同的有符号的整数类型*/ 80: Py_ssize_t ob_refcnt; \\ 81: struct _typeobject *ob_type; 65:/* Define pointers to support a doubly-linked list of all live heap objects. */ 66:#define _PyObject_HEAD_EXTRA \\ 67: struct _object *_ob_next; \\ 68: struct _object *_ob_prev; 从源码目录Python-3.5.4\\Include\\object.h中 , 截取如下片段 : 106:typedef struct _object { 107: _PyObject_HEAD_EXTRA /* 与2.7相比没有发生任何实质性变化 */ 108: Py_ssize_t ob_refcnt; 109: struct _typeobject *ob_type; 110:} PyObject; 82:/* PyObject_HEAD defines the initial segment of every PyObject. */ 83:#define PyObject_HEAD PyObject ob_base; 70:/* Define pointers to support a doubly-linked list of all live heap objects. */ 71:#define _PyObject_HEAD_EXTRA \\ 72: struct _object *_ob_next; \\ 73: struct _object *_ob_prev; 75:#define _PyObject_EXTRA_INIT 0, 0, 78:#else 79:#define _PyObject_HEAD_EXTRA 两个版本源码并没有什么真正意义上的改变 , 从中我们可以看出 , PyObject主要由ob_refcnt , ob_type , _PyObject_HEAD_EXTRA 几个部分组成 , 而对于_PyObject_HEAD_EXTRA , 我们发现它只有在DEBUG模式下才不为空 , 所以我们可以将其忽略 ob_refcnt ob_refcnt 是内存管理机制的核心 , 它实现了基于引用计数的垃圾回收机制 , 例如 : 对于某一个对象A , 当有一个新的PyObject * (对象指针) 引用该对象时 , A的引用计数 (ob_refcnt) 就会增加 ; 而当这个PyObject * 被删除时 , A的引用计数就会减少 , 并且当A的引用计数减少到0时 , A就可以从堆上被删除 , 以释放出内存供别的对象使用 ob_refcnt是一个32位的整型变量 , 这实际蕴含着Python所做的一个假设 , 即对一个对象的引用不会超过一个整型变量的最大值 , 这个假设如果不是恶意代码的话 , 明显是成立的 ob_type ob_type是对象类型的核心 , 源码中我们可以看到 , 它是一个指向_typeobject的结构体的指针 , 该结构体对应的是一种特殊的对象 , 它是用来指定一个对象类型的类型对象 , 也就是说ob_type所指向的位置存放着一个对象的类型信息 Python就是利用ob_type构造了对象类型的基石 PyObject中定义了所有Python对象中都必须有的内容 , 即ob_refcnt和ob_type , 当然一个对象中肯定不止于这些 , 不同的对象中还保存了各自的特殊信息 , 于是才实现了各种基础数据类型 定长对象和变长对象 定长对象 我们把不包含可变长度数据的对象称为 \"定长对象\" , 并且定长对象在内存中所占的大小是一样的 , 比如我们的整数对象 , 内存中 1 和 100占用的内存大小都是sizeof(PyIntObject) 你可能会将定长对象理解为 \"不可变对象\" , 但是实际上并不是这样 , 因为像Python的字符串 , 元组这两者都是 \"不可变对象\" , 但是他们却是 \"变长对象\" , 我们通过源码来看看Python中的整数对象 : 目录Python-2.7\\Include\\intobject.h中 , 截取如下片段 : 23:typedef struct { 24: PyObject_HEAD /*PyObject对象宏 */ 25: long ob_ival; /*PyIntObject的特殊信息*/ 26:} PyIntObject; 如上 , 也就是说在Python 2.x中 , 整数对象都是定长对象 , 因为PyIntObject结构体中没有任何多余的内容 , 但是别忘了数字还有Long类型 , 而Long则是变长对象 源码如下 : Python-2.7\\Include\\longintrepr.h中 , 截取如下片段 : 90:struct _longobject { 91: PyObject_VAR_HEAD /*变长对象基石*/ 92: digit ob_digit[1]; 93:}; 注意 : 在Python 3.x中 , Long类型和Int类型合并到一起去了 , 我们在3.x中所看到的Int类型 , 实际上是Long 类型 , 关于数字类型将会在下一篇中整理 Python 3.x中这部分源码也在logintrepr.h中 , 分别在第89 - 92行 变长对象 上面已经说明了定长对象 , 变长对象则就是包含可变长度数据的对象 定长对象与变长对象的区别在于 : 定长对象占用的内存大小是一样的 , 而变长对象占用的大小不一样 , 实例如下 : >>> a = 1 >>> type(a) >>> a.__sizeof__() 24 >>> b = 100 >>> type(b) >>> b.__sizeof__() 24 注意 : 字符串是变长对象 , Python2.7中源码如下 : // Python2.7\\Include\\stringobject.h 35:typedef struct { 36: PyObject_VAR_HEAD /*变长对象基石*/ 37: long ob_shash; 38: int ob_sstate; 39: char ob_sval[1]; /* 省略注释 */ 49:} PyStringObject; 实例说明 # env : Python 2.x >>> a = \"lyon\" >>> b = \"lyonyang\" >>> a.__sizeof__() 37 >>> b.__sizeof__() 41 PyVarObject PyVarObject就是Python中变长对象的基石 , 上面的PyStringObject中我们已经见过了, 那么继续翻源码 : Python-2.7\\Include\\object.h : 110:typedef struct { 111: PyObject_VAR_HEAD 112:} PyVarObject; /* PyObject_VAR_HEAD defines the initial segment of all variable-size * container objects. These end with a declaration of an array with 1 * element, but enough space is malloc'ed so that the array actually * has room for ob_size elements. Note that ob_size is an element count, * not necessarily a byte count. */ 96:#define PyObject_VAR_HEAD \\ 97: PyObject_HEAD \\ 98: Py_ssize_t ob_size; /* Number of items in variable part */ Python-3.5.4\\Include\\object.h : 112:typedef struct { 113: PyObject ob_base; /* 等价于PyObject_HEAD */ 114: Py_ssize_t ob_size; /* Number of items in variable part */ 115:} PyVarObject; 版本2.7 与 3.5.4无变化 , 我们可以看出 , PyVarObject其实就是在PyObject上的一个扩展而已 , 而这个扩展就是在PyVarObject中多出了一个ob_size变量 , 这是一个整型变量 , 该变量记录的是变长对象中一共容纳了多少个元素 注意 : 变长对象通常都是容器 , 并且ob_size指明的是所容纳元素的个数 , 而不是字节的数量 , 比如一个列表中有5个元素 , 那么ob_size的值就是5 所以对于判断Python底层实现的对象是否是变长对象 , 只需查看其定义中是否具有ob_size属性 类型对象 上面已经提到过了在PyObject中有一个ob_type指针 , 它指向对象的类型信息 , 这样在分配内存空间时 , 就可以根据ob_type所指向的信息来决定对象申请多大的空间 ob_type指向结构体_typeobject , 如下 : Python-2.7\\Include\\object.h : 324:typedef struct _typeobject { 325: PyObject_VAR_HEAD 326: const char *tp_name; /* For printing, in format \".\" */ 327: Py_ssize_t tp_basicsize, tp_itemsize; /* For allocation */ 329: /* Methods to implement standard operations */ ... 338: /* Method suites for standard classes */ ... 344: /* More standard operations (here for binary compatibility) */ ... 411:} PyTypeObject; Python-3.5.4\\Include\\object.h : 343:typedef struct _typeobject { 344: PyObject_VAR_HEAD 345: const char *tp_name; /* For printing, in format \".\" */ 346: Py_ssize_t tp_basicsize, tp_itemsize; /* For allocation */ 348: /* Methods to implement standard operations */ ... 358: /* Method suites for standard classes */ ... 364: /* More standard operations (here for binary compatibility) */ 432:} PyTypeObject; 同样 , 在版本2.7 与 3.5.4之间不能存在差异 我们可以将该结构体主要分为4个部分 : 类型名 , 即tp_name , 主要是Python内部以及调试的时候使用 创建该类型对象时分配内存空间大小的信息 , 即 tp_basicsize , tp_itemsize 与该类型对象相关联的操作信息 , 可以通过源码进行详查 类型的类型信息 由于在PyObject的定义中包含了PyTypeObject , 我们可以认为PyObject对象是继承了PyTypeObject对象 , 而PyTypeObject则是最原始的抽象 因为在实际的Python中确实如此 : object类 (即PyObject) 的基类就是type类 (即PyTypeObject) 我们用Python简单描述 : >>> isinstance(object, type) True 并且由于Python对外提供了C API , 以及Python本身就是用C写成的 , 所以Python内部也大量使用了这些API Python中的API分为两种 : 范型API , 或者称为AOL (Abstract Object Layer) , 这类API都具有诸如Pyobject_*的形式 , 可以应用于任何Python对象上 类型相关API , 或者称为COL (Concrete Object Layer) , 这类API通常只能作用在某一种类型的对象上 , 对于Python内建对象 , 都提供了这样一组API , 诸如PyInt_Type 所以对于Python中的内建类型对象 , 可以利用以上两种API进行创建 : 范型API : PyObject *intobj = PyObject_New(PyObject, &PyInt_Type) 类型API : PyObject *intobj = PyInt_FromLong(10) 注意 : 我们经常所见到的中的 int 代表的就是Python内部的PyInt_Type 总结 : 通过这一篇文章我们已经理清了Python对象机制中的核心定义 以下从上往下依次扩展 PyTypeObject - - 类型对象基石 PyObject - - 对象基石 PyVarObject - - 变长对象基石 "},"01-Python/07-内存篇/02-对象的创建.html":{"url":"01-Python/07-内存篇/02-对象的创建.html","title":"对象的创建","keywords":"","body":"Attack on Python - 对象的创建 🐍 介绍 上一篇关于Python中对象实现中我们知道 , 创建一个对象Python提供了两种API , 即范型API和类型API 而在对象真正创建时 , Python为我们使用的是类型API 因为如果使用范型API , 那么意味着Python要提前为我们准备好PyObject_New 这一系列的API , 对于创建内置类型的对象这并没有问题 , 但是如果对于创建用户自定义的类型这样就非常的不明智了 , 因为需要提前创建好诸多的_New对象 创建对象 我们定义一个类 , 通过这个自定义类来说明Python对象的创建流程 # Python对象的基石,即PyObject class object(): pass # 自定义类 class MyObject(object): pass 创建object对象 在分析自定义类型的对象创建之前 , 我们需要分析一下object对象是如何创建的 , 虽然我们在实际中是不会也不需要去创建object对象的 , 但是这有利于我们下一步的分析 : object对象的创建 : 如上图 , 创建object对象首先调用类型API (PyBaseObject_Type) , 并且会首先调用API中的tp_new , 因为这里是创建object , 所以tp_new中不会为NULL 创建自定义对象 无论是Python 2.x还是3.x , Python中所有的类都是以object类为基础的 , 也就是说所有的类都继承了object类 , 所以自定义类型对象的创建流程如下 : 无论是自定义对象的创建还是object对象的创建 , 其创建对象的流程都是一样的 : 首先都会调用其类型API中的tp_new , 如果我们自定义类型中tp_new为NULL , 那么它将通过tp_base指定的基类继续去寻找tp_new , 直到找到tp_new为止 , 不要担心会找不到 , Python中所有的类都继承了object类 , 而object类中是一定有tp_new的 在找到tp_new之后会回到原点拿取tp_basicsize , 这里面记录了该对象应该占用内存大小的信息 , 拿取后申请内存完成创建 , 返回一个新对象 拿到新对象我们对新对象进行初始化 通过这三大步 , 一个对象的创建基本就完成了 站在Python的角度来看 , tp_new对应的就是特殊操作符中的__new__方法 , 此方法返回一个对象实例 , tp_init 对应的就是特殊操作符中的__init__方法 , 当我们创建一个类时一般都会对__init__方法进行重载以达到我们的目标 当然PyBaseObject_Type并不是类型对象的终点 , 在其之上还存在着一个PyType_Type 更多关于类型对象的信息详见上一篇 , 其中定义了对象的行为 类型的类型 我们知道PyObject中有一个 ob_type指针 , 记录着PyObject的类型信息 , 但是这个结构体也是一个对象 , 就是上一篇中所说的类型对象PyTypeObject 既然是对象 , 那么就肯定有类型 , 而这个类型就是PyType_Type Python-2.7\\Objects\\typeobject.c 2730:PyTypeObject PyType_Type = { PyVarObject_HEAD_INIT(&PyType_Type, 0) \"type\", /* tp_name */ sizeof(PyHeapTypeObject), /* tp_basicsize */ sizeof(PyMemberDef), /* tp_itemsize */ (destructor)type_dealloc, /* tp_dealloc */ 0, /* tp_print */ 0, /* tp_getattr */ 0, /* tp_setattr */ 0, /* tp_compare */ (reprfunc)type_repr, /* tp_repr */ 0, /* tp_as_number */ 0, /* tp_as_sequence */ 0, /* tp_as_mapping */ (hashfunc)_Py_HashPointer, /* tp_hash */ (ternaryfunc)type_call, /* tp_call */ 0, /* tp_str */ (getattrofunc)type_getattro, /* tp_getattro */ (setattrofunc)type_setattro, /* tp_setattro */ 0, /* tp_as_buffer */ Py_TPFLAGS_DEFAULT | Py_TPFLAGS_HAVE_GC | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_TYPE_SUBCLASS, /* tp_flags */ type_doc, /* tp_doc */ (traverseproc)type_traverse, /* tp_traverse */ (inquiry)type_clear, /* tp_clear */ type_richcompare, /* tp_richcompare */ offsetof(PyTypeObject, tp_weaklist), /* tp_weaklistoffset */ 0, /* tp_iter */ 0, /* tp_iternext */ type_methods, /* tp_methods */ type_members, /* tp_members */ type_getsets, /* tp_getset */ 0, /* tp_base */ 0, /* tp_dict */ 0, /* tp_descr_get */ 0, /* tp_descr_set */ offsetof(PyTypeObject, tp_dict), /* tp_dictoffset */ type_init, /* tp_init */ 0, /* tp_alloc */ type_new, /* tp_new */ PyObject_GC_Del, /* tp_free */ (inquiry)type_is_gc, /* tp_is_gc */ 2772:}; 在Python 3.5.4中内容是一样就不列出了 , 行数3328-3369 所有的对象中的类型对象都是由PyType_Type对象进行创建的 , 包括PyObject , 如下 : >>> object.__class__ >>> int.__class__ >>> class A(object): ... pass ... >>> A.__class__ >>> type.__class__ >>> 通过这一实验 , 我们可以知道其实所有类的祖宗实际上是type , 也就是PyType_Type , 所以它在Python中被称为 metaclass(元类) 我们发现就算是type类竟然也是由type (PyType_Type)产生的 , 就像在type类中成了一个 \"圈一样\" , 自己引用自己 , 事实上确实是这样 , 同样以上一小节的例子进行说明 , 如下图 : 也就是说PyType_Type中的ob_type指针最终指向了自己本身 这些基本上就是Python对象的创建流程了 , 但是注意对于Python内部的类型 , 创建时可能存在一些差异 , 但是这些差异并不会影响我们分析的结果 总结 : 这一篇主要整理了对象创建的流程 , 以及对类型对象的整理 tp_new对应到C++中 , 可以视为new操作符 , Python中则是__new__操作符 tp_init则是Python中的__init__ 也就是类的构造函数 , 功能就是对创建的新对象进行初始化 Python中一切皆对象 , 类型也是对象 ; 对象必然具有类型 , PyType_Type是类型对象的创造者 PyType_Type的类型就是其本身 "},"01-Python/07-内存篇/03-整数对象.html":{"url":"01-Python/07-内存篇/03-整数对象.html","title":"整数对象","keywords":"","body":"Attack on Python - 整数对象 🐍 介绍 在Python的应用程序中 , 整数的使用非常地广泛 这就意味着整数对象的创建和销毁肯定是非常的频繁的 , 并且我们知道Python中采用了引用计数机制 , 即一个整数类型的变量ob_refcnt , 这样Python中对于整数对象的创建和销毁会更加的疯狂 , 这样的执行效率明显我们是无法接受的 , 更何况Python已经背负了人们对其执行效率的不满 , 所以Python中大量采用了内存对象池的技术 整数对象必然也使用了内存对象池技术 , 也就是整数对象池 , 当然我们应该从整数对象的创建开始说起 , 以及Python 2.x中与Python 3.x两个版本之间的差异 整数类型 Python 2.x中的整数类型 在Python 2.x中有两种整数类型 , 一种是int 也就是我们通常说的整型 , 另一种是long也就是长整型 , 根据两种对象的源码 , 我们可以知道 , int (PyIntObject) 属于定长对象 , 而long (PyLongObject) 属于变长对象 对于int , 当其进行运算时 , 如果值溢出 , 那么Python将会将值自动转为long类型 , 如下 : # python 2.x >>> n = 2147483647 >>> type(n) # 加法溢出 >>> n = n + 1 >>> n 2147483648L >>> type(n) >>> n = -2147483647 >>> type(n) # 减法溢出 >>> n = n - 2 >>> n -2147483649L >>> type(n) 但是long就不会出现这种溢出情况了 , 因为long是一个变长对象 , 当空间不够存放这个数字值 , 加空间就是了 , 无非是从1Byte 到2 Byte的过程 , 以此类推 Python 3.x中的整数类型 在Python 3.x中 , 只有long了 , 我们所见到的int实际上就是long , 根据源码的注释所说 , 大概意思就是对于未来而言 , long比int好 , 并且在Python 3.x的官方文档中 , 第一句就说明了 : All integers are implemented as “long” integer objects of arbitrary size. 还有一点值得注意的就是 , 在3.x的源码中 , 已经没有intobject.h这个文件了 , 而只有longobject.h , 我们可以在Python-3.5.4\\Objects\\longobject.c中看到long的类型信息 : 5179:PyTypeObject PyLong_Type = { PyVarObject_HEAD_INIT(&PyType_Type, 0) \"int\", /* tp_name */ offsetof(PyLongObject, ob_digit), /* tp_basicsize */ sizeof(digit), /* tp_itemsize */ long_dealloc, /* tp_dealloc */ 0, /* tp_print */ 0, /* tp_getattr */ 0, /* tp_setattr */ 0, /* tp_reserved */ long_to_decimal_string, /* tp_repr */ &long_as_number, /* tp_as_number */ 0, /* tp_as_sequence */ 0, /* tp_as_mapping */ (hashfunc)long_hash, /* tp_hash */ 0, /* tp_call */ long_to_decimal_string, /* tp_str */ PyObject_GenericGetAttr, /* tp_getattro */ 0, /* tp_setattro */ 0, /* tp_as_buffer */ Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_LONG_SUBCLASS, /* tp_flags */ long_doc, /* tp_doc */ 0, /* tp_traverse */ 0, /* tp_clear */ long_richcompare, /* tp_richcompare */ 0, /* tp_weaklistoffset */ 0, /* tp_iter */ 0, /* tp_iternext */ long_methods, /* tp_methods */ 0, /* tp_members */ long_getset, /* tp_getset */ 0, /* tp_base */ 0, /* tp_dict */ 0, /* tp_descr_get */ 0, /* tp_descr_set */ 0, /* tp_dictoffset */ 0, /* tp_init */ 0, /* tp_alloc */ long_new, /* tp_new */ PyObject_Del, /* tp_free */ 5220:}; 注意 : 在此文件中还有一个long_as_number 域 , 其中定义了一个对象作为数值对象时所有可选的操作 , 其中2.7中一共有39个函数指针 , 3.5.2中一共有34个函数指针 , 每一个函数指针都代表着一种可选的操作 , 包括加法 , 减法 , 乘法 , 模运算等等 ; 具体行数见5142-5176 创建方式 对于整数对象的创建 , 其途径都定义在intobject.c或者longobject.c中 , 方式都不止一种 , 例如创建int就有以下3种方式 : 从long值创建 , PyInt_FromLong(long ival) 从Py_UNICODE对象生成 , PyInt_FromUnicode(Py_UNICODE *s, int length, int base) 从字符串生成 , PyInt_FromString(char *s, char **pend, int base) 而对于创建long方法就更多了 , 这些创建方法都定义在Python\\Objects\\目录下对应的.c文件中 小整数对象池 在实际编程中 , 数值比较小的整数 , 比如 1, 2, 29等 , 可能在程序中会非常频繁地使用 ; 在Python中 , 所有的对象都存货在系统堆上 , 也就是说 , 如果没有特殊的机制 , 对于这些频繁使用的小整数对象 , Python将一次又一次使用malloc在堆上申请空间 , 并且不厌其烦地一次次free释放空间 , 这样的操作会严重影响Python的整体性能 所以Python中对于小整数对象使用了对象池技术 , 也就是Python会直接将小整数对象缓存在内存中 , 并将其指针存放在small_ints中 , 这个小整数集合的范围无论是在Python 2.x 还是在Python 3.x , 其范围都设定在[-5, 257) , 源码如下 : Python-2.7\\Objects\\intobject.c 67:#ifndef NSMALLPOSINTS 68:#define NSMALLPOSINTS 257 69:#endif 70:#ifndef NSMALLNEGINTS 71:#define NSMALLNEGINTS 5 72:#endif 73:#if NSMALLNEGINTS + NSMALLPOSINTS > 0 /* References to small integers are saved in this array so that they can be shared. The integers that are saved are those in the range -NSMALLNEGINTS (inclusive) to NSMALLPOSINTS (not inclusive). */ 79:static PyIntObject *small_ints[NSMALLNEGINTS + NSMALLPOSINTS]; Python-3.5.4\\Objects\\longobject.c 12:#ifndef NSMALLPOSINTS 13:#define NSMALLPOSINTS 257 14:#endif 15:#ifndef NSMALLNEGINTS 16:#define NSMALLNEGINTS 5 17:#endif 25:#if NSMALLNEGINTS + NSMALLPOSINTS > 0 /* Small integers are preallocated in this array so that they can be shared. The integers that are preallocated are those in the range -NSMALLNEGINTS (inclusive) to NSMALLPOSINTS (not inclusive). */ 31:static PyLongObject small_ints[NSMALLNEGINTS + NSMALLPOSINTS]; 小整数池测试 # Python 2.7 >>> a = 1 >>> id(a) 87319208L >>> b = 1 >>> id(b) 87319208L # Python 3.5.3 >>> a = 1 >>> id(a) 1852703184 >>> b = 1 >>> id(b) 1852703184 超出小整数集合的整数对象 , 内存地址就不一样了 , 这一点可以自己尝试 对于小整数集合的范围我们是可以修改的 , 但是修改的方法非常原始 , 那就是修改Python的源码然后重新编译 注意 : 小整数对象池中完全地缓存其对象 , 也就是说在执行我们的程序之前小整数对象池就已经激活 通用整数对象池 小整数对象池解决了小整数频繁的使用问题 , 但是我们并不能保证大整数就不会被频繁的使用 , 所以对于这些整数 , Python运行环境将提供一块内存空间 , 供这些大整数轮流使用 , 结构体如下 : Python-2.7\\Objects\\intobject.c 33:#define BLOCK_SIZE 1000 /* 1K less typical malloc overhead */ 34:#define BHEAD_SIZE 8 /* Enough for a 64-bit pointer */ 35:#define N_INTOBJECTS ((BLOCK_SIZE - BHEAD_SIZE) / sizeof(PyIntObject)) 37:struct _intblock { 38: struct _intblock *next; 39: PyIntObject objects[N_INTOBJECTS]; 40:}; 42:typedef struct _intblock PyIntBlock; 44:static PyIntBlock *block_list = NULL; 45:static PyIntObject *free_list = NULL; 在上述结构体中 , N_INTOBJECTS表示所维护的对象的个数 , 在32位的系统上 , 一个int类型所需要的内存为12bytes , 所以可以计算出这个值应该是82 , 这一个值我们也可以通过修改源码进行修改 而PyIntBlock的单向列表通过block_list维护 , 每一个block中都维护了一个PyIntObject数组 , 这就是真正用于存储被缓存的PyIntObject对象的内存 , 而对于这个内存中的空闲内存则是由单向链表free_list进行管理 ; 最开始时这两个指针都指向一个空值 (NULL) 在Python 3.5.4中 , 我没有找到如同2.7一样的源码 , 但是我们可以通过两个版本的实验发现 , 通用对象池机制是一样的 : # Python 2.x >>> id(257),id(258),id(259) (81956248L, 81956224L, 81956200L) >>> n = 258 >>> id(n) 81956248L # Python 3.x >>> id(257),id(258),id(259) (1910529789904, 1910534766192, 1910534766096) >>> n = 258 >>> id(n) 1910529789904 在进行实验时 , 走了很多弯路 , 有兴趣的话可以自己尝试 , 下面是上面实验的结果总结 : 申请完内存之后 , Python解释器就再也不会返回内存给操作系统了 , 就算对象被销毁 创建大整数对象时 , 会到堆里面找最近的那一块空内存 , 注意堆里面存储数据是由高到低进行存储的 也就是说 , 通用整数对象池机制所做的优化就是 , 解决了内存的频繁开辟问题 注意 : 如果第一块空间满了 , 那么就会往第二块进行存储 ; 添加和删除 通过使用PyInt_FromLong API为例 , 创建一个整数对象的过程如下 : Python-2.7\\Objects\\intobject.c 87:PyInt_FromLong(long ival) 88:{ 89: register PyIntObject *v; 90:#if NSMALLNEGINTS + NSMALLPOSINTS > 0 /* 尝试使用小整数对象池 */ 91: if (-NSMALLNEGINTS = 0) 96: quick_int_allocs++; 97: else 98: quick_neg_int_allocs++; 99:#endif 100: return (PyObject *) v; 101: } 102:#endif /* 为通用整数对象池申请新的内存空间 */ 103: if (free_list == NULL) { 104: if ((free_list = fill_free_list()) == NULL) 105: return NULL; 106: } 107: /* Inline PyObject_New */ 108: v = free_list; 109: free_list = (PyIntObject *)Py_TYPE(v); 110: PyObject_INIT(v, &PyInt_Type); 111: v->ob_ival = ival; 112: return (PyObject *) v; 113:} Python-3.5.4\\Objects\\longobject.c 中25行至296行 可以查看到关于Python 3中的一些处理 37:get_small_int(sdigit ival) { PyObject *v; assert(-NSMALLNEGINTS = 0) quick_int_allocs++; else quick_neg_int_allocs++; #endif return v; 50:} 51:#define CHECK_SMALL_INT(ival) \\ do if (-NSMALLNEGINTS 也就是说整数对象的创建会通过两步来完成 : 如果小整数对象池机制被激活 (默认就已激活) , 则尝试使用小整数对象池 如果不能使用小整数对象池 , 则使用通用的整数对象池 对于整数对象的实现大概核心就是这些东西了 , 关于通用对象池的创建 , 可以通过源码或者 , 《Python源码剖析》一书进行探索 "},"01-Python/07-内存篇/04-字符串对象.html":{"url":"01-Python/07-内存篇/04-字符串对象.html","title":"字符串对象","keywords":"","body":"Attack on Python - 字符串对象 🐍 介绍 在前面有提到过 \"定长对象\" 和 \"变长对象\" , 这是一种对对象的二分法 当然不止这一种 , 还有一种就是 \"可变对象(mutable)\" 和 \"不可变对象(immutable)\" , 这种二分法是根据对象维护数据的可变性来进行区分的 , 在Python的官方文档中也是有说到的 可变对象维护的数据在对象被创建后还能再变化 , 比如一个list被创建后 , 可以向其中添加元素或删除元素 , 这些操作都会改变其维护的数据 ; 而不可变对象所维护的数据在对象创建之后就不能再改变了 , 比如Python中的string和tuple , 他们都不支持添加或删除的操作 Python 2.x 与 Python 3.x # Python 2.7 >>> name = 'lyon' >>> type(name) >>> name.decode('utf-8') u'lyon' >>> uname = u'lyon' >>> type(uname) # Python 3.5.4 >>> name = 'lyon' >>> type(name) >>> name.decode('utf-8') Traceback (most recent call last): File \"\", line 1, in AttributeError: 'str' object has no attribute 'decode' >>> uname = u'lyon' >>> type(uname) 在进行对比两种版本的差异前 , 我们需要知道在它们中有哪些字符串类型 : Python 3.x中 , 有3种字符串类型 : str , 表示Unicode文本 (8位的和更宽的) bytes , 表示二进制数据 bytearray , 是bytes的一种可变的变体 Python 2.x中 , 有2中字符串类型 : str , 表示8位文本和二进制数据 unicode , 表示宽字符Unicode文本 虽然在2中没有bytesarray , 但是在Python 2.6 及之后的版本都可以使用bytesarray 总体差异 : 在Python 2.x 与 Python 3.x中 , 字符串的实现主要体现在 , Python 3.x中将Python 2.x中常规的str和Unicode字符串整合到了一个单独的类型str中 , 以支持常规的和Unicode文本 ; 这样的处理使得Python在编码处理方面更加的方便 接下来就来分析Python中的字符串对象了 PyStringObject 在Python中 , PyStringObject是对字符串对象的实现 , PyStringObject 是一个拥有可变长度内存的对象 , 比如 : \"Lyon\" 和 \"KennethReitz\" 这两个字符串对象所需要的内存空间明显是不一样的 同时 , PyStringObject 对象又是一个不可变对象 , 即当创建了一个PyStringObject对象之后 , 该对象内部维护的字符串就不能再被改变了 , 这一点特性使得PyStringObject对象可以作为dict的键 , 但是同时也使得一些字符串的操作效率大大降低 , 比如多个字符串的连接操作 PyStringObject对象的定义如下 : Python-2.7\\Include\\stringobject.h : 35:typedef struct { 36: PyObject_VAR_HEAD /* 在前面的篇章已经介绍过了,变长对象宏 */ 37: long ob_shash; 38: int ob_sstate; 39: char ob_sval[1]; 41: /* Invariants: 42: * ob_sval contains space for 'ob_size+1' elements. 43: * ob_sval[ob_size] == 0. 44: * ob_shash is the hash of the string or -1 if not computed yet. 45: * ob_sstate != 0 iff the string object is in stringobject.c's 46: * 'interned' dictionary; in this case the two references 47: * from 'interned' to this object are *not counted* in ob_refcnt. 48: */ 49:} PyStringObject; 定义说明 : PyObject_VAR_HEAD中有一个ob_size变量保存着对象中维护的可变长度内存的大小 ob_shash变量的作用是缓存该对象的hash值 , 这样可以避免每一次都重新计算该字符串对象的hash值 , 如果一个PyStringObject对象还没有被计算过hash值 , 那么ob_shash的初始值是-1 这个hash值在后期dict类型中发挥了巨大的作用 ob_sstate变量标记了该对象是否已经过intern机制的处理 , intern机制见下文 , 预存的字符串的hash值与intern机制将Python虚拟机的执行效率提升了20% ob_sval在定义中虽然是一个字符的字符数组 , 但是ob_sval实际上是作为一个字符指针指向一段内存的 , 这段内存保存着这个字符串对象所维护的实际字符串 , 而这段内存的实际长度(字节) , 正式通过ob_size来维护的 , 这就是变长对象的实现机制 , 比如一个字符串对象 \"Lyon\" , ob_size的值就是4 在Python 3.x中 , 遗留的字符串定义在unicodeobject.h中 , 不另行说明了 PyString_Type 如下是PyStringObject的类型对象的定义 : Python-2.7\\Objects\\stringobject.c : 3800:PyTypeObject PyString_Type = { PyVarObject_HEAD_INIT(&PyType_Type, 0) \"str\", PyStringObject_SIZE, sizeof(char), ...... string_repr, /* tp_repr */ &string_as_number, /* tp_as_number */ &string_as_sequence, /* tp_as_sequence */ &string_as_mapping, /* tp_as_mapping */ (hashfunc)string_hash, /* tp_hash */ 0, /* tp_call */ ...... &PyBaseString_Type, /* tp_base */ ...... string_new, /* tp_new */ PyObject_Del, /* tp_free */ 3842:}; 对于类型对象就无需多说了 , 在前面的篇章也已经介绍过了 , 这里值得注意的是 , tp_itemsize和ob_size共同决定了应该额外申请的内存之总大小是多少 , tp_itemsize指明了由变长对象保存的元素的单位长度 , 这里就是单个字符在内存中的长度 tp_as_number , tp_as_sequence , tp_as_mapping 三个域都被设置了 , 表示PyStringObject对数值操作 , 序列操作和映射操作都支持 创建PyStringObject对象 Python 2.7 提供了两个接口 : PyString_FromString 和 PyString_FromStringAndSize Python-2.7\\Objects\\stringobject.c : PyString_FromString 119:PyString_FromString(const char *str) { register size_t size; register PyStringObject *op; // 判断字符串长度 assert(str != NULL); size = strlen(str); if (size > PY_SSIZE_T_MAX - PyStringObject_SIZE) { PyErr_SetString(PyExc_OverflowError, \"string is too long for a Python string\"); return NULL; } // 处理null string if (size == 0 && (op = nullstring) != NULL) { #ifdef COUNT_ALLOCS null_strings++; #endif Py_INCREF(op); return (PyObject *)op; } // 处理字符 if (size == 1 && (op = characters[*str & UCHAR_MAX]) != NULL) { #ifdef COUNT_ALLOCS one_strings++; #endif Py_INCREF(op); return (PyObject *)op; } /* Inline PyObject_NewVar */ op = (PyStringObject *)PyObject_MALLOC(PyStringObject_SIZE + size); if (op == NULL) return PyErr_NoMemory(); PyObject_INIT_VAR(op, &PyString_Type, size); op->ob_shash = -1; op->ob_sstate = SSTATE_NOT_INTERNED; Py_MEMCPY(op->ob_sval, str, size+1); /* share short strings */ if (size == 0) { PyObject *t = (PyObject *)op; PyString_InternInPlace(&t); op = (PyStringObject *)t; nullstring = op; Py_INCREF(op); } else if (size == 1) { PyObject *t = (PyObject *)op; PyString_InternInPlace(&t); op = (PyStringObject *)t; characters[*str & UCHAR_MAX] = op; Py_INCREF(op); } return (PyObject *) op; 169:} 传给PyString_FromString的参数必须是一个指向以NUL('\\0') 结尾的字符串的指针 根据定义我们知道 , 在创建PyStringObject时 : 首先会检查该字符串数组的长度 , 如果字符数组的长度大于PY_SSIZE_T_MAX , 那么Python将不会创建对应的PyStringObject对象 , PY_SSIZE_T_MAX是一个与平台相关的值 , 在WIN32系统下 , 该值为2147483647 , 即2GB 接下来检查传入的字符串是不是一个空串 , 对于空串 , Python并不是每一次都会创建相应的PyStringObject ; Python运行时有一个PyStringObject对象指针nullstring专门负责处理空的字符数组 , 如果第一次在一个空字符串基础上创建PyStringObject , 由于nullstring指针被初始化为NULL , 所以iPython会为这个字符建立一个PyStringObject对象 , 将这个对象通过intern机制进行共享 , 然后将nullstring指向这个被共享的对象 , 以后再创建空字符串就直接返回nullstring的引用了 如果不是创建空字符串对象 , 那么就申请内存 , 创建PyStringObject对象 ; 处理申请字符串本身所需要的内存外 , 还会申请额外的内存 , 存放了其他的属性 , 以字符数组\"Python\"为例 , 如下图 PyString_FromStringAndSize Python-2.7\\Objects\\stringobject.c : 61:PyString_FromStringAndSize(const char *str, Py_ssize_t size) { register PyStringObject *op; if (size PY_SSIZE_T_MAX - PyStringObject_SIZE) { PyErr_SetString(PyExc_OverflowError, \"string is too large\"); return NULL; } /* Inline PyObject_NewVar */ op = (PyStringObject *)PyObject_MALLOC(PyStringObject_SIZE + size); if (op == NULL) return PyErr_NoMemory(); PyObject_INIT_VAR(op, &PyString_Type, size); op->ob_shash = -1; op->ob_sstate = SSTATE_NOT_INTERNED; if (str != NULL) Py_MEMCPY(op->ob_sval, str, size); op->ob_sval[size] = '\\0'; /* share short strings */ if (size == 0) { PyObject *t = (PyObject *)op; PyString_InternInPlace(&t); op = (PyStringObject *)t; nullstring = op; Py_INCREF(op); } else if (size == 1 && str != NULL) { PyObject *t = (PyObject *)op; PyString_InternInPlace(&t); op = (PyStringObject *)t; characters[*str & UCHAR_MAX] = op; Py_INCREF(op); } return (PyObject *) op; 116:} PyString_FromStringAndSize 的操作和PyString_FromString几乎一样 , 只有一点 , PyString_FromString传入的参数必须是以NUL('\\0') 结尾的字符数组的指针 , 而PyString_FromStringAndSize则没有这个要求 , 因为通过传的size参数就可以确定需要拷贝的字符的个数 intern机制 从上面两种创建方式的源码中发现 , 无论是PyString_FromString还是PyString_FromStringAndSize , 当字符数组的长度为0或1时 , 需要进行一个特别的操作 : PyString_InternInPlace , 这就是字符串的intern机制 , 也就是上面代码中share short strings 注释下的代码 /* share short strings */ if (size == 0) { PyObject *t = (PyObject *)op; PyString_InternInPlace(&t); op = (PyStringObject *)t; nullstring = op; Py_INCREF(op); } else if (size == 1 && str != NULL) { PyObject *t = (PyObject *)op; PyString_InternInPlace(&t); op = (PyStringObject *)t; characters[*str & UCHAR_MAX] = op; Py_INCREF(op); } return (PyObject *) op; 字符串对象的intern机制的目的是 : 对于被共享之后的字符串 , 比如\"Ruby\" , 在整个Python的运行期间 , 系统中都只有唯一的一个与字符串\"Ruby\"对应的 PyStringObject对象 当判断两个字符串对象是否相同时 , 如果它们都被共享了 , 那么只需要检查它们对应的PyObject *是否相同就可以了 , 这个机制节省了空间 , 如下 : # Python 2.7 >>> str1 = 'lyon' >>> str2 = 'lyon' >>> id(str1) 79116928L >>> id(str2) 79116928L # Python 3.5.4 >>> str1 = 'lyon' >>> str2 = 'lyon' >>> id(str1) 2767446375480 >>> id(str2) 2767446375480 这个例子的创建过程 : 因为'lyon' 对象不存在 , 所以调用接口创建PyStringObject对象 (创建时经过intern机制处理) Python在查找系统中记录的已经被intern机制处理了的PyStringObject 对象 (上一步中同样会进行查找) , 发现'lyon'字符数组对应的PyStringObject已经存在 , 于是返回该对象的引用返回 PyString_InternInPlace 我们已经知道了创建字符串对象时进行了特殊的操作PyString_InternInPlace , 其源码如下 : Python-2.7\\Objects\\stringobject.c : 4712:void PyString_InternInPlace(PyObject **p) { register PyStringObject *s = (PyStringObject *)(*p); PyObject *t; // 对PyStringObject进行类型和状态检查 if (s == NULL || !PyString_Check(s)) Py_FatalError(\"PyString_InternInPlace: strings only please!\"); /* If it's a string subclass, we don't really know what putting it in the interned dict might do. */ if (!PyString_CheckExact(s)) return; if (PyString_CHECK_INTERNED(s)) return; // 创建记录经intern机制处理后的PyStringObject的dict if (interned == NULL) { interned = PyDict_New(); if (interned == NULL) { PyErr_Clear(); /* Don't leave an exception */ return; } } // 检查PyStringObject对象s是否存在对应的intern后的PyStrinObject对象 t = PyDict_GetItem(interned, (PyObject *)s); if (t) { // 调整引用计数 Py_INCREF(t); Py_DECREF(*p); *p = t; return; } // 在interned中记录检查PyStringObject对象s if (PyDict_SetItem(interned, (PyObject *)s, (PyObject *)s) PyString_InternInPlace 首先会进行一系列检查 : 检查传入的对象是否是一个PyStringObject对象 , intern机制只能应用在PyStringObject对象上 , 甚至对于它的派生类对象系统都不会应用intern机制 检查传入的PyStringObject对象是否已经被intern机制处理过 在代码中 , 我们可以清楚的看到 , intern机制的核心在于interned , 它指向一个由PyDict_new创建的对象 , 也就是一个字典 , 也就是说intern机制的关键就是在系统中有一个存在映射关系的集合 , 它的名字叫做interned , 这个集合里面记录了被intern机制处理过的 特殊的引用计数 intern机制进行处理时 , 会将PyStringObject对象的PyObject指针分别作为key和value添加到interned中, 也就是说在这里该对象的引用计数应该加了2 , 如果按照正常的引用计数机制 , 那么明显这个对象是永远都不会被删除的 , 比如a = 1;del a , 我们只能够让引用计数减1 , 却无法让其减2 , 所以这里肯定用了特殊的引用计数机制 特殊就在于 , interned中的指针不能作为对象的有效引用 , 这也是为什么在PyString_InternInPlace的代码清单中第4746行为什么会将引用计数减2的原因 一个对象的引用计数在某个时刻减为0之后 , 系统将会销毁该对象 , 那么字符串中到底是怎么解决的呢 ? 看看string_dealloc代码清单 : Python-2.7\\Objects\\stringobject.c : 582:static void string_dealloc(PyObject *op) { switch (PyString_CHECK_INTERNED(op)) { case SSTATE_NOT_INTERNED: break; case SSTATE_INTERNED_MORTAL: /* revive dead object temporarily for DelItem */ Py_REFCNT(op) = 3; if (PyDict_DelItem(interned, op) != 0) Py_FatalError( \"deletion of interned string failed\"); break; case SSTATE_INTERNED_IMMORTAL: Py_FatalError(\"Immortal interned string died.\"); default: Py_FatalError(\"Inconsistent interned string state.\"); } Py_TYPE(op)->tp_free(op); 602:} 在这份代码清单中 , SSTATE_INTERNED_MORTAL 和 SSTATE_INTERNED_IMMORTAL 表示着PyStringObject的两种状态 , 也就是说被intern机制处理后的PyStringObject对象分为两类 , 这两类的区别在于 , SSTATE_INTERNED_IMMORTAL 状态的PyStringObject对象是永远不会被销毁的 PyString_IntenInPlace 只能创建SSTATE_INTERNED_MORTAL 状态的PyStringObject对象 , 如果想创建SSTATE_INTERNED_IMMORTAL状态的对象 , 必须通过另外的接口 , 在调用了PyString_InternInPlace后 , 强制改变PyStringObject的intern状态 注意 : intern机制节省了内存空间 , 但是在我们创建PyStringObject时 , 无论在interned中是否存在 , 都是会创建一个PyStringObject对象的 , 只不过这是一个临时的对象 , 如果interned中有 , 那么就PyString_InternInPlace 会对这个对象的引用计数减1 , 于是它就会被销毁了 字符缓冲池 与Python整数对象类似 , Python的设计者为PyStringObject中的一个字节的字符对应的PyStringObject对象也设计了一个对象池characters Python-2.7\\Objects\\stringobject.c : 13:static PyStringObject *characters[UCHAR_MAX + 1] 其中UCHAR_MAX是在系统头文件中定义的常量 , 这一个跟平台相关的常量 , 在Win32平台下 : #define UCHAR_MAX 0xff /* maximum unsigned char value */ 在Python的整数对象体系中 , 小整数的缓冲池是在Python初始化时被创建的 , 而字符串对象体系中的字符串缓冲池则是以静态变量的形式存在着的 , 在Python初始化完成之后 , 缓冲池中的所有PyStringObject指针都为空 当我们创建一个字符串对象时 , 无论是通过调用PyString_FromString 还是PyString_FromStringAndSize , 如果字符串实际上是一个字符 , 则会对所创建字符串 (字符) 对象进行intern操作 , 再将intern的结果缓存到字符缓冲池characters中 万恶的加号 字符串拼接绝对是再正常不过的事情了 , 一拼接 , 那么效率问题就来了 Python中提供了 \"+\" 来进行字符串拼接 , 可惜这实际上就是万恶之源 ; 我们除了使用\"+\" 外 , 还有一种方法就是使用list的join方法 , 这也是官方推荐我们使用的 \"+\" 与 join 通过\"+\"操作符对字符串进行拼接时 , 会调用string_concat函数 : 1014:static PyObject * string_concat(register PyStringObject *a, register PyObject *bb) { register Py_ssize_t size; register PyStringObject *op; ...... #define b ((PyStringObject *)bb) /* Optimize cases with empty left or right operand */ ...... // 计算字符串连接后的长度size size = Py_SIZE(a) + Py_SIZE(b); /* Check that string sizes are not negative, to prevent an overflow in cases where we are passed incorrectly-created strings with negative lengths (due to a bug in other code). */ ...... // 创建新的PyStringObject对象,其维护的用于存储字符的内存长度为size op = (PyStringObject *)PyObject_MALLOC(PyStringObject_SIZE + size); if (op == NULL) return PyErr_NoMemory(); PyObject_INIT_VAR(op, &PyString_Type, size); op->ob_shash = -1; op->ob_sstate = SSTATE_NOT_INTERNED; // 将a和b中的字符拷贝到新创建的PyStringObject中 Py_MEMCPY(op->ob_sval, a->ob_sval, Py_SIZE(a)); Py_MEMCPY(op->ob_sval + Py_SIZE(a), b->ob_sval, Py_SIZE(b)); op->ob_sval[size] = '\\0'; return (PyObject *) op; #undef b 1071:} 小结 : 对于任意两个PyStringObject对象的连接 , 就会进行一次内存申请的动作 通过join函数对字符串进行拼接时 , 会调用string_join函数 : 1573:static PyObject * string_join(PyStringObject *self, PyObject *orig) { char *sep = PyString_AS_STRING(self); const Py_ssize_t seplen = PyString_GET_SIZE(self); PyObject *res = NULL; char *p; Py_ssize_t seqlen = 0; size_t sz = 0; Py_ssize_t i; PyObject *seq, *item; // 拼接字符 seq = PySequence_Fast(orig, \"\"); if (seq == NULL) { return NULL; } // 拼接字符长度 seqlen = PySequence_Size(seq); if (seqlen == 0) { Py_DECREF(seq); return PyString_FromString(\"\"); } if (seqlen == 1) { item = PySequence_Fast_GET_ITEM(seq, 0); if (PyString_CheckExact(item) || PyUnicode_CheckExact(item)) { Py_INCREF(item); Py_DECREF(seq); return item; } } /* There are at least two things to join, or else we have a subclass * of the builtin types in the sequence. * Do a pre-pass to figure out the total amount of space we'll * need (sz), see whether any argument is absurd, and defer to * the Unicode join if appropriate. */ // 遍历list中每一个字符串,获取所有字符串长度 for (i = 0; i tp_name); Py_DECREF(seq); return NULL; } sz += PyString_GET_SIZE(item); if (i != 0) sz += seplen; if (sz PY_SSIZE_T_MAX) { PyErr_SetString(PyExc_OverflowError, \"join() result is too long for a Python string\"); Py_DECREF(seq); return NULL; } } /* Allocate result space. */ // 创建长度为sz的PyStringObject对象 res = PyString_FromStringAndSize((char*)NULL, sz); if (res == NULL) { Py_DECREF(seq); return NULL; } /* Catenate everything. */ // 将list中的字符串拷贝到新创建的PyStringObject对象中 p = PyString_AS_STRING(res); for (i = 0; i 小结 : 首先统计出list中的对象个数 , 并统计这些对象的字符串总长度 , 申请一次内存空间 , 将所有的PyStringObject对象维护的字符串都拷贝到新开辟的内存空间中 通过小结可以很直接的得出答案 , 如果要拼接n个字符串对象 , 那么使用 \"+\" 需要申请空间n-1次 , 而使用join则仅需一次 "},"01-Python/07-内存篇/05-List对象.html":{"url":"01-Python/07-内存篇/05-List对象.html","title":"List对象","keywords":"","body":"Attack on Python - List对象 🐍 介绍 元素的一个群是一个非常重要的抽象概念 , 我们可以将符合某一特性的一堆元素聚集为一个群 群的概念对于编程语言十分重要 , C语言就内建了数组的概念 , 每一种实现都为某种目的的元素聚集或元素访问提供极大的方便 PyListObject是Python提供的对列表的抽象 , 它可以支持对元素的插入 , 删除 , 添加等操作 , 所以它是一个可变对象 PyListObject Python-2.7\\Include\\listobject.h 22:typedef struct { 23: PyObject_VAR_HEAD 24: /* Vector of pointers to list elements. list[0] is ob_item[0], etc. */ 25: PyObject **ob_item; 26: 27: /* ob_item contains space for 'allocated' elements. The number 28: * currently in use is ob_size. 29: * Invariants: 30: * 0 分析 : PyObject_VAR_HEAD , Python中的列表是一个变长对象 PyObject **ob_item , ob_item为指向元素列表的指针 , 实际上 , Python中的list[0] 就是ob_item[0] Py_ssize_t allocated , 与PyListObject对象的内存管理有关 实际上 , 在PyObject_VAR_HEAD中的ob_size和allocated 都和PyListObject对象的内存管理有关 : PyListObject采用的内存管理策略和C++中vector采取的内存管理策略是一样的 , 它并不是存了多少东西就申请对应大小的内存 , 因为这样的策略显然是低效的 , 而我们使用列表就是为了用户方便用户频繁地插入或删除元素 , 所以 , 在每一次需要申请内存的时候 , PyListObject总会申请一大块内存 , 这时申请的总内存的大小记录在allocated中 , 而其实际被使用了的内存的数量记录在了ob_size中 假如有一个能容纳10个元素的列表已经装入了5个元素 , 那么这个列表的ob_size就是5 , 而allcoated则是10 即 : 0 在Python-3.5.4\\Include\\listobject.h的22至40行 , 我们可以找到相同的代码 , 也就是说2.7与3.5.4的这一部分是没有区别的 创建与维护 在之前对于Python对象创建方式已有说明 , 为了创建一个列表 , Python只提供了唯一的一条途径 , 就是PyList_New Python-2.7\\Objects\\listobject.c 112:PyObject * PyList_New(Py_ssize_t size) { PyListObject *op; size_t nbytes; #ifdef SHOW_ALLOC_COUNT static int initialized = 0; if (!initialized) { Py_AtExit(show_alloc); initialized = 1; } #endif if (size PY_SIZE_MAX / sizeof(PyObject *)) return PyErr_NoMemory(); // 计算需要使用的内存总量 nbytes = size * sizeof(PyObject *); if (numfree) { // 缓冲池可用 numfree--; op = free_list[numfree]; _Py_NewReference((PyObject *)op); #ifdef SHOW_ALLOC_COUNT count_reuse++; #endif } else { // 缓冲池不可用 op = PyObject_GC_New(PyListObject, &PyList_Type); if (op == NULL) return NULL; #ifdef SHOW_ALLOC_COUNT count_alloc++; #endif } // 为对象中维护的元素列表申请空间 if (size ob_item = NULL; else { op->ob_item = (PyObject **) PyMem_MALLOC(nbytes); if (op->ob_item == NULL) { Py_DECREF(op); return PyErr_NoMemory(); } memset(op->ob_item, 0, nbytes); } Py_SIZE(op) = size; op->allocated = size; _PyObject_GC_TRACK(op); return (PyObject *) op; 163:} 分析 : 这个函数接受一个size参数 , 也就是我们可以在创建时指定PyListObject对象的初始元素个数 在创建时 , 首先计算需要使用的内存总量 , 因为PyList_New指定的仅仅是元素的个数 , 而不是元素实际将占用的内存空间 , 在这里 , Python会检查指定的元素个数是否会大到使所需内存数量产生溢出的程度 , 并根据判断结果做出相应的操作 检查缓冲池是否可用 为维护对象申请内存空间 , 维护对象与PyListOjbect对象本身通过ob_item建立了连接 当Python创建了新的PyListObject对象之后 , 会立即根据调用PyList_New时传递的size参数创建PyListObject对象所维护的元素列表 , 其中每一个元素都被初始化为NULL 在完成了PyListObject对象及维护的列表的创建之后 , Python会调整该PyListObject对象 , 用于维护元素列表中元素数量的ob_size和allocated两个变量 对于缓冲池free_list中的对象个数 , 我们可以在源码中找到 , free_list最多会维护80个PyListObject对象 Python-2.7\\Objects\\listobject.c 94:#ifndef PyList_MAXFREELIST 95:#define PyList_MAXFREELIST 80 96:#endif 97:static PyListObject *free_list[PyList_MAXFREELIST]; 98:static int numfree = 0; Python-3.5.4\\Objects\\listobject.c 95:#ifndef PyList_MAXFREELIST 96:#define PyList_MAXFREELIST 80 97:#endif 98:static PyListObject *free_list[PyList_MAXFREELIST]; 99:static int numfree = 0; 设置元素 在我们创建第一个PyListObject对象时 , 这时候缓冲池是不可用的 , 于是会调用PyObject_GC_New在系统堆上创建一个新的PyListObject对象 , 假如我们创建一个包含6个元素的PyListObject , 那么创建成功之后 , 这个对象的ob_size为6 , allocated为6 , 而ob_item则是指向这些元素的指针 而当我们设置元素时 , 如现有一个列表la = [1, 2, 3] , 当我们执行la[0] = 4时 , 在Python内部 , 会调用PyList_SetItem来完成这个动作 ; 首先Python会进行类型检查 , 随后会进行索引的有效性检查 , 当这两者都通过后 , 将新设置的元素指针放到指定的位置 , 然后调整引用计数 , 将这个位置原来存放的对象的引用计数减1 , 源码如下 : Python-2.7\\Objects\\listobject.c 198:int PyList_SetItem(register PyObject *op, register Py_ssize_t i, register PyObject *newitem) { register PyObject *olditem; register PyObject **p; if (!PyList_Check(op)) { Py_XDECREF(newitem); PyErr_BadInternalCall(); return -1; } if (i = Py_SIZE(op)) { Py_XDECREF(newitem); PyErr_SetString(PyExc_IndexError, \"list assignment index out of range\"); return -1; } p = ((PyListObject *)op) -> ob_item + i; olditem = *p; *p = newitem; Py_XDECREF(olditem); return 0; 220:} Python-3.5.4\\Objects\\listobject.c 215:int PyList_SetItem(PyObject *op, Py_ssize_t i, PyObject *newitem) { PyObject *olditem; PyObject **p; if (!PyList_Check(op)) { Py_XDECREF(newitem); PyErr_BadInternalCall(); return -1; } if (i = Py_SIZE(op)) { Py_XDECREF(newitem); PyErr_SetString(PyExc_IndexError, \"list assignment index out of range\"); return -1; } p = ((PyListObject *)op) -> ob_item + i; olditem = *p; *p = newitem; Py_XDECREF(olditem); return 0; 237:} 在两个版本中 , 没有变化 插入元素 设置元素和插入元素的动作是不同的 , 设置元素不会导致ob_item指向的内存发生变化 , 但是插入元素的动作则有可能使得ob_item指向的内存发生变化 Python内部通过调用PyList_Insert来完成元素的插入动作 , 而PyList_Insert实际上是调用了内部的insl , 如下 : Python-2.7\\Objects\\listobject.c 222:static int ins1(PyListObject *self, Py_ssize_t where, PyObject *v) { Py_ssize_t i, n = Py_SIZE(self); PyObject **items; if (v == NULL) { PyErr_BadInternalCall(); return -1; } if (n == PY_SSIZE_T_MAX) { PyErr_SetString(PyExc_OverflowError, \"cannot add more objects to list\"); return -1; } // 调整列表容量 if (list_resize(self, n+1) == -1) return -1; // 确定插入点 if (where n) where = n; // 插入元素 items = self->ob_item; for (i = n; --i >= where; ) items[i+1] = items[i]; Py_INCREF(v); items[where] = v; return 0; } 255:int 256:PyList_Insert(PyObject *op, Py_ssize_t where, PyObject *newitem) { // 类型检查 if (!PyList_Check(op)) { PyErr_BadInternalCall(); return -1; } return ins1((PyListObject *)op, where, newitem); 263:} Python-3.5.4\\Objects\\listobject.c 239:static int ins1(PyListObject *self, Py_ssize_t where, PyObject *v) { Py_ssize_t i, n = Py_SIZE(self); PyObject **items; if (v == NULL) { PyErr_BadInternalCall(); return -1; } if (n == PY_SSIZE_T_MAX) { PyErr_SetString(PyExc_OverflowError, \"cannot add more objects to list\"); return -1; } // 调整列表容量 if (list_resize(self, n+1) == -1) return -1; // 确定插入点 if (where n) where = n; // 插入元素 items = self->ob_item; for (i = n; --i >= where; ) items[i+1] = items[i]; Py_INCREF(v); items[where] = v; return 0; } 272:int 273:PyList_Insert(PyObject *op, Py_ssize_t where, PyObject *newitem) { // 类型检查 if (!PyList_Check(op)) { PyErr_BadInternalCall(); return -1; } return ins1((PyListObject *)op, where, newitem); 280:} 在insl中 , 为了完成元素的插入工作 , 首先必须保证PyListObject对象有足够的内存来容纳我们期望插入的元素 , 这一步是通过insl中的list_resize函数来实现的 , 正是这个函数改变了PyListObject所维护的PyObject * 列表的大小 Python-2.7\\Objects\\listobject.c 24:static int list_resize(PyListObject *self, Py_ssize_t newsize) { PyObject **items; size_t new_allocated; Py_ssize_t allocated = self->allocated; /* Bypass realloc() when a previous overallocation is large enough to accommodate the newsize. If the newsize falls lower than half the allocated size, then proceed with the realloc() to shrink the list. */ // 不需要重新申请内存 if (allocated >= newsize && newsize >= (allocated >> 1)) { assert(self->ob_item != NULL || newsize == 0); Py_SIZE(self) = newsize; return 0; } /* This over-allocates proportional to the list size, making room * for additional growth. The over-allocation is mild, but is * enough to give linear-time amortized behavior over a long * sequence of appends() in the presence of a poorly-performing * system realloc(). * The growth pattern is: 0, 4, 8, 16, 25, 35, 46, 58, 72, 88, ... */ // 计算重新申请的内存大小 new_allocated = (newsize >> 3) + (newsize PY_SIZE_MAX - newsize) { PyErr_NoMemory(); return -1; } else { new_allocated += newsize; } if (newsize == 0) new_allocated = 0; // 扩展列表 items = self->ob_item; if (new_allocated ob_item = items; Py_SIZE(self) = newsize; self->allocated = new_allocated; return 0; 73:} 同样的 , 在Python-3.5.4\\Objects\\listobject.c 中的第25至74行为该函数的定义 在调整PyListObject对象所维护的列表的内存时 , Python分两种情况处理 : newsize allocated/2 , 也就是说当插入后使用的实际内存大小要小于总内存大小 , 以及要大于总内存大小的一半时 , 就简单调整ob_size值 其他情况 , 调用realloc , 重新分配空间 我们可以发现 , 对于第二种情况 , 比如newsize 时 , Python也会调用realloc来收缩列表的内存空间 , 不得不说这是物尽其用的设计 删除元素 以list对象方法remove为例 , 当我们使用remove方法时 , PyListObject中的listremove操作就会被激活 Python-2.7\\Objects\\listobject.c 2336:static PyObject * listremove(PyListObject *self, PyObject *v) { Py_ssize_t i; for (i = 0; i ob_item[i], v, Py_EQ); if (cmp > 0) { if (list_ass_slice(self, i, i+1, (PyObject *)NULL) == 0) Py_RETURN_NONE; return NULL; } else if (cmp Python-3.5.4\\Objects\\listobject.c 第2197至2215见同上代码清单 首先Python会对整个列表进行遍历 , 在遍历PyListObject中所有元素的过程中 , 将待删除元素与PyListObject中的每个元素一一进行比较 , 比较操作是通过PyObject_RichCompareBool完成的 , 如果返回值大于0 , 则表示要删除的元素与列表中的元素匹配成功 , Python将立即调用list_ass_slice删除该元素 Python-2.7\\Objects\\listobject.c 607:/* a[ilow:ihigh] = v if v != NULL. // 不为空就替换 * del a[ilow:ihigh] if v == NULL. // 为空就删除 * * Special speed gimmick: when v is NULL and ihigh - ilow Python-3.5.4\\Objects\\listobject.c 第572至579见同上代码清单 如上 , 对于list_ass_slice其实是有两种语义的 , 即replace和remove ; 于是 , 在Python列表中删除元素我们还可以这样做 : # Python 2.x & 3.x >>> la = [1,2,3,4,5] >>> la[1:3] = [] >>> la [1, 4, 5] 对于list对象的pop方法 , 同样也是调用list_ass_slice来进行删除 , 源码位于listobject.c文件中 对象缓冲池 在PyList_New中我们见过一个free_list , 这就是PyListObject对象缓冲池 ; 但是我们在PyList_New中并没有看到缓冲池中的PyListObject对象的添加过程 , 这是因为缓冲池对象并不像前面的字符串对象或者整数对象一样 , 是在创建时添加的 , Python列表的缓冲池是在其销毁的时候添加的 Python-2.7\\Objects\\listobject.c 296:static void list_dealloc(PyListObject *op) { Py_ssize_t i; PyObject_GC_UnTrack(op); Py_TRASHCAN_SAFE_BEGIN(op) // 销毁PyListObject对象维护的元素列表 if (op->ob_item != NULL) { /* Do it backwards, for Christian Tismer. There's a simple test case where somehow this reduces thrashing when a *very* large list is created and immediately deleted. */ i = Py_SIZE(op); while (--i >= 0) { Py_XDECREF(op->ob_item[i]); } PyMem_FREE(op->ob_item); } // 释放PyListObject自身 if (numfree tp_free((PyObject *)op); Py_TRASHCAN_SAFE_END(op) 318:} 与PyListObject对象创建一样 , PyListObject对象的销毁也是分离的 , 首先销毁PyListObject对象所维护的元素列表 , 然后再释放PyListObject对象本身 ; 这样的工作无非是改变该对象的引用计数 , 然后再释放内存 , 但是我们发现 , 在释放PyListObject本身时 , Python会检查前面提到的这个缓冲池free_list 首先Python会查看其中缓存的PyListObject对象的数量是否已经满了 , 如果没有 , 就将该待删除的PyListObject对象放到缓冲池中 , 以备后用 注意 , 我们也已经发现了 , 添加进缓冲池的是PyListObject对象本身 , 而不包括它之前维护的元素列表 , 也就是说我们在创建新的PyListObject时 , Python会首先唤醒这些已经 \"死去\" 的PyListObject , 然后赋予它们新的元素列表 , 使其能够重新做 \"人\" 对于每次创建PyListObject对象时必须创建元素列表 , 这是Python为了避免过多的消耗系统内存 , 采取的时间换空间的做法 "},"01-Python/07-内存篇/06-Dict对象.html":{"url":"01-Python/07-内存篇/06-Dict对象.html","title":"Dict对象","keywords":"","body":"Attack on Python - Dict对象 🐍 介绍 为了刻画某种元素之间的对应关系 , 现代编程语言通常都在语言级或标准库中提供某种关联式的容器 ; 关联容器的设计总会极大地关注键的搜索效率 , 因为我们希望根据我们手中已有的某个元素来快速获得与之有某种联系的另一元素 在Python中同样提供关联式容器 , 即PyDictObject 对象 , 与map不同的是 , PyDictObject对搜索的效率要求极其苛刻 , 这也是因为PyDictObject对象在Python本身的实现中被大量采用 ; 比如Python会通过PyDictObject来建立执行Python字节码的运行环境 , 其中会存放变量名和变量值的元素对 , 通过查找变量名获得变量值 , 因此PyDictObject采用的是散列表 (hash table) , 因为理论上 , 在最优情况下 , 散列表能提供O(1)复杂度的搜索效率 散列表 散列表的基本思想 , 是通过一定的函数将需搜索的键值映射为一个整数 , 将这个整数视为索引值去访问某片连续的区域 对散列表这种数据结构的采用是以加速键的搜索过程为终极目标的 , 所以 , 将元素映射为整数的过程对于Python中dict的实现就显得尤为关键 ; 用于映射的函数称为散列函数 (hash function) , 映射后的值称为元素的散列值 (hash value) , 在散列表的实现中 , 所选择的散列函数的优劣直接决定所实现的散列表的搜索效率的高低 在使用散列表的过程中 , 不同的对象经过散列函数的作用 , 可能被映射为相同的散列值 , 这就是散列冲突 根据研究表明 , 当散列表的装载率大于2/3时 , 散列冲突发生的概率就会大大增加 解决散列冲突的方法有很多种 , 在Python中采用的是开放定址法 当产生散列冲突时 , Python会通过一个二次探测函数f , 计算下一个候选位置addr , 如果位置addr可用 , 则可将待插入元素放到位置addr ; 如果位置addr不可用 , 则Python会再次使用探测函数f , 获得下一个候选位置 , 以此依次寻找下去 最后 , 这些位置会形成一个\"冲突探测链\"(或简称探测序列) , 而当我们要删除某条探测链上的某个元素时 , 按照探测链会发生什么样的情况 ; 假如这条链的首元素位置为a , 尾元素的位置为c , 现在需要删除中间的某个位置b上的元素 , 如果直接将位置b上的元素删除 , 则会导致探测链的断裂 , 于是探测函数在探测时将再也不能到达位置c了 , 所以删除某条探测链上的元素时不能进行真正的删除 , 而是进行一种 \"伪删除\" 操作 , 必须要让该元素还存在于探测链上 在Python中 , 这种伪删除是在PyDictObject对象中实现的 PyDictObject 在Python2.7中 , 关联容器的一个(键 , 值)元素对称为一个entry或slot Python-2.7\\Include\\dictobject.h 50:typedef struct { /* Cached hash code of me_key. Note that hash codes are C longs. * We have to use Py_ssize_t instead because dict_popitem() abuses * me_hash to hold a search finger. */ Py_ssize_t me_hash; PyObject *me_key; PyObject *me_value; 58:} PyDictEntry; 在PyDictEntry中 , me_hash域存储的是me_key的散列值 , 利用一个域来记录这个散列值可以避免每次查询的时候都要重新计算一遍散列值 在Python中 , 在一个PyDictObject对象生存变化的过程中 , 其中的entry会在不同的状态间转换 ; PyDictObject中entry可以在3种状态之间转换 : Unused , Active , Dummy Unused : 当一个entry的me_key和me_value都为NULL时 , entry处于Unused态 ; 表明目前该entry中并没有存储(key , value)对 , 而且在此之前 , 也没有存储过它们 , 这时每一个entry在初始化时的状态 , 并且也只有在Unused态下 , entry的me_key域才会为NULL Active : 当entry中存储了一个(key , value)对时 , entry便转到了Active态 , 在Active态下 , me_key和me_value都不能为NULL Dummy : 当entry中存储的(key , value)对被删除后 , entry的状态不能直接从Active态转为Unused态 , 因为这样会导致冲突探测链的中断 , 所以entry中的me_key将指向dummy对象 , 从而entry进入Dummy态 , 这就是\"伪删除\"技术 ; 当Python沿着某条冲突链搜索时 , 如果发现一个entry处于Dummy态 , 说明目前该entry虽然是无效的 , 但是其后的entry可能是有效的 , 是应该被搜索的 , 这样就保证了冲突探测链的连续性 在Python中 , 关联容器是通过PyDictObject对象来实现的 , 而一个PyDictObject 对象实际上是一大堆entry的集合 : Python-2.7\\Include\\dictobject.h 70:typedef struct _dictobject PyDictObject; struct _dictobject { PyObject_HEAD Py_ssize_t ma_fill; /* # Active + # Dummy */ Py_ssize_t ma_used; /* # Active */ /* The table contains ma_mask + 1 slots, and that's a power of 2. * We store the mask instead of the size because the mask is more * frequently needed. */ Py_ssize_t ma_mask; /* ma_table points to ma_smalltable for small tables, else to * additional malloc'ed memory. ma_table is never NULL! This rule * saves repeated runtime null-tests in the workhorse getitem and * setitem calls. */ PyDictEntry *ma_table; PyDictEntry *(*ma_lookup)(PyDictObject *mp, PyObject *key, long hash); PyDictEntry ma_smalltable[PyDict_MINSIZE]; 90:}; 定义说明 : ma_fill , ma_fill域中维护着从PyDictObject对象创建开始直到现在 , 曾经及正处于Active态的entry个数 , 而ma_used则维护着当前正处于Active态的entry的数量 在定义的最后 , 有一个名为ma_smalltable的PyDictEntry数组 , 这个数组意味着当创建一个PyDictObject对象时 , 至少有PyDict_MINSIZE个entry被同时创建 , 在dictobject.h中 , 这个值被设定为8 , 这个值被认为时通过大量的实验得出的最佳值 ; 它既不会态浪费内存空间 , 又能很好地满足Python内部大量使用PyDictObject的环境的需求 ma_table , ma_table域是关联对象的关键所在 , 它将指向一片作为PyDictEntry集合的内存的开始位置 , 当一个PyDictObject对象是一个比较小的dict时 (entry数量少于8) , ma_table域将指向ma_smalltable , 而当PyDictObject中的entry数量超过8个时 , 将会申请额外的内存空间 , 并将ma_table指向这块空间 , 这样 , 无论何时 , ma_table域都不会为NULL , 那么在程序运行时就不需要一次又一次的检查ma_table的有效性了 , 因为ma_table总是有效的 , 这两种ma_table见下图 ma_mask , PyDictObject中的ma_mask记录了一个PyDictObject对象中所拥有的entry数量 创建与维护 Python内部通过PyDict_New来创建一个新的dict对象 Python-2.7\\Include\\dictobject.c 210:#define INIT_NONZERO_DICT_SLOTS(mp) do { \\ (mp)->ma_table = (mp)->ma_smalltable; \\ // PyDict_MINSIZE定义在dictobject.h中,默认值为8 (mp)->ma_mask = PyDict_MINSIZE - 1; \\ } while(0) \\ #define EMPTY_TO_MINSIZE(mp) do { \\ memset((mp)->ma_smalltable, 0, sizeof((mp)->ma_smalltable)); \\ (mp)->ma_used = (mp)->ma_fill = 0; \\ INIT_NONZERO_DICT_SLOTS(mp); \\ 219:} while(0) 220:/* Dictionary reuse scheme to save calls to malloc, free, and memset */ #ifndef PyDict_MAXFREELIST #define PyDict_MAXFREELIST 80 #endif static PyDictObject *free_list[PyDict_MAXFREELIST]; 226:static int numfree = 0; ...... 240:PyObject * PyDict_New(void) { register PyDictObject *mp; // 自动创建dummy对象 if (dummy == NULL) { /* Auto-initialize dummy */ dummy = PyString_FromString(\"\"); if (dummy == NULL) return NULL; #ifdef SHOW_CONVERSION_COUNTS Py_AtExit(show_counts); #endif #ifdef SHOW_ALLOC_COUNT Py_AtExit(show_alloc); #endif #ifdef SHOW_TRACK_COUNT Py_AtExit(show_track); #endif } if (numfree) { // 使用缓冲池 mp = free_list[--numfree]; assert (mp != NULL); assert (Py_TYPE(mp) == &PyDict_Type); _Py_NewReference((PyObject *)mp); if (mp->ma_fill) { EMPTY_TO_MINSIZE(mp); } else { /* At least set ma_table and ma_mask; these are wrong if an empty but presized dict is added to freelist */ INIT_NONZERO_DICT_SLOTS(mp); } assert (mp->ma_used == 0); assert (mp->ma_table == mp->ma_smalltable); assert (mp->ma_mask == PyDict_MINSIZE - 1); #ifdef SHOW_ALLOC_COUNT count_reuse++; #endif } else { // 创建PyDictObject对象 mp = PyObject_GC_New(PyDictObject, &PyDict_Type); if (mp == NULL) return NULL; EMPTY_TO_MINSIZE(mp); #ifdef SHOW_ALLOC_COUNT count_alloc++; #endif } mp->ma_lookup = lookdict_string; #ifdef SHOW_TRACK_COUNT count_untracked++; #endif #ifdef SHOW_CONVERSION_COUNTS ++created; #endif return (PyObject *)mp; 293:} 在定义的开始部分我们可以发现 , 自动创建dummy对象 , 这个dummy对象竟然时一个PyStringObject对象 , 实际上 , 它仅仅时用来作为一种指示标志 , 表明该entry曾被使用过 , 且探测序列下一个位置的entry有可能时有效的 , 从而防止探测序列中断 如果不使用缓冲池 , 创建时将调用EMPTY_TO_MINSIZE , 将ma_smalltable清零 , 同时设置ma_size和ma_fill , 初始时 , 这两个变量都为0 , 随后调用INIT_NONZERO_DICT_SLOTS , 其功能是将ma_table指向ma_smalltable , 并设置ma_mask为7 在创建过程的最后 , 将lookdict_string赋给了ma_lookup , 这个ma_lookup指定了PyDictObjec在entry集合中搜索某一特定entry时需要进行的动作 , 在ma_lookup中 , 包含了散列函数和发生冲突时二次探测函数的具体实现 , 它时PyDictObject的搜索策略 PyDictObject缓冲池见下文 元素搜索 Python为PyDictObject对象提供了两种搜索策略 , lookdict和lookdict_string , 但是实际上 , 这两种策略使用的相同的算法 , lookdict_string只是对lookdict的一种针对PyStringObject对象的特殊形式 , 这是因为以PyStringObject对象作为PyDictObject对象中entry的键在Python中应用非常广泛 lookdict Python-2.7\\Include\\dictobject.c 319:static PyDictEntry * lookdict(PyDictObject *mp, PyObject *key, register long hash) { register size_t i; register size_t perturb; register PyDictEntry *freeslot; register size_t mask = (size_t)mp->ma_mask; PyDictEntry *ep0 = mp->ma_table; register PyDictEntry *ep; register int cmp; PyObject *startkey; // 散列,定位冲突探测链的第一个entry 331: i = (size_t)hash & mask; ep = &ep0[i]; // entry处于Unused态 if (ep->me_key == NULL || ep->me_key == key) return ep; // entry处于Dummy态 if (ep->me_key == dummy) 337: freeslot = ep; else { // 检查Active态entry if (ep->me_hash == hash) { startkey = ep->me_key; Py_INCREF(startkey); cmp = PyObject_RichCompareBool(startkey, key, Py_EQ); Py_DECREF(startkey); if (cmp ma_table && ep->me_key == startkey) { if (cmp > 0) return ep; } else { /* The compare did major nasty stuff to the * dict: start over. * XXX A clever adversary could prevent this * XXX from terminating. */ return lookdict(mp, key, hash); } } freeslot = NULL; } //------------------ 以上为第一检查-------------------- /* In the loop, me_key == dummy is by far (factor of 100s) the least likely outcome, so test for that last. */ // 寻找探测链上的下一个entry for (perturb = hash; ; perturb >>= PERTURB_SHIFT) { i = (i me_key == NULL) return freeslot == NULL ? ep : freeslot; // 检查引用是否相同 if (ep->me_key == key) return ep; // 检查值是否相同 if (ep->me_hash == hash && ep->me_key != dummy) { startkey = ep->me_key; Py_INCREF(startkey); cmp = PyObject_RichCompareBool(startkey, key, Py_EQ); Py_DECREF(startkey); if (cmp ma_table && ep->me_key == startkey) { if (cmp > 0) return ep; } else { /* The compare did major nasty stuff to the * dict: start over. * XXX A clever adversary could prevent this * XXX from terminating. */ return lookdict(mp, key, hash); } } // 设置freeslot else if (ep->me_key == dummy && freeslot == NULL) freeslot = ep; } assert(0); /* NOT REACHED */ return 0; 396:} 第一次检查 PyDictObject中维护的entry的数量是有限的 , 而传入lookdict中的key的hash值却不一定在限定范围内 , 所以这就要求lookdict将hash值映射到某个entry上去 , lookdict采取的策略是 , 直接将hash值与entry的数量做一个&操作(见331行) , 该操作的结果就是entry的数量 , 也就是ma_mask 之所以命名为mask而不是size , 是因为ma_mask会被用来进行大量的&操作 , 所以entry数量相关的变量被命名为ma_mask freeslot指向一个指示失败且立即可用的entry : 在搜索过程中 , 如果探测链中的某个位置上 , entry处于Dummy态 , 那么如果在这个序列中搜索不成功 , 就会返回这个处于Dummy态的entry , 这个freeslot正是用来指向探测序列中第一个处于Dummy态的entry (me_value为NULL); 如果探测序列并没有Dummy态entry , 搜索失败时 , freeslot则指向一个处于Unused态的entry , 同样是一个能指示失败且立即可用的entry 在元素搜索时 , 会先进行两个key的值检查 , 首先检查两个对象的hash值是否相同 , 如果不相同 , 就直接中断 ; 而如果相同 , 那么Python将通过PyObject_RichCompareBool进行比较 , 其原型如下 : int PyObject_RichCompareBool(PyObject *v, PyObject *w, int op) 当v op w成立时 , 返回1 ; 不成立时 , 返回0 ; 如果在比较中发生了错误返回-1 在lookdict代码清单中 , 指定的Py_EQ , 表示进行相等比较操作 对于lookdict代码清单的前半部分 , 也就是第一次检查小结 : 根据hash值获取entry索引 , 这是冲突探测链上的第一个entry索引 两种情况下 , 搜索结束 : entry处于Unused态 , 表明冲突探测链搜索完成 , 搜索失败 ep->me_key == key , 表明entry的key与待搜索的key匹配 , 搜索成功 若当前entry处于Dummy态 , 设置freeslot 检查Active态entry中的key与待查找的key是否值相同 后续操作 在第一个entry检查完毕后 , 后续的动作本质都是一样的 对于lookdict代码清单的前半部分小结 : 根据Python所采用的探测函数 , 获得探测链中的下一个待检查的entry 检查到一个Unused态entry , 表明搜索失败 , 有如下两种结果 : 如果freeslot不为空 , 则返回freeslot 所指entry 如果freeslot为空 , 则返回该Unused态entry 检查entry中的key与待查找的key是否引用相同 检查entry中的key与待查找的key是否值相同 在遍历过程中 , 如果发现Dummy态entry , 且freeslot未设置 , 则设置freeslot lookdict_string Python-2.7\\Include\\dictobject.c 407:static PyDictEntry * lookdict_string(PyDictObject *mp, PyObject *key, register long hash) { register size_t i; register size_t perturb; register PyDictEntry *freeslot; register size_t mask = (size_t)mp->ma_mask; PyDictEntry *ep0 = mp->ma_table; register PyDictEntry *ep; /* Make sure this function doesn't have to handle non-string keys, including subclasses of str; e.g., one reason to subclass strings is to override __eq__, and for speed we don't cater to that here. */ // 选择搜索策略 if (!PyString_CheckExact(key)) { #ifdef SHOW_CONVERSION_COUNTS ++converted; #endif mp->ma_lookup = lookdict; return lookdict(mp, key, hash); } // 检查冲突链上第一个entry i = hash & mask; ep = &ep0[i]; // entry处于Unused态,entry中的key与待搜索的key匹配 if (ep->me_key == NULL || ep->me_key == key) return ep; // 第一个entry处于Dummy态,设置freeslot if (ep->me_key == dummy) freeslot = ep; else { // 检查Active态entry if (ep->me_hash == hash && _PyString_Eq(ep->me_key, key)) return ep; freeslot = NULL; } /* In the loop, me_key == dummy is by far (factor of 100s) the least likely outcome, so test for that last. */ // 遍历冲突链,检查每一个entry for (perturb = hash; ; perturb >>= PERTURB_SHIFT) { i = (i me_key == NULL) return freeslot == NULL ? ep : freeslot; if (ep->me_key == key || (ep->me_hash == hash && ep->me_key != dummy && _PyString_Eq(ep->me_key, key))) return ep; if (ep->me_key == dummy && freeslot == NULL) freeslot = ep; } assert(0); /* NOT REACHED */ return 0; 457:} lookdict_string是一种有条件限制的搜索策略 , 即待搜索的key是一个PyStringObject对象 , 只有当假设成立时 , lookdict_string才会被使用 , 其中_PyString_Eq将保证能正确处理非PyStringObject *参数 其实lookdict_string仅仅是一个lookdict的优化版本 , 因为在Python中大量的使用了PyDictObject对象 , 以用来维护一个命名空间(名字空间)中变量名与变量值之间的对应关系 , 又或者是用来在为函数传递参数名与参数值的对应关系 , 而这些对象几乎都是用PyStringObject对象作为entry中的key , 所以lookdict_string的出现是很有必要的 , 它对Python整体的运行效率都有着重要的影响 插入与删除 PyDictObject对象中元素的插入动作是建立在搜索的基础之上的 Python-2.7\\Include\\dictobject.c 512:static int insertdict(register PyDictObject *mp, PyObject *key, long hash, PyObject *value) { PyObject *old_value; register PyDictEntry *ep; typedef PyDictEntry *(*lookupfunc)(PyDictObject *, PyObject *, long); assert(mp->ma_lookup != NULL); ep = mp->ma_lookup(mp, key, hash); if (ep == NULL) { Py_DECREF(key); Py_DECREF(value); return -1; } MAINTAIN_TRACKING(mp, key, value); // 搜索成功 if (ep->me_value != NULL) { old_value = ep->me_value; ep->me_value = value; Py_DECREF(old_value); /* which **CAN** re-enter */ Py_DECREF(key); } // 搜索失败 else { if (ep->me_key == NULL) mp->ma_fill++; else { assert(ep->me_key == dummy); Py_DECREF(dummy); } ep->me_key = key; ep->me_hash = (Py_ssize_t)hash; ep->me_value = value; mp->ma_used++; } return 0; 546:} insertdict中 , 根据搜索的结果采取不同的动作 : 搜索成功 , 返回处于Active的entry , 并直接替换me_value 搜索失败 , 返回Unused或Dummy态的entry , 完整设置me_key , me_hash 和 me_value 在Python中 , 对PyDictObject对象插入或设置元素两种情况 , 如下代码 : d = {} # entry不存在 d[1] = 1 # entry已存在 d[1] = 2 当这段代码执行时 , Python并不是直接调用insertdict , 因为insertdict需要一个hash值作为调用参数 , 所以在调用insertdict会先调用PyDict_SetItem Python-2.7\\Include\\dictobject.c 747:int PyDict_SetItem(register PyObject *op, PyObject *key, PyObject *value) { register PyDictObject *mp; register long hash; register Py_ssize_t n_used; if (!PyDict_Check(op)) { PyErr_BadInternalCall(); return -1; } assert(key); assert(value); mp = (PyDictObject *)op; // 计算hash值 if (PyString_CheckExact(key)) { hash = ((PyStringObject *)key)->ob_shash; if (hash == -1) hash = PyObject_Hash(key); } else { hash = PyObject_Hash(key); if (hash == -1) return -1; } assert(mp->ma_fill ma_mask); /* at least one empty slot */ // 插入(key, value)元素对 n_used = mp->ma_used; Py_INCREF(value); Py_INCREF(key); // 必要时调整dict的内存空间 if (insertdict(mp, key, hash, value) != 0) return -1; /* If we added a key, we can safely resize. Otherwise just return! * If fill >= 2/3 size, adjust size. Normally, this doubles or * quaduples the size, but it's also possible for the dict to shrink * (if ma_fill is much larger than ma_used, meaning a lot of dict * keys have been * deleted). * * Quadrupling the size improves average dictionary sparseness * (reducing collisions) at the cost of some memory and iteration * speed (which loops over every possible entry). It also halves * the number of expensive resize operations in a growing dictionary. * * Very large dictionaries (over 50K items) use doubling instead. * This may help applications with severe memory constraints. */ // 可转换为 (mp->mafill)/(mp->ma_mask+1) >= 2/3 if (!(mp->ma_used > n_used && mp->ma_fill*3 >= (mp->ma_mask+1)*2)) return 0; return dictresize(mp, (mp->ma_used > 50000 ? 2 : 4) * mp->ma_used); 794:} 我们可以看到 , 在PyDict_SetItem中 , 会首先获取key的hash值 , 随后会调用insertdict来插入元素对 , 再接下来会检查是否需要改变PyDictObject内部ma_table所维护的内存区域的大小 至于如何调整 , 可以查看dictobject.c中的dictresize函数 , 接下来看如何删除一个元素 Python-2.7\\Include\\dictobject.c 796:int PyDict_DelItem(PyObject *op, PyObject *key) { register PyDictObject *mp; register long hash; register PyDictEntry *ep; PyObject *old_value, *old_key; if (!PyDict_Check(op)) { PyErr_BadInternalCall(); return -1; } assert(key); // 同样先获取hash值 if (!PyString_CheckExact(key) || (hash = ((PyStringObject *) key)->ob_shash) == -1) { hash = PyObject_Hash(key); if (hash == -1) return -1; } // 搜索entry mp = (PyDictObject *)op; ep = (mp->ma_lookup)(mp, key, hash); if (ep == NULL) return -1; if (ep->me_value == NULL) { set_key_error(key); return -1; } // 删除entry所维护的元素,将entry的状态转为dummy态 old_key = ep->me_key; Py_INCREF(dummy); ep->me_key = dummy; old_value = ep->me_value; ep->me_value = NULL; mp->ma_used--; Py_DECREF(old_value); Py_DECREF(old_key); return 0; 832:} 与插入操作类似 , 先计算hash值 , 然后搜索相应的entry , 最后删除entry中维护的元素 , 并将entry从Active态变换为Dummy态 , 同时还将调整PyDictObject对象中维护table使用情况的变量 小结 : 无论是插入还是删除元素 , 都会先计算hash值 , 随后进行搜索相应的entry , 随后插入或删除元素 , 转换entry的状态 ; 而PyDictObject对象元素的插入则主要是通过freeslot所指向的entry来进行的 对象缓冲池 在PyDictObject的实现机制中 , 同样使用了缓冲池计数 , 并且其缓冲池机制与PyListObject中使用的缓冲池机制是一样的 Python-2.7\\Include\\dictobject.c 974:static void dict_dealloc(register PyDictObject *mp) { register PyDictEntry *ep; Py_ssize_t fill = mp->ma_fill; PyObject_GC_UnTrack(mp); Py_TRASHCAN_SAFE_BEGIN(mp) // 调整dict中对象的引用计数 for (ep = mp->ma_table; fill > 0; ep++) { if (ep->me_key) { --fill; Py_DECREF(ep->me_key); Py_XDECREF(ep->me_value); } } // 释放从系统堆中申请的内存空间 if (mp->ma_table != mp->ma_smalltable) PyMem_DEL(mp->ma_table); // 将被销毁的PyDictObject对象放入缓冲池 if (numfree tp_free((PyObject *)mp); Py_TRASHCAN_SAFE_END(mp) 995:} 开始时 , 这个缓冲池中什么也没有 , 直到第一个PyDictObject被销毁时 , 这个缓冲池才开始接纳被缓冲的PyDictObject对象 , 与PyListObject对象一样 , 只保留了PyDictObject对象 但是需要注意的是 , 销毁时根据ma_table的两种情况处理方式也是不同的 : 如果ma_table指向的是从系统堆申请的内存空间 (额外的内存) , 那么Python将释放这块内存空间归还给系统堆 如果ma_table指向的是PyDictObject的ma_smalltable , 那么只需要调整ma_smalltable中的对象的引用计数就可以了 在创建新的PyDictObject对象时 , 如果在缓冲池中有可以使用的对象 , 则直接从缓冲池中取出使用 , 而不需要再重新创建 , 这一点在PyDict_New中就已经体现了 至此 , 对于Python 2.7中的dict对象就差不多了 , 对于Python 3.5.4版本的比较待后期继续 , 不过简单的对比之下就可以发现 , 在Python 3.5.4的版本中 , 新增了一个dictnotes.txt文件 , 而且由2.7的3个状态变成了4个状态 , 数据层次也发生了一些改变 , 比如PyDictObject从2.7中的一种形式 , 变成了两种形式 (联合表和分割表) , 新增了PyDictKeyObject对象等 "},"01-Python/07-内存篇/07-Tuple对象.html":{"url":"01-Python/07-内存篇/07-Tuple对象.html","title":"Tuple对象","keywords":"","body":"Attack on Python - Tuple对象 🐍 介绍 Python中的tuple与str一样 , 都属于不可变对象 , 即其所维护的数据在对象创建之后就不能再改变了 直接看PyTupleObject吧 PyTupleObject Python-2.7\\Include\\tupleobject.h: 24:typedef struct { 25: PyObject_VAR_HEAD 26: PyObject *ob_item[1]; 27: 28: /* ob_item contains space for 'ob_size' elements. 29: * Items must normally not be NULL, except during construction when 30: * the tuple is not yet visible outside the function that builds it. 31: */ 32:} PyTupleObject; 通过上面的代码清单 , 我们可以看到 , PyTupleObject除了是一个不可变对象之外 , 它还是一个变长对象 ; 而ob_item 则为指向元素列表的指针 通过前面的整理 , 对于这些再熟悉不过了 创建与维护 PyTupleObject对象的创建同其他对象一样 , 其是通过PyTuple_New来创建的 Python-2.7\\Objects\\tupleobject.c 48:PyObject * PyTuple_New(register Py_ssize_t size) { register PyTupleObject *op; Py_ssize_t i; // 大小为负数 if (size 0 // 如果是空元组,直接取free_list第一个返回 if (size == 0 && free_list[0]) { op = free_list[0]; Py_INCREF(op); #ifdef COUNT_ALLOCS tuple_zero_allocs++; #endif return (PyObject *) op; } // 缓冲池可用 if (size ob_item[0]; numfree[size]--; #ifdef COUNT_ALLOCS fast_tuple_allocs++; #endif /* Inline PyObject_InitVar */ #ifdef Py_TRACE_REFS Py_SIZE(op) = size; Py_TYPE(op) = &PyTuple_Type; #endif _Py_NewReference((PyObject *)op); } // 缓冲池不可用 else #endif { // 通过传入的size参数计算需要的内存总量 Py_ssize_t nbytes = size * sizeof(PyObject *); /* Check for overflow */ if (nbytes / sizeof(PyObject *) != (size_t)size || (nbytes > PY_SSIZE_T_MAX - sizeof(PyTupleObject) - sizeof(PyObject *))) { return PyErr_NoMemory(); } // 创建PyTupleObject对象 op = PyObject_GC_NewVar(PyTupleObject, &PyTuple_Type, size); if (op == NULL) return NULL; } // 初始化每个元素 for (i=0; i ob_item[i] = NULL; #if PyTuple_MAXSAVESIZE > 0 // 第一次分配时将空数组放入缓冲池的第一个位置 if (size == 0) { free_list[0] = op; ++numfree[0]; Py_INCREF(op); /* extra INCREF so that this is never freed */ } #endif #ifdef SHOW_TRACK_COUNT count_tracked++; #endif _PyObject_GC_TRACK(op); return (PyObject *) op; 108:} 分析 : 我们不难发现 , PyTuple_New与PyList_New有很多相同之处 , 首先这个函数同样接受一个size参数 , 也就是我们在创建时指定PyTupleObject对象的初始元素个数 , 不同的地方在于两种对象在计算需要的内存总量的时机不同 随后检查缓冲池是否可用 , 如果可用 , 那么不用多说 ; 如果缓冲池不可用 , 那么现在才计算所需内存总量 , 而在PyList_New中 , 无论缓冲池是否可用都会计算其所需内存总量 缓冲池不可用之后 , 接下来就是创建PyTupleObject对象了 , 再然后初始化每个元素 最后的一步 , 则是将空元组放入缓冲池的第一位置 , 在整个Python的执行过程中 , 这个操作只会执行一次 而对于缓冲池free_list , 如下 : Python-2.7\\Objects\\tupleobject.c 7:#ifndef PyTuple_MAXSAVESIZE 8:#define PyTuple_MAXSAVESIZE 20 /* Largest tuple to save on free list */ 9:#endif 10:#ifndef PyTuple_MAXFREELIST 11:#define PyTuple_MAXFREELIST 2000 /* Maximum number of tuples of each size to save */ 12:#endif 13: 14:#if PyTuple_MAXSAVESIZE > 0 15:/* Entries 1 up to PyTuple_MAXSAVESIZE are free lists, entry 0 is the empty 16: tuple () of which at most one instance will be allocated. 17:*/ 通过定义我们可以看到 , PyTupleObject对象缓冲池中维护的最大个数为2000 , 但是注意 , 不是所有的元组都会放入缓冲池 , 不用想也知道 , 这肯定是有一个界限的 , 也就是要小于PyTuple_MAXSAVESIZE的 , 从上面我们知道 , 这个值为20 , 也就是说只有tuple长度小于20的PyTupleObject才能被放入缓冲池 并且缓冲池的第一个位置是留给()的 (有且仅有一个) , 也就是空元组 ; 对于空元组它是在PyTupleObject对象创建时就已经被放入缓冲池了的 , 而其他的PyTupleObject对象什么时候会放入缓冲池中 , 与PyListObject对象也是一样的 , 就是在对象被销毁时 , 这一点同前面的篇章一样 , 放在最后来说 设置元素 与PyListObject一样 , 在我们创建第一个PyTupleObject对象时 , 这时候缓冲池是不可用的 , 于是会调用PyObject_GC_New在系统堆上创建一个新的PyTupleObject对象 而当我们设置元素时 , 在Python内部会调用PyTupe_SetItem来完成这个动作 135:int PyTuple_SetItem(register PyObject *op, register Py_ssize_t i, PyObject *newitem) { register PyObject *olditem; register PyObject **p; // 类型与引用计数检查 if (!PyTuple_Check(op) || op->ob_refcnt != 1) { Py_XDECREF(newitem); PyErr_BadInternalCall(); return -1; } // 索引有效性检查 if (i = Py_SIZE(op)) { Py_XDECREF(newitem); PyErr_SetString(PyExc_IndexError, \"tuple assignment index out of range\"); return -1; } p = ((PyTupleObject *)op) -> ob_item + i; olditem = *p; *p = newitem; Py_XDECREF(olditem); return 0; 156:} 与PyListObject非常相似 , 首先进行类型检查 ,随后进行索引的有效性检查 , 当这两者都通过后 , 将新设置的元素指针放到指定的位置 , 然后调整引用计数 , 将这个位置原来存放的对象的引用计数减1 PyTupleObject对象是不可变对象 , 所以没有类似于PyListObject对象的插入等操作 对象缓冲池 通过前面我们已经知道 , PyTupleObject对象的缓冲池机制在创建PyTupleObject对象时 , 仅仅会将空元组加入缓冲池中 , 而对于其他的PyTupleObject对象并没有出现在PyTuple_New中 其实PyTupleObject对象的缓冲池与PyListObject对象是一样 , 是在其销毁时添加的 Python-2.7\\Objects\\tupleobject.c 210:static void tupledealloc(register PyTupleObject *op) { register Py_ssize_t i; register Py_ssize_t len = Py_SIZE(op); PyObject_GC_UnTrack(op); Py_TRASHCAN_SAFE_BEGIN(op) // 销毁PyTupeObject对象维护的元素列表 if (len > 0) { i = len; while (--i >= 0) Py_XDECREF(op->ob_item[i]); #if PyTuple_MAXSAVESIZE > 0 // 检查是否满足放入缓冲池的条件 if (len ob_item[0] = (PyObject *) free_list[len]; numfree[len]++; free_list[len] = op; goto done; /* return */ } #endif } Py_TYPE(op)->tp_free((PyObject *)op); done: Py_TRASHCAN_SAFE_END(op) 236:} 根据上面的代码清单 , 可以看出 , 在PyTupleObject对象进行销毁时 , 首先会销毁PyTupleObject对象维护的元素列表 , 然后判断该PyTupleObject的大小是否超过缓冲池可缓冲的最大大小 (PyTuple_MAXSAVESIZE=20) , 以及缓冲池是否已满 , 对象是否为PyTupleObject对象 随后 , 如果满足使用缓冲池的要求 , 那么就将这个PyTupleObject对象放入缓冲池中 , 这时这个PyTupleObject对象中的元素列表是已经被销毁了的 ; 如果不满足就直接销毁整个PyTupleObject对象 小结 : 通过与PyListObject对象的实现相比较 , 其与PyTupleObject的差异基本取决于一个是可变对象 , 一个是不可变对象 , 我们可以看到在设置元素和缓冲池机制 , 在两种对象的源码上差别都非常的小 ; 而在对象创建时有所不同的是 , PyTupleObject对象会在创建时将空元组放入缓冲池中 (第一个位置) , 而PyListObject对象则不会 , 如下小实验 : # Python 2.7 >>> list1 = [] >>> list2 = [] >>> id(list1) 79581256L >>> id(list2) 79684744L >>> tuple1 = () >>> tuple2 = () >>> id(tuple1) 77598792L >>> id(tuple2) 77598792L # Python 3.5.3结果相同 由于缓冲池实现的小差异 , 空元组是不会反复创建的 , 并且在缓冲池的第一位置 "},"01-Python/07-内存篇/08-垃圾回收.html":{"url":"01-Python/07-内存篇/08-垃圾回收.html","title":"垃圾回收","keywords":"","body":"Attack on Python - 垃圾回收 🐍 介绍 引用计数在对Python内置数据类型的分析时 , 已经见过太多次了 , 就是通过对象中的ob_refcnt变量来实现的 在Python中引用计数是一种垃圾收集机制 , 并且是一种最直观 , 最简单的垃圾收集技术 虽然引用计数必须在每次分配和释放内存的时候加入管理引用计数的动作 , 然而与其他主流的垃圾收集技术相比 , 引用计数有一个最大的优点 , 即实时性 , 任何内存 , 一旦没有指向它的引用 , 就会立即被回收 ; 而其他的垃圾收集计数必须在某种特殊条件下 (比如内存分配失败) 才能进行无效内存的回收 引用计数机制所带来的维护引用计数的额外操作与Python运行中所进行的内存分配和释放 , 引用赋值的次数是成正比的 , 这是Python的一个弱点 , 因此在Python内置数据类型中就大量使用了对象缓冲池机制 , 就是为了竭力弥补引用计数机制的软肋 除了执行效率这个软肋之外 , 引用计数还存在一个致命的弱点 , 那就是循环引用 循环引用 我们知道 , 当一个对象的引用被创建或复制时 , 对象的引用计数就会加1 ; 而当一个对象的引用被销毁时 , 对象的引用计数就会减1 ; 如果对象的引用计数减少为0 , 那么就以为着这个对象不会被任何人使用 , 那么就可以进行回收了 而引用计数的另一个现象就是循环引用了 , 就相当于有两个对象a和b , 其中a引用了b , b引用了a , 这样a和b的引用计数都为1 , 并且永远都不会为0 , 这就意味着 , 这两个对象永远都不会被回收了 , 这就是循环引用 , a与b形成了一个引用循环 , 示例如下 : # 我们让list1中包含list2的引用,而list2中又包含list1的引用,形成引用循环 >>> list1 = [] >>> list2 = [] >>> list1.append(list2) # 此时还没有形成引用循环 >>> list1 [[]] # 循环引用 >>> list2.append(list1) >>> l1 [[[...]]] >>> l2 [[[...]]] ''' [...]:这就是list循环引用的结果 ''' 除了上述两个对象互相引用之外 , 还可以引用自身 , 示例如下 : >>> list3 = [] >>> list3.append(list3) >>> list3 [[...]] 循环引用与手动进行内存管理所产生的内存泄漏毫无区别 , 不过循环引用对于int或者str类型明显是不存在的 所以为了解决循环引用的问题 , Python引入了主流垃圾收集技术中的标记——清除和分代收集两种技术来填补其内存管理机制中最致命的漏洞 标记清除 垃圾收集机制一般分为两个阶段 : 垃圾检测和垃圾回收 垃圾检测是从所有的已分配的内存中区别出可以回收的内存和不可回收的内存 , 而垃圾回收则是使系统重新掌握在垃圾检测阶段被标识出来的可回收内存块 对于标记——清除方法其简要工作过程如下 : 寻找根对象的集合 , 所谓根对象就是一些全局引用和函数栈中的引用 , 这些引用的对象是不可被删除的, 而这个根对象集合也是垃圾检测动作的起点 从根对象的集合 , 沿着根对象集合中的每一个引用 , 如果能到达某个对象A , 则A称为可达的 , 可达的对象也不可被删除 , 这个阶段就是垃圾检测阶段 当垃圾检测阶段结束后 , 所有的对象分为了可达的和不可达的两部分 , 所有的可达的对象都必须予以保留 , 而所有的不可达对象所占用的内存将被回收 , 这就是垃圾回收阶段 分代回收 我们的开发程序 , 其一定比例的内存块的生存周期都比较短 , 通常是几百万条机器指令的时间 , 而只有剩下的极少部分内存块 , 生存周期比较长 , 而对于不同的语言 , 不同的应用程序 , 生存周期比较短的内存块的比例通常在80%到98%之间游走 从上面我们知道 , 标记——清除技术所带来的额外操作实际上与系统中总的内存块的数量是相关的 , 当需要回收的内存块越多时 , 垃圾检测带来的额外操作就越多 , 而垃圾回收带来的额外操作就越少 所以通常为了提高垃圾收集的效率 , 我们就可以采用一种以空间换时间的策略 , 分代回收计数 , 这也是当前支撑着Java的关键技术 分代回收 : 将系统中的所有内存块根据其存活时间划分为不同的集合 , 每一个集合就称为一个 \"代\" , 垃圾收集的频率随着 \"代\" 的存活时间的增大而减小 也就是说 , 活得越长的对象 , 就越可能不是垃圾 , 就应该越少去收集 . 而这个存活时间通常就是利用经过了几次垃圾收集动作来衡量 ; 如果一个对象经过的垃圾收集次数越多, 那么显然 , 其存活时间就越长 在Python中 , 一个 \"代\" 就是一个链表 , Python采用了三代的分代收集机制 Python-2.7\\Modules\\gcmodule.c 32:struct gc_generation { 33: PyGC_Head head; /* 回收阀值 */ 34: int threshold; /* collection threshold */ /* 实时个数 */ 35: int count; /* count of allocations or collections of younger 36: generations */ 37:}; 39:#define NUM_GENERATIONS 3 40:#define GEN_HEAD(n) (&generations[n].head) 41: 42:/* linked lists of container objects */ 43:static struct gc_generation generations[NUM_GENERATIONS] = { 44: /* PyGC_Head, threshold, count */ /* 第0代,可收集700个container对象,一旦超出就立即触发垃圾回收机制 */ 45: {{{GEN_HEAD(0), GEN_HEAD(0), 0}}, 700, 0}, 46: {{{GEN_HEAD(1), GEN_HEAD(1), 0}}, 10, 0}, 47: {{{GEN_HEAD(2), GEN_HEAD(2), 0}}, 10, 0}, 48:}; 49: 50:PyGC_Head *_PyGC_generation0 = GEN_HEAD(0); "},"01-Python/07-内存篇/09-元类.html":{"url":"01-Python/07-内存篇/09-元类.html","title":"元类","keywords":"","body":"Attack on Python - 元类 🐍 介绍 元类 ( metaclass ) , 是一种实例是类的类 普通的类定义的是特定对象的行为 , 元类定义的则是特定的类及其对象的行为 , 不是所有面向对象编程语言都支持元类 type 元类在 Wiki 中的解释已经说的很明确了 , 它是一种实例是类的类 , 这也就意味着元类可以创造类 这么说你可能会不太清晰 , 我们从问题出发 , 在 Python 中是谁创建了类 , 也就是说 Python 中的元类是谁? 如果你看过 Python 这一部分的源码 , 那么想必你对这个问题肯定了然于心 , 没错就是 type 类 >>> object.__class__ 至于 type 类为什么是元类 , 你可以从我的另一篇文章中获得答案 《对象的创建》 看下面的例子 : >>> class Foo: ... pass ... >>> f = Foo() 在这个例子中 , Foo() , 也就是调用 Foo 的 __call__ 方法 , 它会做两件事情 : 调用 __new__ , 创建对象 调用 __init__ , 初始化对象 但是注意 , 这个 __call__ 是 object 类的 , 因为 Python 3 中所有的类都默认继承了 object , 至于 Python 2 没什么好谈的 , 相信你查查就能知道 我们本就可以通过重载 __new__ 来控制对象的创建 , 如下 : def new(cls): x = object.__new__(cls) x.attr = 100 return x Foo.__new__ = new f = Foo() print(f.attr) g = Foo() print(g.attr) \"\"\" 执行结果如下: 100 100 \"\"\" 但是不同的是 , 你对 type 不能这么干 , Python 也不允许你这么干 , 如果唯一的元类都被动了 , 那就乱套了 def new(cls): x = type.__new__(cls) x.attr = 100 return x type.__new__ = new \"\"\" Traceback (most recent call last): File \"\", line 1, in TypeError: can't set attributes of built-in/extension type 'type' \"\"\" 所以你从这里也可以知道 , type 和 object 的区别就在于 : type 的 __new__ , 返回了一个类 object 的 __new__ , 返回了一个对象实例 如果我们要定义一个元类 , 只需要如下 : class Meta(type): def __new__(cls, name, bases, dct): x = super().__new__(cls, name, bases, dct) x.attr = 100 return x 当然你也看出来了 , 这只是继承 , 要让它真正成为元类 , 你还需要如下 : class Foo(metaclass=Meta): pass print(Foo.attr) 我们再看看这个 Foo 和普通的对象有什么不同 : class Meta(type): def __new__(cls, name, bases, dct): x = super().__new__(cls, name, bases, dct) x.attr = 100 return x class Foo(metaclass=Meta): pass class Bar(Foo): pass print(type(Meta)) print(type(object)) print(type(Foo)) print(type(Bar)) \"\"\" 执行结果如下: \"\"\" 当指定了 metaclass 之后 , 类的创建将不再由 type 负责 , 而是由元类 Meta 负责 , 也就是说 type 类与这类的 Meta 类都是元类 , 大家是同一级 元类的作用 元类可以用来改变类的行为 , 这和类并没有什么差别 , 因为我们定义类也可以改变对象的行为 , 我们来看一个例子 class Foo: pass # 调用__call__ f = Foo() # 如果我们想改变 () 也就是 __call__的行为要怎么做? # 当然不可能是在Foo类中重载 __call__ 因为那是控制 Foo 实例化出来的对象的 # 所以我们需要用元类来控制它 # 单例模式直接用metaclass来实现, 而且它是线程安全的 class SingletonMeta(type): _instances = {} def __call__(self, *args, **kwargs): if self not in self._instances: self._instances[self] = super(SingletonMeta, self).__call__(*args, **kwargs) return self._instances[self] class Singleton(metaclass=SingletonMeta): pass a = Singleton() b = Singleton() c = Singleton() d = Singleton() e = Singleton() 如果你想要改变类的行为 , 除了 Python 默认提供的一个魔术方法 (__new__) , 你必须通过元类来改变 因为 __new__ 是唯一一个第一个参数不是 self 而是 cls 的魔术方法 所以上面这个例子 , 除了用元类 , 你也可以通过覆盖 __new__ 来实现 元类其实就是一个类工厂 , 而类则是对象工厂 , 但是实际上我们不需要使用元类同样可以达到生产的目的 , 因为通常我们不会需要去改变类的行为 , 需要改变的是对象的行为 看下面几个例子 继承 >>> class Base: ... attr = 100 ... >>> class X(Base): ... pass ... >>> class Y(Base): ... pass ... >>> class Z(Base): ... pass ... >>> X.attr 100 >>> Y.attr 100 >>> Z.attr 100 类装饰器 >>> def decorator(cls): ... class NewClass(cls): ... attr = 100 ... return NewClass ... >>> @decorator ... class X: ... pass ... >>> @decorator ... class Y: ... pass ... >>> @decorator ... class Z: ... pass ... >>> X.attr 100 >>> Y.attr 100 >>> Z.attr 100 总而言之 , 元类的作用就是用来创造类的 , 我们通常更多的是使用继承 (也就是利用抽象) 的方式来达到我们的目的 Python 之禅中这么说到 : 元类是深层次的魔术代码 , 99% 的用户都不需要关心它 , 如果你好奇你是否需要 , 那你就不需要 , 真正需元类的人 , 是很清楚他们需要的 , 并且 , 不需要一个理由来解释 简单的说 , 元类不适合在生产的代码中使用 , 它更适合用来设计 , 比如 Django , SQLAlchemy 中 , 你就能发现它的身影 , 总而言之 , 元类控制类 , 类控制对象 "},"01-Python/08-番外篇/":{"url":"01-Python/08-番外篇/","title":"番外篇","keywords":"","body":"Attack on Python - 番外篇 🐍 "},"01-Python/08-番外篇/04-Python - 第三方库之PyMySQL.html":{"url":"01-Python/08-番外篇/04-Python - 第三方库之PyMySQL.html","title":"Python - 第三方库之PyMySQL","keywords":"","body":"Python - 第三方库之PyMySQL 介绍 🍀 pymysql是用于Python 3.x 链接MySQL数据库的一个第三方库 , 其使用方法和MySQLdb几乎相同 , pymysql的目的就是为了称为MySQLdb的替代品 , 因为MySQLdb不支持Python 3.x以后的版本 安装 $ pip install PyMySQL 包内容 PACKAGE CONTENTS _compat _socketio charset connections constants (package) converters cursors err optionfile tests (package) times util 使用 🍀 包中我们主要需要了解connectinos.py 中的内容 在pymysql包中我们只需要使用Connect() 来创建一个Connection对象 def Connect(*args, **kwargs): \"\"\" Connect to the database; see connections.Connection.__init__() for more information. \"\"\" from .connections import Connection return Connection(*args, **kwargs) # 返回一个Connection对象 Connection.__init __() 参数如下 Connect(*args, **kwargs) Establish a connection to the MySQL database. Accepts several arguments: host: Host where the database server is located user: Username to log in as password: Password to use. database: Database to use, None to not use a particular one. port: MySQL port to use, default is usually OK. (default: 3306) bind_address: When the client has multiple network interfaces, specify the interface from which to connect to the host. Argument can be a hostname or an IP address. unix_socket: Optionally, you can use a unix socket rather than TCP/IP. charset: Charset you want to use. sql_mode: Default SQL_MODE to use. read_default_file: Specifies my.cnf file to read these parameters from under the [client] section. conv: Conversion dictionary to use instead of the default one. This is used to provide custom marshalling and unmarshaling of types. See converters. use_unicode: Whether or not to default to unicode strings. This option defaults to true for Py3k. client_flag: Custom flags to send to MySQL. Find potential values in constants.CLIENT. cursorclass: Custom cursor class to use. init_command: Initial SQL statement to run when connection is established. connect_timeout: Timeout before throwing an exception when connecting. (default: 10, min: 1, max: 31536000) ssl: A dict of arguments similar to mysql_ssl_set()'s parameters. For now the capath and cipher arguments are not supported. read_default_group: Group to read from in the configuration file. compress; Not supported named_pipe: Not supported autocommit: Autocommit mode. None means use server default. (default: False) local_infile: Boolean to enable the use of LOAD DATA LOCAL command. (default: False) max_allowed_packet: Max size of packet sent to server in bytes. (default: 16MB) Only used to limit size of \"LOAD LOCAL INFILE\" data packet smaller than default (16KB). defer_connect: Don't explicitly connect on contruction - wait for connect call. (default: False) auth_plugin_map: A dict of plugin names to a class that processes that plugin. The class will take the Connection object as the argument to the constructor. The class needs an authenticate method taking an authentication packet as an argument. For the dialog plugin, a prompt(echo, prompt) method can be used (if no authenticate method) for returning a string from the user. (experimental) db: Alias for database. (for compatibility to MySQLdb) passwd: Alias for password. (for compatibility to MySQLdb) 连接数据库 🍀 import pymysql # 连接MySQL数据库 connection = pymysql.connect(host='localhost', port=3306, user='root', password='myroot', db='mydatabase', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor) pymysql包中的cursors.py 中的class Cursor(object) 可供我们建立与数据库进行交互的对象 , cursor(游标) , 下面就开始与数据库进行交互了 创建表 🍀 import pymysql.cursors # 连接MySQL数据库 connection = pymysql.connect(host='localhost', port=3306, user='root', password='myroot', db='mydatabase', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor) try: # 创建游标实例 with connection.cursor() as cursor: sql = \"\"\"CREATE TABLE EMPLOYEE ( FIRST_NAME CHAR(20) NOT NULL, LAST_NAME CHAR(20), AGE INT, SEX CHAR(1), INCOME FLOAT );\"\"\" # 执行sql,并返回受影响行数 cursor.execute(sql) # executemany()可一次性执行多个sql语句,提高了多行插入的性能 # 提交,不然无法保存新建或者修改的数据 connection.commit() finally: connection.close() execute介绍 def execute(self, query, args=None): \"\"\"Execute a query :param str query: Query to execute. :param args: parameters used with query. (optional) :type args: tuple, list or dict :return: Number of affected rows :rtype: int If args is a list or tuple, %s can be used as a placeholder in the query. If args is a dict, %(name)s can be used as a placeholder in the query. \"\"\" # list example cursor.execute(\"update hosts set host = '1.1.1.2' where nid > %s\", (1,)) # tuple example cursor.execute(\"insert into hosts(host,color_id) values(%s,%s)\", [(\"1.1.1.11\",1),(\"1.1.1.11\",2)]) 查询表 🍀 Python查询MySQL获取数据使用方法如下 : fetchone(self) : 获取下一行查询结果 fetchmany(self, size=None) : 获取size行数的查询结果 fetchall(self) : 获取全部的返回结果 rowcount : 这是一个只读属性 , 并返回执行execute() 方法后影响的行数 在fetch数据时按照顺序进行 , 可以使用scroll(num, mode)来移动游标位置 , 如 : cursor.scroll(1, mode='relative') , 相对当前位置移动 cursor.scroll(2, mode='absolute') , 相对绝对位置移动 import pymysql.cursors # 连接MySQL数据库 connection = pymysql.connect(host='localhost', port=3306, user='root', password='myroot', db='mydatabase', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor) try: # 创建游标实例 with connection.cursor() as cursor: sql = \"SELECT * FROM user_info\" # 执行sql,并返回受影响行数 cursor.execute(sql) # 查询结果 result = cursor.fetchall() print(result) # 提交 connection.commit() finally: connection.close() ''' 执行结果: [{'username': 'Lyon', 'id': 1, 'password': '456'}] ''' 注意 : fetch默认获取的数据是元组类型 , 可以在建立cursor(游标)对象时 , 设置cursor属性进行修改 , 如设置为字典类型 : cursor(cursor=pymysql.cursors.DictCursor) 获取最新自增ID : cursor.lastrowid 修改表 🍀 import pymysql.cursors # 连接MySQL数据库 connection = pymysql.connect(host='localhost', port=3306, user='root', password='myroot', db='mydatabase', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor) try: # 创建游标实例 with connection.cursor() as cursor: sql = \"UPDATE user_info SET password = '456' WHERE username = 'Lyon'\" # 执行sql,并返回受影响行数 effect_row = cursor.execute(sql) print(effect_row) # 提交 connection.commit() except: # 发生错误时回滚 connection.rollback() # 关闭连接 connection.close() 删除表 🍀 import pymysql.cursors # 连接MySQL数据库 connection = pymysql.connect(host='localhost', port=3306, user='root', password='myroot', db='mydatabase', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor) try: # 创建游标实例 with connection.cursor() as cursor: sql = \"DROP TABLE EMPLOYEE\" # 执行sql,并返回影响行数 cursor.execute(sql) # 提交 connection.commit() finally: # 关闭连接 connection.close() "},"01-Python/08-番外篇/05-Python - 第三方库之MySQLdb.html":{"url":"01-Python/08-番外篇/05-Python - 第三方库之MySQLdb.html","title":"Python - 第三方库之MySQLdb","keywords":"","body":"Python - 第三方库之MySQLdb 介绍 🍀 MySQLdb是用于Python链接MySQL数据库的接口 , 它实现了Python数据库API规范V2.0 , 基于MySQL C API 上建立的 Python DB-API使用流程 : 导入API模块 获取与数据的连接 执行SQL语句和存储过程 关闭数据库连接 MySQLdb只支持Python 3.x之前的版本 , 在Python 3.x中则是用PyMySQL来代替 安装 https://sourceforge.net/projects/mysql-python/ # 安装相关教程可以通过google,baidu等进行查找 在上一篇已经介绍了PyMySQL , MySQLdb的用户与PyMySQL是一样的 , 所以这篇直接以实例进行整理 , 并补充对于事务的说明 连接数据库 🍀 import MySQLdb connection = MySQLdb.Connect(host='localhost', user='root', passwd='myroot', db='test', port='3306', charset='utf8') 创建表 🍀 import MySQLdb # 连接数据库 connection = MySQLdb.Connect(host='localhost', user='root', passwd='myroot', db='test', port='3306', charset='utf8') # 创建游标 cursor = connection.cursor() # 定义sql语句 sql = \"\"\"CREATE TABLE EMPLOYEE ( FIRST_NAME CHAR(20) NOT NULL, LAST_NAME CHAR(20), AGE INT, SEX CHAR(1), INCOME FLOAT )\"\"\" # 执行sql cursor.execute(sql) # 关闭连接 connection.close() 查询表 🍀 查询方法如下 : fetchone() : 获取下一条查询结果 , 结果集是一个对象 fetchall() : 获取全部查询结果 rowcount : 这是一个只读属性 , 并返回执行execute() 方法后的影响行数 import MySQLdb # 连接数据库 connection = MySQLdb.Connect(host='localhost', user='root', passwd='myroot', db='test', port='3306', charset='utf8') # 创建游标 cursor = connection.cursor() # 定义sql语句 sql = \"SELECT * FROM EMPLOYEE \\ WHERE INCOME > '%d'\" % (1000) try: # 执行SQL语句 cursor.execute(sql) # 获取所有记录列表 results = cursor.fetchall() for row in results: fname = row[0] lname = row[1] age = row[2] sex = row[3] income = row[4] # 打印结果 print \"fname=%s,lname=%s,age=%d,sex=%s,income=%d\" % \\ (fname, lname, age, sex, income ) except: print \"Error: unable to fecth data\" # 关闭连接 connection.close() 修改表 🍀 插入数据 import MySQLdb # 连接数据库 connection = MySQLdb.Connect(host='localhost', user='root', passwd='myroot', db='test', port='3306', charset='utf8') # 创建游标 cursor = connection.cursor() # 定义sql语句 sql = \"\"\"INSERT INTO EMPLOYEE(FIRST_NAME, LAST_NAME, AGE, SEX, INCOME) VALUES ('Mac', 'Mohan', 20, 'M', 2000)\"\"\" try: # 执行sql语句 cursor.execute(sql) # 提交到数据库执行 connection.commit() except: # 出现异常回滚 connection.rollback() # 关闭连接 connection.close() 更新数据 import MySQLdb # 连接数据库 connection = MySQLdb.Connect(host='localhost', user='root', passwd='myroot', db='test', port='3306', charset='utf8') # 创建游标 cursor = connection.cursor() # 定义sql语句 sql = \"UPDATE EMPLOYEE SET AGE = AGE + 1 WHERE SEX = '%c'\" % ('M') try: # 执行SQL语句 cursor.execute(sql) # 提交到数据库执行 connection.commit() except: # 发生错误时回滚 connection.rollback() # 关闭连接 connection.close() 删除表 🍀 import MySQLdb # 连接数据库 connection = MySQLdb.Connect(host='localhost', user='root', passwd='myroot', db='test', port='3306', charset='utf8') # 创建游标 cursor = connection.cursor() # 定义sql语句 sql = \"DELETE FROM EMPLOYEE WHERE AGE > '%d'\" % (20) try: # 执行SQL语句 cursor.execute(sql) # 提交修改 connection.commit() except: # 发生错误时回滚 connection.rollback() # 关闭连接 connection.close() 事务 🍀 事务机制是为了确保数据的一致性 事务应该具有4个属性 : 原子性 : 一个事务是一个不可分割的工作单位 , 事务中包括的诸操作要么都做 , 要么都不做 一致性 : 事务必须是数据库从一个一致性状态变到另一个一致性状态 , 一致性与原子性是密切相关的 隔离性 : 一个事务的执行不能被其他事务干扰 , 即一个事务内部的操作及使用的数据对并发的其他事务是隔离的 , 并发执行的各个事务之间不能互相干扰 持久性 : 也成为永久性 , 指一个事务一旦提交 , 它对数据库中数据的改变就应该是永久性的 , 接下来的其他操作或故障不应该对其有任何影响 Python DB-API 2.0的事务提供了两个方法 commit 和rollback , 在上述实例中已经见过了 try: # 执行SQL语句 cursor.execute(sql) # 向数据库提交 connection.commit() except: # 发生错误时回滚 connection.rollback() "},"01-Python/08-番外篇/06-Python - 第三方库之SQlAlchemy.html":{"url":"01-Python/08-番外篇/06-Python - 第三方库之SQlAlchemy.html","title":"Python - 第三方库之SQlAlchemy","keywords":"","body":"Python - 第三方库之SQlAlchemy SQLAlchemy官方文档 介绍 🍀 在介绍SQLAlchemy之前先介绍一下什么是ORM ORM ORM即Object Relational Mapping , 简称ORM , 中文意思就是对象关系映射 ; 是一种程序技术 , 用于实现面向对象编程语言里不同类型系统的数据之间的转换 换一个方式介绍 , 我们知道面向对象是从软件工程基本原则(如耦合 , 聚合 , 封装) 的基础上发展起来的 , 而关系型数据库是从数学理论发展而来的 , 两套理论完全是不匹配的 , 那么正是为了解决这个问题 , 对象关系映射技术诞生了 SQLAlchemy SQLAlchemy是Python中最有名的一款ORM框架 , 该框架建立在数据库API之上 , 使用关系对象映射进行数据库操作 SQLAlchemy对象关系映射代表了用户使用Python定义类来与数据库中的表相关联的一种方式 , 类的实例则对应数据表中的一行数据 , SQLAlchemy包括了一套将对象中的变化同步到数据库表中的系统 , 这套系统被称之为工作单元(unit of work) , 同时也提供了使用类查询来实现数据库查询以及查询表之间关系的功能 安装 $ pip3 install SQLAlchemy 版本检查 >>>import sqlalchemy >>>sqlalchemy.__version__ '1.1.14' 各数据库Dialect MySQL-Python mysql+mysqldb://:@[:]/ pymysql mysql+pymysql://:@/[?] MySQL-Connector mysql+mysqlconnector://:@[:]/ cx_Oracle oracle+cx_oracle://user:pass@host:port/dbname[?key=value&key=value...] -- 更多详见：http://docs.sqlalchemy.org/en/latest/dialects/index.html 内部处理 SQLAlchemy操作数据库是利用Engine/ConnectionPooling/Dialect进行的 , Engine(引擎)使用ConnectionPooling连接数据库 , 然后再通过Dialect执行SQL语句 , SQLAlchemy Core如下 SQLAlchemy Core +-----------------+ +-------------------------+ +-----------------+ | Schema/Types | | SQL Expression Language | | Engine | +-----------------+ +-------------------------+ +-----------------+ ↓ +------------------+ +-------+ |Connection Pooling| |Dialect| +------------------+ +-------+ --------------------------------------------------------------------- DBAPI 连接数据库 🍀 from sqlalchemy import create_engine engine = create_engine(\"mysql+pymysql://root:myroot@localhost:3306/t1\", echo=True) echo参数是用来设置SQLAlchemy日志的 , 通过Python标准库logging模块实现 ; 设置为True表示所有操作记录可见 , 也可设置为False来减少日志的输出 create_engine() 的返回值是Engine的一个实例 , 此实例代表了操作数据库的核心接口 , 通过Dialect来处理数据库和数据库的API PS : 初次调用create_engine()时并不会真正的去连接数据库 , 只有在真正执行一条命令的时候才会去简历真正的DBAPI连接 ; 很多地方都会使用这种方式 , 以达到省资源的目的 声明映射 🍀 当使用ORM的时候 , 配置过程以描述数据库的表来开始 , 然后定义与之匹配的类 ; 而在SQLAlchemy中 , 这两个过程一般结合在一起 , 通过一个声明(Declarative)系统实现 , 该系统帮我们定义类以及实现与表的对应 声明系统实现类与表的对应是通过一系列基类实现的 , 即声明基类(Declarative Base Class) , 我们的应用程序经常只有一个此基类的实例 from sqlalchemy.ext.declarative import declarative_base Base = declarative_base() 根据声明的基类\"Base\" , 我们就可以通过它定义任何数量的映射类 使用原生SQL from sqlalchemy import create_engine from consts import DB_URI eng = create_engine(DB_URI) with eng.connect() as con: con.execute('drop table if exists users') con.execute('create table users(Id INT PRIMARY KEY AUTO_INCREMENT, ' 'Name VARCHAR(25))') con.execute(\"insert into users(name) values('Lyon')\") con.execute(\"insert into users(name) values('Kenneth')\") rs = con.execute('select * from users') for row in rs: print(row) 使用表达式 SQLAlchemy 支持使用表达式的方式来操作数据库 from sqlalchemy import (create_engine, Table, MetaData, Column, Integer, String, tuple_) from sqlalchemy.sql import select, asc, and_ from consts import DB_URI eng = create_engine(DB_URI) meta = MetaData(eng) users = Table( 'Users', meta, Column('Id', Integer, primary_key=True, autoincrement=True), Column('Name', String(50), nullable=False), ) if users.exists(): users.drop() users.create() # 创建表 def execute(s): print('-' * 20) rs = con.execute(s) for row in rs: print(row['Id'], row['Name']) with eng.connect() as con: for username in ('xiaoming', 'wanglang', 'lilei'): user = users.insert().values(Name=username) con.execute(user) stm = select([users]).limit(1) execute(stm) k = [(2,)] stm = select([users]).where(tuple_(users.c.Id).in_(k)) execute(stm) stm = select([users]).where(and_(users.c.Id > 2, users.c.Id ORM功能使用 🍀 流程如下 : 使用者通过ORM对象提交命令 将命令给SQLAlchemy Core转换成SQL 匹配使用者事先配置好的engine engine从连接池中取出一个链接 基于该链接通过Dialect调用DBAPI , 将SQL转交给数据库去执行 创建表 🍀 # 创建单表 from sqlalchemy import create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy import Column, Integer, String, Index, UniqueConstraint # 根据Dialet创建引擎,echo=True表示输出所有操作日志 engine = create_engine('mysql+pymysql://root:myroot@localhost:3306/test', echo=True) # 声明基类 Base = declarative_base() # 定义映射类 class Userinfo(Base): # 表名 __tablename__ = 'user_info' # 设置主键自增列 id = Column(Integer, primary_key=True, autoincrement=True) name = Column(String(32)) extra = Column(String(16)) __table_args__ = ( # 唯一索引,索引名为uix_id_name UniqueConstraint('id', 'name', name='uix_id_name'), # 联合索引 Index('ix_id_name', 'name', 'extra'), ) # 定义格式 def __repr__(self): return \"\" % (self.id, self.name) # 初始化函数 def init_db(): # 将所有继承Base类的类,创建表结构 Base.metadata.create_all(engine) def drop_db(): # 将所有继承Base类的类,删除表 Base.metadata.drop_all(engine) init_db() 对应的SQL语句 CREATE TABLE `UserInfo` ( id INTEGER NOT NULL AUTO_INCREMENT, name VARCHAR(32), extra VARCHAR(16), PRIMARY KEY (id), CONSTRAINT uix_id_name UNIQUE (id, name) ) 创建其他表 # 创建单表:业务线 class Business(Base): __tablename__='business' id=Column(Integer,primary_key=True,autoincrement=True) bname=Column(String(32),nullable=False,index=True) # 一对多:多个服务可以属于一个业务线,多个业务线不能包含同一个服务 class Service(Base): __tablename__='service' id=Column(Integer,primary_key=True,autoincrement=True) sname=Column(String(32),nullable=False,index=True) ip=Column(String(15),nullable=False) port=Column(Integer,nullable=False) business_id=Column(Integer,ForeignKey('business.id')) __table_args__=( UniqueConstraint(ip,port,name='uix_ip_port'), Index('ix_id_sname',id,sname) ) # 一对一:一种角色只能管理一条业务线,一条业务线只能被一种角色管理 class Role(Base): __tablename__='role' id=Column(Integer,primary_key=True,autoincrement=True) rname=Column(String(32),nullable=False,index=True) priv=Column(String(64),nullable=False) business_id=Column(Integer,ForeignKey('business.id'),unique=True # 多对多:多个用户可以是同一个role,多个role可以包含同一个用户 class Users(Base): __tablename__='users' id=Column(Integer,primary_key=True,autoincrement=True) uname=Column(String(32),nullable=False,index=True) class Users2Role(Base): __tablename__='users2role' id=Column(Integer,primary_key=True,autoincrement=True) uid=Column(Integer,ForeignKey('users.id')) rid=Column(Integer,ForeignKey('role.id')) __table_args__=( UniqueConstraint(uid,rid,name='uix_uid_rid'), ) class Favor(Base): __tablename__ = 'favor' nid = Column(Integer, primary_key=True, autoincrement=True) caption = Column(String(50), default='red', unique=True) class Person(Base): __tablename__ = 'person' nid = Column(Integer, primary_key=True, autoincrement=True) favor_id = Column(Integer, ForeignKey(\"favor.nid\")) ''' 设置外键的另一种方式 ForeignKeyConstraint(['other_id'], ['othertable.other_id']) ''' 扩展分析 : 根据流程可以发现 , 如果我们不依赖于SQLAlchemy的转换而自己写好sql语句 , 那么我们完全可以只用SQLAlchemy执行纯sql语句 , 即利用配置好的engine执行 , engine.execute() 删除表 🍀 Base.metadata.drop_all(engine) # 把所有继承Base类的类，删除表 操作表 🍀 ORM处理数据库的方式是通过Session来实现的 , 当我们需要与数据库进行对话时 , 就需要创建一个Session实例 : engine对象已经创建完成时 from sqlalchemy.orm import sessionmaker # 创建Session工厂,并连接engine Session = sessionmaker(bind=engine) # 创建Session实例 session = Session() engine未创建时 from sqlalchemy.orm import sessionmaker from sqlalchemy import create_engine # 创建Session工厂 Session = sessionmaker() # 创建引擎 engine = create_engine() # 连接Session与engine Session.configure(bind=engine) # 创建Session实例 session = Session() 增加数据 🍀 单条数据 Session = sessionmaker(bind=engine) session = Session() # 创建一条数据 users = Userinfo(name='Hello', password='World') # 把数据添加到表内 session.add(users) # 提交生效 session.commit() 多条数据 session.add_all([ Userinfo(name='Lyon',extra='xxx'), Userinfo(name='Kenneth Reitz',extra='xxx'), ]) session.commit() 删除数据 🍀 session.query(Userinfo).filter(Userinfo.name == 'Kenneth Reitz').delete() session.commit() 修改数据 🍀 session.query(Userinfo).filter(Users.id > 2).update({\"name\" : \"099\"}) # synchronize_session同步会话 session.query(Userinfo).filter(Users.id > 2).update({Users.name: Users.name + \"099\"}, synchronize_session=False) # 设置评估标准 session.query(Userinfo).filter(Users.id > 2).update({\"num\": Users.num + 1}, synchronize_session=\"evaluate\") session.commit() ''' 更多synchronize_session的参数可以查看官方文档 ''' 查询数据 🍀 # 查所有,取所有字段 res = session.query(Userinfo).all() print(res) # 查所有,取指定字段,按照id排序 res = session.query(Userinfo.name).order_by(Userinfo.id).all() print(res) # 查所有,取指定字段,第一条信息 res = session.query(Userinfo.name).first() print(res) # 过滤查,逗号分隔,默认为and res = session.query(Userinfo).filter(Userinfo.id > 1,Userinfo.id ] [('Lyon',)] ('Lyon',) [] ''' 其他查询 🍀 #　条件 ret = session.query(MyClass).filter_by(name = 'some name') ret = session.query(MyClass).filter(MyClass.id > 1, MyClass.name == 'Lyon').all() ret = session.query(MyClass).filter(MyClass.id.between(1, 3), MyClass.name == 'eric').all() ret = session.query(MyClass).filter(MyClass.id.in_([1,2,3])).all() ret = session.query(MyClass).filter(~MyClass.id.in_([1,2,3])).all() ret = session.query(MyClass).filter(MyClass.id.in_(session.query(MyClass.id).filter_by(name='Lyon'))).all() from sqlalchemy import and_, or_ ret = session.query(MyClass).filter(and_(MyClass.id > 3, MyClass.name == 'Lyon')).all() ret = session.query(MyClass).filter(or_(MyClass.id 3), MyClass.extra != \"\" )).all() # 通配符 ret = session.query(MyClass).filter(MyClass.name.like('e%')).all() ret = session.query(MyClass).filter(~MyClass.name.like('e%')).all() # 限制 ret = session.query(MyClass)[1:2] # 排序 ret = session.query(MyClass).order_by(MyClass.name.desc()).all() ret = session.query(MyClass).order_by(MyClass.name.desc(), MyClass.id.asc()).all() # 分组 from sqlalchemy.sql import func ret = session.query(MyClass).group_by(MyClass.extra).all() ret = session.query( func.max(MyClass.id), func.sum(MyClass.id), func.min(MyClass.id)).group_by(MyClass.name).all() ret = session.query( func.max(MyClass.id), func.sum(MyClass.id), func.min(MyClass.id)).group_by(MyClass.name).having(func.min(MyClass.id) >2).all() # 连表 ret = session.query(Users, Favor).filter(Users.id == Favor.nid).all() ret = session.query(Person).join(Favor).all() ret = session.query(Person).join(Favor, isouter=True).all() # 组合 q1 = session.query(MyClass.name).filter(MyClass.id > 2) q2 = session.query(Favor.caption).filter(Favor.nid 2) q2 = session.query(Favor.caption).filter(Favor.nid "},"02-Go/":{"url":"02-Go/","title":"Go","keywords":"","body":"Golang 动一笔先 , 基础的语法等不会写 , 所以本目录下更新会比较慢 ... OOP 封装 : Golang 中的封装就不用说了 , 通过大小写控制 , 和 Python 的 __ 相比 , 差异还是比较大的 继承 : Golang 中的继承通过内嵌的方式实现 多态 : Golang 不像 Python , 天生多态 , 在 Golang 中 , 多态通过 interface 实现 , 详细后期慢慢慢慢更新... "},"02-Go/Golang - 语言基础.html":{"url":"02-Go/Golang - 语言基础.html","title":"Golang - 语言基础","keywords":"","body":"Golang - 语言基础 变量 在 Go 语言中定义变量有两种方式 : 一般声明和简短声明 一般声明 一般声明就是使用关键字 var 进行声明 , 格式如下 // 声明单个变量 var variableName type // 声明多个变量 var variableName1, variableName2, variableName3 type 或 var ( variableName1 type variableName2 type variableName3 type ) // 初始化值 variableName = value variableName1, variableName2, variableName3 = value1, value2, value3 // 声明并初始化值 var variableName type = value var variableName1, variableName2, variableName3 type = value1, value2, value3 var ( variableName1 type = value1 variableName2 type = value2 variableName3 type = value3 ) // 如果初始化值存在, 可以省略类型, 变量会从初始值中获取类型 var variableName1, variableName2, variableName3 = value1, value2, value3 当一个变量声明之后 , Go 会自动赋予它该类型的零值 简短声明 简短声明是使用 := 来定义变量 , 但是它只能在函数中使用 , 也就是说它只能用来定义局部变量 , 如果要定义全局变量 , 还是需要通过一般声明 var 来进行 func main() { variableName := value variableName1, variableName2, variableName3 := value1, value2, value3 } 常量 常量用于存储不会改变的数据 , 它在编译阶段就已经被确定了 , 并且在程序运行时无法改变 常量的声明与变量类似 , 只不过是使用 const 关键字 , 但是常量只能使用一般声明 , 不能使用简短声明 ( := ) const constantName = value // 你也可以明确指定常量的类型 const constantName type = value // 定义多个常量 const constantName1, constantName2, constantName3 = value1, value2, value3 基础类型 Go 的数据类型有 bool string int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uintptr byte // uint8 的别名 rune // int32 的别名 // 表示一个 Unicode 码点 float32 float64 complex64 complex128 关于 Go 中的其他派生类型 , 如 array , slice , map 等 , 后续会详细介绍 流程控制 if if 条件表达式 { } // 也可使用初始化语句 if 初始化语句; 条件表达式 { } 如下 if x if-else if 条件表达式 { } else if 条件表达式 { } else { } // 使用初始化语句 if 初始化语句; 条件表达式 { } else if 初始化语句; 条件表达式 { } else { } 如下 if x 5 { } else { } } switch switch 是编写一连串 if - else 语句的简便方法 , 它运行第一个值等于条件表达式的 case 语句 switch 值 { case 值: ... case 值, 值...: ... default: ... } // 没有条件的switch switch { case 条件表达式: ... case 条件表达式, 条件表达式...: ... default: ... } // 带有初始化语句 switch 初始化语句; 值 { case 值: ... case 值, 值...: ... default: ... } switch 初始化语句; { case 条件表达式: ... case 条件表达式, 条件表达式...: ... default: ... } 如下 i := 10 switch i { case 1: case 2, 3, 4: case 10: default: } // 没有条件的switch switch { case i 1, i 1, i switch 语句从上到下执行 , 当匹配成功的时候停止 , 如果需要往下继续执行 , 可以使用 fallthrough 强制执行后面的 case i := 10 switch i { case 10: fmt.Println(10) fallthrough case 9: fmt.Println(9) fallthrough case 8: fmt.Println(8) case 7: fmt.Println(7) } /* 执行结果: 10 9 8 */ for for 初始化语句; 条件表达式; 后置语句 { } // 初始化语句和后置语句是可选的 for ; 条件表达式; { } // for去掉分号之后将会变成while for 条件表达式 { } // 当省略条件表达式时, for将变成无限循环 for 初始化语句; ; 后置语句 { } // while同样 for { } 如下 for i := 0; i 当然 break 和 continue 就不用说了 for-range 用于迭代可迭代的结构 , 如 array 和 map for k, v := range map { } goto goto 必须与标签配合使用 func main() { i := 0 HERE: print(i) i++ if i == 5 { return } goto HERE } 特别注意 : 使用标签和 goto 语句是不被鼓励的 , 因为它们会很快导致非常糟糕的程序设置 , 而且总有更加可读的代替方案来实现相同的需求 除了 goto 之外 , fot , switch 和 select 与都也可以与标签配合使用 func main() { LABEL1: for i := 0; i 函数 // 无返回值 func funcName(input1 type1, input2 type2) { return value1, value2 } // 一个返回值 func funcName(input1 type1, input2 type2) type { return value1 } // 多个返回值 func funcName(input1 type1, input2 type2) (output1 type1, output2 type2) { return value1, value2 } // 可变参数 func funcName(input1 type1, input2 ...type2) (output1 type1, output2 type2) { return value1, value2 } 如下 func max(a, b int) int { if a > b { return a } return b } // 可变参数 func sum(a ...int) int { total := 0 for _, value := range a { total = total + value } return total } defer defer 语句会在函数执行都最后时执行 func main() { defer fmt.Println(\"world\") fmt.Println(\"hello\") } "},"03-MySQL/":{"url":"03-MySQL/","title":"MySQL","keywords":"","body":"MySQL 前言 为了科学地组织和存储数据 , 实现高效获取和维护数据 , 我们需要使用一个系统软件 , 数据库管理系统 (DataBase Management System , BDMS) , 简称数据库 数据库又分为关系型和非关系型两种 : 关系型数据库 : MySQL , Oracle , SqlServer , Access , Db2 , SQLite , MariaDB 非关系型数据库 : Redis , MongoDB , Memcached , Cassandra MySQL介绍 MySQL是一种快速易用的关系型数据库管理系统(RDBMS) , 由瑞典MySQL AB公司开发 , 目前属于Oracle旗下公司 , MySQL是目前最流行的关系型数据库管理系统 , 在Web应用方面MySQL是最好的RDBMS应用软件之一 具备以下优点 : 开源 , 免费使用 自身功能非常强大 , 足以匹敌绝大多数功能强大但却价格昂贵的数据库软件 使用业内所熟悉的标准SQL数据库语言(SQL语句通用) 可运行于多个操作系统 , 支持多种语言 , 包括PHP , C , C++ , Java等语言 非常迅速 , 即使面对大型数据集也毫无滞涩 非常适用Web应用开发 支持大型数据库 , 最高可在一个表中容纳5千多万行 ; 每张表的默认文件大小限制4GB , 不过如果操作系统支持 , 可以将期理论限制增加到800万TB 可以自定义 , 开源GPL许可保证了程序员可以自由修改MySQL , 以便适应各自特殊的开发环境 RDBMS术语 术语 描述 数据库(Database) 数据库是带有相关数据的表的集合 表(Table) 表是带有数据的矩阵 , 数据库中的表就像一种简单的电子表格 列(Column) 每一列 (数据元素) 都包含着同种类型的数据 , 比如邮编 行(Row) 行 (又被称为元组 , 项或者记录) 是一组相关数据 , 比如有关订阅量的数据 冗余(Redundancy) 存储两次数据 , 以便使系统更快速 主键(Primary key) 主键是唯一的 , 同一张表中不允许出现同样两个键值 , 一个键值只对应着一行 外键(Foreign key) 用于连接两张表 复合键(Compound key) 复合键 (又称组合键) 是一种由多列组成的键 , 因为一列并不足以确定唯一性 索引(Index) 它在数据库中的作用就像书后的索引一样 引用完性(Referential Integrity) 用来确保外键一直指向已存在的一行 "},"03-MySQL/01-MySQL - 库操作.html":{"url":"03-MySQL/01-MySQL - 库操作.html","title":"MySQL - 库操作","keywords":"","body":"MySQL - 库操作 SQL介绍 🍀 SQL是Structured Query Language(结构化查询语言)的缩写 , SQL是转为数据库而建立的操作命令集 , 是一种功能齐全的数据库语言 SQL分类 SQL语句主要可以划分为一下3个类别 : DDL(Data Definition Languages) 语句 : 数据定义语句 , 这些语句定义了不同的数据段 , 数据库 , 表 , 列 , 索引等数据库对象 ; 常用的语句关键字主要包括create , drop , alter等 DML(Data Manipulation Language) 语句 : 数据操纵语句 , 用于添加 , 删除 , 更新和查询数据库记录 , 并检查数据完整性 ; 常用的语句关键字主要包括insert , delete , update和select等 DCL(Data Control Language) 语句 : 数据控制语句 , 用于控制不同数据段直接许可和访问级别的语句 , 这些语句定义了数据库 , 表 , 字段 , 用户的访问权限和安全级别 ; 主要的语句关键字包括grant , revoke等 SQL规范 在数据库系统中 , SQL语句不区分大小写 (建议用大写) , 但字符串常量区分大小写 ; 建议命令大写 , 表名库名小写 SQL语句可单行或多行书写 , 以\" ; \"结尾 , 关键字不能跨多行或简写 用空格和缩进来提高语句的可读性 , 子句通常位于独立行 , 便于编辑 , 提高可读性 单行注释 : -- 多行注释 : / *... */ SQL语句可拆行操作 数据库操作 🍀 在MySQL数据中有如下默认数据库 默认数据库 描述 information_schema 虚拟库 , 不占用磁盘空间 , 存储的是数据库启动后的一些参数 , 如用户表信息 , 列信息 , 权限信息 , 字符信息等 test 用户用来测试的数据库 (MySQL 5.7没有) mysql 授权库 , 主要存储系统用户的权限信息 performance_schema MySQL 5.5 后新增的 , 主要用于收集数据库服务器性能参数 , 记录处理查询请求时发生的各种事件 , 锁等现象 sys 包含了一系列视图、函数和存储过程 查看数据库 🍀 SHOW DATABASES; 查看所有数据库 SHOW CREATE DATABASE dbname; 查看数据库的创建信息 实例 mysql> SHOW DATABASES; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.00 sec) /* 查看mysql库的创建信息 */ mysql> show create database mysql; +----------+----------------------------------------------------------------+ | Database | Create Database | +----------+----------------------------------------------------------------+ | mysql | CREATE DATABASE `mysql` /*!40100 DEFAULT CHARACTER SET utf8 */ | +----------+----------------------------------------------------------------+ 1 row in set (0.00 sec) 创建数据库 🍀 CREATE DATABASE dbname DEFAULT CHARSET utf8 COLLATE utf8_general_ci; 创建字符串为utf-8的数据库 CREATE DATABASE dbname DEFAULT CHARACTER SET gbk COLLATE gbk_chinese_ci; 创建字符串为gbk的数据库 实例 mysql> CREATE DATABASE test DEFAULT CHARSET utf8 COLLATE utf8_general_ci; Query OK, 1 row affected (0.00 sec) /* 查看所有数据库 */ mysql> SHOW DATABASES; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | test | +--------------------+ 5 rows in set (0.00 sec) 删除数据库 🍀 DROP DATABASE dbname; 删除数据库 使用数据库 🍀 USE dbname; 进入数据库 SHOW TABLES; 查看当前数据库中所有的表 SELECT DATABASE(); 查看当前使用的数据库 实例 mysql> SELECT DATABASE(); +------------+ | DATABASE() | +------------+ | NULL | +------------+ 1 row in set (0.00 sec) /* 使用mysql数据库 */ mysql> USE mysql; Database changed mysql> SELECT DATABASE(); +------------+ | DATABASE() | +------------+ | mysql | +------------+ 1 row in set (0.00 sec) 用户管理 🍀 CREATE USER 'lyon'@'%' IDENTIFIED BY '123'; 创建`lyon`用户,允许任意IP访问,密码为`123` RENAME USER 'lyon'@'%' TO 'mylyon'@'127.0.0.1'; 修改用户 SET PASSWORD FOR 'mylyon'@'127.0.0.1' = PASSWORD('456'); 修改密码为`456` DROP USER 'mylyon'@'127.0.0.1'; 删除用户`lyon` 实例 /* 用户权限信息都在mysql库中,先进入mysql库 */ mysql> USE mysql; Database changed /* 查看原有用户 */ mysql> SELECT HOST,USER FROM USER; +-----------+-----------+ | HOST | USER | +-----------+-----------+ | localhost | mysql.sys | | localhost | root | +-----------+-----------+ 2 rows in set (0.00 sec) /* 创建新用户'lyon',允许任意IP */ mysql> CREATE USER 'lyon'@'%' IDENTIFIED BY '123'; Query OK, 0 rows affected (0.00 sec) /* 查看所有用户 */ mysql> SELECT HOST,USER FROM USER; +-----------+-----------+ | HOST | USER | +-----------+-----------+ | % | lyon | | localhost | mysql.sys | | localhost | root | +-----------+-----------+ 3 rows in set (0.00 sec) /* 修改用户 */ mysql> RENAME USER 'lyon'@'%' TO 'mylyon'@'127.0.0.1'; Query OK, 0 rows affected (0.00 sec) /* 查看所有用户 */ mysql> SELECT HOST,USER FROM USER; +-----------+-----------+ | HOST | USER | +-----------+-----------+ | 127.0.0.1 | mylyon | | localhost | mysql.sys | | localhost | root | +-----------+-----------+ 3 rows in set (0.00 sec) /* 修改用户密码 */ mysql> SET PASSWORD FOR 'mylyon'@'127.0.0.1' = PASSWORD('456'); Query OK, 0 rows affected, 1 warning (0.00 sec) /* 删除用户 */ mysql> DROP USER 'mylyon'@'127.0.0.1'; Query OK, 0 rows affected (0.00 sec) /* 查看所有用户 */ mysql> SELECT HOST,USER FROM USER; +-----------+-----------+ | HOST | USER | +-----------+-----------+ | localhost | mysql.sys | | localhost | root | +-----------+-----------+ 2 rows in set (0.00 sec) 注意 : 'username'@'IP' 用户只能在该IP下才能访问 'username'@'127.0.0.1' 用户只能在该IP段下才能访问(通配符%表示任意) 'username'@'%' 用户可以再任意IP下访问(默认IP地址为%) 授权管理 🍀 SHOW GRANTS FOR 'username'@'IP'; 查看用户权限 GRANT 权限 ON dbname.表 TO 'username'@'IP'; 授权 REVOKE 权限 ON dbname.表 FROM 'username'@'IP'; 取消授权 实例 /* 创建用户lyon@%,密码为123 */ mysql> CREATE USER 'lyon'@'%' IDENTIFIED BY '123'; Query OK, 0 rows affected (0.00 sec) /* 查看用户lyon@%的权限 */ mysql> SHOW GRANTS FOR 'lyon'@'%'; +----------------------------------+ | Grants for lyon@% | +----------------------------------+ | GRANT USAGE ON *.* TO 'lyon'@'%' | +----------------------------------+ 1 row in set (0.00 sec) /* 授予用户lyon@% select权限,即将Select_priv改成Y */ mysql> GRANT SELECT ON *.* TO 'lyon'@'%'; Query OK, 0 rows affected (0.00 sec) /* 查看用户lyon@%的权限 */ mysql> SHOW GRANTS FOR 'lyon'@'%'; +-----------------------------------+ | Grants for lyon@% | +-----------------------------------+ | GRANT SELECT ON *.* TO 'lyon'@'%' | +-----------------------------------+ 1 row in set (0.00 sec) /* 取消对用户lyon@%的SELECT授权 */ mysql> REVOKE SELECT ON *.* FROM 'lyon'@'%'; Query OK, 0 rows affected (0.00 sec) /* 查看用户lyon@%的权限 */ mysql> SHOW GRANTS FOR 'lyon'@'%'; +----------------------------------+ | Grants for lyon@% | +----------------------------------+ | GRANT USAGE ON *.* TO 'lyon'@'%' | +----------------------------------+ 1 row in set (0.00 sec) 也可以用SELECT * FROM USER WHERE USER='lyon' AND HOST='%' \\G; 命令查看 , 具体如下 : *************************** 1. row *************************** Host: % User: lyon Select_priv: N ... *************************** 1. row *************************** Host: % User: lyon Select_priv: Y ... 权限介绍 all privileges 除grant外的所有权限 select 仅查权限 select,insert 查和插入权限 usage 无访问权限 alter 使用alter table alter routine 使用alter procedure和drop procedure create 使用create table create routine 使用create procedure create temporary tables 使用create temporary tables create user 使用create user,drop user,rename user和revoke all privileges create view 使用create view delete 使用delete drop 使用drop table execute 使用call和存储过程 file 使用select into outfile 和 load data infile grant option 使用grant 和 revoke index 使用index insert 使用insert lock tables 使用lock table process 使用show full processlist select 使用select show databases 使用show databases show view 使用show view update 使用update reload 使用flush shutdown 使用mysqladmin shutdown(关闭MySQL) super 使用change master,kill,logs,purge,master和set global,还允许 mysqladmin调试登陆 replication client 服务器位置的访问 replication slave 由复制从属使用 flush privileges　　　 　将数据读取到内存中,从而立即生效 PS : 代表所有 , \\.* 代表所有数据库中的所有表 "},"03-MySQL/02-MySQL - 数据类型.html":{"url":"03-MySQL/02-MySQL - 数据类型.html","title":"MySQL - 数据类型","keywords":"","body":"MySQL - 基本数据类型 介绍 🍀 MySQL中定义数据字段的类型对数据库的优化是非常重要的 MySQL支持多种数据类型 , 主要包括 : 数值类型 日期时间类型 字符串类型 本篇内容 , 以MySQL 5.0 版本为例 , 因为不同的版本可能有所差异 , 不过差异不大 数值类型 🍀 MySQL支持所有标准SQL中的数值类型 , 其中包括严格数值类型 , 以及近似数值数据类型 , 并在此基础上做了扩展 , 增加了TINYINT , MEDIUMINT , BIGINT 3种长度不同的整型 , 并增加了BIT类型 , 用来存放位数据 整数类型介绍 整数类型 字节 最小值 最大值 TINYINT 1 有符号 : -128 无符号 : 0 有符号 : 127无符号 : 255 SMALLINT 2 有符号 : -32768无符号 : 0 有符号 : 32767无符号 : 65535 MEDIUMINT 3 有符号 : -8388608无符号 : 0 有符号 : 8388607无符号 : 1677215 INT , INTEGER 4 有符号 : -2147483648无符号 : 0 有符号 : 2147483647无符号 : 4294967295 BIGINT 8 有符号 : -9223372036854775808无符号 : 0 有符号 : 9223372036854775807无符号 : 18446744073709551615 浮点数类型介绍 浮点数类型 字节 最小值 最大值 FLOAT 4 ±1.175494351E-38 ±3.402823466E+38 DOUBLE 8 ±2.2250738585072014E-308 ±1.7976931348623157E+308 定点数类型介绍 定点数类型 字节 描述 DEC (M , D) DECIMAL (M , D) M + 2 最大取值范围与DOUBLE相同 , 给定DECIMAL的有效取值范围由 M 和 D 决定 位类型介绍 位类型 字节 最小值 最大值 BIT (M) 1~8 BIT (1) BIT (64) 特别的 : MySQL中无布尔值 , 可以使用TINYINT(1)构造 , 0为假非0为真 , 如下 : mysql> SELECT IF(0, 'true', 'false'); +------------------------+ | IF(0, 'true', 'false') | +------------------------+ | false | +------------------------+ 1 row in set (0.00 sec) mysql> SELECT IF(1, 'true', 'false'); +------------------------+ | IF(1, 'true', 'false') | +------------------------+ | true | +------------------------+ 1 row in set (0.00 sec) 但是真假的值只有1和0 , 而不是非0的值都为真 , 如下 : mysql> SELECT IF(0 = FALSE, 'true', 'false'); +--------------------------------+ | IF(0 = FALSE, 'true', 'false') | +--------------------------------+ | true | +--------------------------------+ 1 row in set (0.01 sec) mysql> SELECT IF(1 = TRUE, 'true', 'false'); +-------------------------------+ | IF(1 = TRUE, 'true', 'false') | +-------------------------------+ | true | +-------------------------------+ 1 row in set (0.00 sec) mysql> SELECT IF(2 = TRUE, 'true', 'false'); +-------------------------------+ | IF(2 = TRUE, 'true', 'false') | +-------------------------------+ | false | +-------------------------------+ 1 row in set (0.00 sec) mysql> SELECT IF(2 = FALSE, 'true', 'false'); +--------------------------------+ | IF(2 = FALSE, 'true', 'false') | +--------------------------------+ | false | +--------------------------------+ 1 row in set (0.00 sec) 日期时间类型 🍀 MySQL中有多种数据类型可以用于日期和时间的表示 , 这些数据类型的主要区别如下 : 表示年月日 , 通常用DATE来表示 表示年月日时分秒 , 通常用DATETIME表示 只表示时分秒 , 通常用TIME来表示 日期和时间类型介绍 日期和时间类型 字节 最小值 最大值 DATE 4 1001-01-01 9999-12-31 DATETIME 8 1000-01-01 00 : 00 : 00 9999-12-31 23 : 59 : 59 TIMESTAMP 4 19700101080001 2038年的某个时刻 TIME 3 -838 : 59 : 59 838 : 59 : 59 YEAR 1 1901 2155 每种日期时间类型都有一个有效值范围 , 如果超出这个范围 , 在默认的SQLMode下 , 系统会进行错误提示 , 并将以零值来进行存储 , 不同日期类型零值的表示如下 : 日期和时间类型的零值表示 数据类型 零值表示 DATETIME 0000-00-00 00 : 00 : 00 DATE 0000-00-00 TIMESTAMP 00000000000000 TIME 00 : 00 : 00 YEAR 0000 字符串类型 🍀 MySQL包括了CHAR , VARCHAR , BINARY , VARBINARY , BLOB , TEXT , ENUM 和 SET等多种字符串类型 字符串类型介绍 字符串类型 字节 描述及存储需求 CHAR(M) M M为0~255之间的整数 VARCHAR(M) M为0~65535之间的整数 , 值的长度+1个字节 TINYBLOB 允许长度0~255字节 , 值的长度+1个字节 BLOB 允许长度0~65535字节 , 值的长度+2个字节 MEDIUMBLOB 允许长度0~167772150字节 , 值的长度+3个字节 LONGBLOB 允许长度0~4294967295字节 , 值的长度+4个字节 TINYTEXT 允许长度0~255字节 , 值的长度+2个字节 TEXT 允许长度0~65535字节 , 值的长度+2个字节 MEDIUMTEXT 允许长度0~167772150字节 , 值的长度+3个字节 LONGTEXT 允许长度0~4294967295字节 , 值的长度+4个字节 VARBINARY(M) 允许长度0~M个字节的变长字节字符串 , 值的长度+1个字节 BINARY(M) M 允许长度0~M个字节的定长字节字符串 CHAR 与 VARCHAR 两者类似 , 但保存和检索方式不同 CHAR长度固定 , VARCHAR长度可变 在检索时 , CHAR列删除了尾部的空格 , 而VARCHAR则保留这些空格 BINARY 与 VARBINARY BINARY 与 VARBINARY 类似于 CHAR 与 VARCHAR , 不同的是它们包含二进制字符串而不包含非二进制字符串 , 也就是说 , 它们包含字节字符串而不是字符字符串 , 它们没有字符集 , 并且排序和比较基于列值字节的数值值 ENUM ENUM中文名称叫枚举类型 , 它的值范围需要在创建表时通过枚举方式显示指定 , 意思就是字段的值只能在给定范围中选择 , 对1~255个成员的枚举需要1个字节存储 ; 对于255~65535个成员 , 需要2个字节存储 , 最多允许由65535个成员 使用ENUM时需注意 : ENUM类型是忽略大小写的 , 在存储时会将小写都转成大写 对于插入不在ENUM指定范围内的值时 , 并不会返回警告 , 而是插入enum()中的第一个值 ENUM类型只允许从值集合中选取单个值 , 而不能一次选取多个值 SET SET 和 ENUM类型非常类似 , 也是一个字符串对象 , 里面可以包含0~64个成员 1~8成员的集合 , 占1个字节 9~16成员的集合 , 占2个字节 17~24成员的集合 , 占3个字节 25~32成员的集合 , 占4个字节 33~64成员的集合 , 占8个字节 SET 和 ENUM除了存储之外 , 最主要的区别在于SET类型一次可以选取多个成员 , ENUM则只能选一个 mysql> CREATE TABLE myset(col SET('a','b','c','d')); Query OK, 0 rows affected (0.32 sec) mysql> INSERT INTO myset VALUES('a,b'),('a,d,a'),('a,b'),('a,c'),('a'); Query OK, 5 rows affected (0.12 sec) Records: 5 Duplicates: 0 Warnings: 0 mysql> SELECT * FROM myset; +------+ | col | +------+ | a,b | | a,d | | a,b | | a,c | | a | +------+ 5 rows in set (0.00 sec) 对于超出允许值范围的值 , 将不允许注入到设置的SET类型中 , 而对于包含重复成员的集合将只取一次 , 集合去重 "},"03-MySQL/03-MySQL - 存储引擎.html":{"url":"03-MySQL/03-MySQL - 存储引擎.html","title":"MySQL - 存储引擎","keywords":"","body":"MySQL - 存储引擎 介绍 🍀 插件式存储引擎是MySQL数据库最重要的特性之一 , 用户可以根据应用的需要选择如何存储和索引数据 , 是否使用实务等 , MySQL默认支持多种存储引擎(表类型) , 用户还可以按照自己的需要定制和使用自己的存储引擎 MySQL 5.0支持的存储引擎包括MyISAM , InnoDB , BDB , MEMORY , MERGE , EXAMPLE , NDB Cluster , ARCHIVE , CSV , BLACKHOLE 等 , 其中InnoDB 和 BDB提供事物安全表 , 其他存储引擎都是非安全事物安全表 创建新表时如果不指定存储引擎 , 那么系统就会使用默认存储引擎 , MySQL 5.5 之前的默认存储引擎是MyISAM , 5.5 之后改为了InnoDB , 如果修改默认的存储引擎 , 可以在参数文件中红设置 default-table-type 查看当前默认存储引擎 , 可以使用以下命令 : SHOW VARIABLES LIKE 'table_type'; 查询当前数据库支持的存储引擎 , 可以使用一下命令 : -- 第一种方法 SHOW ENGINES \\G -- 第二种方法 SHOW VARIABLES LIKE 'have%'; 常用存储引擎对比 特点 MyISAM InnoDB MEMORY MERGE NDB 存储限制 有 64TB 有 没有 有 事物安全 支持 锁机制 表锁 行锁 表锁 表锁 行锁 B树索引 支持 支持 支持 支持 支持 哈希索引 支持 支持 全文索引 支持 集群索引 支持 数据缓存 支持 支持 支持 索引缓存 支持 支持 支持 支持 支持 数据可压缩 支持 空间使用 低 高 N/A 低 低 内存使用 低 高 中等 低 高 批量插入的速度 高 低 高 高 高 支持外键 支持 下面重点介绍最常使用的4中存储引擎 : MyISAM , InnoDB , MEMORY 和 MERGE MyISAM 🍀 MyISAM不支持事务 , 也不支持外键 , 其优势是访问的速度快 , 对事务完整性没有要求或者以SELECT , INSERT为主的应用基本上都可以使用这个引擎来创建表 每个MyISAM在磁盘上存储成3个文件 , 其文件名都和表明相同 , 但扩展名分别是.frm(存储表定义) , .MYD(MYDate , 存储数据) , .MYI(MYIndex , 存储索引) ; MyISAM的表还支持3种不同的存储格式 , 分别是 : 静态表(固定长度) , 动态表 , 压缩表 数据文件和索引文件可以放置在不同的目录 , 平局分布IO , 获得更快的速度 ; 不同MyISAM表的索引文件和数据文件可以放置到不同的路经下 , 文件路经要是绝对路经 , 并且具有访问权限 MyISAM类型的表可能会损坏 , 原因可能是多种多样的 , 损坏后的表可能不能被访问 , 会提示需要修复或者访问返回错误的结果 ; MyISAM类型的表提供修复的工具 , 可以用CHECK TABLE语句来检查MyISAM表的健康 , 并用REPAIR TABLE语句修复一个损坏的MyISAM表 . MyISAM另一个与众不同的地方是 , 它的缓冲池只缓存(cache)索引文件 , 而不缓存数据文件 , 这与大多数的数据库都不相同 InnoDB 🍀 InnoDB存储引擎提供了具有提交 , 回滚和崩溃恢复能力的事务安全 , 但是对比MyISAM , InnoDB写的处理效率差一些 , 并且会占用更多的磁盘空间以保留数据和索引 InnoDB是MySQL数据库最为常用的存储引擎 , 其不同于其他存储引擎的表的特点如下 自动增长列 🍀 InnoDB表的自增列可以手工插入 , 但是插入的值如果是空或者0 , 则实际插入的将是自动增长后的值 自增实例 mysql> USE mydatabase; Database changed mysql> CREATE TABLE autoincre_demo( -> id int NOT NULL AUTO_INCREMENT, -- 设置id列自增 -> name varchar(10), -> PRIMARY KEY(id) -- 将id列设为主键 -> )ENGINE=InnoDB; Query OK, 0 rows affected (0.28 sec) /* 插入数据 */ mysql> INSERT INTO autoincre_demo VALUES(1,'lyon'),(0,'leon'),(NULL,'kenneth'); Query OK, 3 rows affected (0.11 sec) Records: 3 Duplicates: 0 Warnings: 0 /* 查看表 */ mysql> SELECT * FROM autoincre_demo; +----+---------+ | id | name | +----+---------+ | 1 | lyon | | 2 | leon | | 3 | kenneth | +----+---------+ 3 rows in set (0.00 sec) PS : 可以通过ALTER TABLE *** AUTO_INCREMENT = n; 语句强制设置自动增长列的初始值 , 默认从1开始 , 但是该强制的默认值是保留在内存中的 , 如果该值在使用之前数据库重新启动 , 那么这个强制的默认值就会丢失 , 就需要在数据库启动以后重新设置 可以使用LAST_INSERT_ID() 查询当前线程最后插入记录使用的值 , 如果一次插入多条记录 , 则返回第一条记录使用的自动增长值 LAST_INSERT_ID()实例 /* 从1开始,所以自增值为2 */ mysql> SELECT LAST_INSERT_ID(); +------------------+ | LAST_INSERT_ID() | +------------------+ | 2 | +------------------+ 1 row in set (0.00 sec) /* 再插3条 */ mysql> INSERT INTO autoincre_demo(name) VALUES('FIVE'),('SIX'),('SEVEN'); Query OK, 3 rows affected (0.12 sec) Records: 3 Duplicates: 0 Warnings: 0 /* 默认开始有3条,所以自增值为5 */ mysql> SELECT LAST_INSERT_ID(); +------------------+ | LAST_INSERT_ID() | +------------------+ | 5 | +------------------+ 1 row in set (0.00 sec) 对于InnoDB表 , 自动增长列必须是索引(主键) , 如果是组合索引 , 也必须是组合索引的第一列 但是对于MyISAM表 , 自增列可以是组合索引的其他列 外键约束 🍀 MySQL支持外键的存储引擎只有InnoDB , 在创建外键的时候 , 要求父表必须有对应的索引 , 子表在创建外键的时候也会自动创建对应的索引 实例 /* 创建父表country */ mysql> CREATE TABLE country( -> country_id SMALLINT UNSIGNED NOT NULL AUTO_INCREMENT, -- 创建自增列(索引) -> country VARCHAR(50) NOT NULL, -> last_update TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, -- 最后更新时间 -> PRIMARY KEY (country_id) -- 设置主键索引 -> )ENGINE=InnoDB DEFAULT CHARSET=utf8; Query OK, 0 rows affected (0.32 sec) /* 创建子表city */ mysql> CREATE TABLE city( -> city_id SMALLINT UNSIGNED NOT NULL AUTO_INCREMENT, -- 创建自增列 -> city VARCHAR(50) NOT NULL, -> country_id SMALLINT UNSIGNED NOT NULL, -- 外键 -> last_update TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, -> PRIMARY KEY (city_id), -- 设置主键索引 -> KEY idx_fk_country_id (country_id), -> CONSTRAINT fk_city_country FOREIGN KEY (country_id) REFERENCES country (country_id) ON DELETE RESTRICT ON UPDATE CASCADE -- 设置外键,对应country表的主键country_id -> )ENGINE=InnoDB DEFAULT CHARSET=utf8; Query OK, 0 rows affected (0.40 sec) 在创建索引时 , 可以指定在删除 , 更新父表时 , 对子表进行的相应操作 , 包括RESTRICT , CASCADE , SET NULL 和 NO ACTION . 其中RESETRICT 和 NO ACTION 相同 , 是指限制在子表有关联记录的情况下父表不能更新 ; CASCADE表示父表在更新或者删除时 , 更新或者删除子表对应记录 ; SET NULL 则表示父表在更新或者删除的时候 , 子表的对应字段被设置为NULL /* 子表的外键指定是 ON DELETE RESTRICT ON UPDATE CASCADE 方式,即主表删除记录时,如果子表有对应记录,则不允许删除,主表在更新记录时,如果子表有对应记录,则子表对应更新 */ -> CONSTRAINT fk_city_country FOREIGN KEY (country_id) REFERENCES country (country_id) ON DELETE RESTRICT ON UPDATE CASCADE 当某个表被其他表创建了外键参照 , 那么该表的对应索引或者主键禁止被删除 关闭外键 在导入多个表的数据时 , 如果需要忽略表之前的导入顺序 , 可以暂时关闭外键的检查 ; 在执行LOAD DATE 和 ALTER TABLE操作的时候 , 可以通过暂时关闭外键约束来加快处理的速度 关闭命令 : SET FOREIGN_KEY_CHECKS = 0; 打开外键则将0改为1即可 查看外键信息 命令 : SHOW CREATE TABLE或者SHOW TABLE STATUS 实例 : mysql> SHOW TABLE STATUS LIKE 'city' \\G; *************************** 1. row *************************** Name: city Engine: InnoDB Version: 10 Row_format: Dynamic Rows: 0 Avg_row_length: 0 Data_length: 16384 Max_data_length: 0 Index_length: 16384 Data_free: 0 Auto_increment: 1 Create_time: 2017-10-18 17:16:18 Update_time: NULL Check_time: NULL Collation: utf8_general_ci Checksum: NULL Create_options: Comment: 1 row in set (0.01 sec) ERROR: No query specified 存储方式 🍀 InnoDB存储表和索引有两种方式 : 使用共享表空间存储 使用多表空间存储 , 需要设置参数 innodb_file_per_table , 并且重新启动才生效 对于表中数据的存储 , InnoDB 存储引擎采用了聚集(clustered)的方式 , 每张表都是按主键的顺序进行存储的 , 如果没有显式地在表定义时指定主键 , InnoDB 存储引擎会为每一行生成一个 6 字节的 ROWID , 并以此作为主键 深入了解InnoDB存储引擎的工作 , 原理 , 实现和应用 , 可以参考《MySQL技术内幕 : InnoDB存储引擎》一书 MEMORY 🍀 MEMORY存储引擎使用存在于内存中的内容来创建表 每个MEMORY表只实际对应一个磁盘文件 , 格式是.frm , MEMORY类型的表访问非常地快 , 因为它的数据是存放在内存中的 , 并且默认使用HASH索引 , 但是一旦服务关闭 , 表中的数据就会丢失 给MEMORY创建索引的时候 , 可以指定使用HASH索引还是BTREE索引 实例 mysql> CREATE TABLE tab_memory ENGINE=MEMORY -> SELECT city_id,city,country_id -> FROM city GROUP BY city_id; Query OK, 0 rows affected (0.07 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql> CREATE INDEX mem_hash USING HASH ON tab_memory(city_id); Query OK, 0 rows affected (0.10 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql> SHOW INDEX FROM tab_memory \\G; *************************** 1. row *************************** Table: tab_memory Non_unique: 1 Key_name: mem_hash Seq_in_index: 1 Column_name: city_id Collation: NULL Cardinality: 0 Sub_part: NULL Packed: NULL Null: Index_type: HASH Comment: Index_comment: 1 row in set (0.00 sec) ERROR: No query specified 服务器需要足够内存来维持所有在同一时间使用的MEMORY表 , 当不再需要MEMOY表的内容之时 , 要释放被MEMORY表使用的内存 , 应该执行DELETE FROM 或TRUNCATE TABLE , 或者整个地删除(使用DROP TABLE操作) MEMORY类型的存储引擎主要用于那些内容变化不频繁的代码表 , 或者作为统计操作的中间结果表 , 便于高效地对中间结果进行分析并得到最终的统计结果 ; 对存储引擎为MEMORY的表进行更新操作要谨慎 , 因为数据并没有实际写入到磁盘中 , 所以一定要对下次重新启动服务后如何获得这些修改后的数据有所考虑 MERGE 🍀 MERGE存储是一组MyISAM表的组合 , 这些MyISAM表必须结构完全相同 , MERGE表本身并没有数据 , 对MERGE类型的表可以进行查询 , 更新 , 删除操作 , 这些操作实际上是对内部的MyISAM表进行的 可以对MERGE表进行DROP操作 , 这个操作只是删除MERGE的定义 , 对内部的表没有任何的影响 通常我们使用MERGE表来透明地对多个表进行查询和更新操作 , 而对按照时间记录的操作日志则可以透明地进行插入操作 TokuDB 🍀 前面的都是MySQL自带的存储引擎 , 除了这些之外 , 还有一些常见的第三方存储引擎 , 在某些特定应用中也有广泛使用 , 比如列式存储引擎Infobright , 高写性能高压缩的TokuDB就是其中非常有代表性的两种 TokuDB是一个高性能 , 支持事务处理的MySQL和MariaDB的存储引擎 , 具有高扩展性 , 高压缩率 , 高效的写入性能 , 支持大多数在线DDL操作 主要特性 : 使用Fractal树索引保证高效的插入性能 优秀的压缩特性 , 比InnoDB高近10倍 Hot Schema Changes 特性支持在线创建索引和添加 , 删除属性列等DDL操作 使用Bulk Loader达到快速加载大量数据 提供了主从延迟消除技术 支持ACID和MVCC 适用场景 : 日志数据 , 因为日志通常插入频繁切存储量大 历史数据 , 通常不会再有写操作 , 可以利用TokuDB的高压缩特性进行存储 在线DDL较频繁的场景 , 使用TokuDB可以大大增加系统的可用性 "},"03-MySQL/04-MySQL - 表操作.html":{"url":"03-MySQL/04-MySQL - 表操作.html","title":"MySQL - 表操作","keywords":"","body":"MySQL - 表操作 介绍 🍀 该部分语句属于DDL语句 , 对表的定义 , 结构的修改 与DML语句的区别在于 , DML语句仅对表内部数据进行操作 , 即数据的增删改查 DDL语句更多地由数据库管理员(DBA)使用 , 开发人员一般很少使用 创建表 🍀 -- 创建数据库 mysql> CREATE DATABASE mydatabase DEFAULT CHARSET utf8 COLLATE utf8_general_ci; Query OK, 1 row affected (0.00 sec) -- 使用数据库 mysql> USE mydatabase; Database changed -- 创建表tb mysql> CREATE TABLE tb( -> id int(5) NOT NULL AUTO_INCREMENT, -> name char(15) NOT NULL, -> alias varchar(10) DEFAULT NULL, -> email varchar(30) DEFAULT NULL, -> password varchar(20) NOT NULL, -> phone char(11) DEFAULT '00000000000', -> PRIMARY KEY(id,name) -> )ENGINE=InnoDB DEFAULT CHARSET=utf8; Query OK, 0 rows affected (0.24 sec) -- 查看表定义 mysql> DESC tb; +----------+-------------+------+-----+-------------+----------------+ | Field | Type | Null | Key | Default | Extra | +----------+-------------+------+-----+-------------+----------------+ | id | int(5) | NO | PRI | NULL | auto_increment | | name | char(15) | NO | PRI | NULL | | | alias | varchar(10) | YES | | NULL | | | email | varchar(30) | YES | | NULL | | | password | varchar(20) | NO | | NULL | | | phone | char(11) | YES | | 00000000000 | | +----------+-------------+------+-----+-------------+----------------+ 6 rows in set (0.00 sec) -- 查看表详细定义,\\G的含义是使记录按照字段竖向排列 mysql> SHOW CREATE TABLE tb \\G; *************************** 1. row *************************** Table: tb Create Table: CREATE TABLE `tb` ( `id` int(5) NOT NULL AUTO_INCREMENT, `name` char(15) NOT NULL, `alias` varchar(10) DEFAULT NULL, `email` varchar(30) DEFAULT NULL, `password` varchar(20) NOT NULL, `phone` char(11) DEFAULT '00000000000', PRIMARY KEY (`id`,`name`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 1 row in set (0.00 sec) ERROR: No query specified 删除表 🍀 mysql> DROP TABLE tb; Query OK, 0 rows affected (0.21 sec) 修改表 🍀 修改表类型 🍀 语法 : ALTER TABLE tablename MODIFY [COLUMN] column_name column_type [FIRST|AFTER col_name]; -- [...]表示中间的可以省略不写 实例 -- 创建emp表 mysql> CREATE TABLE emp( -> ename VARCHAR(10), -> hiredate DATE, -> sal DECIMAL(10,2), -> deptno INT(2) -> )ENGINE=InnoDB DEFAULT CHARSET=utf8; Query OK, 0 rows affected (0.27 sec) -- 修改emp表中ename字段的类型 mysql> ALTER TABLE emp MODIFY ename varchar(20); Query OK, 0 rows affected (0.08 sec) Records: 0 Duplicates: 0 Warnings: 0 -- 查看表定义 mysql> DESC emp; +----------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+---------------+------+-----+---------+-------+ | ename | varchar(20) | YES | | NULL | | | hiredate | date | YES | | NULL | | | sal | decimal(10,2) | YES | | NULL | | | deptno | int(2) | YES | | NULL | | +----------+---------------+------+-----+---------+-------+ 4 rows in set (0.00 sec) 增加表字段 🍀 语法 : ALTER TABLE tablename ADD [COLUMN] column_name column_type [FIRST|AFTER col_name]; ALTER TABLE tablename ADD PRIMARY KEY(column_name); ALTER TABLE slave_table ADD CONSTRAINT symbol(如:FK_slave_primary) FOREIGN KEY slave_table(foreign_name) REFERENCES primary_table(primary_name); 实例 : -- 增加age字段 mysql> ALTER TABLE emp ADD age int(3); Query OK, 0 rows affected (0.63 sec) Records: 0 Duplicates: 0 Warnings: 0 -- 增加id字段 mysql> ALTER TABLE emp ADD id int(5); Query OK, 0 rows affected (0.53 sec) Records: 0 Duplicates: 0 Warnings: 0 -- 将id字段设置为主键 mysql> ALTER TABLE emp ADD PRIMARY KEY(id); Query OK, 0 rows affected (0.41 sec) Records: 0 Duplicates: 0 Warnings: 0 -- 查看表定义 mysql> DESC emp; +----------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+---------------+------+-----+---------+-------+ | ename | varchar(20) | YES | | NULL | | | hiredate | date | YES | | NULL | | | sal | decimal(10,2) | YES | | NULL | | | deptno | int(2) | YES | | NULL | | | age | int(3) | YES | | NULL | | | id | int(5) | NO | PRI | NULL | | +----------+---------------+------+-----+---------+-------+ 6 rows in set (0.00 sec) 删除表字段 🍀 语法 : ALTER TABLE tablename DROP [COLUMN] column_name; ALTER TABLE DROP FOREIGN KEY foreign_key_name; ALTER TABLE DROP PRIMARY KEY; 实例 : -- 删除age字段 mysql> ALTER TABLE emp DROP age; Query OK, 0 rows affected (0.42 sec) Records: 0 Duplicates: 0 Warnings: 0 -- 删除主键 mysql> ALTER TABLE emp DROP PRIMARY KEY; Query OK, 0 rows affected (0.62 sec) Records: 0 Duplicates: 0 Warnings: 0 -- 查看表定义 mysql> DESC emp; +----------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+---------------+------+-----+---------+-------+ | ename | varchar(20) | YES | | NULL | | | hiredate | date | YES | | NULL | | | sal | decimal(10,2) | YES | | NULL | | | deptno | int(2) | YES | | NULL | | | id | int(5) | NO | | NULL | | +----------+---------------+------+-----+---------+-------+ 5 rows in set (0.00 sec) 字段改名 🍀 语法 : ALTER TABLE tablename CHANGE [COLUMN] old_col_name new_col_name column_type [FIRST|AFTER col_name]; 实例 : -- 将sal字段修改为salary mysql> ALTER TABLE emp CHANGE sal salary decimal(10,2); Query OK, 0 rows affected (0.09 sec) Records: 0 Duplicates: 0 Warnings: 0 -- 查看表定义 mysql> DESC emp; +----------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+---------------+------+-----+---------+-------+ | ename | varchar(20) | YES | | NULL | | | hiredate | date | YES | | NULL | | | salary | decimal(10,2) | YES | | NULL | | | deptno | int(2) | YES | | NULL | | | id | int(5) | NO | | NULL | | +----------+---------------+------+-----+---------+-------+ 5 rows in set (0.00 sec) PS : change 和 modify都可以修改表类型 , 不同的是change后面需要写两次列名 ; 并且change可以修改列名 , modify则不能 修改字段排列顺序 🍀 前面介绍的字段增加和修改语法(ADD/CHANGE/MODIFY)中, 都有一个可选项 [FIRST|AFTER col_name] ,这个选项可以用来修改字段在表中的位置 , ADD增加的新字段默认是加在表的最后位置 , 而CHANGE/MODIFY默认都不会改变字段的位置 将新增的字段birth date加在ename之后 mysql> DESC emp; +----------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+---------------+------+-----+---------+-------+ | ename | varchar(20) | YES | | NULL | | | hiredate | date | YES | | NULL | | | salary | decimal(10,2) | YES | | NULL | | | deptno | int(2) | YES | | NULL | | | id | int(5) | NO | | NULL | | +----------+---------------+------+-----+---------+-------+ 5 rows in set (0.00 sec) -- 新增birth date字段在ename之后 mysql> ALTER TABLE emp ADD birth date AFTER ename; Query OK, 0 rows affected (0.45 sec) Records: 0 Duplicates: 0 Warnings: 0 -- 查看表定义 mysql> DESC emp; +----------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+---------------+------+-----+---------+-------+ | ename | varchar(20) | YES | | NULL | | | birth | date | YES | | NULL | | | hiredate | date | YES | | NULL | | | salary | decimal(10,2) | YES | | NULL | | | deptno | int(2) | YES | | NULL | | | id | int(5) | NO | | NULL | | +----------+---------------+------+-----+---------+-------+ 6 rows in set (0.00 sec) 修改salary字段 , 将它放在最前面 mysql> ALTER TABLE emp MODIFY salary DECIMAL(10,2) FIRST; Query OK, 0 rows affected (0.55 sec) Records: 0 Duplicates: 0 Warnings: 0 -- 查看表定义 mysql> DESC emp; +----------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+---------------+------+-----+---------+-------+ | salary | decimal(10,2) | YES | | NULL | | | ename | varchar(20) | YES | | NULL | | | birth | date | YES | | NULL | | | hiredate | date | YES | | NULL | | | deptno | int(2) | YES | | NULL | | | id | int(5) | NO | | NULL | | +----------+---------------+------+-----+---------+-------+ 6 rows in set (0.00 sec) PS : CHANGE/FIRST|AFTER COLUMN 这些关键字都属于MySQL在标准SQL上的扩展 , 在其他数据库上不一定适用 更改表名 🍀 语法 : ALTER TABLE tablename RENAME [TO] new_tablename; 实例 : mysql> ALTER TABLE emp RENAME emp1; Query OK, 0 rows affected (0.16 sec) -- 表明改变 mysql> DESC emp1; +----------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+---------------+------+-----+---------+-------+ | salary | decimal(10,2) | YES | | NULL | | | ename | varchar(20) | YES | | NULL | | | birth | date | YES | | NULL | | | hiredate | date | YES | | NULL | | | deptno | int(2) | YES | | NULL | | | id | int(5) | NO | | NULL | | +----------+---------------+------+-----+---------+-------+ 6 rows in set (0.00 sec) 默认值 🍀 语法 : -- 修改默认值 ALTER TABLE tablename ALTER field_name SET DEFAULT v; -- 删除默认值 ALTER TABLE tablename ALTER field_name DROP DEFAULT; 实例 : -- 将salary字段默认值修改为2000 mysql> ALTER TABLE emp1 ALTER salary SET DEFAULT 2000; Query OK, 0 rows affected (0.12 sec) Records: 0 Duplicates: 0 Warnings: 0 -- 查看表定义 mysql> DESC emp1; +----------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+---------------+------+-----+---------+-------+ | salary | decimal(10,2) | YES | | 2000.00 | | | ename | varchar(20) | YES | | NULL | | | birth | date | YES | | NULL | | | hiredate | date | YES | | NULL | | | deptno | int(2) | YES | | NULL | | | id | int(5) | NO | | NULL | | +----------+---------------+------+-----+---------+-------+ 6 rows in set (0.00 sec) -- 删除salary的默认值 mysql> ALTER TABLE emp1 ALTER salary DROP DEFAULT; Query OK, 0 rows affected (0.09 sec) Records: 0 Duplicates: 0 Warnings: 0 -- 查看表定义 mysql> DESC emp1; +----------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+---------------+------+-----+---------+-------+ | salary | decimal(10,2) | YES | | NULL | | | ename | varchar(20) | YES | | NULL | | | birth | date | YES | | NULL | | | hiredate | date | YES | | NULL | | | deptno | int(2) | YES | | NULL | | | id | int(5) | NO | | NULL | | +----------+---------------+------+-----+---------+-------+ 6 rows in set (0.00 sec) 语句合集 🍀 创建表 /* 是否可空 */ CREATE TABLE tablename( column_name type NULL, /* 列名,类型,可为空 */ column_name type NOT NULL /* 列名,类型,不可为空 */ )ENGINE=InnoDB DEFAULT CHARSET=utf8 /* 默认值 */ CREATE TABLE tablename( column_name type DEFAULT NULL , /* 列名,类型,默认为空 */ 　column_name type NOT NULL DEFAULT 2 /* 列名,类型,默认为空 */ )ENGINE=InnoDB DEFAULT CHARSET=utf8 /* 自增 */ CREATE TABLE tablename( column_name type NOT NULL auto_increment PRIMARY KEY, )ENGINE=InnoDB DEFAULT CHARSET=utf8 或 CREATE TABLE tablename( column_name type NOT NULL auto_increment, INDEX(column_name) )ENGINE=InnoDB DEFAULT CHARSET=utf8 对于自增列,必须是索引(含主键) 对于自增可以设置步长和起始值 SHOW SESSION VARIABLES LIKE 'auto_inc%'; SET SESSION auto_increment_increment=2; SET SESSION auto_increment_offset=10; SHOW GLOBAL VARIABLES LIKE 'auto_inc%'; SET GLOBAL auto_increment_increment=2; SET GLOBAL auto_increment_offset=10; /* 主键 */ 主键,一种特殊的唯一索引,不允许有空值,如果主键使用单个列,则它的值必须唯一,如果是多列,则其组合必须唯一 CREATE TABLE tablename( column_name type NOT NULL auto_increment PRIMARY KEY, ) 或 CREATE TABLE tablename( column_name type NOT NULL, PRIMARY KEY(column_name) ) /* 外键 */ CREATE TABLE tablename( CONSTRAINT symbol FOREIGN KEY (id) REFERENCES table_child(id); ) 删除表 DROP TABLE tablename; 清空表 -- 如果清空的表又自增列,那么在清空之后会继续上次自增的值继续自增 DELETE FROM tablename; -- 如果清空的表又自增列,那么在清空之后再次添加数据自增的值会从新开始计算 TRUNCATE TABLE tablename; 修改表 -- 添加列 ALTER TABLE tablename ADD column_name column_type -- 删除列 ALTER TABLE tablename DROP COLUMN column_name -- 修改列 ALTER TABLE tablename MODIFY COLUMN column_name column_type; -- 修改类型 ALTER TABLE tablename CHANGE originalname newname column_type; -- 修改列名与类型 -- 添加主键 ALTER TABLE tablename ADD PRIMARY KEY(columnname); -- 删除主键 ALTER TABLE tablename DROP PRIMARY KEY; ALTER TABLE tablename MODIFY columnname INT, DROP PRIMARY KEY; -- 添加外键 ALTER TABLE slave_table ADD CONSTRAINT symbol(ex:FK_slave_primary) FOREIGN KEY slave_table(foreign_key_field) REFERENCES primary_table(primary_field); -- 删除外键 ALTER TABLE tablename DROP FOREIGN KEY foreign_key_field -- 修改默认值 ALTER TABLE testalter_tbl ALTER i SET DEFAULT 1000; -- 删除默认值 ALTER TABLE testalter_tbl ALTER i DROP DEFAULT; "},"03-MySQL/05-MySQL - 数据操作.html":{"url":"03-MySQL/05-MySQL - 数据操作.html","title":"MySQL - 数据操作","keywords":"","body":"MySQL - 数据操作 介绍 🍀 DML操作是指对数据库中表记录的操作 , 即数据操作 主要包括表记录的插入(insert) , 更新(update) , 删除(delete) 和查询(select) , 是开发人员日常使用最频繁的操作 插入记录 🍀 语法 : INSERT INTO tablename(field1,field2,...,fieldn) VALUES(value1,value2,...,valuen); 实例 : mysql> DESC emp; +----------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+---------------+------+-----+---------+-------+ | ename | varchar(10) | YES | | NULL | | | hiredate | date | YES | | NULL | | | sal | decimal(10,2) | YES | | NULL | | | deptno | int(2) | YES | | NULL | | +----------+---------------+------+-----+---------+-------+ 4 rows in set (0.00 sec) -- 插入数据,values中的顺序需排列一致 mysql> INSERT INTO emp(ename,hiredate,sal,deptno) VALUES('lyon','2000-01-01','2000',1); Query OK, 1 row affected (0.08 sec) -- 插入第二条数据 mysql> INSERT INTO emp(ename,hiredate,sal,deptno) VALUES('kenneth','2000-01-01','1000',2); Query OK, 1 row affected (0.10 sec) -- 没写的字段自动设置为NULL,默认值,自增的下一个数字 mysql> INSERT INTO emp(ename,sal) VALUES('alex',500); Query OK, 1 row affected (0.24 sec) -- 查看emp表中的所有记录 mysql> SELECT * FROM emp; +---------+------------+---------+--------+ | ename | hiredate | sal | deptno | +---------+------------+---------+--------+ | lyon | 2000-01-01 | 2000.00 | 1 | | kenneth | 2000-01-01 | 1000.00 | 2 | | alex | NULL | 500.00 | NULL | +---------+------------+---------+--------+ 3 rows in set (0.00 sec) 一次性插入多条 : INSERT INTO tablename(field1,field2,...,fieldn) VALUES (value1,value2,...,valuen), -- 以逗号分隔 (value1,value2,...,valuen), (value1,value2,...,valuen), (value1,value2,...,valuen); 更新记录 🍀 语法 : UPDATE tablename SET field1=value1,field2=value2,...,fieldn=valuen [WHERE CONDITION]; 实例 : mysql> UPDATE emp SET sal=100000 WHERE ename='lyon'; Query OK, 1 row affected (0.10 sec) Rows matched: 1 Changed: 1 Warnings: 0 同时更新多个表中数据 UPDATE t1,t2,...,tn SET t1.field1=expr1,tn.fieldn=exprn [WHERE CONDITION]; 删除记录 🍀 语法 : DELETE FROM tablename [WHERE CONDITION]; 实例 : mysql> DELETE FROM emp WHERE ename='alex'; Query OK, 1 row affected (0.06 sec) 一次删除多个表的数据 DELETE t1,t2,...,tn FROM t1,t2,...,tn [WHERE CONDITION]; 实例 : -- 查询记录 mysql> SELECT * FROM dept; +--------+----------+ | deptno | deptname | +--------+----------+ | 1 | tech | | 2 | sale | | 3 | hr | +--------+----------+ 3 rows in set (0.00 sec) -- 查询记录 mysql> SELECT * FROM emp; +---------+------------+-----------+--------+ | ename | hiredate | sal | deptno | +---------+------------+-----------+--------+ | lyon | 2000-01-01 | 100000.00 | 1 | | kenneth | 2000-01-01 | 1000.00 | 2 | +---------+------------+-----------+--------+ 2 rows in set (0.00 sec) -- 删除两个表中deptno为2的记录 mysql> DELETE a,b FROM emp a,dept b WHERE a.deptno=b.deptno and a.deptno=2; Query OK, 2 rows affected (0.11 sec) -- 查询记录 mysql> SELECT * FROM emp; +-------+------------+-----------+--------+ | ename | hiredate | sal | deptno | +-------+------------+-----------+--------+ | lyon | 2000-01-01 | 100000.00 | 1 | +-------+------------+-----------+--------+ 1 row in set (0.00 sec) -- 查询记录 mysql> SELECT * FROM dept; +--------+----------+ | deptno | deptname | +--------+----------+ | 1 | tech | | 3 | hr | +--------+----------+ 2 rows in set (0.00 sec) PS : 不管是单表还是多表 , 不加where条件会把表的所有记录删除 , 所以操作时一定要小心 查询记录 🍀 语法 : SELECT * FROM tablename [WHERE CONDITION]; \"*\"表示要将所有的记录都选出来 , 也可以用逗号分割所有的字段来代替 如上面例子中 mysql> SELECT ename,hiredate,sal,deptno FROM emp; 查询不重复的记录 🍀 mysql> SELECT DISTINCT deptno FROM emp; +--------+ | deptno | +--------+ | 1 | +--------+ 1 row in set (0.00 sec) 条件查询 🍀 mysql> SELECT * FROM emp WHERE deptno=1; +-------+------------+-----------+--------+ | ename | hiredate | sal | deptno | +-------+------------+-----------+--------+ | lyon | 2000-01-01 | 100000.00 | 1 | +-------+------------+-----------+--------+ 1 row in set (0.00 sec) 排序和限制 🍀 -- 按工资高低进行显示 mysql> SELECT * FROM emp order by sal; +---------+------------+-----------+--------+ | ename | hiredate | sal | deptno | +---------+------------+-----------+--------+ | kenneth | 2000-01-01 | 5000.00 | 2 | | alex | 2000-01-01 | 8000.00 | 3 | | lyon | 2000-01-01 | 100000.00 | 1 | +---------+------------+-----------+--------+ 3 rows in set (0.00 sec) 对于排序后的记录 , 如果希望只显示一部分 , 可以使用LIMIT关键字来实现 SELECT ...[LIMIT offset_start,row_count] -- offset_start表示记录的起始偏移量,默认为0;row_count表示显示的行数 实例 mysql> SELECT * FROM emp ORDER BY sal LIMIT 2; +---------+------------+---------+--------+ | ename | hiredate | sal | deptno | +---------+------------+---------+--------+ | kenneth | 2000-01-01 | 5000.00 | 2 | | alex | 2000-01-01 | 8000.00 | 3 | +---------+------------+---------+--------+ 2 rows in set (0.00 sec) PS : limit属于MySQL扩展SQL92后的语法 , 在其他数据库上并不能通用 聚合 🍀 用于进行汇总操作 语法 : SELECT [field1,field2,...,fieldn] fun_name FROM tablename [WHERE where_condition] [GROUP BY field1,dield2,,...,fieldn [WITH ROLLUP]] [HAVING where_contition] /* 参数说明 */ fun_name 表示要做的聚合操作,也就是聚合函数,常用的有 sum(求和),count(*)(记录表),max(最大值),min(最小值) GROUP BY 关键字表示要进行分类聚合的字段,比如要按照部门分类统计员工数量,部门就应该写在 group by后面 WITH ROLLUP 是可选语法,表明是否对分类聚合后的结果进行再汇总 HAVING 关键字表示对分类后的结果再进行条件的过滤 -- 注意 : having 和 where的区别在于,having是对聚合后的结果进行条件的过滤,而 where是在聚合前就对记录进行过滤,所以我们应该尽可能用 where先过滤记录,使结果集减小,会对聚合的效率大大提高 实例 : /* 统计emp表中公司的总人数 */ mysql> SELECT count(1) FROM emp; +----------+ | count(1) | +----------+ | 5 | +----------+ 1 row in set (0.00 sec) /* 统计个部门的总人数 */ mysql> SELECT count(1) FROM emp GROUP BY deptno; +----------+ | count(1) | +----------+ | 2 | | 2 | | 1 | +----------+ 3 rows in set (0.00 sec) /* 既统计各部门人数,又统计总人数 */ mysql> SELECT deptno,count(1) FROM emp GROUP BY deptno WITH ROLLUP; +--------+----------+ | deptno | count(1) | +--------+----------+ | 1 | 2 | | 2 | 2 | | 3 | 1 | | NULL | 5 | +--------+----------+ 4 rows in set (0.00 sec) /* 统计人数大于1的部门 */ mysql> SELECT deptno,count(1) FROM emp GROUP BY deptno HAVING count(1)>1; +--------+----------+ | deptno | count(1) | +--------+----------+ | 1 | 2 | | 2 | 2 | +--------+----------+ 2 rows in set (0.00 sec) /* 统计公司所有员工的薪水总额,最高和最低薪水 */ mysql> SELECT sum(sal),max(sal),min(sal) FROM emp; +-----------+-----------+----------+ | sum(sal) | max(sal) | min(sal) | +-----------+-----------+----------+ | 113000.00 | 100000.00 | 5000.00 | +-----------+-----------+----------+ 1 row in set (0.00 sec) 表连接 🍀 同时显示多个表中的字段 , 分为内连接和外连接 内连接仅选出两张表中互相匹配的记录 , 外连接会选出其他不匹配的记录 , 我们最常用的是内连接 外连接又分为左连接和右连接 内连接 : mysql> SELECT * FROM emp; +---------+------------+-----------+--------+ | ename | hiredate | sal | deptno | +---------+------------+-----------+--------+ | lyon | 2000-01-01 | 100000.00 | 1 | | kenneth | 2000-01-01 | 5000.00 | 2 | | alex | 2000-01-01 | 8000.00 | 3 | | egon | NULL | NULL | 1 | | eva | NULL | NULL | 2 | +---------+------------+-----------+--------+ 5 rows in set (0.00 sec) mysql> SELECT * FROM dept; +--------+----------+ | deptno | deptname | +--------+----------+ | 1 | tech | | 3 | hr | | 2 | sale | +--------+----------+ 3 rows in set (0.00 sec) mysql> SELECT ename,deptname FROM emp,dept WHERE emp.deptno=dept.deptno; +---------+----------+ | ename | deptname | +---------+----------+ | lyon | tech | | kenneth | sale | | alex | hr | | egon | tech | | eva | sale | +---------+----------+ 5 rows in set (0.00 sec) 外连接 : 根据上述实例 , 语句为 左连接 : SELECT ename,deptname FROM emp LEFT JOIN dept ON emp.deptno=dept.deptno; 右连接 : SELECT ename,deptname FROM emp RIGHT JOIN dept ON emp.deptno=dept.deptno; 子查询 🍀 查询时 , 需要的条件是另一个select语句的结果 mysql> SELECT * FROM emp WHERE deptno in(SELECT deptno FROM dept); +---------+------------+-----------+--------+ | ename | hiredate | sal | deptno | +---------+------------+-----------+--------+ | lyon | 2000-01-01 | 100000.00 | 1 | | kenneth | 2000-01-01 | 5000.00 | 2 | | alex | 2000-01-01 | 8000.00 | 3 | | egon | NULL | NULL | 1 | | eva | NULL | NULL | 2 | +---------+------------+-----------+--------+ 5 rows in set (0.00 sec) 如果子查询记录数唯一 , 可以用 = 代替 in , 即SELECT * FROM emp WHERE deptno = (SELECT deptno FROM dept); 某些情况下 , 子查询可以转化为表连接 , 如下 mysql> SELECT * FROM emp WHERE deptno in(SELECT deptno FROM dept); +---------+------------+-----------+--------+ | ename | hiredate | sal | deptno | +---------+------------+-----------+--------+ | lyon | 2000-01-01 | 100000.00 | 1 | | kenneth | 2000-01-01 | 5000.00 | 2 | | alex | 2000-01-01 | 8000.00 | 3 | | egon | NULL | NULL | 1 | | eva | NULL | NULL | 2 | +---------+------------+-----------+--------+ 5 rows in set (0.00 sec) PS : MySQL 4.1 以前的版本不支持子查询 , 需要用表连接来实现子查询 表连接在很多情况下用于优化子查询 记录联合 🍀 union 和union all 关键字可以实现 , 将多个表的数据按照一定的查询条件查询出来后 , 将结果合并到一起显示 语法 : SELECT * FROM t1 UNION\\UNION ALL SELECT * FROM t2 ... UNION\\UNION ALL SELECT * FROM tn; 实例 : mysql> SELECT deptno FROM emp -> UNION -> SELECT deptno FROM dept; +--------+ | deptno | +--------+ | 1 | | 2 | | 3 | +--------+ 3 rows in set (0.00 sec) PS : union all是把结果集直接合并在一起 , 而union是将union all后的结果进行一次distinct , 去除重复记录后的结果 "},"03-MySQL/06-MySQL - 索引.html":{"url":"03-MySQL/06-MySQL - 索引.html","title":"MySQL - 索引","keywords":"","body":"MySQL - 索引 介绍 🍀 索引是数据库中最常用也是最重要的手段之一 , 是数据库中专门用于帮助用户快速查询数据的一种数据结构 , 类似于字典中的目录 , 查找字典内容时可以根据目录查找到数据的存放位置 , 然后直接获取值即可 索引是在MySQL的存储引擎层中实现的 而不是在服务器层实现的 , 所以每种存储引擎的索引都不一定完全相同 , 也不是所有的存储引擎都支持所有的索引类型 三个常用引擎支持的索引类型比较 索引 MyISAM引擎 InnoDB引擎 Memory引擎 B-Tree索引 支持 支持 支持 HASH索引 不支持 不支持 支持 R-Tree索引 支持 不支持 不支持 Full-text索引 支持 5.6版本后支持 不支持 下面对比较常用的两个索引类型进行说明 B-Tree索引与HASH索引 🍀 B-Tree索引 B-Tree索引是最常见的索引 , 构造类似二叉树 , 所以可以根据键值进行快速的访问 , 通常只需要很少的读操作就可以找到正确的行 , 不过B-Tree中的B不代表二叉树 , 而是代表平衡树 , B-Tree并不是一棵二叉树 B-Tree索引适用于全关键字 , 关键字范围和关键字前缀查询 HASH索引 HASH索引相对简单 , 只有Memory/Heap引擎支持 HASH索引适用于 Key - Value查询 , 通过HASH索引要比通过B-Tree索引查询更迅速 , 但是HASH不适用范围查询 , 例如 : , =这类操作 ; 如果使用Memory/Heap引擎并且where条件中不使用 \"=\" 今夕in个索引列 , 那么不会用到索引 , Memory/Heap引擎只有在 \"=\" 的条件下才会使用索引 MySQL索引管理 🍀 索引的功能就是为了加速查找和约束 , 下面对常用索引进行介绍 查看索引 : SHOW INDEX FROM tablename \\G; 普通索引 🍀 普通索引仅有一个功能 , 就是加速查找 创建方式 方式一 : /* 创建索引 */ CREATE INDEX indexname ON tablename(column_name(length)); -- 注意如过是CHAR,VARCHAR类型,length可以小于字段长度 -- 如果是BLOB和TEXT类型,必须指定length 方式二 : /* 修改表结构 */ ALTER TABLE tablename ADD INDEX indexname (column_name); 方式三 : /* 创建表时直接指定 */ CREATE TABLE mytable( ID INT NOT NULL, name VARCHAR(16) NOT NULL, INDEX [indexname] (name(length)) -- indexname可不写 ); 删除索引 DROP INDEX indexname ON tablename; ALTER TABLE tablename DROP INDEX column_name; 唯一索引 🍀 唯一索引有两个功能 : 加速查找和唯一约束(可含NULL) 与普通索引类似 , 不同的就是 : 索引列的值必须唯一 , 但允许有空值 , 如果是组合索引 , 则列值的组合必须唯一 创建方式 方式一 : /* 创建索引 */ CREATE UNIQUE INDEX indexname ON tablename(column_name(length)); 方式二 : /* 修改表结构 */ ALTER TABLE tablename ADD UNIQUE [indexname] (column_name(length)); 方式三 : /* 创建表时指定 */ CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, UNIQUE [indexname] (username(length)) ); 删除索引 DROP INDEX indexname ON tablename; ALTER TABLE tablename DROP INDEX column_name; 主键索引 🍀 主键有两个功能 : 加速查找和唯一约束(不可NULL) 创建方式 方式一 : /* 修改表结构 */ ALTER TABLE tablename ADD PRIMARY KEY column_name; 方式二 : /* 创建表时指定 */ CREATE TABLE mytable( id INT NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(32) NOT NULL ); -- or CREATE TABLE mytable( id INT NOT NULL AUTO_INCREMENT, name VARCHAR(32) NOT NULL PRIMARY KEY(id) ); 删除主键 ALTER TABLE tablename DROP PRIMARY KEY; ALTER TABLE tablename MODIFY column_name column_type, drop primary key; 　组合索引 🍀 组合索引是将n个列组合成一个索引 , 专门用于组合搜索 , 其效率大于索引合并 应用场景 : 频繁的同时使用n列来进行查询 , 如 : where n1 = 'alex' and n2 = 666 创建索引 /* 创建表 */ CREATE TABLE mytable( nid INT NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(32) NOT NULL, email VARCHAR(64) NOT NULL, extra TEXT ); /* 创建组合索引 */ CREATE INDEX ix_name_email ON mytable(name,email); 如上创建索引后 , 查询 : name and email -- 使用索引 name -- 使用索引 email -- 不使用索引 PS : 对于同时搜索n个条件时 , 组合索引的性能好于多个单一索引合并 全文索引 : 对文本的内容进行分词 , 进行搜索 索引合并 : 使用多个单列索引组合搜索 覆盖索引 : MySQL只需要通过索引就可以返回查询所需要的数据 , 而不必在查到索引之后进行回表操作 , 减少IO , 提供效率 当你对一个sql 使用explain statement 查看一个sql的执行计划时 , 在EXPLAIN的Extra列出现Using Index提示时 , 就说明该select查询使用了覆盖索引 使用索引 🍀 使用索引可以加速查找 , 但是如果以错误的方式使用 , 即使建立索引也不会生效 以\"%\"开头的LIKE查询不走索引 mysql> EXPLAIN SELECT * FROM actor WHERE last_name LIKE '%NI' \\G; -- 不走索引 mysql> EXPLAIN SELECT * FROM actor WHERE last_name LIKE 'NI%' \\G; -- 走索引 数据类型出现隐式转换不走索引 , MySQL默认把输入的常量值进行转换后才进行检索 , 如下last_name列为字符串类型 mysql> EXPLAIN SELECT * FROM actor WHERE last_name = 1 \\G; -- 不走索引,全表扫描 mysql> EXPLAIN SELECT * FROM actor WHERE last_name = '1' \\G; -- 走索引 -- 一定记得在where条件中把字符常量值用引号引起来 组合索引情况下 , 查询条件不包含索引列最左边部分 , 不走索引 mysql> EXPLAIN SELECT * FROM mytable WHERE name='lyon' AND email='myemail' \\G; -- 使用索引 mysql> EXPLAIN SELECT * FROM mytable WHERE email='myemail' AND name='lyon' \\G; -- 不使用索引 -- 最左原则 如果MySQL估计使用索引比全表扫描更慢 , 不走索引 , MySQL 5.6版本中 , 能够通过Trace清晰地看到优化器选择的过程 用or分割开的条件中有未建立索引的列 , 不走索引 mysql> SELECT * FROM tb1 WHERE nid = 1 OR email = 'myemail'; -- 不使用索引,email列未建立 mysql> SELECT * FROM tb1 WHERE nid = 1 OR name = 'myemail'; -- 使用索引,两者都建立 普通索引的\"!=\"和\">\"不走索引 , 特别的走 -- != mysql> SELECT * FORM tb1 WHERE name != 'lyon'; -- 不走索引 mysql> SELECT * FORM tb1 WHERE nid != 123; -- 走索引,nid为主键 -- > mysql> SELECT * FORM tb1 WHERE name > 'lyon'; -- 不走索引 mysql> SELECT * FORM tb1 WHERE nid > 123; -- 走索引,nid为主键或索引是整数类型 排序条件为索引 , 选择的映射如果不是索引 , 则不走索引 mysql> SELECT email FROM tb1 ORDER BY name desc; -- 不走索引 mysql> SELECT * FROM tb1 ORDER BY nid desc; -- 走索引,如果对主键排序,则还是走 注意 : -- 避免使用select * -- count(1)或count(列) 代替 count(*) -- 创建表时尽量时 char 代替 varchar -- 表的字段顺序固定长度的字段优先 -- 组合索引代替多个单列索引（经常使用多个条件查询时） -- 尽量使用短索引 -- 使用连接（JOIN）来代替子查询(Sub-Queries) -- 连表时注意条件类型需一致 -- 索引散列值（重复少）不适合建索引，例：性别不适合 "},"03-MySQL/07-MySQL - 视图.html":{"url":"03-MySQL/07-MySQL - 视图.html","title":"MySQL - 视图","keywords":"","body":"MySQL - 视图 介绍 🍀 视图是一种虚拟存在的表 , 对于使用视图的用户来说基本上是透明的 视图并不在数据库中实际存在 , 行和列数据来自定义视图的查询中使用的表 , 并且是在使用视图时动态生成的 视图相对于普通的表的优势主要包括一下几点 : 简单 : 使用视图的用户完全不需要关心后面对应的表的结构 , 关联条件和筛选条件 , 对用户来说已经是过滤好的复杂条件的结果集 安全 : 使用视图的用户只能访问他们被允许查询的结果集 , 对表的权限管理并不能限制到某个行某个列 , 但是通过视图就可以简单的实现 数据独立 : 一旦视图的结构确定了 , 可以屏蔽表结构变化对用户的影响 , 源表增加列对视图没有影响 ; 源表修改列名 , 则可以通过修改视图来解决 , 不会造成对访问者的影响 视图操作 🍀 视图的操作包括创建或者修改视图 , 删除视图 , 以及查看视图定义 创建视图 🍀 创建视图需要有CREATE VIEW的权限 , 并且对于查询涉及的列有SELECT权限 ; 如果使用CREATE OR REPLACE或者ALTER修改视图 , 那么还需要该视图的DROP权限 语法 : -- 创建视图 CREATE VIWE 视图名称 AS SQL语句 -- 完整 CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED|MERGE|TEMPTABLE}] VIEW view_name [(column_list)] AS select_statement [WITH [CASCADED|LOCAL] CHECK OPTION] 实例 mysql> CREATE VIEW lyon_view AS SELECT * FROM test; Query OK, 0 rows affected (0.08 sec) /* 注意 : MySQL视图的定义有一些限制,例如,在FROM关键字后面不能包含子查询,这和其他数据库是不同的 如果视图是从其他数据库迁移过来的,那么可能需要因此做一些改动,可以将子查询的内容先定义成 一个视图,然后对该视图再创建视图就可以实现类似的功能了 再次注意 : 使用视图后无需每次都重写子查询的sql , 但是这样效率并不高 , 还不如我们写子查询的效率高 一个致命的问题 : 视图是存放在数据库中的 , 如果我们程序中的sql过分依赖于数据库中存放的视图 , 那么意味着 , 一旦sql需要修改且涉及到视图的部分 , 则必须去数据库中进行修改 , 而通常在公司中数据库有专门的DBA负责 , 你要想完成修改 , 必须付出大量的沟通成本DBA可能才会帮你完成修改 , 极其的不方便 修改视图 🍀 语法 : -- 修改视图 ALTER VIEW 视图名称 AS SQL语句 -- 完整如下 ALTER [ALGORITHM = {UNDEFINED|MERGE|TEMPTABLE}] VIEW view_name [(column_list)] AS select_statement [WITH [CASCADED|LOCAL] CHECK OPTION] 实例 mysql> ALTER VIEW lyon_view AS SELECT * FROM test WHERE id > 4; Query OK, 0 rows affected (0.09 sec) 视图的可更新性 对于视图中的数据 , 由于视图是虚拟的 , 所以如果更新数据那么原始表也跟着被更新 , 即视图的可更新性 视图的可更新性和视图中查询的定义有关 , 一下类型的视图是不可更新的 : 包含一下关键字的SQL语句 : 聚合函数(SUM , MIN , MAX , COUNT等) , DISTINCT , GROUP BY , HAVING , UNION 或者 UNION ALL 常量视图 SELECT中包含子查询 JION FROM一个不能更新的视图 WHERE字句的子查询引用了FROM字句中的表 实例 -- 包含聚合函数 mysql> CREATE OR REPLACE VIEW payment_sum AS -> SELECT staff_id,SUM(count) FROM payment GROUP BY staff_id; Query OK, 0 rows affected(0.00 sec) -- 常量视图 mysql> CREATE OR REPLACE VIEW pi AS -> SELECT 3.1415926 AS pi; Query OK, 0 rows affected(0.00 sec) -- select中包含子查询 mysql> CREATE VIEW city_view AS -> SELECT (SELECT city FROM city WHERE city_id = 1); Query OK, 0 rows affected(0.00 sec) [WITH [CASCADED|LOCAL] CHECK OPTION] 该语句决定了是否允许更新数据使记录不再满足视图的条件 , 其中 : LOCAL只要满足本视图的条件就可以更新 CASCADED则必须满足所有针对该视图的所有视图的条件才可以更新 默认为CASCADED 实例 mysql> CREATE OR REPLACE VIEW payment_view AS -> SELECT payment_id,amount FROM payment -> WHERE amount CREATE OR REPLACE VIEW payment_view1 AS -> SELECT payment_id,amount FROM payment_view -> WHERE amount CREATE OR REPLACE VIEW payment_view2 AS -> SELECT payment_id,amount FROM payment_view -> WHERE amount > 5 WITH CASCADED CHECK OPTION; Query OK, 0 rows affected (0.00 sec) mysql> SELECT * FROM payment_view1 limit 1; +------------+--------+ | payment_id | amount | +------------+--------+ | 3 | 5.99 | +------------+--------+ 1 row in set (0.00 sec) mysql> UPDATE payment_view1 SET amount=10 -> WHERE payment_id = 3; Query OK, 1 row affected (0.03 sec) Rows matched: 1 changed: 1 warnings: 0 mysql> UPDATE payment_view2 SET amount=10 -> WHERE payment_id = 3; ERROR 1369 (HY000): CHECK OPTION failed 'sakila.payment_view2' 删除视图 🍀 用户可以一次删除一个或者多个视图 , 前提是必须有该视图的DROP权限 语法 : -- 删除视图 DROP VIEW view_name -- 完整如下 DROP VIEW [IF EXISTS] view_name [,view_name]...[RESTRICT|CASCADE]; 查看视图 🍀 从MySQL 5.1开始 , 使用SHOW TABLES命令的时候不仅显示表的名字 , 同时也会显示视图的名字 , 而不存在单独显示视图的SHOW VIEWS命令 , 如下 : mysql> SHOW TABLES; +----------------------+ | Tables_in_mydatabase | +----------------------+ | ... | | salary_view | | ... | +----------------------+ 11 rows in set (0.00 sec) 同样使用SHOW TABLE STATUS命令时 , 不但可以显示表的信息 , 同时也可以显示视图的信息 SHOW TABLE STATUS LIKE 'view_name' \\G; 查询某个视图的定义 SHOW CREATE VIEW view_name \\G; 通过查看系统表information_schema.views也可以查看视图的相关信息 SELECT * FROM VIEWS WHERE table_name = 'view_name' \\G; "},"03-MySQL/08-MySQL - 存储过程与函数.html":{"url":"03-MySQL/08-MySQL - 存储过程与函数.html","title":"MySQL - 存储过程与函数","keywords":"","body":"MySQL - 存储过程和函数 介绍 🍀 存储过程和函数是实现经过编译并存储在数据库中的一段SQL语句的集合 , 调用存储过程和函数可以简化应用开发人员的很多工作 , 减少数据在数据库和应用服务器之间的传输 , 对于提高数据处理的效率是有好处的 存储过程和函数的区别在于函数必须有返回值 , 而存储过程没有 , 存储过程的参数可以使用IN , OUT , INOUT类型 , 而函数的参数只能是IN类型的 , 如果有函数从其他类型的数据库迁移到MySQL , 那么就可能因此需要将函数改成成存储过程 参数介绍 : IN 仅用于传入参数用 OUT 仅用于返回值用 INOUT 既可以传入又可以当做返回值 在对存储过程或函数进行操作时 , 需要首先确认用户是否具有相应的权限 ' 例如 : 创建存储过程或者函数需要CREATE ROUTINE权限 , 修改或者删除存储过程或者函数需要ALTER ROUTINE权限 , 执行存储过程或者函数需要EXECUTE权限 创建存储过程或函数 🍀 语法 : /* 创建存储过程 */ CREATE PROCEDURE sp_name([proc_parameter[,...]]) [characteristic ...] routine_body /* 创建函数 */ CREATE FUNCTION sp_name([func_parameter[,...]]) RETURNS type [characteristic ...] routine_body -- 参数介绍 proc_parameter:[IN\\ OUT\\ INOUT] param_name type func_parameter:param_name type characteristic: LANGUAGE SQL |[NOT] DETERMINISTIC -- 修改SQL语句的结束符 |{CONTAINS SQL|NO SQL|READS SQL DATA|MODIFIES SQL DATA} |SQL SECURITY{DEFINER|INVOKER} -- SQL语句块 |COMMENT 'string' -- END 结束 实例 -- 修改SQL语句的结束符为$$ mysql> DELIMITER $$ -- 创建存储过程 mysql> CREATE PROCEDURE film_in_stock(IN p_film_id INT,IN p_store_id INT,OUT p_film_count INT) -> READS SQL DATA -> BEGIN -- 开始 -> SELECT inventory_id -- SQL语句块 -> FROM inventory -> WHERE film_id = p_film_id -> AND store_id = p_store_id -> AND inventory_in_stock(inventory_id); -> SELECT FOUND_ROWS() INTO p_film_count; -> END $$ -- 结束 Query OK, 0 rows affected (0.11 sec) -- 把SQL语句的结束符改为 ; mysql> DELIMITER ; characteristic特征值部分说明 特征值 说明 LANGUAGE SQL 说明下面过程的body是使用SQL语言编写 , 系统默认的 , 为今后MySQL会支持的除SQL外的其他语言做准备 [NOT] DETERMINISTIC DETERMINISTIC确定的 , 即每次输入一样输出也一样的程序 , NOT DETERMINISTIC非确定的 , 这个值当前还没有被优化程序使用 {CONTAINS SQL丨NO SQL丨READS SQL DATA丨MODIFIES SQL DATA} CONTAINS SQL表示子程序不包含读或写数据的语句NO SQL 表示子程序不包含SQL语句READS SQL DATA表示子程序包含读数据的语句 , 但不包含写数据的语句MODIFIES SQL DATA表示子程序包含写数据的语句默认值为CONTAINS SQL SQL SECURITY {DEFINER丨INVOKER} 可以用来指定子程序该用创建子程序者的许可来执行 , 还是使用调用者的许可来执行 , 默认为DEFINER(定义者) COMMENT 'string' 存储过程或者函数的注释信息 通过call调用存储过程 CALL sp_name([parameter[,...]]) 事务 修改存储过程或函数 🍀 语法 : ALTER {PRCEDURE|FUNCTION} sp_name [characteristic...] -- characteristic {CONTAINS SQL|NO SQL|READS SQL DATA|MODIFIES SQL DATA} |SQL SECURITY {DEFINER|INVOKER} |COMMENT 'string' 删除存储过程或函数 🍀 一次只能删除一个存储过程或者函数 , 删除过程或者函数需要有该过程或者函数的ALTER ROUTINE权限 语法 : DROP {PROCEDURE|FUNCTION} [IF EXISTS] sp_name 实例 mysql> DROP PROCEDURE film_in_stock; Query OK, 0 rows affected (0.00 sec) 查看存储过程或函数 🍀 查看状态 SHOW PROCEDURE STATUS LIKE 'film_in_stock' \\G; 查看定义 SHOW CREATE {PROCEDURE|FUNCTION} sp_name; 通过查看information_schema.Routines查看 SELECT * FROM ROUTINES WHERE ROUTINE_NAME = 'film_in_stock' \\G; 变量的使用 🍀 存储过程和和函数可以使用变量 , 而且在MySQL 5.1版本中 , 变量不区分大小写 变量的定义 通过DECLARE定义一个局部变量 , 该变量的作用域只能在BEGIN...END块中 , 可以用在嵌套的块中 ; 变量的定义必须卸载复合语句的开头 , 并且在任何其他语句的前面 , 可以一次声明多个相同类型的变量 , DEFAULT可以设置默认值 语法 : DECLARE var_name[,...] type [DEFAULT value]; 实例 DECLARE last_month_start DATE; 变量的赋值 变量可以直接赋值 , 或者通过查询赋值 ; 直接赋值使用SET , 可以赋厂里爱那个或者赋表达式 语法 : SET var_name = expr [,var_name = expr]... 给刚才定义的last_month_start复制 SET last_month_start = DATE_SUB(CURRENT_DATE(),INTERVAL 1 MONTH); 可以通过查询将结果赋给变量 , 但是结果必须只有一行 SELECT col_name [,...] INTO var_name [,...] table_expr; 实例如下 DECLARE v_payments DECIMAL(5,2); -- SUM OF PAYMENTS MADE PREVIOUSLY 条件处理 🍀 用于定义在处理过程中遇到问题时相应的处理步骤 定义条件 DECLARE condition_name CONDITION FOR condition_value condition_value: SQLSTATE [VALUE] sqlstate_value |mysql_error_code 处理条件 DECLARE handler_type HANDLER FOR condition_value [,...] sp_statement handler_type: CONTINUE |EXIT -- 执行终止 |UNDO -- 现在还不支持 condition_value: SQLSTATE [VALUE] sqlstate_value |condition_name |SQLWARNING |NOT FOUND |SQLEXCEPTION |mysql_error_code 实例 /* 事务 */ mysql> DELIMITER $$ mysql> CREATE PROCEDURE p1( -> OUT p_return_code TINYINT -> ) -> BEGIN -> DECLARE EXIT HANDLER FOR SQLEXCEPTION -> BEGIN -- ERROR -> SET p_return_code = 1; -> ROLLBACK; -- 回滚 -> END; -> DECLARE EXIT HANDLER FOR SQLWARNING -> BEGIN -- WARNING -> SET p_return_code = 2; -> ROLLBACK; -> END; -> START TRANSACTION; -- 开始事务 -> DELETE FROM tb1; -> INSERT INTO tb2(name) values('lyon'); -> COMMIT; -> SET p_return_code = 0; -- SUCCESS -> END $$ Query OK, 0 rows affected (0.10 sec) 光标 🍀 在存储过程或函数中 , 可以使用光标对结果集进行循环处理 声明光标 DECLARE curor_name CURSOR FOR select_statement OPEN光标 OPEN cursor_name FETCH光标 FETCH cursor_name INTO var_name [,var_name]... CLOSE光标 CLOSE cursor_name 实例 mysql> DELIMITER $$ mysql> CREATE PROCEDURE p3() -> BEGIN -> DECLARE ssid INT; -- 定义变量 -> DECLARE ssname VARCHAR(50); -> DECLARE done INT DEFAULT FALSE; -> DECLARE my_cursor CURSOR FOR SELECT sid,sname FROM student; -> DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE; -> OPEN my_cursor; -> ins:LOOP -> FETCH my_cursor INTO ssid,ssname; -> IF done THEN -> LEAVE ins; -> END IF; -> INSERT INTO teacher(tname) values(ssname); -> END LOOP ins; -> CLOSE my_cursor; -> END $$ Query OK, 0 rows affected (0.02 sec) 流程控制 🍀 IF语句 IF search_condition THEN statement_list [ELSEIF search_condition THEN statement_list]... [ELSE statement_list] END IF CASE语句 CASE case_value WHEN when_value THEN statement_list [WHEN when_value THEN statement_list]... [ELSE statement_list] END CASE -- or CASE WHEN search_condition THEN statement_list [WHEN search_codition THEN statement_list]... [ELSE statement_list] END CASE LOOP语句 [begin_label:] LOOP statement_list END LOOP [end_label] LEAVE语句 /* 用来从标注流程构造中退出,通常和BEGIN...END或者循环一起使用 */ BEGIN ... ins:LOOP ... LEAVE ins; ... ... END LOOP ins; END; ITERATE语句 /* 相当于continue */ BEGIN ... ins:LOOP IF ... THEN ITERATE ins; END LOOP ins; ... END; REPEAT语句 /* 满足条件退出循环 */ [begin_lable:] REPEAT statement_list UNTIL search_condition END REPEAT [end_late] WHILE语句 /* 满足条件才执行 */ [begin_lable:] WHILE search_condition DO statement_list END WHILE [end_label] 内置函数 🍀 对于一些内置函数 , 就直接看官方的吧 https://dev.mysql.com/doc/refman/5.7/en/functions.html 需要注意的是 , 上述为自定义函数的操作 , 而对于自定义函数 , 在功能块中不要写SQL语句 , 否则会报错 , 函数仅仅只是一个功能 , 一个在SQL中被应用的功能 , 如果要写SQL则应该使用存储过程 "},"03-MySQL/09-MySQL - 触发器.html":{"url":"03-MySQL/09-MySQL - 触发器.html","title":"MySQL - 触发器","keywords":"","body":"MySQL - 触发器 介绍 🍀 触发器是与表有关的数据库对象 , 在满足定义条件时触发 , 并执行触发器中定义的语句集合 创建触发器 🍀 语法 : CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW TRIGGER_stmt /* 触发器只能创建在永久性(Permanent Table)上,不能对临时表(Temporary Table)创建触发器 */ trigger_time: -- 触发器的触发时间 BEFORE|AFTER -- BEFORE:在检查约束前触发 -- AFTER:在检查约束后触发 trigger_event: -- 触发器触发的事件 INSERT|UPDATE|DELETE PS:对于同一个表相同触发时间的相同触发事件,只能定义一个触发器 详细如下 -- 插入前 CREATE TRIGGER tri_before_insert_tb1 BEFORE INSERT ON tb1 FOR EACH ROW BEGIN ... END -- 插入后 CREATE TRIGGER tri_after_insert_tb1 AFTER INSERT ON tb1 FOR EACH ROW BEGIN ... END -- 删除前 CREATE TRIGGER tri_before_delete_tb1 BEFORE DELETE ON tb1 FOR EACH ROW BEGIN ... END -- 删除后 CREATE TRIGGER tri_after_delete_tb1 AFTER DELETE ON tb1 FOR EACH ROW BEGIN ... END -- 更新前 CREATE TRIGGER tri_before_update_tb1 BEFORE UPDATE ON tb1 FOR EACH ROW BEGIN ... END -- 更新后 CREATE TRIGGER tri_after_update_tb1 AFTER UPDATE ON tb1 FOR EACH ROW BEGIN ... END 实例 -- 插入前触发 DELIMITER $$ CREATE TRIGGER tri_before_insert_tb1 BEFORE INSERT ON tb1 FOR EACH ROW BEGIN IF NEW.NAME == 'LYON' THEN INSERT INTO tb2 (NAME) VALUES ('aa') END END $$ 删除触发器 🍀 语法 : DROP TRIGGER [schema_name.]trigger_name -- 如果没有指定schema_name,默认为当前数据库 实例 mysql> DROP TRIGGER ins_film Query OK, 0 rows affected (0.00 sec) 查看触发器 🍀 可通过执行SHOW TRIGGERS \\G; 命令查看触发器的状态 , 语法等信息 , 但是因为不能查询指定的触发器 , 所以每次都返回所有的触发器信息 , 使用不方便 查询系统表information_schema.triggers表 , 该方式可以查询指定触发器的指定信息 DESC TRIGGERS; SELECT * FROM TRIGGERS WHERE trigger_name = '...' \\G; 使用触发器 🍀 触发器的语句有以下两个限制 触发程序不能调用将数据返回客户端的存储程序 , 也不能采用CALL语句的动态SQL语句 , 但是允许存储程序通过参数将数据返回触发程序 , 也就是存储过程或者函数通过OUT或者INOUT类型的参数将数据返回触发器是可以的 , 但是不能调用直接返回数据的过程 不能在触发器中使用以显示或隐式方式开始或结束事务的语句 , 如START TRANSACTION , COMMIT或ROLLBACK 总之触发器无法由用户直接调用 "},"03-MySQL/10-MySQL - 事务.html":{"url":"03-MySQL/10-MySQL - 事务.html","title":"MySQL - 事务","keywords":"","body":"MySQL - 事务 介绍 🍀 事务就是满足 ACID 特性的一组操作 在 MySQL 中 , 可以用 START TRANSACTION 或 BEGIN 开启一个事务 , 然后使用 COMMIT 提交事务将修改的数据持久保留 , 或者使用 ROLLBACK 撤销所有的修改 在 MySQL 中默认自动提交 (Autocommit) , 如果需要通过明确的 COMMIT 和 ROLLBACK 来提交和回滚事务 , 那么就需要通过明确的事务控制命令来开始事务 ACID 🍀 原子性 (Atomicity) 一个事务必须被视为一个不可分割的最小工作单元 , 整个事务中的所有操作要么全部提交成功 , 要么全部失败回滚 , 对于一个事务来说 , 不可能只执行其中的一部分操作 一致性 (Consistency) 数据库在事务执行前后都保持一致性状态 , 在一致性状态下 , 所有事务对同一个数据的读取结果都是相同的 隔离性 (Isolation) 一个事务所做的修改在最终提交以前 , 对其他事务是不可见的 持久性 (Durability) 一旦事务提交 , 则其所做的修改就会永久保存到数据库中 , 此时即使系统崩溃 , 修改的数据也不会丢失 并发问题 🍀 在没有并发的情况下 , 事务串行执行 , 隔离性一定能够满足 , 但是在并发的情况下 , 多个事务并行执行 , 事务的隔离性就无法得到保证 丢失修改 丢失修改是指一个事务的更新操作被另外一个事务的更新操作覆盖 例如 : T1 和 T2 两事务同时获取了一个数据 , T1 先修改并提交生效 , 随后 T2 又修改并提交 , 这个时候 T1 的修改就会被 T2 覆盖掉 读脏数据 读脏数据是指在不同的事务下 , 一个事务可以读到另一个事务未提交的数据 例如 : T1 和 T2 两个事务 , T1 先修改某个数据并提交生效 , 随后 T2 获取该数据 , 但是这个时候 T1 由于某些原因把刚才的修改撤销了 , 也就是回滚了 , 这个时候 T2 获取的数据就是脏数据 不可重复读 不可重复读是指一个事务中多次读取同一个数据 , 由于中途被其他事务修改了 , 导致读取的结果不一致 例如 : T1 和 T2 两个事务 , T1 先读取了一个数据 , 这时 T2 将这个数据进行修改 , 之后 T1 又读取这个数据 , 发现两次读取操作获取的结果不一致 , 注意 T1 的两次读取是一组操作 幻影读 幻影读本质上也属于不可重复读 , 它也是一个事务中多次读取数据 , 但是由于中途被其他事务新增或者删除了 , 导致读取的条数不一致 例如 : T1 和 T2 两个事务 , T1 读取某个范围的数据 , T2 在这个范围内新增或者删除了某条数据 , 这个时候 T1 再次读取这个范围的数据 , 这个时候读取的结果和第一次读取的结果不一致 产生并发不一致问题的主要原因是破坏了事务之间的隔离性 , 要解决这个问题就是要通过并发控制来保证隔离性 , 使一个事务的执行不受其他事务的干扰 , 从而避免造成过数据的不一致性 , 而实现并发控制的一个非常重要的技术就是封锁 封锁 🍀 封锁粒度 在 MySQL 中 , 不同的存储引擎 , 提供的封锁粒度是不一样的 , 各个存储引擎对封锁粒度的支持如下 : 存储引擎 MyISAM InnoDB MEMORY MERGE NDB 封锁粒度 表级锁 行级锁 表级锁 表级锁 行级锁 除了表级锁和行级锁之外 , 在早期 MySQL 5.1 之前 , 还有 BDB 存储引擎 , 支持页面锁 , 这三种锁的特性如下 : 表级锁 : 开销小 , 加锁快 ; 不会出现死锁 ; 锁定粒度大 , 发生锁冲突的概率最高 , 并发度最低 行级锁 : 开销大 , 加锁慢 ; 会出现死锁 ; 锁定粒度最小 , 发生锁冲突的概率最低 , 并发度也最高 页面锁 : 开销和加锁时间介于表锁和行锁之间 ; 会出现死锁 ; 锁定粒度介于表锁和行锁之间 , 并发度一般 从上述特点可见 , 很难笼统地说哪种锁更好 , 在选择封锁粒度时 , 需要在开销和并发度之间做一个权衡 封锁类型 基本封锁类型有两种 , 共享锁 (Shared) , 简写为 S 锁 , 又称读锁 ; 排他锁 (Exclusive) 也可以叫互斥锁 , 简写为 X 锁 , 又称写锁 共享锁 : 一个事务对数据对象 A 加了 S 锁 , 可以对 A 进行读取操作 , 但是不能进行更新操作 , 加锁期间其它事务能对 A 加 S 锁 , 但是不能加 X 锁 排他锁 : 一个事务对数据对象 A 加了 X 锁 , 就可以对 A 进行读取和更新操作 , 加锁期间其它事务不能对 A 加任何锁 除了共享锁和排他锁之外 , 为了允许行锁和表锁共存 , 实现多粒度锁机制 , 还有意向锁 (Intention Locks) 意向共享锁 (IS) : 一个事务在获取某个数据行对象的 S 锁之前 , 必须 先获得表的 IS 锁或者更强的锁 意向排他锁 (IX) : 一个事务在获取某个数据行对象的 X 锁之前 , 必须先获得表的 IX 锁 意向锁兼容关系如下 : 锁 X IX S IS X 冲突 冲突 冲突 冲突 IX 冲突 兼容 冲突 兼容 S 冲突 冲突 兼容 兼容 IS 冲突 兼容 兼容 兼容 解释如下 : 任意 IS/IX 锁之间都是兼容的 , 因为它们只表示想要对表加锁 , 而不是真正加锁 这里的兼容关系针对的表级锁 , 而表锁的 IX 锁和行级的 X 锁兼容 , 两个事务可以对两个数据行加 X 锁 封锁协议 在运用锁对数据对象加锁时 , 还需要约定一些规则 , 例如应何时申请X锁或S锁 , 持锁时间 , 何时释放等 , 这就是封锁协议 (Locking Protocol) 一级封锁协议 事务 T 在修改数据 A 之前必须先对其加 X 锁 , 直到事务结束才释放 一级封锁协议可防止丢失修改 , 因为不能同时有两个事务对同一个数据进行修改 , 所以事务的修改不会被覆盖 二级封锁协议 在一级封锁协议的基础上 , 要求读取数据 A 之前必须先对其加 S 锁 , 读完后即可释放 S 锁 二级封锁协议除了防止丢失修改 , 还可以防止读脏数据 , 因为根据一级封锁协议 , 在修改数据时 , 会加 X 锁 , 这样就无法再加 S 锁了 , 也就不会再次读取数据 三级封锁协议 在一级封锁协议的基础上 , 要求读取数据 A 之前必须对其加 S 锁 , 直到事务结束才能释放 S 锁 三级封锁协议除了防止丢失修改和读脏数据之外 , 还进一步防止了不可重复读 两阶段锁定协议 加锁和解锁分为两个阶段进行 , 在事务执行过程追踪 , 随时都可以执行锁定 , 锁只有在执行 COMMIT 或者 ROLLBACK 的时候才会释放 , 并且所有的锁是在同一时刻释放 , InnoDB 采用的正式这种协议 隐式和显式锁定 InnoDB 会根据隔离级别在需要的时候自动加锁 (隐式) , 同时也支持通过特定的语句进行显式锁定 SELECT ... LOCK IN SHARE MODE; SELECT .. FOR UPDATE; 隔离级别 🍀 并发控制可以通过封锁来实现 , 但是封锁需要用户自己控制如何加锁 , 以及加锁和解锁的时机 , 相当复杂 , 所以绝大多数数据库以及开发工具都提供了事务的隔离级别 , 让用户以一种更轻松的方式处理并发一致性问题 未提交读 (READ UNCOMMITTED) 事务中的修改，即使没有提交，对其它事务也是可见的 , 也就意味着会出现脏读现象 这个级别会导致很多问题 , 从性能上来说 , READ UNCOMMITTED 不会比其他的级别好太多 , 但却缺乏其他级别的很多好处 , 除非真的有非常必要的理由 , 在实际应用中一般很少使用 提交读 (READ COMMITTED) 一个事务只能读取已经提交的事务所做的修改 , 换句话说 , 一个事务从开始直到提交之前 , 所做的任何修改对其他事务都是不可见的 大多数数据库系统的默认隔离级别都是 READ COMMITTED 可重复读 (REPEATABLE READ) 保证在同一个事务中多次读取同一数据的结果是一致的 但是理论上 , 可重复读隔离级别还是无法解决幻读问题 , 可重复读是 MySQL 的默认事务隔离级别 可串行化 (SERIALIZABLE) 强制事务串行执行 , 这样多个事务互不干扰，不会出现并发一致性问题 该隔离级别需要加锁实现 , 因为要使用加锁机制保证同一时间只有一个事务执行 , 也就是保证事务串行执行 SERIALIZABLE 会在读取的每一行数据上都加锁 , 所以可能导致大量的超时和锁争用的问题 , 实际应用中很少用到这个隔离级别 , 只有在非常需要确保数据的一致性而且可以接受没有并发的情况下 , 才考虑采用该级别 这四个隔离级别对比如下 : 隔离级别 读数据一致性 脏读 不可重复读 幻影读 未提交读 最低级别 , 只能保证不读取物理上损坏的数据 是 是 是 已提交读 语句级 否 是 是 可重复读 事务级 否 否 是 可序列化 最高级别 , 事务级 否 否 否 MySQL中使用事务 语法 : START TRANSACTION|BEGIN[WORK] COMMIT [WORK] [AND [NO] CHAIN] [[NO] RELEASE] ROLLBACK [WORK] [AND [NO] CHAIN] [[NO] RELEASE] SET AUTOCOMMIT = {0/1} -- START TRANSACTION 或 BEGIN 语句可以开始一项新的事务 -- COMMIT 和 ROLLBACK 用来提交或者回滚事务 -- CHAIN 和 RELEASE子句分别用来定义在事务提交或者回滚之后的操作,CHAIN会立即启动一个新事务,宾且和刚才的事务具有相同的隔离级别,RELEASE则会断开和客户端的连接 -- SET AUTOCOMMIT可以修改当前连接的提交方式,如果设置了 SET AUTOCOMMIT=0,则设置之后的所有事务都需要通过明确的命令进行提交或者回滚 实例 mysql> DELIMITER $$ mysql> CREATE PROCEDURE p1( -> OUT p_return_code TINYINT -> ) -> BEGIN -> DECLARE EXIT HANDLER FOR SQLEXCEPTION -> BEGIN -- ERROR -> SET p_return_code = 1; -> ROLLBACK; -- 回滚 -> END; -> DECLARE EXIT HANDLER FOR SQLWARNING -> BEGIN -- WARNING -> SET p_return_code = 2; -> ROLLBACK; -> END; -> START TRANSACTION; -- 开始事务 -> DELETE FROM tb1; -> INSERT INTO tb2(name) values('lyon'); -> COMMIT; -> SET p_return_code = 0; -- SUCCESS -> END $$ Query OK, 0 rows affected (0.10 sec) -- 调用 SET @i = 0; CALL p1(@i); SELECT @i; "},"03-MySQL/11-MySQL - SQL注入.html":{"url":"03-MySQL/11-MySQL - SQL注入.html","title":"MySQL - SQL注入","keywords":"","body":"MySQL - SQL注入 介绍 🍀 SQL注入(SQL Injection)就是利用某些数据的外部接口将用户数据插入到实际的数据库操作语言(SQL)当中 , 从而达到入侵数据库乃至操作系统的目的 , 它的产生主要是由于程序对用户输入的数据没有进行严格的过滤 , 导致非法数据库查询语句的执行 SQL注入攻击具有很大的危害 , 攻击者可以利用它读取 , 修改或者删除数据库内的数据 , 获取数据库中的用户名和密码等敏感信息 , 甚至可以获得数据库管理原的权限 , 而且 , SQL注入也很难防范 , 网站管理员无法通过安装系统补丁或者进行简单的安全配置进行自我保护 , 一般的防火墙也无法拦截SQl注入攻击 SQL注入实例(PHP程序实例) 创建用户表user CREATE TABLE user( userid INT(11) NOT NULL AUTO_INCREMENT, username VARCHAR(20) NOT NULL DEFAULT '', password VARCHAR(20) NOT NULL DEFAULT '', PRIMARY KEY(userid) )TYPE=MyISAM AUTO_INCREMENT=3; 添加一条用户记录 INSERT INTO 'user' VALUES(1,'angel','mypass'); 验证用户root登录localhost服务器 SQL Query:$sql\"; ?> 提交URL http://127.0.0.1/injection/user.php?username=angel' or '1=1 结果发现 , 这个URL可以成功登录系统 , 显然这并不是我们预期的结果 ; 溶氧也可以利用SQL的注释语句实现SQL注入 , 如下 : http://127.0.0.1/injection/user.php?username=angel'/* http://127.0.0.1/injection/user.php?username=angel'# 因为在SQL语句中 , \"/*\" 或者 \"#\" 都可以将后面的语句注释掉 这样上述语句就可以通过这两个注释符中的任意一个将后面的语句给注释掉 结果导致只根据用户名而没有密码的URL都成功进行了登录 \"or\" 是利用逻辑运算 , 注释符是根据MySQL的特性 , 这个比逻辑运算简单多了 , 两者都实现了SQL注入效果 应对措施 🍀 对于SQL注入隐患 , 后果可想而知 , 轻则获得数据信息 , 重则可以将数据进行非法更改 , 一下则是常用的防范方法 PrepareStatemen + Bind - Variable 🍀 MySQL服务器端并不存在共享池的概念 , 所以在MySQL上使用绑定变量(Bind Variable) , 最大的好处主要是为了避免SQL注入 , 增加安全性 即使用PrepareStatement语句(如Java)来实现 , 输入的参数中的单引号会被正常转义 , 导致后续的\"or 1=1\" 作为username条件内容出现 , 而不会作为SQL的一个单独条件被解析 , 避免了SQL注入的风险 ; 同样的 , 在使用绑定变量的情况下 , 企图通过注释 \"/*\" 或 \"#\" 让后续条件失效也是会失败的 需要注意 , PrepareStatement语句是由JDBC驱动来支持的 , 他仅仅做了简单的替换和转义 , 斌不是MySQL提供了PreparedStatement的特性 使用应用程序提供的转换函数 🍀 很多应用程序接口都提供了对特殊字符进行转换的函数 , 恰当地使用这些函数 , 可以防止应用程序用户输入使应用程序生成不期望的语句 MySQL C API : 使用mysql_real_escape_string() API调用 MySQL++ : 使用escape 和quote修饰符 PHP : 使用mysql_real_escape_string()函数 Perl DBI : 使用placeholders 或者quote()方法 Ruby DBI : 使用paceholders 或者quote()方法 自己定义函数进行检验 🍀 如果现有的转换函数任然不能满足要求 , 则需要自己编写函数进行输入校验 目前最好的解决方法就是 , 对用用户提交或者可能改变的数据进行简单分类 , 分别应用正则表达式来对用户提供的输入数据进行严格的检测和验证 Python中的pymysql模块 🍀 通过Python的pymysql模块来进行SQL的执行 , 在pymysql模块内部会自动把 \" ' \"(单引号做一个特殊的处理 , 来预防上述的错误) ...... effect_row = cursor.execute(\"select username from user_info where username='%s' and password = '%s'\", (username, pwd)) ...... "},"03-MySQL/SQL优化.html":{"url":"03-MySQL/SQL优化.html","title":"SQL优化","keywords":"","body":"SQL优化 一般步骤 MySQL 客户端连接成功后 , 通过 mysql> show [session|global]status 可以提供服务器状态信息 , 也可以在操作系统上使用 mysqladmin extended-status 命令获得这些消息 对于 session 与 global 参数 : session : 显示当前连接的统计结果 , 默认使用session global : 显示自数据库上次启动至今的统计结果 示例 mysql> show status like 'Com_%'; +-----------------------------+-------+ | Variable_name | Value | +-----------------------------+-------+ | Com_admin_commands | 0 | | Com_assign_to_keycache | 0 | | Com_alter_db | 0 | | Com_alter_db_upgrade | 0 | | Com_alter_event | 0 | | Com_alter_function | 0 | | Com_alter_instance | 0 | | Com_alter_procedure | 0 | | Com_alter_server | 0 | | Com_alter_table | 0 | ... Com_xxx 表示每个 xxx 语句执行的次数 , 我们通常比较关心以下几个统计参数 : Com_select : 执行 SELECT 操作的次数 , 一次查询只累加 1 Com_insert : 执行 INSERT 操作的次数 , 对于批量插入的 INSERT 操作 , 只累加一次 Com_update : 执行 UPDATE 操作的次数 Com_delete : 执行 DELETE 操作的次数 以上 4 个参数对于所有存储引擎的表操作都会进行累加 , 下面这几个参数只是针对 InnoDB 存储引擎的 , 累加的算法也略有不同 : Innodb_rows_read : SELECT 查询返回的行数 Innodb_rows_inserted : 执行 INSERT 操作插入的行数 Innodb_rows_upddated : 执行 UPDATE 操作更新的行数 Innodb_rows_deleted : 执行 DELETE 操作删除的行数 通过以上几个参数 , 可以容易地了解当前数据库的应用是以插入更新为主还是以查询操作为主 , 以及各种类型的 SQL 大致的执行比例是多少 ; 对于更新操作的技术 , 是对执行次数的技术 , 不论提交还是回滚都会进行累加 对于事务型的应用 , 通过 Com_commit 和 Com_rollback 可以了解事务提交和回滚的情况 , 对于回滚操作非常频繁的数据 , 可能意味着应用编写存在问题 , 此外还有几个参数便于用户了解数据库的基本情况 : Connections : 视图连接 MySQL 服务器的次数 Uptime : 服务器工作时间 Slow_queries : 慢查询的次数 定位低效SQL 有两种方式定位执行效率较低的 SQL : 通过慢查询日志定位那些执行效率较低的 SQL 语句 , 用 --log-slow-queries[= file_name] 选项启动时 , mysqld 写一个包含所有执行时间超过 long_query_time 秒的 SQL 语句的日志文件 慢查询日志在查询结束以后才记录 , 所以在应用反映执行效率出现问题的时候查询慢查询日志并不能定位问题 , 可以使用 show processlist 命令查看当前 MySQL 在进行的线程 , 包括线程的状态 , 是否锁表等 , 可以实时地查看 SQL 的执行情况 , 同时对一些锁表操作进行优化 EXPLAIN分析低效SQL 通过上一步定位效率低的 SQL 语句后 , 可以通过 EXPLAIN 或者DESC 命令获取 MySQL 如何执行 SELECT 语句的信息 , 包括在 SELECT 语句执行过程中表如何连接和连接的顺序 , 比如想统计某个 email 为租赁电影拷贝所支付的总金额 , 需要关联客户表 customer 和付款表 payment , 并且对付款金额 amount 字段做求和 (sum) 操作 , 相应的 SQL 执行计划如下 : mysql>explain select sum(amount) from custoer a, payment b where l=l and a.customer_id = b.customer_id and email = 'lyon.yang@qq.com'\\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: a type: ALL possible_keys: PRIMARY key: NULL key_len: NULL ref: NULL rows: 583 Extra: Using where *************************** 2. row *************************** id: 1 select_type: SIMPLE table: b type: ref possible_keys: idx_fk_customer_id key: idx_fk_customer_id key_len: 2 ref: sakila.a.customer_id rows: 12 Extra: 2 rows in set (0.00 sec) id 查询标识符 select_type SELECT类型 , 类型如下表 值 说明 SIMPLE 简单的查询 , 没有 UNION 或者子查询 PRIMARY 带有子查询的最外层查询 UNION DEPENDENT UNION UNION RESULT SUBQUERY DEPENDENT SUBQUERY DERIVED DEPENDENT DERIVED MATERIALLZED UNCACHEABLE SUBQUERY UNCACHEABLE UNION 表示 SELECT 的类型 , 常见的取值有 SIMPLE (简单表 , 即不使用表连接或者子查询) , PRIMARY (主查询 , 即外层的查询) , UNION (UNION 中的第二个或者后面的查询语句) , SUBQUERY (子查询中的第一个 SELECT ) 等 table 输出结果的表 , 也可以是以下值之一 : : 该行是指具有和id值的行 的 M并集 N。 ：该行是指用于与该行的派生表结果id的值 N。派生表可能来自（例如）FROM子句中的子查询 。 ：该行是指该行的物化子查询的结果，其id 值为N。请参见 第8.2.2.2节“通过实现来优化子查询”。 partitions type type : 表示 MySQL 在表中找到所需行的方式 , 或者叫访问类型 , 常见类型有 : # 从左至右,性能由最差到最好 +------+--------+-------+-----+--------+--------------+------+ | ALL | index | range | ref | eq_ref | const,system | NULL | +------+--------+-------+-----+--------+--------------+------+ ALL : 全表扫描 , MySQL 遍历全表来找到匹配的行 # type=ALL explain select * from film where rating > 9\\G; index : 索引全扫描 , MySQL 遍历整个索引来查询匹配的行 # type=index explain select title from film\\G; range : 索引范围扫描 , 常见与 , >= , between 等操作符 # type=range explain select * from payment where customer_id >= 300 and customer_id ref : 使用非唯一索引扫描或唯一索引的前缀扫描 , 返回匹配某个单独值的记录行 # type=ref explain select * from payment where customer_id = 350\\G; eq_ref : 类似 ref , 区别就在使用的索引是唯一索引 , 对于每个索引键值 , 表中只有一条记录匹配 ; 简单来说 , 就是多表连接中使用主键或者联合索引作为关联条件 # type=eq_ref explain select * from film a, film_text b where a.film_id = b.film_id\\G; const/system : 单表中最多有一个匹配行 , 查询起来非常迅速 , 所以这个匹配行中的其他列的值可以被优化器在当前查询中当做常量来处理 , 如 , 根据主键或唯一索引进行的查询 # type=const/system explain select * from (select * from customer where email='lyon.yang@qq.com')a\\G; NULL : MySQL 不用访问表或者索引 , 直接就能得到结果 # type=NULL explain select 1 from dual where 1\\G; type 还有其他值 , 如 ref_or_null (与 ref 类似 , 区别在于条件中包含对 NULL 的查询) , index_merge (索引合并优化) , unique_subquery (in 的后面是一个查询主键字段的子查询) , index_subquery (与 unique_subquery 类似 , 区别在于 in 的后面是查询非唯一索引字段的子查询) possible_keys : 表示查询时可能使用的索引 key : 表示实际使用的索引 key_len : 使用到索引字段的长度 rows : 扫描行的数量 Extra : 执行情况的说明和描述 , 包含不适合在其他列中显示但是对执行计划非常重要的额外信息 show profile分析SQL 查看是否支持 profile mysql> select @@have_profiling; +------------------+ | @@have_profiling | +------------------+ | YES | +------------------+ 1 row in set, 1 warning (0.00 sec) 默认 profiling 是关闭的 , 在 Session 级别开启 profiling : mysql> select @@profiling; +-------------+ | @@profiling | +-------------+ | 0 | +-------------+ 1 row in set, 1 warning (0.00 sec) mysql> set profiling=1; Query OK, 0 rows affected, 1 warning (0.05 sec) mysql> select @@profiling; +-------------+ | @@profiling | +-------------+ | 1 | +-------------+ 1 row in set, 1 warning (0.00 sec) 通过 show profiles 语句可以查看 SQL 的 Query ID : mysql> show profiles; +----------+------------+--------------------+ | Query_ID | Duration | Query | +----------+------------+--------------------+ | 1 | 0.00050550 | select @@profiling | +----------+------------+--------------------+ 1 row in set, 1 warning (0.00 sec) 通过 show profile for query 语句查看执行过程中线程的每个状态和消耗的时间 mysql> show profile for query 1; +----------------------+----------+ | Status | Duration | +----------------------+----------+ | starting | 0.000081 | | checking permissions | 0.000005 | | Opening tables | 0.000004 | | init | 0.000009 | | optimizing | 0.000336 | | executing | 0.000012 | | end | 0.000004 | | query end | 0.000004 | | closing tables | 0.000003 | | freeing items | 0.000036 | | cleaning up | 0.000013 | +----------------------+----------+ 11 rows in set, 1 warning (0.03 sec) 参考 : https://dev.mysql.com/doc/refman/8.0/en/explain-output.html "},"04-前端/":{"url":"04-前端/","title":"前端","keywords":"","body":"Web development 介绍 本目录为Web开发前端部分整理 整理路线如下 : - HTML - CSS - JavaScirpt - BOM - DOM - jQuery - Ajax 更多等待后期扩展添加...... "},"04-前端/02-Web开发 - HTML-head.html":{"url":"04-前端/02-Web开发 - HTML-head.html","title":"Web开发 - HTML-head","keywords":"","body":"Web开发之路 - HTML-head 介绍 🍀 HTML是什么 ? 超文本标记语言(Hyper Text Markup Language , HTML) , 是一种用于创建网页的标准标记语言 HTML使用一套标记标签来描述网页 , HTML文档 = 网页 , Web浏览器的作用是读取HTML文档 , 并以网页的形式显示出来 , 浏览器不会显示HTML标签 , 而是使用标签来解释页面的内容 HTML标签 HTML标签是由尖括号包围的关键字 , Content \"HTML 标签\" 和 \"HTML 元素\" 通常都是描述同样的意思 , 但是严格来讲 , 一个HTML元素包含了开始标签与结束标签 HTML元素 HTML元素以开始标签起始(起始标签) , 以结束标签终止(闭合标签) , 元素的内容是开始标签与结束标签之间的内容 HTML元素分为块级元素和内联元素 , 块级元素在浏览器显示时 , 通常会以新的行来开始(和结束) , 而内联元素显示时通常就在行内开始(和结束) , 如下 : 块级元素 : , , , 等 内联元素 : , , , 等 HTML属性 HTML标签可以拥有属性 , 属性提供了有关HTML元素的更多信息 , 属性是以键/值对的形式出现 , 如 : name=\"Lyon\" ; 属性在HTML元素的起始标签中规定 , 如 :点我进百度 HTML中有很多标准属性 , 当然也可以自定义属性 , 适用于大多数HTML元素的属性如下 : class , 为html元素定义一个或多个类名(类名从样式文件引入) id , 定义元素的id style , 规定元素的行内样式 title , 描述了元素的额外信息 HTML文档 file.html ├── └── ├── │ ├── │ ├── │ ├── │ ├── │ ├── │ └── └── 🍀 DOCTYPE告诉浏览器使用什么样的html或xhtml规范来解析html文档 如果html文档没有DOCTYPE声明 , 那么compatMode 默认为BackCompat(标准兼容模式未开启,称为怪异模式或混杂模式) , 使用该模式时 , 浏览器会按照自己的方式解析渲染页面 , 并且在不同的浏览器就会显示不同的样式 如果html文档添加了声明 , 那么compatMode就是CSS1Compat(标准兼容模式已开启,称为严格模式) , 即按照W3C的标准解析渲染页面 , 这样所有的浏览器显示的就是一个样子了 🍀 元素是HTML页面的根元素 , HTML文档由嵌套的HTML元素构成 一般包括与 , 如下 🍀 元素包含了所有的头部标签元素 , 在元素中可以插入脚本(Script) , 样式(CSS) , 及各种meta信息 🍀 meta标签描述了一些基本的元数据 页面编码 刷新和跳转 关键词 描述 X-UA-Compatible 🍀 定义网页头部信息 , 在HTML/XHTML文档中是必须的 title元素 : 定义了浏览器工具栏的标题 当网页添加收藏夹时 , 显示在收藏夹中的标题 显示在搜索引擎结果页面的标题 Lyon 🍀 标签描述了基本的链接地址/链接目标 , 该标签作为HTML文档中所有的链接默认链接 🍀 标签定义了文档与外部资源之间的关系 , 通常用于链接到样式表 : 🍀 标签定义HTML文档的样式文件引用地址 , 在 元素中也可以直接添加样式来渲染HTML文档 Lyon body { background-color: yellowgreen; font-size: inherit; } I am Lyon!! 🍀 标签用于加载脚本文件 , 也可直接写脚本代码 🍀 body元素是html文档中的主体 , 表示网页的主题部分 , 也就是用户可以看到的内容 , 可以包含文本 , 图片 , 音频 , 视频等各种内容 为避免篇幅过长 , body部分请看下一篇 "},"04-前端/03-Web开发 - HTML-body.html":{"url":"04-前端/03-Web开发 - HTML-body.html","title":"Web开发 - HTML-body","keywords":"","body":"Web开发之路 - HTML-body 前言 🍀 为避免上一篇篇幅过长 , body部分在这一篇进行整理 , 主要介绍常用标签 🍀 定义标题 , ~ , 标题从大到小 标题一 标题二 标题三 标题四 标题五 标题六 and 🍀 标签用于定义段落 , 默认段落之间是有间隔的 , 并且浏览器会自动地在段落前后添加空行(块级标签) 标签用于换行 , 即不产生一个新段落的情况下进行换行 这是第一个段落 这是第二个段落 这是第三个段落 and 🍀 定义文本粗体和斜体 我是粗体 我是斜体 更多 我是下划线 我是删除线 🍀 定义列表 , 区别如下 : ul , 无序列表 ol , 有序列表 li , 定义列表项 dl , 自定义列表 dt , 自定义列表项 dd , 自定义列表项的描述 Coffee Milk Coffee Milk Coffee - black hot drink Milk - white cold drink 🍀 定义下拉列表 , 元素中的 标签定义了列表中的可用选项 上海 北京 广州 上海 北京 广州 武汉 上海 北京 广州 武汉 武汉 湖里 石家庄 河里 🍀 插入图像 是空标签 , 只包含属性 , 没有闭合标签 , 属性如下 : src , 源属性(source) , 值为图像的url地址 alt , 定义图像的预备可替换文本 , 即无法载入图像时显示 🍀 定义表格 , 每个表格均有若干行( 标签定义) , 每行分为若干单元格( 标签定义) , 以及表格的标头(由 标签定义) 标签也可以用于页面的布局 , 用法与 一样 row 1, cell 1 row 1, cell 2 row 2, cell 1 row 2, cell 2 and 🍀 HTML可以通过 和 将元素组合起来 为块级标签 ; 为内联标签 , 可以作文本的容器 块级标签与内联标签的区别 : 块级标签是另起一行开始渲染 , 而行内联标签则不需要另起一行 单独在页面中插入这两个标签, 不会对页面产生任何影响 , 这两个标签是专门为定义CSS样式而生的 关于标签嵌套 : 通常块级标签可以包含内联标签或某些块级标签 ; 但是内联标签不能包含块级标签 , 它只能包含其他内联标签 Welcome To Lyon's Blog 菜单 HTML CSS JavaScript Welcom to lyon's blog! 版权 © Lyon.com 🍀 为表单标签 , 表单是一个包含表单元素的区域 表单元素允许用户在表单中输入内容 , 比如 : 文本域(textarea) , 下拉列表 , 单选框(radio-buttons) , 复选框(checkboxes)等 , 格式如下 : . input 元素 . 实例 username: password: 🍀 标签规定了用户可以在其中输入数据的输入字段 , 输入字段可以通过多种方式改变 , 取决于type属性 湖北 湖南 北京 男 女 男 女 password: 以上简单的介绍了几个属性 , 更多input元素属性可以通过访问W3school进行学习 🍀 标签为input元素定义标注(标记) lable元素不会向用户呈现任何特殊效果 , 不过当鼠标点击时 , 浏览器会自动将焦点转到和标签相关的表单控件上 标签的for属性应当与相关元素的id属性相同 姓名： 婚否： 姓名： 婚否： 🍀 用于定义多行文本 可以通过cols和rows属性来规定textarea的尺寸 , 不过更好的办法是使用CSS的 height和width属性 如果需要启动自动换行功能 , 可以通过设置wrap属性为virtual或physical 🍀 标签用于将表单内容的一部分打包 , 生成一组相关表单的字段 , 浏览器会以特殊方式来显示 , 它们可能有特殊的边界 , 3D效果 , 或者甚至可创建一个子表单来处理这些元素 标签为fieldset元素定义标题 登录 用户名： 密码： 更多:dash: "},"04-前端/05-Web开发 - CSS.html":{"url":"04-前端/05-Web开发 - CSS.html","title":"Web开发 - CSS","keywords":"","body":"Web开发之路 - CSS 介绍 🍀 CSS指的是层叠样式表(Cascading Style Sheets) , 用于定义如何显示HTML元素 CSS是在HTML 4 开始使用的 , 是为了更好的渲染HTML元素而引入的 , CSS可以通过以下方式加到HTML中 : 内联样式 : 在HTML元素中使用 \"style\" 属性 内部样式表 : 在HTML文档头部区域使用 元素来包含CSS 外部引用 : 是用外部CSS文件 最好的方式就是通过外部引用CSS文件 , 三种方式如下 : 内联样式 标题 段落 内部样式表 body {background-color:yellow;} p {color:blue;} 外部引用 选择器 🍀 CSS规则由两个主要的部分构成 : 选择器 , 以及一条或多条声明 , 如下 : selector {declaration1;declaration2;... declarationN} 选择器的种类有很多 , 下面就开始介绍各种选择器 元素选择器 🍀 元素选择器又称标签选择器 文档的元素就是最基本的选择器 div {background-color:red;} Class选择器 🍀 class选择器用于描述一组元素的样式 , class可以在多个元素中使用 class选择器在HTML中以class属性表示 , 在CSS中 , 类选择器以一个点 \".\" 号显示 .center {text-align:center;} p.center {text-align:center;} 类名的第一个字符不能使用数字 , 它无法在Mozilla或Firefox中起作用/ ID选择器 🍀 ID选择器可以为标有特定id的HTML元素指定特定的样式 CSS中ID选择器以\"#\"来定义 #para1 { color:red; } ID属性不要以数字开头 , 数字开头的ID在Mozilla/Firefox浏览器中不起作用 属性选择器 🍀 选择拥有某些属性的元素 , 也可设置特定属性 *[title] {color:red;} a[href][title] {color:red;} p[class=\"important warning\"] {color: red;} p[class~=\"important\"] {color: red;} 后代选择器 🍀 后代选择器又称包含选择器 , 后代选择器可以选择作为某元素后代的元素 h1 em {color:red;} 子元素选择器 🍀 与后代选择器相比 , 子元素选择器只能选择作为某元素子元素中的元素 h1 > strong {color:red;} 相邻兄弟选择器 🍀 相邻兄弟选择器可以选择紧接在另一元素后的元素 , 且二者有相同父元素 h1 + p {margin-top:50px;} 伪类 🍀 伪类用于向某些选择器添加特殊的效果 伪类的语法 : selector : pseudo-class {property:value} selector.class : pseduo-class {property:value} 实例 a.red : visited {color: #FF0000} CSS Syntax 伪元素 🍀 伪元素用于向某些选择器设置特殊效果 伪元素的语法 : selector:pseudo-element {property:value;} selector.class:pseudo-element {property:value;} p:first-line { color:#ff0000; font-variant:small-caps; } 组合选择器 : input,div,p { background-color:red; } 关联选择器 : idselect p { background-color:red; } 常用属性 🍀 background 🍀 背景色 background-color属性可以为元素设置背景色 /* 把元素背景设置为灰色 */ p {background-color: gray;} background-color属性默认值为transparent , 即\"透明\" 背景图像 background-image属性可以把图像作为背景 /* 设置图像时,必须为该属性设置一个url值 */ body {background-image: url(/i/eg_bg_04.gif);} background-image属性默认值为none , 表示背景上没有放置任何图像 背景重复 background-repeat属性可以将背景图像在页面上进行平铺 属性repeat导致图像在水平垂直方向都平铺 , repeat-x和repeat-y分别导致图像只在水平或垂直方向上重复 , no-repeat则不允许图像在任何方向上平铺 /* 默认从左上角开始,垂直方向上平铺 */ body { background-image: url(/i/eg_bg_03.gif); background-repeat: repeat-y; } 背景定位 background-position属性可以改变图像在背景中的位置 body { background-image:url('/i/eg_bg_03.gif'); background-repeat:no-repeat; background-position:center; } background-position有以下关键字 : top , bottom , left , right 和center , 这些关键字通常成对出现 , 也可以使用长度值 , 如 : 100px或5cm(长度值是元素边距区左上角的便宜偏移) , 以及使用百分数 body { background-image:url('/i/eg_bg_03.gif'); background-repeat:no-repeat; background-position:50% 50%; } 背景关联 background-attachment属性可以使图像相对于可视区固定 , 即不会受到滚动的影响 body { background-image:url(/i/eg_bg_02.gif); background-repeat:no-repeat; background-attachment:fixed } border 🍀 border属性允许规定元素边框的样式 , 宽度和颜色 元素的边框是围绕元素内容和内边距的一条或多条线 /* 一个边框定义多个样式 */ p.aside {border-style: solid dotted dashed double;} 看看我的边框 更多边框样式及边框相关 , 在我这里 margin 🍀 margin属性可以用来设置外边距 , 接受任何长度单位 , 百分数甚至负值 围绕在元素边框的空白区域就是外边距 , 设置外边距会在元素外创建额外的\"空白\" /* h1元素的各个边上设置了1/4英寸的空白 */ h1 {margin : 0.25in;} h1 {margin : 10px 0px 15px 5px;} /* margin: top right bottom left */ 值复制 如果缺少左外边距的值 , 则使用右外边距的值 如果缺少下外边距的值 , 则使用上外边距的值 如果缺少右外边距的值 , 则使用上外边距的值 h1 {margin: 0.25em 1em 0.5em;} /* 等价于 0.25em 1em 0.5em 1em */ h2 {margin: 0.5em 1em;} /* 等价于 0.5em 1em 0.5em 1em */ p {margin: 1px;} /* 等价于 1px 1px 1px 1px */ 单边外边距 margin-top margin-right margin-bottom margin-left padding 🍀 padding属性可以设置内边距 , 接受长度值或百分比值 , 但不允许使用负值 元素的内边距在边框和内容区之间 /* 同样具有值复制规则,与margin一样 */ h1 {padding: 10px;} h1 {padding: 10px 0.25em 2ex 20%;} 单边内边距 padding-top padding-right padding-bottom padding-left display 🍀 display属性规定元素应该生成的框的类型 /* 使段落生出行内框 */ p.inline { display:inline; } display属性常用的值 : none , 此元素不会被显示 block , 此元素将显示为块级元素 , 此元素前后会带有换行符 inline , 默认 , 此元素会被显示为内联元素 , 元素前后没有换行符 inline-block , 行内块元素 更多display属性 cursor 🍀 cursor属性规定要显示的光标的类型 该属性定义了鼠标指针放在一个元素边界范围内时锁用的光标形状 值 描述 url 需使用的自定义光标的 URL , 末端始终定义一种普通的光标 , 以防没有由 URL 定义的可用光标 default 默认光标(通常是一个箭头) auto 默认 , 浏览器设置的光标 crosshair 光标呈现为十字线 pointer 光标呈现为指示链接的指针(一只手) move 此光标指示某对象可被移动 e-resize 此光标指示矩形框的边缘可被向右(东)移动 ne-resize 此光标指示矩形框的边缘可被向上及向右移动(北/东) nw-resize 此光标指示矩形框的边缘可被向上及向左移动北(西) n-resize 此光标指示矩形框的边缘可被向上(北)移动。 se-resize 此光标指示矩形框的边缘可被向下及向右移动(南/东) sw-resize 此光标指示矩形框的边缘可被向下及向左移动(南/西) s-resize 此光标指示矩形框的边缘可被向下移动(南) w-resize 此光标指示矩形框的边缘可被向左移动(西) text 此光标指示文本 wait 此光标指示程序正忙(通常是一只表或沙漏) help 此光标指示可用的帮助(通常是一个问号或一个气球) 请把鼠标移动到单词上，可以看到鼠标指针发生变化： Auto Crosshair Default Pointer Move e-resize ne-resize nw-resize n-resize se-resize sw-resize s-resize w-resize text wait help position 🍀 CSS有三种基本的定位机制 : 普通流 , 浮动和绝对定位 ; 除非专门指定 , 否则所有框都在普通流中定位 position属性有以下属性值 : static , 默认值 , 元素没有被定位 , 而且在文档中出现在它应该在的位置 , 一般不用指定 , 除非需要覆盖之前设置的定位 relative , 元素框偏移某个距离 , 元素实际上依然占据文档中的原有位置 , 只是视觉上相对于它在文档中的原有位置移动了 , 相对定位 absolute , 元素在文档中不占据位置 , 定位后生成一个块级框 , 不论原来它在正常流中生成何种类型的框 , 绝对定位 fixed , 是特殊的absolute , 即fixed总是以body为定位对象的 , 按照浏览器的窗口进行定位 相对定位 h2.pos_left { position:relative; left:-20px } h2.pos_right { position:relative; left:20px } 这是位于正常位置的标题 这个标题相对于其正常位置向左移动 这个标题相对于其正常位置向右移动 相对定位会按照元素的原始位置对该元素进行移动。 样式 \"left:-20px\" 从元素的原始左侧位置减去 20 像素。 样式 \"left:20px\" 向元素的原始左侧位置增加 20 像素。 绝对定位 h2.pos_abs { position:absolute; left:100px; top:150px } 这是带有绝对定位的标题 通过绝对定位，元素可以放置到页面上的任何位置。下面的标题距离页面左侧 100px，距离页面顶部 150px。 固定定位 p.one { position:fixed; left:5px; top:5px; } p.two { position:fixed; top:30px; right:5px; } 一些文本。 更多的文本。 float 🍀 float属性可以实现浮动的框 /* 图像向右浮动 */ img { float:right; } float属性值 : left , 元素向左浮动 right , 元素向右浮动 none , 默认值 , 元素不浮动 inherit , 规定应该从父元素继承float属性的值 注意 : 假如在一行之上只有极少的空间可供浮动元素，那么这个元素会跳至下一行，这个过程会持续到某一行拥有足够的空间为止。 clear属性可以阻止框围绕浮动框 , clear属性值 : left , 在左侧不允许浮动元素 right , 在右侧不允许浮动元素 both , 在左右两侧均不允许浮动元素 none , 默认值 , 允许浮动元素出现在两侧 inherit , 规定应该从父元素继承 clear 属性的值 /* 图像左侧和右侧均不允许出现浮动元素 */ img { float:left; clear:both; } opacity 🍀 opacity属性用于定义透明效果 , opacity属性能设置的值从0.0到1.0 , 值越小则越透明 img { opacity:0.4; filter:alpha(opacity=40); /* 针对 IE8 以及更早的版本 */ } 更过CSS内容 , CSS教程 "},"04-前端/06-Web开发 - JavaScript.html":{"url":"04-前端/06-Web开发 - JavaScript.html","title":"Web开发 - JavaScript","keywords":"","body":"Web开发之路 - JavaScript 介绍 🍀 JavaScript是属于网络的脚本语言 , 是因特网上最流行的脚本语言 JavaScript被数百万计的网页用来改进设计 , 验证表单 , 检测浏览器 , 创建cookies , 以及更多的应用 浏览器内置了JavaScript语言的解释器 , 所以浏览器上按照JavaScript的规则编写相应代码 , 浏览器可以解释并作出相应的处理 完整的JavaScript实现是由一下三个不同部分组成的 : 核心 , ECMAScript 文档对象模型 (DOM) , Document Object Model (整合JS , CSS , HTML) 浏览器对象模型 (BOM) , Broswer Object Model (整合JS和浏览器) 简单的说 , ECMAScript描述了JavsScript语言本身的相关内容 存在形式 js代码内容 存放位置 HTML的head中 HTML的body代码块底部 由于HTML代码是从上到下执行的 , 如果Head中的JS代码耗时严重 , 会导致用户长时间无法看到页面 , 所以将JS代码防止HTML的body代码块底部是最好的 , 因为不会影响用户看到页面效果 , 只是JS实现特效慢而已 JavaScript注释 单行注释 : // 多行注释 : /* ... */ , (CSS注释也是如此) 变量 🍀 在JavaScript中 , 变量的声明默认表示声明的全局变量 , 局部变量必须以var开头 生命周期 : JavaScript变量的声明周期从它们被声明的时间开始 局部变量会在函数运行以后被删除 全局变量会在页面关闭后被删除 // 全局变量 name = 'Lyon'; function func(){ // 局部变量 var age = 18; // 全局变量 gender = \"男\" } 注意 : JavaScript中严格区分大小写 , 并且以 \";\" 号结束 数据类型 🍀 JavaScript中的数据类型有字符串 , 数字 , 布尔 , 数组 , 对象 , Null , Undefined 其中Null是JavaScript语言的关键字 , 它表示一个特殊的值 , 常用来描述\"空值\" , Undefined是一个特殊值 , 表示变量未定义 , 即声明后若不定义则变量的值为Undefined 数字(Number) JavaScript中不区分整数值和浮点数值 , 所有数字均用浮点数值表示 , 还有两个特殊值NaN和Infinity , 如下 123; // 整数 0.456; //浮点数 1.2345e3; // 科学计数法表示1.2345x1000，等同于1234.5 NaN; // NaN表示Not a Number，当无法计算结果时用NaN表示 Infinity; // Infinity表示无限大，当数值超过了JavaScript的Number所能表示的最大值时，就表示为Infinity Number可以直接做四则运算 , 规则和数学一致 : 1 + 2; // 3 (1 + 2) * 5 / 2; // 7.5 2 / 0; // Infinity 0 / 0; // NaN 10 % 3; // 取余,1 10.5 % 3; // 1.5 如果要执行常见的算数任务 , 可以使用Math对象进行计算 , Math对象 字符串(String) 字符串是由字符组成的数组 , 在JavaScript中字符串不可变的 常见功能 string.length // 长度 string.trim() //移除空白 string.trimLeft() string.trimRight) string.charAt(n) //返回字符串中的第n个字符 string.concat(value, ...) //拼接 string.indexOf(substring,start) //子序列位置 string.lastIndexOf(substring,start) //子序列位置 string.substring(from, to) //根据索引获取子序列 string.slice(start, end) //切片 string.toLowerCase() //大写 string.toUpperCase() //小写 string.split(delimiter, limit) //分割 string.search(regexp) //从头开始匹配,返回匹配成功的第一个位置(g无效) string.match(regexp) //全局搜索,如果正则中有g表示找到全部，否则只找到第一个。 string.replace(regexp, replacement) //替换,正则中有g则替换所有,否则只替换第一个匹配项 布尔 布尔类型即true和false 比较运算符 == //比较值相等 != //不等于 === //比较值和类型相等 !=== //不等于 || //或 && //且 数组 JavaScript中的数组与Python中的列表类似 , 是一组按顺序排列的集合 , 集合的每个值称为元素 [1, 2, 3.14, 'Hello', null, true]; // 通过Array()函数创建 new Array(1, 2, 3); 常用功能 array.length //数组的大小 array.push(ele) //尾部追加元素 array.pop() //尾部获取一个元素 array.unshift(ele) //头部插入元素 array.shift() //头部移除元素 array.splice(start, deleteCount, value, ...) //插入,删除或替换数组的元素 array.splice(n,0,val) //指定位置插入元素 array.splice(n,1,val) //指定位置替换元素 array.splice(n,1) //指定位置删除元素 array.slice( ) //切片 array.reverse( ) //反转 array.join(sep) //将数组元素连接起来以构建一个字符串 array.concat(val,..) //连接数组 array.sort( ) //对数组元素进行排序 对象 对象由花括号分隔 , 在括号内部 , 对象的属性以名称和值对的形式定义 , 属性由逗号分隔 var person={ name:\"Lyon\", age:20 } 要获取对象的属性 , 只需用对象变量.属性名即可 , 也可对象变量[属性名] 语句与异常 🍀 条件语句 JavaScript中支持两个条件语句 , 分别是if和switch // if语句 if(条件){ } else if(条件){ } else{ } // switch语句,选择要执行的多个代码块之一 switch(n){ case 1: 执行代码块 1 break; case 2: 执行代码块 2 break; default: n 与 case 1 和 case 2 不同时执行的代码 } 注意 : 使用switch语句时 , 如果不使用break , 那么程序将继续往下执行 循环语句 与Python一样 , for循环和while循环 // for循环 var names = [\"Lyon\", \"Kenneth\", \"Eva\"]; for(var i=0;i 异常处理 try { // 捕获异常 } catch (e) { // 如果try代码块中捕获到了异常，catch代码块中的代码就会被执行。 //e是一个局部变量，用来指向Error对象或者其他抛出的对象 } finally { //无论发生啥,最后都要执行,即使遇到return } // throw Error(e) 主动抛出异常,如Python中的raise 函数 🍀 基本函数 JavaScript中函数基本上可以分为以下三类 // 普通函数 function func(arg){ return true; } // 匿名函数 var func = function(arg){ return \"Lyon\"; } // 自执行函数 (function(arg){ console.log(arg); })('123') 上例中函数的参数都属于显示参数(Parameters) , 显示参数在函数定义时列出 ; 还有隐式参数(Arguments) , 其在函数调用时传递给函数真正的值 , 如果函数在调用时未提供隐式参数 , 参数会默认设置为undefined JavaScript函数有个内置的对象arguments对象 , 包含了函数调用的参数数组 , 通过这种方式可以方便的找到最大的参数的值 , 或者创建一个函数用来统计所有数值的和 x = findMax(1, 123, 500, 115, 44, 88); function findMax() { var i, max = arguments[0]; if(arguments.length max) { max = arguments[i]; } } return max; } 对于Python来讲 , 显示与隐式不过就是实参与形参而已 ; 在JavaScript中都是按照位置参数进行传递的 , 特殊对象arguments的存在 , 使得JavaScript中实际参数可能小于形式参数的个数 作用域 JavaScript中每个函数都有自己的作用域 , 当出现函数嵌套时 , 就出现了作用域链 , 即函数使用变量时 , 会随着作用域链从内而外的一层层寻找 , 找不到就异常 function f2(){ var arg= 111; function f3(){ console.log(arg); } return f3; } ret = f2(); ret(); PS1 : 所有的作用域在创建函数且未执行时就已经存在 PS2 : JavaScript函数在被执行之前 , 所有的变量都已经声明 , 但是却不会赋值 , 所以即使你在使用参数的后面再定义变量也不会报错 , 因为它是存在的并且值为undefined function Mytest(){ console.log(name); var name = \"Lyon\"; } Mytest(); // 输出结果:undefined 闭包 闭包在很多语言中都可以看到 , 就像Python中 , 闭包必须是内部定义的函数(嵌套函数) , 该函数包含对外部作用域而不是全局作用域名字的引用 , JavaScript中也一样 闭包是一个函数 , 它记住了周围发生了什么 由于作用域链只能从内而外的找 , 默认外部无法获取函数内部变量 , 闭包则实现了在外部获取函数内部的变量 function f2(){ var arg= [11,22]; function f3(){ return arg; } return f3; } ret = f2(); ret(); 面向对象 function Foo (name,age) { this.Name = name; this.Age = age; this.Func = function(arg){ return this.Name + arg; } } var obj = new Foo('Lyon', 18); var ret = obj.Func(\"Kenneth\"); console.log(ret); 上述代码中 : Foo充当构造函数 this代指对象 创建对象时需要使用new 但是在上述代码中每个对象中均保存了一个相同的Func函数 , 可以使用原型优化处理 function Foo (name,age) { this.Name = name; this.Age = age; } Foo.prototype = { GetInfo: function(){ return this.Name + this.Age }, Func : function(arg){ return this.Name + arg; } } 其他 🍀 序列化 - JSON.stringify(obj) 序列化 - JSON.parse(str) 反序列化 转义 - decodeURI( ) URl中未转义的字符 - decodeURIComponent( ) URI组件中的未转义字符 - encodeURI( ) URI中的转义字符 - encodeURIComponent( ) 转义URI组件中的字符 - escape( ) 对字符串转义 - unescape( ) 给转义字符串解码 - URIError 由URl的编码和解码方法抛出 eval // JavaScript中的eval是Python中eval和exec的合集,既可以编译代码也可以获取返回值 -eval() -EvalError // 执行字符串中的JavaScript代码 正则表达式 定义正则表达式 /.../ 用于定义正则表达式 /.../g 表示全局匹配 /.../i 表示不区分大小写 /.../m 表示多行匹配 // JS正则匹配时本身就是支持多行,此处多行匹配只是影响正则表达式^和$,m模式也会使用^$来匹配换行的内容 实例 var pattern = /^Java\\w*/gm; var text = \"JavaScript is more fun than \\nJavaEE or JavaBeans!\"; result = pattern.exec(text) result = pattern.exec(text) result = pattern.exec(text) // 定义正则表达式也可以使用RegExp对象 匹配 JavaScript中主要提供了以下两个功能 : test(string) 检查字符串中是否和正则匹配 n = 'uui99sdf' reg = /\\d+/ reg.test(n) ---> true // 只要正则在字符串中存在就匹配,如果想要开头和结尾匹配的话,就需要在正则前后加^和$ exec(string) 获取正则表达式匹配的内容 , 如果未匹配 , 值为null , 否则 , 获取匹配成功的数组 非全局模式 // 获取匹配结果数组，注意：第一个元素是第一个匹配的结果，后面元素是正则子匹配（正则内容分组匹配） var pattern = /\\bJava\\w*\\b/; var text = \"JavaScript is more fun than Java or JavaBeans!\"; result = pattern.exec(text) var pattern = /\\b(Java)\\w*\\b/; var text = \"JavaScript is more fun than Java or JavaBeans!\"; result = pattern.exec(text) 全局模式 // 需要反复调用exec方法，来一个一个获取结果,直到匹配获取结果为null表示获取完毕 var pattern = /\\bJava\\w*\\b/g; var text = \"JavaScript is more fun than Java or JavaBeans!\"; result = pattern.exec(text) var pattern = /\\b(Java)\\w*\\b/g; var text = \"JavaScript is more fun than Java or JavaBeans!\"; result = pattern.exec(text) 字符串中相关方法 obj.search(regexp) // 获取索引位置,搜索整个字符串,返回匹配成功的第一个位置(g模式无效) obj.match(regexp) // 获取匹配内容,搜索整个字符串,获取找到第一个匹配内容,如果正则是g模式找到全部 obj.replace(regexp, replacement) //替换匹配替换,正则中有g则替换所有,否则只替换第一个匹配项 // $数字：匹配的第n个组内容； // $&：当前匹配的内容； // $`：位于匹配子串左侧的文本； // $'：位于匹配子串右侧的文本 // $$：直接量$符号 时间处理 Date对象用于处理日期和时间 获取系统当前时间 var now = new Date(); now; // Wed Jun 24 2015 19:49:22 GMT+0800 (CST) now.getFullYear(); // 2015, 年份 now.getMonth(); // 5, 月份，注意月份范围是0~11，5表示六月 now.getDate(); // 24, 表示24号 now.getDay(); // 3, 表示星期三 now.getHours(); // 19, 24小时制 now.getMinutes(); // 49, 分钟 now.getSeconds(); // 22, 秒 now.getMilliseconds(); // 875, 毫秒数 now.getTime(); // 1435146562875, 以number形式表示的时间戳 // 注意当前时间是浏览器从本机操作系统获取的时间,所以不一定准确 创建一个指定日期和时间的Date对象 var d = new Date(2015, 5, 19, 20, 15, 30, 123); d; // Fri Jun 19 2015 20:15:30 GMT+0800 (CST) 第二中创建方式 var d = Date.parse('2015-06-24T19:49:22.875+08:00'); d; // 1435146562875 PS : Date对象表示的时间总是按浏览器所在的时区显示的 , 我们可以进行调整 var d = new Date(1435146562875); d.toLocaleString(); // '2015/6/24 下午7:49:22'，本地时间（北京时区+8:00），显示的字符串与操作系统设定的格式有关 d.toUTCString(); // 'Wed, 24 Jun 2015 11:49:22 GMT'，UTC时间，与本地时间相差8小时 更多JavaScript内容可以通过 , 以下链接学习 , 菜鸟教程 , 廖雪峰官网教程 , w3school "},"04-前端/07-Web开发 - BOM.html":{"url":"04-前端/07-Web开发 - BOM.html","title":"Web开发 - BOM","keywords":"","body":"Web开发之路 - BOM 介绍 🍀 由于JavaScript的出现就是为了能在浏览器中运行 , BOM , 即浏览器对象模型(Browser Object Model) , BOM使JavaScript有能力与浏览器进行\"对话\" 由于现代浏览器已经(几乎) 实现了JavaScript交互性方面的相同方法和属性 , 因此常被认为是BOM的方法和属性 BOM是为了控制浏览器的行为而出现的接口 , 不同浏览器之间定义与实现存在差异 下面就开始介绍浏览器对象啦 Window 🍀 所有的浏览器都支持window对象 , 它表示浏览器窗口 , 所有JavaScript全局对象 , 函数以及变量均自动成为window对象的成员 , 也就是说Window对象是客户端JavaScript最高层对象之一 由于window对象是全局对象 , 所有的表达式都在当前的环境中计算 , 所以要引用当前窗口不需要特殊的语法 , 可以直接把窗口的属性作为全局变量来使用 , 如 : window.document可以直接写document Window对象属性 属性 描述 closed 返回窗口是否已被关闭 defaultStatus 设置或返回窗口状态栏中的默认文本 document 文档对象 history 包含窗口的浏览历史 innerheight 返回窗口的文档显示区的高度 innerwidth 返回窗口的文档显示区的宽度 length 设置或返回窗口中的框架数量 location 包含有关当前URL的信息 name 设置或返回窗口的名称 navigator 包含有关浏览器的信息 opener 返回对创建此窗口的窗口的引用 outerheight 返回窗口的外部高度 outerwidth 返回窗口的外部宽度 pageXOffset 设置或返回当前页面相对于窗口显示区左上角的 X 位置 pageYOffset 设置或返回当前页面相对于窗口显示区左上角的 Y 位置 parent 返回父窗口。 screen 包含有关显示浏览器屏幕的信息 self 返回对当前窗口的引用 , 等价于 Window 属性 status 设置窗口状态栏的文本 top 返回最顶层的先辈窗口 window window 属性等价于 self 属性 , 它包含了对窗口自身的引用 screenLeftscreenTopscreenXscreenY 只读整数 , 声明了窗口的左上角在屏幕上的的 x 坐标和 y 坐标 , IE、Safari 和 Opera 支持 screenLeft 和 screenTop , 而 Firefox 和 Safari 支持 screenX 和 screenY Window对象方法 方法 描述 alert() 显示带有一段消息和一个确认按钮的警告框 blur() 把键盘焦点从顶层窗口移开 clearInterval() 取消由 setInterval() 设置的 timeout clearTimeout() 取消由 setTimeout() 方法设置的 timeout close() 关闭浏览器窗口 confirm() 显示带有一段消息以及确认按钮和取消按钮的对话框 createPopup() 创建一个 pop-up 窗口 focus() 把键盘焦点给予一个窗口 moveBy() 可相对窗口的当前坐标把它移动指定的像素 moveTo() 把窗口的左上角移动到一个指定的坐标 open() 打开一个新的浏览器窗口或查找一个已命名的窗口 print() 打印当前窗口的内容 prompt() 显示可提示用户输入的对话框 resizeBy() 按照指定的像素调整窗口的大小 resizeTo() 把窗口的大小调整到指定的宽度和高度 scrollBy() 按照指定的像素值来滚动内容 scrollTo() 把内容滚动到指定的坐标 setInterval() 按照指定的周期（以毫秒计）来调用函数或计算表达式 setTimeout() 在指定的毫秒数后调用函数或计算表达式 实例 // 返回窗口尺寸 alert('window inner size:' + window.innerWidth + 'x' + window.innerHeight); // 直接在浏览器中consle下执行 document 🍀 每个载入浏览器的HTML文档都会成为document对象 , document对象使我们可以从脚本中对HTML页面中的所有元素进行访问 document对象是Window对象的一部分 , 可通过window.document属性对其进行访问 , 或直接使用document document对象属性 属性 描述 body 提供对 元素的直接访问 , 对于定义了框架集的文档 , 该属性引用最外层的 cookie 设置或返回与当前文档有关的所有 cookie domain 返回当前文档的域名 lastModified 返回文档被最后修改的日期和时间 referrer 返回载入当前文档的文档的 URL title 返回当前文档的标题 URL 返回当前文档的 URL document对象方法 方法 描述 close() 关闭用 document.open() 方法打开的输出流 , 并显示选定的数据 getElementById() 返回对拥有指定 id 的第一个对象的引用 getElementsByName() 返回带有指定名称的对象集合 getElementsByTagName() 返回带有指定标签名的对象集合 open() 打开一个流 , 以收集来自任何 document.write() 或 document.writeln() 方法的输出 write() 向文档写 HTML 表达式 或 JavaScript 代码 writeln() 等同于 write() 方法 , 不同的是在每个表达式之后写一个换行符 实例 // 改变title document.title = '努力学习JavaScript!'; navigator 🍀 navigator对象包含有关浏览器的信息 , 所有浏览器中都支持 , navigator对象的实例是唯一的 , 它是Window对象的子对象 , 所以可以用Window对象的navigator属性来引用它 , 即window.navigator , 当然也可以直接navigator navigator对象属性 属性 描述 appCodeName 返回浏览器的代码名 appMinorVersion 返回浏览器的次级版本 appName 返回浏览器的名称 appVersion 返回浏览器的平台和版本信息 browserLanguage 返回当前浏览器的语言 cookieEnabled 返回指明浏览器中是否启用 cookie 的布尔值 cpuClass 返回浏览器系统的 CPU 等级 onLine 返回指明系统是否处于脱机模式的布尔值 platform 返回运行浏览器的操作系统平台 systemLanguage 返回 OS 使用的默认语言 userAgent 返回由客户机发送服务器的 user-agent 头部的值 userLanguage 返回 OS 的自然语言设置 navigator对象方法 方法 描述 javaEnabled() 规定浏览器是否启用 Java taintEnabled() 规定浏览器是否启用数据污点 (data tainting) 实例 alert('appName = ' + navigator.appName + '\\n' + 'appVersion = ' + navigator.appVersion + '\\n' + 'language = ' + navigator.language + '\\n' + 'platform = ' + navigator.platform + '\\n' + 'userAgent = ' + navigator.userAgent); screen 🍀 screen对象中存放着有关显示浏览器屏幕的信息 , 可用Window对象中的screen属性直接引用 , 即window.screen , 或者screen , 所有浏览器都支持 screen对象属性 属性 描述 availHeight 返回显示屏幕的高度 (除 Windows 任务栏之外) availWidth 返回显示屏幕的宽度 (除 Windows 任务栏之外) bufferDepth 设置或返回调色板的比特深度 colorDepth 返回目标设备或缓冲器上的调色板的比特深度 deviceXDPI 返回显示屏幕的每英寸水平点数 deviceYDPI 返回显示屏幕的每英寸垂直点数 fontSmoothingEnabled 返回用户是否在显示控制面板中启用了字体平滑 height 返回显示屏幕的高度 logicalXDPI 返回显示屏幕每英寸的水平方向的常规点数 logicalYDPI 返回显示屏幕每英寸的垂直方向的常规点数 pixelDepth 返回显示屏幕的颜色分辨率(比特每像素) updateInterval 设置或返回屏幕的刷新率 width 返回显示器屏幕的宽度 实例 alert('screen size = ' + screen.width + ' x ' + screen.height); history 🍀 history对象最初设计来表示窗口的浏览历史 , 但出于隐私方面的原因 , history对象不在允许脚本访问已经访问过的实际URL , 唯一保持使用的功能只有back()、forward() 和 go() 方法 可通过window.history或者history进行访问 history对象属性 属性 描述 length 返回浏览器历史列表中的 URL 数量 history对象方法 方法 描述 back() 加载 history 列表中的前一个 URL forward() 加载 history 列表中的下一个 URL go() 加载 history 列表中的某个具体页面 实例 // 该操作与单击后退按钮执行的操作一样 history.back() // 返回结果:undefined location 🍀 location对象包含有关当前URL的信息 , location对象是Window对象的一部分 , 可通过window.location属性来访问 , 或者location location对象属性 属性 描述 hash 设置或返回从井号 (#) 开始的 URL（锚） host 设置或返回主机名和当前 URL 的端口号 hostname 设置或返回当前 URL 的主机名 href 设置或返回完整的 URL pathname 设置或返回当前 URL 的路径部分 port 设置或返回当前 URL 的端口号 protocol 设置或返回当前 URL 的协议 search 设置或返回从问号 (?) 开始的 URL（查询部分） location 对象方法 属性 描述 assign() 加载新的文档 reload() 重新加载当前文档 replace() 用新的文档替换当前文档 实例 function currLocation() { alert(window.location) } function newLocation() { window.location=\"/index.html\" } "},"04-前端/08-Web开发 - DOM.html":{"url":"04-前端/08-Web开发 - DOM.html","title":"Web开发 - DOM","keywords":"","body":"Web开发之路 - DOM 介绍 🍀 上一篇中了解了BOM , 其存在是为了控制浏览器的行为 , 而这一篇所说的DOM则是为了操作HTML和XML文档出现的接口 DOM全称为Document Object Model , 也就是文档对象模型 , DOM是W3C(万维网联盟)的标准 , 所有浏览器公共遵守的标准 当网页被加载时 , 浏览器会创建页面的DOM ; DOM是一个树形结构 , 在HTML DOM中 , 每一个元素都是节点 同样的 , 通过JavaScript可以来操作DOM DOM查找 🍀 直接查找 document.getElementById() // 获取指定ID节点 document.getElementsByName() // 获取指定name属性节点 document.getElementsByClassName() // 获取指定class属性节点 document.getElementsByTagName() // 获取指定标签节点 间接查找 parentNode // 父节点 childNodes // 所有子节点 firstChild // 第一个子节点 lastChild // 最后一个子节点 nextSibling // 下一个兄弟节点 previousSibling // 上一个兄弟节点 parentElement // 父节点标签元素 children // 所有子标签 firstElementChild // 第一个子标签元素 lastElementChild // 最后一个子标签元素 nextElementtSibling // 下一个兄弟标签元素 previousElementSibling // 上一个兄弟标签元素 实例 // 返回ID为'test'的节点： var test = document.getElementById('test'); // 先定位ID为'test-table'的节点，再返回其内部所有tr节点： var trs = document.getElementById('test-table').getElementsByTagName('tr'); // 先定位ID为'test-div'的节点，再返回其内部所有class包含red的节点： var reds = document.getElementById('test-div').getElementsByClassName('red'); // 获取节点test下的所有直属子节点: var cs = test.children; // 获取节点test下第一个、最后一个子节点： var first = test.firstElementChild; var last = test.lastElementChild; DOM修改 🍀 对于DOM的修改操作有很多, 比如内容 , 样式 , 属性等等 , 都是可以用DOM进行修改的 内容 🍀 innerHTML // 设置或获取位于对象起始和结束标签内的HTML,符合W3C标准 innerText // 设置或获取位于对象起始和结束标签内的文本 outerHTML // 设置或获取对象及其内容的HTML形式 outerText // 设置(包括标签)或获取(不包括标签)对象的文本 value // 设置或返回隐藏输入域的value属性的值 实例 // 获取... var p = document.getElementById('p-id'); // 设置文本为abc: p.innerHTML = 'ABC'; // ABC // 设置HTML: p.innerHTML = 'ABC RED XYZ'; // ...的内部结构已修改 属性 🍀 attribute // 获取所有标签属性 setAttribute(key,value) // 设置标签属性 getAttribute(key) // 获取指定标签属性 实例 // 创建class属性 var atr = document.createAttribute(\"class\"); // 设置属性值 atr.nodeValue=\"democlass\"; // 设置属性节点 document.getElementById('n1').setAttributeNode(atr); Class 🍀 className // 获取所有类名 classList.remove(cls) // 删除指定类 classList.add(cls) // 添加类 标签 🍀 创建标签 // 方式一 var tag = document.createElement('a') tag.innerText = \"baidu\" tag.className = \"c1\" tag.href = \"http://www.baidu.com/\" // 方式二 var tag = \"baidu\" 插入标签 // 方式一 var obj = \"\"; xxx.insertAdjacentHTML(\"beforeEnd\",obj); xxx.insertAdjacentElement('afterBegin',document.createElement('p')) //注意：第一个参数只能是'beforeBegin'、 'afterBegin'、 'beforeEnd'、 'afterEnd' // 方式二 var tag = document.createElement('a') xxx.appendChild(tag) xxx.insertBefore(tag,xxx[1]) 样式 🍀 var obj = document.getElementById('i1') obj.style.fontSize = \"32px\"; obj.style.backgroundColor = \"red\"; 实例 function Focus(ths){ ths.style.color = \"black\"; if(ths.value == '请输入关键字' || ths.value.trim() == \"\"){ ths.value = \"\"; } } function Blur(ths){ if(ths.value.trim() == \"\"){ ths.value = '请输入关键字'; ths.style.color = 'gray'; }else{ ths.style.color = \"black\"; } } 位置 🍀 document.documentElement.offsetHeight // 总文档高度 document.documentElement.clientHeight // 当前文档占屏幕高度 tag.offsetHeight // 自身高度 tag.offsetTop // 距离上级定位高度 tag.offsetParent // 父定位标签 tag.scrollTop // 滚动高度 /* clientHeight -> 可见区域：height + padding clientTop -> border高度 offsetHeight -> 可见区域：height + padding + border scrollHeight -> 全文高：height + padding 特别的： document.documentElement代指文档根节点 */ 实例 var i1 = document.getElementById('i1'); console.log(i1.clientHeight); // 可见区域：height + padding console.log(i1.clientTop); // border高度 console.log('====='); console.log(i1.offsetHeight); // 可见区域：height + padding + border console.log(i1.offsetTop); // 上级定位标签的高度 console.log('====='); console.log(i1.scrollHeight); //全文高：height + padding console.log(i1.scrollTop); // 滚动高度 console.log('====='); 表单 🍀 document.geElementById('form').submit() 其他 🍀 console.log // 输出框 alert // 弹出框 confirm // 确认框 // URL和刷新 location.href // 获取URL location.href = \"url\" // 重定向 location.reload() // 重新加载 // 定时器 setInterval // 多次定时器 clearInterval // 清除多次定时器 setTimeout // 单次定时器 clearTimeout // 清除单次定时器 DOM事件 🍀 鼠标事件 属性 描述 DOM onclick 当用户点击某个对象时调用的事件句柄 2 oncontextmenu 在用户点击鼠标右键打开上下文菜单时触发 ondblclick 当用户双击某个对象时调用的事件句柄 2 onmousedown 鼠标按钮被按下 2 onmouseenter 当鼠标指针移动到元素上时触发 2 onmouseleave 当鼠标指针移出元素时触发 2 onmousemove 鼠标被移动 2 onmouseover 鼠标移到某元素之上 2 onmouseout 鼠标从某元素移开 2 onmouseup 鼠标按键被松开 2 键盘事件 属性 描述 DOM onkeydown 某个键盘按键被按下 2 onkeypress 某个键盘按键被按下并松开 2 onkeyup 某个键盘按键被松开 2 更多DOM事件 , HTML DOM事件对象 搜索框实例 Title .gray{ color:gray; } .black{ color:black; } function Enter(){ var id= document.getElementById(\"tip\") id.className = 'black'; if(id.value=='请输入关键字'||id.value.trim()==''){ id.value = '' } } function Leave(){ var id= document.getElementById(\"tip\") var val = id.value; if(val.length==0||id.value.trim()==''){ id.value = '请输入关键字' id.className = 'gray'; }else{ id.className = 'black'; } } 跑马灯实例 欢迎blue shit莅临指导&nbsp;&nbsp; function Go(){ var content = document.title; var firstChar = content.charAt(0) var sub = content.substring(1,content.length) document.title = sub + firstChar; } setInterval('Go()',1000); "},"04-前端/09-Web开发 - jQuery.html":{"url":"04-前端/09-Web开发 - jQuery.html","title":"Web开发 - jQuery","keywords":"","body":"Web开发之路 - jQuery 介绍 🍀 为了使写更少的代码完成更多的功能 , JavaScript (helper) 库应运而生 , 这些JavaScript库常被成为JavaScript框架 广受欢迎的JavaScript框架如下 : jQuery Prototype MooTools jQuery是目前最受欢迎的JavaScript框架 , jQuery库包含以下功能 : HTML元素选取 HTML元素操作 CSS操作 HTML事件函数 JavaScript特效和动画 HTML DOM 遍历和修改 AJAX Utilities 除此之外 , jQuery还提供了大量的插件 , jQuery受欢迎与其兼容性是密不可分的 下载jQuery http://jquery.com/download/ jQuery库是一个JavaScript文件 , 所以可以直接使用 标签引用 也可通过CDN (内容分发网络) 进行引用 $符号 🍀 $是jQuery符号 , jQuery把所有功能全部封装在一个全局变量jQuery中 , 而$也是一个合法的变量名 , 它是变量jQuery的别名 window.jQuery; // jQuery(selector, context) window.$; // jQuery(selector, context) $ === jQuery; // true typeof($); // 'function' $本质上就是一个函数 , 但是函数也是对象 , 于是$除了可以直接调用外 , 也可以有很多其他属性 PS : 如果$变量被占用了 , 那么我们就只能使用jQuery这个变量了 jQuery对象 jQuery对象就是通过jQuery包装DOM对象后产生的对象 , jQuery对象是jQuery独有的 , 如果一个对象是jQuery对象 , 那么它就可以使用jQuery里面的方法 , 如 : $(\"test\").html(); 约定 : 如果获取的是jQuery对象 , 那么要在变量前面加上$ var $variable = jQuery对象 var variable = DOM对象 PS : 虽然jQuery对象是包含DOM对象后产生的 , 但是jQuery无法使用DOM对象的任何方法 , 同理DOM对象也不能使用jQuery里的方法 jQuery语法 语法 : $(selector).action() 美元符号定义 jQuery 选择符(selector)\"查询\"和\"查找\" HTML 元素 jQuery 的 action() 执行对元素的操作 jQuery查找 🍀 选择器 🍀 ID $('#test') // id=\"test\" Class $(\".c1\") // class=\"c1\" 标签 $('p') // 所有p标签 组合 $('a') $('.c2') $('a,.c2,#i10') 层级 $(\".outer div\") //后代,子子孙孙 $(\".outer>div\") //子元素 $(\".outer+div\") //相邻,同一父元素 $(\".outer~div\") //包含 基本 :first //第一行 :last //最后一行 :eq() //匹配一个给定索引值的元素 属性 $('[Lyon]') //具有Lyon属性的所有标签 $('[Lyon=\"123\"]') //Lyon属性等于123的标签 左侧菜单实例 left_menu function show(self){ //console.log($(self).text()) $(self).next().removeClass(\"hide\") $(self).parent().siblings().children(\".con\").addClass(\"hide\") } .menu{ height: 500px; width: 30%; background-color: gainsboro; float: left; } .content{ height: 500px; width: 70%; background-color: rebeccapurple; float: left; } .title{ line-height: 50px; background-color: #425a66; color: forestgreen;} .hide{ display: none; } 菜单一 111 111 111 菜单二 111 111 111 菜单三 111 111 111 筛选器 🍀 $('#i1').next() // 下一个同级标签 $('#i1').nextAll() // 下面的所有同级标签 $('#i1').nextUntil('#ii1') // 往下找,直到找到为止 $('#i1').prev() // 上一个同级标签 $('#i1').prevAll() // 上面所有同级标签 $('#i1').prevUntil('#ii1') // 往上找,直到找到为止 $('#i1').parent() // 父级标签 $('#i1').parents() // 所有父级标签 $('#i1').parentsUntil() // 找所有父级标签,直到找到为止 $('#i1').children() // 找子标签 $('#i1').siblings() // 找兄弟标签 $('#i1').find() // 搜索指定的元素 $('li:eq(1)') // li标签中索引为1的元素 $('li').eq(1) // 索引为1的li标签 first() // 匹配找到的第一个元素 last() // 匹配找到的最后一个元素 hasClass(class) // 是否含有指定的类 jQuery操作 🍀 属性 🍀 // 专门用于做自定义属性 $(..).attr('n') // 查找属性 $(..).attr('n','v') // 设置属性 $(..).removeAttr('n') // 删除属性 // 专门用于chekbox,radio $(..).prop('checked') $(..).prop('checked', true) CSS 🍀 $('t1').css('样式名称', '样式值') //位置 $(window).scrollTop() // 获取 $(window).scrollTop(0) // 设置 scrollLeft([val]) offset().left // 指定标签在html中的坐标 offset().top // 指定标签在html中的坐标 position() // 指定标签相对父标签(relative)标签的坐标 $('i1').height() // 获取标签的高度 纯高度 $('i1').innerHeight() // 获取边框 + 纯高度 + ？ $('i1').outerHeight() // 获取边框 + 纯高度 + ？ $('i1').outerHeight(true) // 获取边框 + 纯高度 + ？ 文本操作 🍀 $(..).text() // 获取文本内容 $(..).text(\"1\") // 设置文本内容 $(..).html() // 获取html内容 $(..).html(\"1\") // 设置html内容 $(..).val() // 获取值 $(..).val(..) // 设置值 文档处理 🍀 // 内部插入 $(\"#c1\").append(\"hello\") // 追加到内容后面 $(\"p\").appendTo(\"div\") // 追加到指定位置后面 prepend() // 追加到内容前面 prependTo() // 追加到指定位置前面 // 外部插入 before() // 插入到元素之前 insertBefore() // 插入到指定元素之前 after // 插入到元素之后 insertAfter() // 插入到指定元素之后 replaceWith() // 替换 remove() // 删除 empty() // 匹配空元素 clone() // 克隆 事件 🍀 jQuery： $('.c1').click() $('.c1').bind('click',function(){}) $('.c1').unbind('click',function(){}) $('.c').delegate('a', 'click', function(){}) $('.c').undelegate('a', 'click', function(){}) $('.c1').on('click', function(){}) $('.c1').off('click', function(){}) // 阻止事件发生 return false // 当页面框架加载完成之后，自动执行 $(function(){ $(...) }) 拖动面板实例 标题 内容 $(function(){ // 页面加载完成之后自动执行 $('#title').mouseover(function(){ $(this).css('cursor','move'); }).mousedown(function(e){ //console.log($(this).offset()); var _event = e || window.event; // 原始鼠标横纵坐标位置 var ord_x = _event.clientX; var ord_y = _event.clientY; var parent_left = $(this).parent().offset().left; var parent_top = $(this).parent().offset().top; $(this).bind('mousemove', function(e){ var _new_event = e || window.event; var new_x = _new_event.clientX; var new_y = _new_event.clientY; var x = parent_left + (new_x - ord_x); var y = parent_top + (new_y - ord_y); $(this).parent().css('left',x+'px'); $(this).parent().css('top',y+'px'); }) }).mouseup(function(){ $(this).unbind('mousemove'); }); }) 效果 🍀 // 基本 show([s,[e],[fn]]) hide([s,[e],[fn]]) toggle([s],[e],[fn]) // 滑动 slideDown([s],[e],[fn]) slideUp([s,[e],[fn]]) slideToggle([s],[e],[fn]) // 淡入淡出 fadeIn([s],[e],[fn]) fadeOut([s],[e],[fn]) fadeTo([[s],o,[e],[fn]]) fadeToggle([s,[e],[fn]]) // 自定义 animate(p,[s],[e],[fn])1.8* stop([c],[j])1.7* delay(d,[q]) finish([queue])1.9+ // 设置 jQuery.fx.off jQuery.fx.interval 回调函数实例 Title $(document).ready(function(){ $(\"button\").click(function(){ $(\"p\").hide(1000,function(){ alert('动画结束') }) //$(\"p\").css('color','red').slideUp(1000).slideDown(2000) }) }); 隐藏 helloworld helloworld helloworld 扩展 🍀 jQuery.fn.extend(object) jQuery.extend(object) 更多详细内容 jQuery API中文文档 "},"04-前端/11-Web开发 - Ajax.html":{"url":"04-前端/11-Web开发 - Ajax.html","title":"Web开发 - Ajax","keywords":"","body":"Web开发之路 - Ajax 介绍 🍀 Ajax : Asynchronous JavaScript and XML , 意思就是用JavaScript执行异步网络请求 Ajax是一种用于创建快速动态网页的技术 , 通过在后台与服务器进行少量数据交换 , Ajax可以使网页实现异步更新 , 这意味着可以在不重新加载整个网页的情况下 , 对网页的某部分进行更新 2005年 , Google通过其Google Suggest使用Ajax创造出动态性极强的Web界面 , 于是Ajax开始流行起来 Ajax是基于现有的Internet标准 , 并且联合使用它们 : XMLHttpRequest对象 (异步的与服务器交换数据) JavaScript/DOM (信息显示/交互) CSS (给数据定义样式) XML (作为转换数据的格式) AJAX应用程序与浏览器和平台无关的 XHR创建对象 🍀 XMLHttpRequest是Ajax的基础 , 所有现代浏览器均支持XMLHttpRequest对象 (IE5 和 IE6使用ActiveXObject) XMLHttpRequest用于在后台与服务器交换数据 , 也就是完成不重新加载整个页面实现某部分进行更新 创建XMLHttpRequest对象 variable=new XMLHttpRequest(); variable=new ActiveXObject(\"Microsoft.XMLHTTP\"); // IE5和IE6使用ActiveX对象 应对所有浏览器 var xmlhttp; if (window.XMLHttpRequest) { // IE7+, Firefox, Chrome, Opera, Safari 浏览器执行代码 xmlhttp=new XMLHttpRequest(); } else { // IE6, IE5 浏览器执行代码 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); } XHR请求 🍀 将请求发送到服务器 , 使用XMLHttpRequest对象的open() 和 send() 方法 描述 open(method,url,async) 规定请求的类型 , URL 以及是否异步处理请求 method : 请求的类型 , GET 或 POSTurl : 文件在服务器上的位置async : true(异步)或 false(同步) send(string) 将请求发送到服务器 , string : 仅用于 POST 请求 GET 与 POST 与 POST 相比 , GET 更简单也更快 , 并且在大部分情况下都能用 然而 , 在以下情况中 , 请使用 POST 请求 : 无法使用缓存文件 (更新服务器上的文件或数据库) 向服务器发送大量数据 (POST 没有数据量限制) 发送包含未知字符的用户输入时 , POST 比 GET 更稳定也更可靠 GET请求 xmlhttp.open(\"GET\",\"/try/ajax/demo_get.php\",true); // url-服务器上的文件 xmlhttp.send(); POST请求 xmlhttp.open(\"POST\",\"/try/ajax/demo_post.php\",true); xmlhttp.send(); HTML表单POST数据使用setRequestHeader(header,value) 来添加HTTP头 xmlhttp.open(\"POST\",\"/try/ajax/demo_post2.php\",true); xmlhttp.setRequestHeader(\"Content-type\",\"application/x-www-form-urlencoded\"); xmlhttp.send(\"fname=Henry&lname=Ford\"); XHR响应 🍀 获取服务器的响应 , 可以使用XMLHttpRequest对象的responseText或responseXML属性 属性 描述 responseText 获得字符串形式的响应数据 responseXML 获得 XML 形式的响应数据 responseText document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; responseXML xmlDoc=xmlhttp.responseXML; txt=\"\"; x=xmlDoc.getElementsByTagName(\"ARTIST\"); for (i=0;i\"; } document.getElementById(\"myDiv\").innerHTML=txt; XHR readyState 🍀 onreadystatechange 事件 当请求被发送到服务器时 , 我们需要执行一些基于响应的任务 ; 每当readyState改变时 , 就会触发onreadystatechange事件 XMLHttpRequest对象的三个重要属性 : 属性 描述 onreadystatechange 存储函数(或函数名) , 每当 readyState 属性改变时 , 就会调用该函数 readyState 存有 XMLHttpRequest 的状态 , 从 0 到 4 发生变化0: 请求未初始化1: 服务器连接已建立2: 请求已接收3: 请求处理中4: 请求已完成 , 且响应已就绪 status 200 : \"OK\"404 : 未找到页面 在onreadystatechange事件中 , 我们规定当服务器响应已做好被处理的准备时所执行的任务 xmlhttp.onreadystatechange=function() { if (xmlhttp.readyState==4 && xmlhttp.status==200) { document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; } } // onreadystatechange 事件被触发 5 次(0-4),对应着 readyState 的每个变化 回调函数 function myFunction() { loadXMLDoc(\"/try/ajax/ajax_info.txt\",function() { if (xmlhttp.readyState==4 && xmlhttp.status==200) { document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; } }); } Ajax数据库 🍀 Ajax可用来与数据库进行动态通信 , 使用showCustomer()函数 showCustomer() 函数执行以下任务 : 检查是否已选择某个客户 创建 XMLHttpRequest 对象 当服务器响应就绪时执行所创建的函数 把请求发送到服务器上的文件 请注意我们向 URL 添加了一个参数 q (带有输入域中的内容) 实例 function showCustomer(str) { var xmlhttp; if (str==\"\") { document.getElementById(\"txtHint\").innerHTML=\"\"; return; } if (window.XMLHttpRequest) { // IE7+, Firefox, Chrome, Opera, Safari 浏览器执行代码 xmlhttp=new XMLHttpRequest(); } else { // IE6, IE5 浏览器执行代码 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); } xmlhttp.onreadystatechange=function() { if (xmlhttp.readyState==4 && xmlhttp.status==200) { document.getElementById(\"txtHint\").innerHTML=xmlhttp.responseText; } } xmlhttp.open(\"GET\",\"/try/ajax/getcustomer.php?q=\"+str,true); xmlhttp.send(); } jQuery Ajax方法 🍀 在jQuery中我们为我们封装了有关Ajax的操作 , 方法如下 : 方法 描述 $.ajax() 执行异步 AJAX 请求 $.ajaxPrefilter() 在每个请求发送之前且被 $.ajax() 处理之前 , 处理自定义 Ajax 选项或修改已存在选项 $.ajaxSetup() 为将来的 AJAX 请求设置默认值 $.ajaxTransport() 创建处理 Ajax 数据实际传送的对象 $.get() 使用 AJAX 的 HTTP GET 请求从服务器加载数据 $.getJSON() 使用 HTTP GET 请求从服务器加载 JSON 编码的数据 $.getScript() 使用 AJAX 的 HTTP GET 请求从服务器加载并执行 JavaScript $.param() 创建数组或对象的序列化表示形式（可用于 AJAX 请求的 URL 查询字符串） $.post() 使用 AJAX 的 HTTP POST 请求从服务器加载数据 ajaxComplete() 规定 AJAX 请求完成时运行的函数 ajaxError() 规定 AJAX 请求失败时运行的函数 ajaxSend() 规定 AJAX 请求发送之前运行的函数 ajaxStart() 规定第一个 AJAX 请求开始时运行的函数 ajaxStop() 规定所有的 AJAX 请求完成时运行的函数 ajaxSuccess() 规定 AJAX 请求成功完成时运行的函数 load() 从服务器加载数据，并把返回的数据放置到指定的元素中 serialize() 编码表单元素集为字符串以便提交 serializeArray() 编码表单元素集为 names 和 values 的数组 GET var jqxhr = $.get('/path/to/resource', { name: 'Bob Lee', check: 1 }); POST var jqxhr = $.post('/path/to/resource', { name: 'Bob Lee', check: 1 }); JSON var jqxhr = $.getJSON('/path/to/resource', { name: 'Bob Lee', check: 1 }).done(function (data) { // data已经被解析为JSON对象了 }); "},"04-前端/Vue/":{"url":"04-前端/Vue/","title":"Vue","keywords":"","body":"Vue "},"04-前端/Vue/01-Vue - 介绍.html":{"url":"04-前端/Vue/01-Vue - 介绍.html","title":"Vue - 介绍","keywords":"","body":" Vue - 介绍 Vue.js是什么 🍀 Vue 是一套用于构建用户界面的渐进式框架 , 与其它大型框架不同的是 , Vue 被设计为可以自底向上逐层应用 , Vue 的和核心库只关注视图层 , 不仅易于上手 , 还便于与第三方库或已有项目整合 , 另一方面 , 当与现代化的工具链以及各种支持类库结合使用时 , Vue 也完全能够为复杂的单页应用提供驱动 起步 🍀 尝试 Vue.js 最简单的方法是使用 Jsfiddle 的 Vue 工具 , 可以直接从官网提供的 Hello World 例子开始 我们也可以自己创建一个 .html 文件 , 然后在里面引入 Vue : 或者 : 这里我们就自己创建一个 index.html 来完成教程 , 为了将 Javascript 与 HTML 分离 , 我们再创建一个 index.js 存放 Javascript 代码 , 以下不再说明 声明式渲染 🍀 Vue.js 的核心是一个允许采用简介的模板语法来声明式地将数据渲染进 DOM 的系统 : 我们只截取重要代码 , index.html 如下 : {{ message }} 在 index.js 中 var app = new Vue({ el: '#app', data: { message: 'Hello Vue!' } }); 浏览器页面效果如下 : {{ message }} 我们已经成功创建了第一个 Vue 应用 , 看起来这跟渲染一个字符串模板非常类似 , 但是 Vue 在背后做了大量工作 , 现在数据和 DOM 已经被建立了关联 , 所有东西都是响应式的 , 我们可以就在当前页面打开浏览器的 Javascript控制台 , 并修改 app.message 的值 , 你将看到上面的例子会相应地更新 除了文本插值 , 我们还可以像这样来绑定元素特性 : 鼠标悬停几秒钟查看此处动态绑定的提示信息！ var app2 = new Vue({ el: '#app-2', data: { message: '页面加载于 ' + new Date().toLocaleString() } }) 鼠标悬停几秒钟查看此处动态绑定的提示信息 ! 这里我们遇到了一点新东西 , 你看到的 v-bind 特性被成为指令 , 指令带有前缀 v- , 以表示它们是 Vue 提供的特殊特性 , 这与 AngularJS 的 ng- 很相似 . 它们会在渲染的 DOM 上应用特殊的响应式行为 , 在这里 , 该指令的意思是 : \"将这个元素节点的 title 特性和 Vue 实例的 message 属性保持一致\" 如果你再次打开浏览器的 JavaScript 控制台 , 输入 app2.message = '新消息' , 就会再一次看到这个绑定了 title 特性的 HTML 已经进行了更新 条件与循环 🍀 像其他模板语言语法一样 , Vue 当然有条件与循环 现在你看到我了 var app3 = new Vue({ el: '#app-3', data: { seen: true } }) 现在你看到我了 继续在控制台输入 app3.seen = false , 你会发现之前显示的消息消失了 这个例子演示了我们不仅可以把数据绑定到 DOM 文本或特性 , 还可以绑定到 DOM 结构 此外 , Vue 也提供了一个强大的过渡效果系统 , 可以在 Vue 插入/更新/移除元素时自动应用过渡效果 还有其它很多指令 , 每个都有特殊的功能 , 例如 , v-for 指令可以绑定数组的数据来渲染一个项目列表 : {{ todo.text }} var app4 = new Vue({ el: '#app-4', data: { todos: [ { text: '学习 JavaScript' }, { text: '学习 Vue' }, { text: '整个牛项目' } ] } }) {{ todo.text }} 在控制台里 , 输入 app4.todos.push({text:'新项目'}) , 你会发现列表最后添加了一个新项目 处理用户输入 🍀 为了让用户和你的应用进行交互 , 我们可以用 v-on 指令添加一个事件监听器 , 通过它调用在 Vue 实例中定义的方法 : {{ message }} 逆转消息 var app5 = new Vue({ el: '#app-5', data: { message: 'Hello Vue.js!' }, methods: { reverseMessage: function () { this.message = this.message.split('').reverse().join('') } } }) {{ message }} 逆转消息 注意在 reverseMessage 方法中 , 我们更新了应用的状态 , 但没有触碰 DOM ——所有的 DOM 操作都有 Vue 来处理 , 你编写的代码只需要关注逻辑层面即可 Vue 还提供了 v-model 指令 , 它能轻松实现表单输入和应用状态之间的双向绑定 {{ message }} var app6 = new Vue({ el: '#app-6', data: { message: 'Hello Vue!' } }) {{ message }} 组件化应用构建 🍀 组件系统是 Vue 的另一个重要概念 , 因为它是一种抽象 , 允许我们是使用小型 , 独立和通常可复用的组件构建大型应用 , 仔细想想 , 几乎任意类型的应用界面都可以抽象为一个组件树 : 在 Vue 里 , 一个组件本质上是一个拥有预定义选项的一个 Vue 实例 , 在 Vue 中注册组件很简单 : // 定义名为 todo-item 的新组件 Vue.component('todo-item', { template: '这是个待办项' }) 现在你可以用它构建另一个组件模板 : 但是这样会为每个代办项渲染同样的文本 , 为了使其更加酷炫 , 我们应用从父作用域将数据传到子组件才对 , 我们修改一下组件定义 , 使之能够接受一个 prop : Vue.component('todo-item', { // todo-item 组件现在接受一个 // \"prop\"，类似于一个自定义特性。 // 这个 prop 名为 todo。 props: ['todo'], template: '{{ todo.text }}' }) 现在 , 我们可以使用 v-bind 指令将代办项传到循环输出的每个组件中 : Vue.component('todo-item', { props: ['todo'], template: '{{ todo.text }}' }) var app7 = new Vue({ el: '#app-7', data: { groceryList: [ { id: 0, text: '蔬菜' }, { id: 1, text: '奶酪' }, { id: 2, text: '随便其它什么人吃的东西' } ] } }) 尽管这只是一个刻意设计的例子 , 但是我们已经设法将应用分割成两个更小的单元 , 子单元通过 prop 接口与父单元进行了良好的解藕 , 我们现在可以进一步改进 组件 , 提供更为复杂的模板和逻辑 , 而不会影响到父单元 在一个大型应用中 , 有必要将整个应用程序划分为组件 , 以使开发更易管理 , 在后续教程中我们将详述组件 , 不过这里有一个假想的例子 , 以展示使用了组件的应用模板是什么样的 : 与自定义元素的关系 🍀 你可能已经注意到 Vue 组件非常类似于自定义元素——它是 Web 组件规范的一部分 , 这是因为 Vue 的组件语法部分参考了该规范 , 例如 Vue 组件实现了 Slot API 与 is 特性 , 但是 , 还是有几个关键差别 : Web 组件规范仍然处于草案阶段 , 并且未被所有浏览器原生实现 , 相比之下 , Vue 组件不需要任何 polyfill , 并且在所有支持的浏览器 (IE9 及更高版本) 之下表现一致 , 必要时 , Vue 组件也可以包装于原生自定义元素之内 Vue 组件提供了纯自定义元素所不具备的一些重要功能 , 最突出的是跨组件数据流 , 自定义事件通信以及构建工具集成 下一步 🍀 我们刚才简单介绍了 Vue 核心最基本的功能——本教程的其余部分将涵盖这些功能以及其它高级功能更详细的细节 , 所以请务必读完整个教程 ! var app = new Vue({ el: '#app', data: { message: 'Hello Vue!' } }); var app2 = new Vue({ el: '#app-2', data: { message: '页面加载于 ' + new Date().toLocaleString() } }); var app3 = new Vue({ el: '#app-3', data: { seen: true } }); var app4 = new Vue({ el: '#app-4', data: { todos: [ {text: '学习 JavaScript'}, {text: '学习 Vue'}, {text: '整个牛项目'} ] } }); var app5 = new Vue({ el: '#app-5', data: { message: 'Hello Vue.js!' }, methods: { reverseMessage: function () { this.message = this.message.split('').reverse().join('') } } }); var app6 = new Vue({ el: '#app-6', data: { message: 'Hello Vue!' } }); Vue.component('todo-item', { props: ['todo'], template: '{{ todo.text }}' }); var app7 = new Vue({ el: '#app-7', data: { groceryList: [ {id: 0, text: '蔬菜'}, {id: 1, text: '奶酪'}, {id: 2, text: '随便其它什么人吃的东西'} ] } }); "},"04-前端/Vue/02-Vue - 实例.html":{"url":"04-前端/Vue/02-Vue - 实例.html","title":"Vue - 实例","keywords":"","body":"Vue - 实例 创建一个Vue实例 🍀 每个 Vue 应用都是通过用 Vue 函数创建一个新的 Vue 实例开始的 : var vm = new Vue({ // 选项 }) 虽然没有完全遵循 MVVM 模型 , 但是 Vue 的设计也受到了它的启发 , 因此在文档中经常会使用 vm (ViewModel 的缩写) 这个变量名表示 Vue 实例 当创建一个 Vue 实例时 , 你可以传入一个选项对象 , 这篇教程主要描述的就是如何使用这些选项来创建你想要的行为 , 作为参考 , 你也可以在 API 文档中浏览完整的选项列表 一个 Vue 应用由一个通过 new Vue 创建的根 Vue 实例 , 以及可选的嵌套的 , 可复用的组件树组成 , 举个例子 , 一个 todo 应用的组件树可以是这样的 : 根实例 └─ TodoList ├─ TodoItem │ ├─ DeleteTodoButton │ └─ EditTodoButton └─ TodoListFooter ├─ ClearTodosButton └─ TodoListStatistics 我们会在稍后的组件系统章节具体展开 , 不过现在 , 你只需要明白所有的 Vue 组件都是 Vue 实例 , 并且接受相同的选项对象 (一些根实例特有的选项除外) 数据与方法 🍀 当一个 Vue 实例被创建时 , 它向 Vue 的响应式系统中加入了其 data 对象中能找到的所有的属性 , 当这些属性的值发生改变时 , 视图将会产生 \"响应\" , 即匹配更新为新的值 // 我们的数据对象 var data = { a: 1 } // 该对象被加入到一个 Vue 实例中 var vm = new Vue({ data: data }) // 获得这个实例上的属性 // 返回源数据中对应的字段 vm.a == data.a // => true // 设置属性也会影响到原始数据 vm.a = 2 data.a // => 2 // 反之亦然 data.a = 3 vm.a // => 3 当这些数据改变时 , 视图会进行重新渲染 , 值得注意的是只有当实例被创建时 data 中存在的属性才是响应式的 , 也就是说如果你添加一个新的属性 , 比如 : vm.b = 'hi' 那么对 b 的改动将不会触发任何视图的更新 , 如果你知道你会在晚些时候需要一个属性 , 但是一开始它为空或者不存在 , 那么你仅需要设置一些初始值 , 比如 : data: { newTodoText: '', visitCount: 0, hideCompletedTodos: false, todos: [], error: null } 这里唯一的例外是使用 Object.freeze() , 这会组织修改现有的属性 , 也意味着响应系统无法再追踪变化 var obj = { foo: 'bar' } Object.freeze(obj) new Vue({ el: '#app', data: obj }) {{ foo }} Change it 除了数据属性 , Vue 实例还暴露了一些有用的实例属性与方法 , 它们都有前缀 $ , 以便与用户定义的属性区分开来 , 例如 : var data = { a: 1 } var vm = new Vue({ el: '#example', data: data }) vm.$data === data // => true vm.$el === document.getElementById('example') // => true // $watch 是一个实例方法 vm.$watch('a', function (newValue, oldValue) { // 这个回调将在 `vm.a` 改变后调用 }) 以后你可以在 API 参考中查阅到完整的实例属性和方法的列表 实例生命周期钩子 🍀 每个 Vue 实例在被创建时都要经过一系列的初始化过程——例如 , 需要设置数据监听 , 编译模板 , 将实例挂载到 DOM 并在数据变化时更新 DOM 等 , 同时在这个过程中也会运行一些叫做生命周期钩子的函数 , 这给了用户在不同阶段添加自己的代码的机会 比如 created 钩子可以用来在一个实例被创建之后执行代码 : new Vue({ data: { a: 1 }, created: function () { // `this` 指向 vm 实例 console.log('a is: ' + this.a) } }) // => \"a is: 1\" 也有一些其它的钩子 , 在实例生命周期的不同阶段被调用 , 如 mounted , updated 和destroyed , 生命周期钩子的 this 上下文指向调用它的 Vue 实例 不要在选项属性或回调上使用箭头函数 , 比如created: () => console.log(this.a) 或vm.$watch('a', newValue => this.myMethod()) , 因为箭头函数是和父级上下文绑定在一起的 , this 不会是如你所预期的 Vue 实例 , 经常导致Uncaught TypeError: Cannot read property of undefined 或Uncaught TypeError: this.myMethod is not a function 之类的错误 生命周期图示 🍀 下图展示了实例的生命周期 , 你不需要立马弄明白所有的东西 , 不过随着你的不断学习和使用，它的参考价值会越来越高 "},"04-前端/Vue/03-Vue - 模板语法.html":{"url":"04-前端/Vue/03-Vue - 模板语法.html","title":"Vue - 模板语法","keywords":"","body":"Vue - 模板语法 介绍 🍀 Vue.js 使用了基于 HTML 的模板语法 , 允许开发者声明式地将 DOM 绑定至底层 Vue 实例的数据 , 所有 Vue.js 的模板都是合法的 HTML , 所以能被遵循规范的浏览器和 HTML 解析器解析 在底层的实现上 , Vue 将模板编译成虚拟 DOM 渲染函数 , 结合响应系统 , Vue 能够智能地计算出最少需要重新渲染多少组件 , 并把 DOM 操作次数减到最少 如果你熟悉虚拟 DOM 并且偏爱 JavaScript 的原始力量 , 你也可以不用模板 , 直接写渲染 (render) 函数 , 使用可选的 JSX 语法。 插值 🍀 文本 🍀 数据绑定最常见的形式就是使用 Mustache 语法 (双大括号) 的文本插值 : Message: {{ msg }} Mustache 标签将会被替代为对应数据对象上 msg 属性的值 , 无论何时 , 绑定的数据对象上 msg 属性发生了改变 , 插值处的内容都会更新 通过使用 v-once 指令 , 你也能执行一次性地插值 , 当数据改变时 , 插值处的内容不会更新 , 但请留心这会影响到该节点上的其它数据绑定 : 这个将不会改变: {{ msg }} 原始HTML 🍀 双大括号会将数据解释为普通文本 , 而非 HTML 代码 , 为了输出真正的 HTML , 你需要使用 v-html 指令 : Using mustaches: {{ rawHtml }} Using v-html directive: Using mustaches: Using v-html directive: 这个 span 的内容将会被替换成为属性值 rawHtml , 直接作为 HTML——会忽略解析属性值中的数据绑定 , 注意 , 你不能使用 v-html 来复合局部模板 , 因为 Vue 不是基于字符串的模板引擎 ; 反之 , 对于用户界面 (UI) , 组件更适合作为可重用和可组合的基本单位 你的站点上动态渲染的任意 HTML 可能会非常危险 , 因为它很容易导致 XSS 攻击。请只对可信内容使用 HTML 插值 , 绝不要对用户提供的内容使用插值 特性 🍀 Mustache 语法不能作用在 HTML 特性上 , 遇到这种情况应该使用 v-bind 指令 : 在布尔特性的情况下 , 它们的存在即暗示为 true , v-bind 工作起来略有不同 , 在这个例子中 : Button 如果 isButtonDisabled 的值是 null、undefined 或 false , 则 disabled 特性甚至不会被包含在渲染出来的 元素中 使用JavaScript表达式 🍀 迄今为止 , 在我们的模板中 , 我们一直都只绑定简单的属性键值 ; 但实际上 , 对于所有的数据绑定 , Vue.js 都提供了完全的 JavaScript 表达式支持 {{ number + 1 }} {{ ok ? 'YES' : 'NO' }} {{ message.split('').reverse().join('') }} 这些表达式会在所属 Vue 实例的数据作用域下作为 JavaScript 被解析 , 有个限制就是 , 每个绑定都只能包含单个表达式 , 所以下面的例子都不会生效 {{ var a = 1 }} {{ if (ok) { return message } }} 模板表达式都被放在沙盒中 , 只能访问全局变量的一个白名单 , 如 Math 和 Date , 你不应该在模板表达式中试图访问用户定义的全局变量 指令 🍀 指令 (Directives) 是带有 v- 前缀的特殊特性 , 指令特性的值预期是单个 JavaScript 表达式(v-for 是例外情况)。指令的职责是 , 当表达式的值改变时 , 将其产生的连带影响 , 响应式地作用于 DOM , 如 : 现在你看到我了 这里 , v-if 指令将根据表达式 seen 的值的真假来插入/移除 元素 参数 🍀 一些指令能够接收一个“参数” , 在指令名称之后以冒号表示 , 例如 , v-bind 指令可以用于响应式地更新 HTML 特性 : ... 在这里 href 是参数 , 告知 v-bind 指令将该元素的 href 特性与表达式 url 的值绑定 另一个例子是 v-on 指令 , 它用于监听 DOM 事件 : ... 在这里参数是监听的事件名。我们也会更详细地讨论事件处理。 修饰符 🍀 修饰符 (Modifiers) 是以半角句号 . 指明的特殊后缀 , 用于指出一个指令应该以特殊方式绑定 , 例如 , .prevent 修饰符告诉 v-on 指令对于触发的事件调用 event.preventDefault() : ... 在接下来对 v-on 和 v-for 等功能的探索中 , 你会看到修饰符的其它例子 缩写 🍀 v- 前缀作为一种视觉提示 , 用来识别模板中 Vue 特定的特性 ; 当你在使用 Vue.js 为现有标签添加动态行为 (dynamic behavior) 时 , v- 前缀很有帮助 , 然而 , 对于一些频繁用到的指令来说 , 就会感到使用繁琐 . 同时 , 在构建由 Vue.js 管理所有模板的单页面应用程序 (SPA - single page application) 时 , v- 前缀也变得没那么重要了 , 因此 , Vue.js 为 v-bind 和 v-on 这两个最常用的指令 , 提供了特定简写 : v-bind 缩写如下 : ... ... v-on 缩写如下 : ... ... 它们看起来可能与普通的 HTML 略有不同 , 但 : 与 @ 对于特性名来说都是合法字符 , 在所有支持 Vue.js 的浏览器都能被正确地解析 , 而且 , 它们不会出现在最终渲染的标记中 . 缩写语法是完全可选的 , 但随着你更深入地了解它们的作用 , 你会庆幸拥有它们 "},"04-前端/Vue/04-Vue - 计算属性和侦听器.html":{"url":"04-前端/Vue/04-Vue - 计算属性和侦听器.html","title":"Vue - 计算属性和侦听器","keywords":"","body":"Vue - 计算属性和侦听器 计算属性 🍀 模板内的表达式非常便利 , 但是设计它们的初衷是用于简单运算的 , 在模板中放入太多的逻辑会让模板过重且难以维护 , 例如 : {{ message.split('').reverse().join('') }} 在这个地方 , 模板不再是简单的声明式逻辑 , 你必须看一段时间才能意识到 , 这里是想要显示变量 message 的翻转字符串 , 当你想要在模板中多次引用此处的翻转字符串时 , 就会更加难以处理 所以 , 对于任何复杂逻辑 , 你都应当使用计算属性 基础例子 🍀 Original message: \"{{ message }}\" Computed reversed message: \"{{ reversedMessage }}\" var vm = new Vue({ el: '#example', data: { message: 'Hello' }, computed: { // 计算属性的 getter reversedMessage: function () { // `this` 指向 vm 实例 return this.message.split('').reverse().join('') } } }) 结果 : Original message: \"{{ message }}\" Computed reversed message: \"{{ reversedMessage }}\" 这里我们声明了一个计算属性 reversedMessage , 我们提供的函数将用作属性 vm.reversedMessage 的 getter 函数 : console.log(vm.reversedMessage) // => 'olleH' vm.message = 'Goodbye' console.log(vm.reversedMessage) // => 'eybdooG' 你可以打开浏览器的控制台 , 自行修改例子中的 vm , vm.reversedMessage 的值始终取决于 vm.message 的值 你可以像绑定普通属性一样在模板中绑定计算属性 , Vue 知道 vm.reversedMessage 依赖于 vm.message , 因此当 vm.message 发生改变时 , 所有依赖 vm.reversedMessage 的绑定也会更新 , 而且最妙的是我们已经以声明的方式创建了这种依赖关系 : 计算属性的 getter 函数是没有副作用 (side effect) 的 , 这使它更易于测试和理解 计算属性缓存与方法 🍀 你可能已经注意到我们可以通过在表达式中调用方法来达到同样的效果 : Reversed message: \"{{ reversedMessage() }}\" // 在组件中 methods: { reversedMessage: function () { return this.message.split('').reverse().join('') } } 我们可以将同一函数定义为一个方法而不是一个计算属性 , 两种方式的最终结果确实是完全相同的 , 然而 , 不同的是计算属性是基于它们的依赖进行缓存的 , 计算属性只有在它的相关依赖发生改变时才会重新求值 , 这就意味着只要 message 还没有发生改变 , 多次访问 reversedMessage 计算属性会立即返回之前的计算结果 , 而不必再次执行函数 这也同样意味着下面的计算属性将不再更新 , 因为 Date.now() 不是响应式依赖 : computed: { now: function () { return Date.now() } } 相比之下 , 每当触发重新渲染时 , 调用方法将总会再次执行函数 我们为什么需要缓存 ? 假设我们有一个性能开销比较大的计算属性 A , 它需要遍历一个巨大的数组并做大量的计算 , 然后我们可能有其他的计算属性依赖于 A , 如果没有缓存 , 我们将不可避免的多次执行 A 的 getter ! 如果你不希望有缓存 , 可以使用方法来替代计算属性 计算属性和侦听属性 🍀 Vue 提供了一种更通用的方式来观察和响应 Vue 实例上的数据变动 : 侦听属性 ; 当你有一些数据需要随着其它数据变动而变动时 , 你很容易滥用 watch——特别是如果你之前使用过 AngularJS , 然而 , 通常更好的做法是使用计算属性而不是命令式的 watch 回调 , 看看如下例子 : {{ fullName }} // 侦听器版本 var vm = new Vue({ el: '#demo', data: { firstName: 'Foo', lastName: 'Bar', fullName: 'Foo Bar' }, watch: { firstName: function (val) { this.fullName = val + ' ' + this.lastName }, lastName: function (val) { this.fullName = this.firstName + ' ' + val } } }) 上面代码是命令式且重复的 , 将它与计算属性的版本进行比较 : // 计算属性版本 var vm = new Vue({ el: '#demo', data: { firstName: 'Foo', lastName: 'Bar' }, computed: { fullName: function () { return this.firstName + ' ' + this.lastName } } }) 明显 , 计算属性版本更加简洁 计算属性的setter 🍀 计算属性默认只有 getter , 不过在需要时你也可以提供一个 setter : // ... computed: { fullName: { // getter get: function () { return this.firstName + ' ' + this.lastName }, // setter set: function (newValue) { var names = newValue.split(' ') this.firstName = names[0] this.lastName = names[names.length - 1] } } } // ... 现在再运行 vm.fullName = 'John Doe' 时 , setter 会被调用 , vm.firstName 和 vm.lastName 也会相应地被更新 侦听器 🍀 虽然计算属性在大多数情况下更合适 , 但有时也需要一个自定义的侦听器 , 这就是为什么 Vue 通过 watch 选项提供了一个更通用的方法 , 来响应数据的变化 , 当需要在数据变化时执行异步或开销较大的操作时 , 这个方式是最有用的 例如 : Ask a yes/no question: {{ answer }} // 因为 AJAX 库和通用工具的生态已经相当丰富, Vue 核心代码没有重复 // 提供这些功能以保持精简, 这也可以让你自由选择自己更熟悉的工具 var watchExampleVM = new Vue({ el: '#watch-example', data: { question: '', answer: 'I cannot give you an answer until you ask a question!' }, watch: { // 如果 `question` 发生改变 , 这个函数就会运行 question: function (newQuestion, oldQuestion) { this.answer = 'Waiting for you to stop typing...' this.debouncedGetAnswer() } }, created: function () { // `_.debounce` 是一个通过 Lodash 限制操作频率的函数 // 在这个例子中, 我们希望限制访问 yesno.wtf/api 的频率 // AJAX 请求直到用户输入完毕才会发出. 更多关于 // `_.debounce` 函数, 请参考 : https://lodash.com/docs#debounce this.debouncedGetAnswer = _.debounce(this.getAnswer, 500) }, methods: { getAnswer: function () { if (this.question.indexOf('?') === -1) { this.answer = 'Questions usually contain a question mark. ;-)' return } this.answer = 'Thinking...' var vm = this axios.get('https://yesno.wtf/api') .then(function (response) { vm.answer = _.capitalize(response.data.answer) }) .catch(function (error) { vm.answer = 'Error! Could not reach the API. ' + error }) } } }) 结果 : Ask a yes/no question: {{ answer }} 在这个示例中 , 使用 watch 选项允许我们执行异步操作 (访问一个 API) , 限制我们执行该操作的频率 , 并在我们得到最终结果前 , 设置中间状态。这些都是计算属性无法做到的。 除了 watch 选项之外 , 您还可以使用命令式的 vm.$watch API var watchExampleVM = new Vue({ el: '#watch-example', data: { question: '', answer: 'I cannot give you an answer until you ask a question!' }, watch: { // 如果 `question` 发生改变 , 这个函数就会运行 question: function (newQuestion, oldQuestion) { this.answer = 'Waiting for you to stop typing...' this.debouncedGetAnswer() } }, created: function () { this.debouncedGetAnswer = _.debounce(this.getAnswer, 500) }, methods: { getAnswer: function () { if (this.question.indexOf('?') === -1) { this.answer = 'Questions usually contain a question mark. ;-)' return } this.answer = 'Thinking...' var vm = this axios.get('https://yesno.wtf/api') .then(function (response) { vm.answer = _.capitalize(response.data.answer) }) .catch(function (error) { vm.answer = 'Error! Could not reach the API. ' + error }) } } }); var vm = new Vue({ el: '#example', data: { message: 'Hello' }, computed: { // 计算属性的 getter reversedMessage: function () { // `this` 指向 vm 实例 return this.message.split('').reverse().join('') } } }); "},"04-前端/Vue/05-Vue - Class与Style 绑定.html":{"url":"04-前端/Vue/05-Vue - Class与Style 绑定.html","title":"Vue - Class与Style 绑定","keywords":"","body":"Vue - Class与Style绑定 介绍 🍀 操作元素的 class 列表和内联样式是数据绑定的一个常见需求 , 因为它们都是属性 , 所以我们可以用 v-bind 处理它们 : 只需要通过表达式计算出字符串结果即可 , 不过 , 字符串拼接麻烦且易错 , 因此 , 在将 v-bind 用于 class 和 style 时 , Vue.js 做了专门的增强 , 表达式结果的类型除了字符串之外 , 还可以是对象或数组 绑定Class 🍀 对象语法 🍀 我们可以传给 v-bind:class 一个对象 , 以动态地切换 class : 上面的语法表示 active 这个 class 存在与否将取决于数据属性 isActive 的 truthiness 你可以在对象中传入更多属性来动态切换多个 class , 此外 , v-bind:class 指令也可以与普通的 class 属性共存 , 当有如下模板 : 和如下 data : data: { isActive: true, hasError: false } 结果渲染为 : 当 isActive 或者 hasError 变化时 , class 列表将相应地更新 , 例如 , 如果 hasError的值为 true , class 列表将变为 \"static active text-danger\" 绑定的数据对象不必内联定义在模板里 : data: { classObject: { active: true, 'text-danger': false } } 渲染的结果和上面一样 , 我们也可以在这里绑定一个返回对象的计算属性 , 这是一个常用且强大的模式 : data: { isActive: true, error: null }, computed: { classObject: function () { return { active: this.isActive && !this.error, 'text-danger': this.error && this.error.type === 'fatal' } } } 数组语法 🍀 我们可以把一个数组传给 v-bind:class , 以应用一个 class 列表 : data: { activeClass: 'active', errorClass: 'text-danger' } 渲染为 : 如果你也想根据条件切换列表中的 class , 可以用三元表达式 : 这样写将始终添加 errorClass , 但是只有在 isActive 是 truthy 时才添加 activeClass 不过 , 当有多个条件 class 时这样写有些繁琐 , 所以在数组语法中也可以使用对象语法 : 组件上使用 🍀 这个章节假设你已经对 Vue 组件有一定的了解 , 当然你也可以先跳过这里 , 稍后再回过头来看 当在一个自定义组件上使用 class 属性时 , 这些类将被添加到该组件的根元素上面 , 这个元素上已经存在的类不会被覆盖 例如 , 如果你声明了这个组件 : Vue.component('my-component', { template: 'Hi' }) 然后在使用它的时候添加一些 class : HTML 将被渲染为: Hi 对于带数据绑定 class 也同样适用 : 当 isActive 为 truthy 时 , HTML 将被渲染成为 : Hi 绑定内联样式 🍀 对象语法 🍀 v-bind:style 的对象语法十分直观——看着非常像 CSS , 但其实是一个 JavaScript 对象。CSS 属性名可以用驼峰式 (camelCase) 或短横线分隔 (kebab-case , 记得用单引号括起来) 来命名 : data: { activeColor: 'red', fontSize: 30 } 直接绑定到一个样式对象通常更好 , 这会让模板更清晰 : data: { styleObject: { color: 'red', fontSize: '13px' } } 同样的 , 对象语法常常结合返回对象的计算属性使用 数组语法 🍀 v-bind:style 的数组语法可以将多个样式对象应用到同一个元素上 : 自动添加前缀 🍀 当 v-bind:style 使用需要添加浏览器引擎前缀的 CSS 属性时 , 如 transform , Vue.js 会自动侦测并添加相应的前缀 多重值 🍀 2.3.0+ 从 2.3.0 起你可以为 style 绑定中的属性提供一个包含多个值的数组 , 常用于提供多个带前缀的值 , 例如 : 这样写只会渲染数组中最后一个被浏览器支持的值。在本例中 , 如果浏览器支持不带浏览器前缀的 flexbox , 那么就只会渲染 display: flex "},"04-前端/Vue/06-Vue - 条件渲染.html":{"url":"04-前端/Vue/06-Vue - 条件渲染.html","title":"Vue - 条件渲染","keywords":"","body":"Vue - 条件渲染 v-if 🍀 在字符串模板中 , 比如 Handlebars , 我们得像这样写一个条件块 : {{#if ok}} Yes {{/if}} 在 Vue 中 , 我们使用 v-if 指令实现同样的功能 : Yes 也可以用 v-else 添加一个“else 块” : Yes No 在template元素上使用v-if条件渲染分组 🍀 因为 v-if 是一个指令 , 所以必须将它添加到一个元素上 , 但是如果想切换多个元素呢 ? 此时可以把一个 元素当做不可见的包裹元素 , 并在上面使用 v-if , 最终的渲染结果将不包含 元素 Title Paragraph 1 Paragraph 2 v-else 🍀 你可以使用 v-else 指令来表示 v-if 的“else 块” : 0.5\"> Now you see me Now you don't v-else 元素必须紧跟在带 v-if 或者 v-else-if 的元素的后面 , 否则它将不会被识别 v-else-if 🍀 2.1.0 新增 v-else-if , 顾名思义 , 充当 v-if 的“else-if 块” , 可以连续使用 : A B C Not A/B/C 类似于 v-else , v-else-if 也必须紧跟在带 v-if 或者 v-else-if 的元素之后 用key管理可复用的元素 🍀 Vue 会尽可能高效地渲染元素 , 通常会复用已有元素而不是从头开始渲染 , 这么做除了使 Vue 变得非常快之外 , 还有其它一些好处 , 例如 , 如果你允许用户在不同的登录方式之间切换 : Username Email 那么在上面的代码中切换 loginType 将不会清除用户已经输入的内容 , 因为两个模板使用了相同的元素 , 不会被替换掉——仅仅是替换了它的 placeholder 自己动手试一试 , 在输入框中输入一些文本 , 然后按下切换按钮 : Username Email Toggle login type 这样也不总是符合实际需求 , 所以 Vue 为你提供了一种方式来表达“这两个元素是完全独立的 , 不要复用它们” , 只需添加一个具有唯一值的 key 属性即可 : Username Email 现在 , 每次切换时 , 输入框都将被重新渲染 , 请看 : Username Email Toggle login type 注意 , 元素仍然会被高效地复用 , 因为它们没有添加 key 属性 v-show 🍀 另一个用于根据条件展示元素的选项是 v-show 指令 , 用法大致一样 : Hello! 不同的是带有 v-show 的元素始终会被渲染并保留在 DOM 中 , v-show 只是简单地切换元素的 CSS 属性 display 注意 , v-show 不支持 元素 , 也不支持 v-else v-if与v-show 🍀 v-if 是“真正”的条件渲染 , 因为它会确保在切换过程中条件块内的事件监听器和子组件适当地被销毁和重建 v-if 也是惰性的 : 如果在初始渲染时条件为假 , 则什么也不做——直到条件第一次变为真时 , 才会开始渲染条件块 相比之下 , v-show 就简单得多——不管初始条件是什么 , 元素总是会被渲染 , 并且只是简单地基于 CSS 进行切换 一般来说 , v-if 有更高的切换开销 , 而 v-show 有更高的初始渲染开销 , 因此 , 如果需要非常频繁地切换 , 则使用 v-show 较好 ; 如果在运行时条件很少改变 , 则使用 v-if 较好 v-if与v-for一起使用 🍀 当 v-if 与 v-for 一起使用时 , v-for 具有比 v-if 更高的优先级。 new Vue({ el: '#no-key-example', data: { loginType: 'username' }, methods: { toggleLoginType: function () { return this.loginType = this.loginType === 'username' ? 'email' : 'username' } } }); new Vue({ el: '#key-example', data: { loginType: 'username' }, methods: { toggleLoginType: function () { return this.loginType = this.loginType === 'username' ? 'email' : 'username' } } }); "},"04-前端/Vue/07-Vue - 列表渲染.html":{"url":"04-前端/Vue/07-Vue - 列表渲染.html","title":"Vue - 列表渲染","keywords":"","body":"Vue - 列表渲染 v-for迭代列表 🍀 我们用 v-for 指令根据一组数组的选项列表进行渲染 , v-for 指令需要使用 item in items 形式的特殊语法 , items 是源数据数组并且 item 是数组元素迭代的别名 {{ item.message }} var example1 = new Vue({ el: '#example-1', data: { items: [ { message: 'Foo' }, { message: 'Bar' } ] } }) 结果 : {{ item.message }} 在 v-for 块中 , 我们拥有对父作用域属性的完全访问权限 , v-for 还支持一个可选的第二个参数为当前项的索引 {{ parentMessage }} - {{ index }} - {{ item.message }} var example2 = new Vue({ el: '#example-2', data: { parentMessage: 'Parent', items: [ { message: 'Foo' }, { message: 'Bar' } ] } }) 结果 : {{ parentMessage }} - {{ index }} - {{ item.message }} 你也可以用 of 替代 in 作为分隔符 , 因为它是最接近 JavaScript 迭代器的语法 : v-for迭代对象 🍀 你也可以用 v-for 通过一个对象的属性来迭代 {{ value }} new Vue({ el: '#v-for-object', data: { object: { firstName: 'John', lastName: 'Doe', age: 30 } } }) 结果 : {{ value }} 你也可以提供第二个参数为键名 : {{ key }}: {{ value }} : 第三个参数为索引 : {{ index }}. {{ key }}: {{ value }} . : 在遍历对象时 , 是按 Object.keys() 的结果遍历 , 但是不能保证它的结果在不同的 JavaScript 引擎下是一致的 key 🍀 当 Vue.js 用 v-for 正在更新已渲染过的元素列表时 , 它默认用 “就地复用” 策略 , 如果数据项的顺序被改变 , Vue 将不会移动 DOM 元素来匹配数据项的顺序 , 而是简单复用此处每个元素 , 并且确保它在特定索引下显示已被渲染过的每个元素 , 这个类似 Vue 1.x 的 track-by=\"$index\" 这个默认的模式是高效的 , 但是只适用于不依赖子组件状态或临时 DOM 状态 (例如 : 表单输入值) 的列表渲染输出 为了给 Vue 一个提示 , 以便它能跟踪每个节点的身份 , 从而重用和重新排序现有元素 , 你需要为每项提供一个唯一 key 属性 , 理想的 key 值是每项都有的且唯一的 id , 这个特殊的属性相当于 Vue 1.x 的 track-by , 但它的工作方式类似于一个属性 , 所以你需要用 v-bind 来绑定动态值 (在这里使用简写) : 建议尽可能在使用 v-for 时提供 key , 除非遍历输出的 DOM 内容非常简单 , 或者是刻意依赖默认行为以获取性能上的提升 因为它是 Vue 识别节点的一个通用机制 , key 并不与 v-for 特别关联 , key 还具有其他用途 , 我们将在后面的指南中看到其他用途 数组更新检测 🍀 变异方法 🍀 Vue 包含一组观察数组的变异方法 , 所以它们也将会触发视图更新 , 这些方法如下 : push() , 从末尾添加 pop() , 从某位删除 shift() , 从头部添加 unshift() , 从头部删除 splice() , 删除元素 , 删除索引为 1 的元素 : splice(index, 1) sort() , 排序 reverse() , 反转 你可以打开控制台 , 然后用前面例子的 items 数组调用变异方法 : example1.items.push({ message: 'Baz' }) 替换数组 🍀 变异方法 (mutation method) , 顾名思义 , 会改变被这些方法调用的原始数组 , 相比之下 , 也有非变异 (non-mutating method) 方法 , 例如 : filter(), concat() 和 slice() , 这些不会改变原始数组 , 但总是返回一个新数组 , 当使用非变异方法时 , 可以用新数组替换旧数组 : example1.items = example1.items.filter(function (item) { return item.message.match(/Foo/) }) 你可能认为这将导致 Vue 丢弃现有 DOM 并重新渲染整个列表 , 幸运的是 , 事实并非如此 , Vue 为了使得 DOM 元素得到最大范围的重用而实现了一些智能的、启发式的方法 , 所以用一个含有相同元素的数组去替换原来的数组是非常高效的操作 注意事项 🍀 由于 JavaScript 的限制 , Vue 不能检测以下变动的数组 : 当你利用索引直接设置一个项时 , 例如 : vm.items[indexOfItem] = newValue 当你修改数组的长度时 , 例如 : vm.items.length = newLength 举个例子 : var vm = new Vue({ data: { items: ['a', 'b', 'c'] } }) vm.items[1] = 'x' // 不是响应性的 vm.items.length = 2 // 不是响应性的 为了解决第一类问题 , 以下两种方式都可以实现和 vm.items[indexOfItem] = newValue 相同的效果 , 同时也将触发状态更新 : // Vue.set Vue.set(vm.items, indexOfItem, newValue) // Array.prototype.splice vm.items.splice(indexOfItem, 1, newValue) 你也可以使用 vm.$set 实例方法 , 该方法是全局方法 Vue.set 的一个别名 : vm.$set(vm.items, indexOfItem, newValue) 为了解决第二类问题 , 你可以使用 splice : vm.items.splice(newLength) 对象更改检测 🍀 还是由于 JavaScript 的限制 , Vue 不能检测对象属性的添加或删除 : var vm = new Vue({ data: { a: 1 } }) // `vm.a` 现在是响应式的 vm.b = 2 // `vm.b` 不是响应式的 对于已经创建的实例 , Vue 不能动态添加根级别的响应式属性 , 但是 , 可以使用 Vue.set(object, key, value) 方法向嵌套对象添加响应式属性 , 例如 , 对于 : var vm = new Vue({ data: { userProfile: { name: 'Anika' } } }) 你可以添加一个新的 age 属性到嵌套的 userProfile 对象 : Vue.set(vm.userProfile, 'age', 27) 你还可以使用 vm.$set 实例方法 , 它只是全局 Vue.set 的别名 : vm.$set(vm.userProfile, 'age', 27) 有时你可能需要为已有对象赋予多个新属性 , 比如使用 Object.assign() 或 _.extend() , 在这种情况下 , 你应该用两个对象的属性创建一个新的对象。所以 , 如果你想添加新的响应式属性 , 不要像这样 : Object.assign(vm.userProfile, { age: 27, favoriteColor: 'Vue Green' }) 你应该这样做 : vm.userProfile = Object.assign({}, vm.userProfile, { age: 27, favoriteColor: 'Vue Green' }) 显示过滤/排序结果 🍀 有时 , 我们想要显示一个数组的过滤或排序副本 , 而不实际改变或重置原始数据 , 在这种情况下 , 可以创建返回过滤或排序数组的计算属性 例如 : {{ n }} data: { numbers: [ 1, 2, 3, 4, 5 ] }, computed: { evenNumbers: function () { return this.numbers.filter(function (number) { return number % 2 === 0 }) } } 在计算属性不适用的情况下 (例如 , 在嵌套 v-for 循环中) 你可以使用一个 method 方法 : {{ n }} data: { numbers: [ 1, 2, 3, 4, 5 ] }, methods: { even: function (numbers) { return numbers.filter(function (number) { return number % 2 === 0 }) } } v-for取值范围 🍀 v-for 也可以取整数 , 在这种情况下 , 它将重复多次模板 {{ n }} 结果 : template里使用v-for 🍀 类似于 v-if , 你也可以利用带有 v-for 的 渲染多个元素 , 比如 : {{ item.msg }} v-for与v-if 🍀 当它们处于同一节点 , v-for 的优先级比 v-if 更高 , 这意味着 v-if 将分别重复运行于每个 v-for 循环中 , 当你想为仅有的一些项渲染节点时 , 这种优先级的机制会十分有用 , 如下 : {{ todo }} 上面的代码只传递了未完成的 todos 而如果你的目的是有条件地跳过循环的执行 , 那么可以将 v-if 置于外层元素 (或 )上 , 如 : {{ todo }} No todos left! 组件中使用v-for 🍀 在自定义组件里 , 你可以像任何普通元素一样用 v-for 2.2.0+ 的版本里 , 当在组件中使用 v-for 时 , key 现在是必须的 然而 , 任何数据都不会被自动传递到组件里 , 因为组件有自己独立的作用域 , 为了把迭代数据传递到组件里 , 我们要用 props : 不自动将 item 注入到组件里的原因是 , 这会使得组件与 v-for 的运作紧密耦合 , 明确组件数据的来源能够使组件在其他场合重复使用 下面是一个简单的 todo list 的完整例子 : Add a todo Add 注意这里的 is=\"todo-item\" 属性 , 这种做法在使用 DOM 模板时是十分必要的 , 因为在 元素内只有 元素会被看作有效内容。这样做实现的效果与 相同 , 但是可以避开一些潜在的浏览器解析错误。查看 DOM 模板解析说明 来了解更多信息 Vue.component('todo-item', { template: '\\ \\ {{ title }}\\ Remove\\ \\ ', props: ['title'] }) new Vue({ el: '#todo-list-example', data: { newTodoText: '', todos: [ { id: 1, title: 'Do the dishes', }, { id: 2, title: 'Take out the trash', }, { id: 3, title: 'Mow the lawn' } ], nextTodoId: 4 }, methods: { addNewTodo: function () { this.todos.push({ id: this.nextTodoId++, title: this.newTodoText }) this.newTodoText = '' } } }) Add a todo Add Vue.component('todo-item', { template: '\\ \\ \\ Remove\\ \\ ', props: ['title'] }); new Vue({ el: '#todo-list-example', data: { newTodoText: '', todos: [ { id: 1, title: 'Do the dishes', }, { id: 2, title: 'Take out the trash', }, { id: 3, title: 'Mow the lawn' } ], nextTodoId: 4 }, methods: { addNewTodo: function () { this.todos.push({ id: this.nextTodoId++, title: this.newTodoText }) this.newTodoText = '' } } }); new Vue({ el: '#v-for-object', data: { object: { firstName: 'John', lastName: 'Doe', age: 30 } } }); var example2 = new Vue({ el: '#example-2', data: { parentMessage: 'Parent', items: [ { message: 'Foo' }, { message: 'Bar' } ] } }); var example1 = new Vue({ el: '#example-1', data: { items: [ { message: 'Foo' }, { message: 'Bar' } ] } }); "},"04-前端/Vue/08-Vue - 事件处理.html":{"url":"04-前端/Vue/08-Vue - 事件处理.html","title":"Vue - 事件处理","keywords":"","body":"Vue - 事件处理 监听事件 🍀 可以用 v-on 指令监听 DOM 事件 , 并在触发时运行一些 JavaScript 代码 示例 : Add 1 The button above has been clicked {{ counter }} times. var example1 = new Vue({ el: '#example-1', data: { counter: 0 } }) 结果 : Add 1 The button above has been clicked {{ counter }} times. 事件处理 🍀 然而许多事件处理逻辑会更为复杂 , 所以直接把 JavaScript 代码写在 v-on 指令中是不可行的 , 因此 v-on 还可以接收一个需要调用的方法名称 示例 : Greet var example2 = new Vue({ el: '#example-2', data: { name: 'Vue.js' }, // 在 `methods` 对象中定义方法 methods: { greet: function (event) { // `this` 在方法里指向当前 Vue 实例 alert('Hello ' + this.name + '!') // `event` 是原生 DOM 事件 if (event) { alert(event.target.tagName) } } } }) // 也可以用 JavaScript 直接调用方法 example2.greet() // => 'Hello Vue.js!' 结果 : Greet 内联处理 🍀 除了直接绑定到一个方法 , 也可以在内联 JavaScript 语句中调用方法 : Say hi Say what new Vue({ el: '#example-3', methods: { say: function (message) { alert(message) } } }) 结果 : Say hi Say what 有时也需要在内联语句处理器中访问原始的 DOM 事件 , 可以用特殊变量 $event 把它传入方法 : Submit // ... methods: { warn: function (message, event) { // 现在我们可以访问原生事件对象 if (event) event.preventDefault() alert(message) } } 事件修饰符 🍀 在事件处理程序中调用 event.preventDefault() 或 event.stopPropagation() 是非常常见的需求 , 尽管我们可以在方法中轻松实现这点 , 但更好的方式是 : 方法只有纯粹的数据逻辑 , 而不是去处理 DOM 事件细节 为了解决这个问题 , Vue.js 为 v-on 提供了事件修饰符 , 之前提过 , 修饰符是由点开头的指令后缀来表示的 : .stop .prevent .capture .self .once .passive 用法实例如下 : ... ... 使用修饰符时 , 顺序很重要 ; 相应的代码会以同样的顺序产生 , 因此 , 用 v-on:click.prevent.self 会阻止所有的点击 , 而 v-on:click.self.prevent 只会阻止对元素自身的点击 2.1.4 新增 不像其它只能对原生的 DOM 事件起作用的修饰符 , .once 修饰符还能被用到自定义的组件事件上 , 如果你还没有阅读关于组件的文档 , 现在大可不必担心 2.3.0 新增 Vue 还对应 addEventListener 中的 passive 选项提供了 .passive 修饰符 ... 这个 .passive 修饰符尤其能够提升移动端的性能 不要把 .passive 和 .prevent 一起使用 , 因为 .prevent 将会被忽略 , 同时浏览器可能会向你展示一个警告 , 请记住 , .passive 会告诉浏览器你不想阻止事件的默认行为 按键修饰符 🍀 在监听键盘事件时 , 我们经常需要检查常见的键值 , Vue 允许为 v-on 在监听键盘事件时添加按键修饰符 : 记住所有的 keyCode 比较困难 , 所以 Vue 为最常用的按键提供了别名 : 全部的按键别名 : .enter .tab .delete .esc .space .up .down .left .right 可以通过全局 config.keyCodes 对象自定义按键修饰符别名 : // 可以使用 `v-on:keyup.f1` Vue.config.keyCodes.f1 = 112 自动匹配按键修饰符 🍀 2.5.0 新增 你也可直接将 KeyboardEvent.key 暴露的任意有效按键名转换为 kebab-case 来作为修饰符 : 在上面的例子中 , 处理函数仅在 $event.key === 'PageDown' 时被调用 有一些按键 (.esc 以及所有的方向键) 在 IE9 中有不同的 key 值 , 如果你想支持 IE9 , 它们的内置别名应该是首选 系统修饰键 🍀 2.1.0 新增 可以用如下修饰符来实现仅在按下相应按键时才触发鼠标或键盘事件的监听器 .ctrl .alt .shift .meta 注意 : 在 Mac 系统键盘上 , meta 对应 command 键 (⌘) ; 在 Windows 系统键盘 meta 对应 Windows 徽标键 (⊞) ; 在 Sun 操作系统键盘上 , meta 对应实心宝石键 (◆) ; 在其他特定键盘上 , 尤其在 MIT 和 Lisp 机器的键盘、以及其后继产品 , 比如 Knight 键盘、space-cadet 键盘 , meta 被标记为“META” ; 在 Symbolics 键盘上 , meta 被标记为“META”或者“Meta” 例如 : Do something 请注意修饰键与常规按键不同 , 在和 keyup 事件一起用时 , 事件触发时修饰键必须处于按下状态 , 换句话说 , 只有在按住 ctrl 的情况下释放其它按键 , 才能触发 keyup.ctrl , 而单单释放 ctrl 也不会触发事件 , 如果你想要这样的行为 , 请为 ctrl 换用 keyCode : keyup.17 .exact修饰符 🍀 2.5.0 新增 .exact 修饰符允许你控制由精确的系统修饰符组合触发的事件 A A A 鼠标按钮修饰符 🍀 2.2.0 新增 .left .right .middle 这些修饰符会限制处理函数仅响应特定的鼠标按钮 HTML中监听事件 🍀 你可能注意到这种事件监听的方式违背了关注点分离 (separation of concern) 这个长期以来的优良传统 , 但不必担心 , 因为所有的 Vue.js 事件处理方法和表达式都严格绑定在当前视图的 ViewModel 上 , 它不会导致任何维护上的困难 , 实际上 , 使用 v-on 有几个好处 : 扫一眼 HTML 模板便能轻松定位在 JavaScript 代码里对应的方法 因为你无须在 JavaScript 里手动绑定事件 , 你的 ViewModel 代码可以是非常纯粹的逻辑 , 和 DOM 完全解耦 , 更易于测试 当一个 ViewModel 被销毁时 , 所有的事件处理器都会自动被删除 , 你无须担心如何自己清理它们 var example1 = new Vue({ el: '#example-1', data: { counter: 0 } }); var example2 = new Vue({ el: '#example-2', data: { name: 'Vue.js' }, // 在 `methods` 对象中定义方法 methods: { greet: function (event) { // `this` 在方法里指向当前 Vue 实例 alert('Hello ' + this.name + '!') // `event` 是原生 DOM 事件 if (event) { alert(event.target.tagName) } } } }); new Vue({ el: '#example-3', methods: { say: function (message) { alert(message) } } }); "},"04-前端/Vue/09-Vue - 组件基础.html":{"url":"04-前端/Vue/09-Vue - 组件基础.html","title":"Vue - 组件基础","keywords":"","body":"Vue - 组件基础 基本示例 🍀 这里有一个 Vue 组件的示例 : // 定义一个名为 button-counter 的新组件 Vue.component('button-counter', { data: function () { return { count: 0 } }, template: 'You clicked me {{ count }} times.' }) 组件是可复用的 Vue 实例 , 且带有一个名字 : 在这个例子中是 , 我们可以在一个通过 new Vue 创建的 Vue 根实例中 , 把这个组件作为自定义元素来使用 : new Vue({ el: '#components-demo' }) 因为组件是可复用的 Vue 实例 , 所以它们与 new Vue 接收相同的选项 , 例如 data、computed、watch、methods 以及生命周期钩子等 , 仅有的例外是像 el 这样根实例特有的选项 组件的复用 🍀 你可以将组件进行任意次数的复用 : 注意当点击按钮时 , 每个组件都会各自独立维护它的 count , 因为你每用一次组件 , 就会有一个它的新实例被创建 组件的data 🍀 当我们定义这个 组件时 , 你可能会发现它的 data 并不是像这样直接提供一个对象 : data: { count: 0 } 取而代之的是 , 一个组件的 data 选项必须是一个函数 , 因此每个实例可以维护一份被返回对象的独立的拷贝 : data: function () { return { count: 0 } } 组件的组织 🍀 通常一个应用会以一棵嵌套的组件树的形式来组织 : 例如 , 你可能会有页头、侧边栏、内容区等组件 , 每个组件又包含了其它的像导航链接、博文之类的组件 为了能在模板中使用 , 这些组件必须先注册以便 Vue 能够识别 , 这里有两种组件的注册类型 : 全局注册和局部注册 , 至此 , 我们的组件都只是通过 Vue.component 全局注册的 : Vue.component('my-component-name', { // ... options ... }) 全局注册的组件可以用在其被注册之后的任何 (通过 new Vue) 新创建的 Vue 根实例 , 也包括其组件树中的所有子组件的模板中 Prop 🍀 早些时候 , 我们提到了创建一个博文组件的事情 , 问题是如果你不能向这个组件传递某一篇博文的标题或内容之类的我们想展示的数据的话 , 它是没有办法使用的 , 这也正是 prop 的由来 Prop 是你可以在组件上注册的一些自定义特性 , 当一个值传递给一个 prop 特性的时候 , 它就变成了那个组件实例的一个属性 , 为了给博文组件传递一个标题 , 我们可以用一个 props 选项将其包含在该组件可接受的 prop 列表中 : Vue.component('blog-post', { props: ['title'], template: '{{ title }}' }) 一个组件默认可以拥有任意数量的 prop , 任何值都可以传递给任何 prop , 在上述模板中 , 你会发现我们能够在组件实例中访问这个值 , 就像访问 data 中的值一样 一个 prop 被注册之后 , 你就可以像这样把数据作为一个自定义特性传递进来 : 然而在一个典型的应用中 , 你可能在 data 里有一个博文的数组 : new Vue({ el: '#blog-post-demo', data: { posts: [ { id: 1, title: 'My journey with Vue' }, { id: 2, title: 'Blogging with Vue' }, { id: 3, title: 'Why Vue is so fun' } ] } }) 并想要为每篇博文渲染一个组件 : 如上所示 , 你会发现我们可以使用 v-bind 来动态传递 prop , 这在你一开始不清楚要渲染的具体内容 , 比如从一个 API 获取博文列表的时候 , 是非常有用的 Prop详细 单个根元素 🍀 当构建一个 组件时 , 你的模板最终会包含的东西远不止一个标题 : {{ title }} 最最起码 , 你会包含这篇博文的正文 : {{ title }} 然而如果你在模板中尝试这样写 , Vue 会显示一个错误 , 并解释道 every component must have a single root element (每个组件必须只有一个根元素) , 你可以将模板的内容包裹在一个父元素内 , 来修复这个问题 , 例如 : {{ title }} 看起来当组件变得越来越复杂的时候 , 我们的博文不只需要标题和内容 , 还需要发布日期、评论等等 , 为每个相关的信息定义一个 prop 会变得很麻烦 : 所以是时候重构一下这个 组件了 , 让它变成接受一个单独的 post prop : Vue.component('blog-post', { props: ['post'], template: ` {{ post.title }} ` }) 上述的这个和一些接下来的示例使用了 JavaScript 的模板字符串来让多行的模板更易读 , 它们在 IE 下并没有被支持 , 所以如果你需要在不 (经过 Babel 或 TypeScript 之类的工具) 编译的情况下支持 IE , 请使用折行转义字符取而代之 现在 , 不论何时为 post 对象添加一个新的属性 , 它都会自动地在 内可用。 通过事件向父级组件发送消息 🍀 在我们开发 组件时 , 它的一些功能可能要求我们和父级组件进行沟通 , 例如我们可能会引入一个可访问性的功能来放大博文的字号 , 同时让页面的其它部分保持默认的字号 在其父组件中 , 我们可以通过添加一个 postFontSize 数据属性来支持这个功能 : new Vue({ el: '#blog-posts-events-demo', data: { posts: [/* ... */], postFontSize: 1 } }) 它可以在模板中用来控制所有博文的字号 : 现在我们在每篇博文正文之前添加一个按钮来放大字号 : Vue.component('blog-post', { props: ['post'], template: ` {{ post.title }} Enlarge text ` }) 问题是这个按钮不会做任何事 : Enlarge text 当点击这个按钮时 , 我们需要告诉父级组件放大所有博文的文本 , 幸好 Vue 实例提供了一个自定义事件的系统来解决这个问题 , 我们可以调用内建的 $emit 并传入事件的名字 , 来向父级组件触发一个事件 : Enlarge text 然后我们可以用 v-on 在博文组件上监听这个事件 , 就像监听一个原生 DOM 事件一样 : 使用事件抛出一个值 🍀 有的时候用一个事件来抛出一个特定的值是非常有用的 , 例如我们可能想让 组件决定它的文本要放大多少 , 这时可以使用 $emit 的第二个参数来提供这个值 : Enlarge text 然后当在父级组件监听这个事件的时候 , 我们可以通过 $event 访问到被抛出的这个值 : 或者 , 如果这个事件处理函数是一个方法 : 那么这个值将会作为第一个参数传入这个方法 : methods: { onEnlargeText: function (enlargeAmount) { this.postFontSize += enlargeAmount } } 组件上使用 v-model 🍀 自定义事件也可以用于创建支持 v-model 的自定义输入组件 , 记住 : 等价于 : 当用在组件上时 , v-model 则会这样 : 为了让它正常工作 , 这个组件内的 必须 : 将其 value 特性绑定到一个名叫 value 的 prop 上 在其 input 事件被触发时 , 将新的值通过自定义的 input 事件抛出 写成代码之后是这样的 : Vue.component('custom-input', { props: ['value'], template: ` ` }) 现在 v-model 就应该可以在这个组件上完美地工作起来了 : 通过插槽分发内容 🍀 和 HTML 元素一样 , 我们经常需要向一个组件传递内容 , 像这样 : Something bad happened. 幸好 , Vue 自定义的 元素让这变得非常简单 : Vue.component('alert-box', { template: ` Error! ` }) 如你所见 , 我们只要在需要的地方加入插槽就行了——就这么简单 ! 更多插槽相关 : 插槽 动态组件 🍀 有的时候 , 在不同组件之间进行动态切换是非常有用的 , 比如在一个多标签的界面里 : Home component 上述内容可以通过 Vue 的 元素加一个特殊的 is 特性来实现 : 在上述示例中 , currentTabComponent 可以包括 已注册组件的名字 , 或 一个组件的选项对象 解析 DOM 模板时的注意事项 🍀 有些 HTML 元素 , 诸如 、、 和 , 对于哪些元素可以出现在其内部是有严格限制的 , 而有些元素 , 诸如 、 和 , 只能出现在其它某些特定的元素内部 这会导致我们使用这些有约束条件的元素时遇到一些问题 , 例如 : 这个自定义组件 会被作为无效的内容提升到外部 , 并导致最终渲染结果出错。幸好这个特殊的 is 特性给了我们一个变通的办法 : 需要注意的是 , 如果我们从以下来源使用模板的话 , 这条限制是不存在的 : 字符串 (例如 : template: '...') 单文件组件 (.vue) `` "},"04-前端/Vue/10-Vue - Vue-cli.html":{"url":"04-前端/Vue/10-Vue - Vue-cli.html","title":"Vue - Vue-cli","keywords":"","body":"Vue - Vue-cli 介绍 🍀 Vue CLI 是一个用于 vue.js 快速开发的系统 , 提供 : 交互式工程脚手架 @vue/cli 零配置快速成型 @vue/cli + @vue/cli-service-global 运行时依赖项 @vue/cli-service : 可升级 建立在 WebPack 之上 , 有合理的默认设置 可通过项目内配置文件进行配置 可扩展的插件 丰富的官方插件 , 集成了前端生态系统中最好的工具 Vue CLI 旨在成为 VUE 生态系统的标准工具基线 , 它确保各种构建工具与合理的缺省值一起顺利工作 , 这样你就可以专注于编写应用程序 , 而不是花费数天时间与配置进行争论 , 同时 , 它仍然可以灵活地调整每个工具的配置 使用 vue-cli 可以快速构建我们的 Vue 项目 快速开始 🍀 安装 Node.js 🍀 在安装 vue-cli 之前 , 我们需要安装 Node 环境 , 因为我们需要使用 npm 包管理器 官网下载安装 安装 vue-cli 🍀 安装好 node 之后 , 我们就可以安装 vue-cli 了 : npm install -g vue-cli 待安装完成后 , 在中断使用 vue -help 出现以下信息说明安装成功 : C:\\Users\\Lyon>vue -help Usage: vue [options] Options: -V, --version output the version number -h, --help output usage information Commands: init generate a new project from a template list list available official templates build prototype a new project create (for v3 warning only) help [cmd] display help for [cmd] 可用的官方模板如下 : C:\\Users\\Lyon>vue list Available official templates: ★ browserify - A full-featured Browserify + vueify setup with hot-reload, linting & unit testing. ★ browserify-simple - A simple Browserify + vueify setup for quick prototyping. ★ pwa - PWA template for vue-cli based on the webpack template ★ simple - The simplest possible Vue setup in a single HTML file ★ webpack - A full-featured Webpack + vue-loader setup with hot reload, linting, testing & css extraction. ★ webpack-simple - A simple Webpack + vue-loader setup for quick prototyping. 初始化项目 🍀 使用 webpack 模块初始化 my-project 项目 $ vue init webpack my-project ? Project name my-project ? Project description A Vue.js project ? Author lyonyang ? Vue build standalone ? Install vue-router? Yes ? Use ESLint to lint your code? No ? Set up unit tests No ? Setup e2e tests with Nightwatch? No ? Should we run `npm install` for you after the project has been created? (recommended) npm vue-cli · Generated \"my-project\". # Installing project dependencies ... # ======================== ..... # Project initialization finished! # ======================== To get started: cd my-project npm run dev Documentation can be found at https://vuejs-templates.github.io/webpack 切换到项目目录 🍀 $ cd my-project 下载项目依赖包 🍀 $ npm install 启动当前项目 🍀 $ npm run dev > my-project@1.0.0 dev D:\\my-project > webpack-dev-server --inline --progress --config build/webpack.dev.conf.js 95% emitting DONE Compiled successfully in 4755ms 10:39:13 I Your application is running here: http://localhost:8080 访问 http://localhost:8000 项目结构 🍀 注释部分为未启用功能部分 . ├── build/ # webpack config files │ └── ... ├── config/ │ ├── index.js # main project config │ └── ... ├── node_modules/ # library root ├── src/ │ ├── main.js # app entry file │ ├── App.vue # main app component │ ├── components/ # ui components │ │ └── ... │ └── assets/ # module assets (processed by webpack) │ └── ... ├── static/ # pure static assets (directly copied) # ├── test/ # │ └── unit/ # unit tests # │ │ ├── specs/ # test spec files # │ │ ├── eslintrc # config file for eslint with extra settings only for unit tests # # │ │ ├── index.js # test build entry file # │ │ ├── jest.conf.js # Config file when using Jest for unit tests # │ │ └── karma.conf.js # test runner config file when using Karma for unit tests # │ │ ├── setup.js # file that runs before Jest runs your unit tests # │ └── e2e/ # e2e tests # │ │ ├── specs/ # test spec files # │ │ ├── custom-assertions/ # custom assertions for e2e tests # │ │ ├── runner.js # test runner script # │ │ └── nightwatch.conf.js # test runner config file ├── .babelrc # babel config ├── .editorconfig # indentation, spaces/tabs and similar settings for your editor # ├── .eslintrc.js # eslint config # ├── .eslintignore # eslint ignore rules ├── .gitignore # sensible defaults for gitignore ├── .postcssrc.js # postcss config ├── index.html # index.html template ├── package.json # build scripts and dependencies └── README.md # Default README file 构建命令 🍀 所有的构建命令都是通过 NPM 脚本执行的 npm run dev 开启一个本地 Node.js 开发服务器 npm run build 打包生产资源 , 如 JavaScript , HTML , CSS 等 npm run unit 在 JSDOM 中运行单元测试 npm run e2e 运行端到端的测试 npm run lint 运行 eslint 并报告错误链接 "},"05-Web框架/":{"url":"05-Web框架/","title":"Web框架","keywords":"","body":"Web框架介绍 主流Web框架 🍀 Django 🍀 Django 是当前 Python 世界中最负盛名且最成熟的网络框架 , 最初用来制作在线新闻的 Web 站点 , 目前已经发展为应用最广泛的 Python 网络框架 Django 的各模块之间结合得比较紧密 , 所以在功能强大的同时又是一个相对封闭的系统 , 但是它提供了非常齐备的官方文档 , 提供了译站式的解决方案 , 其中包含缓存 , ORM , 管理后台 , 验证 , 表单处理等 , 使得开发复杂的数据库驱动的网站变得很简单 ; 当然也因为系统耦合度太高 , 替换掉内置的功能往往需要花费一些功夫 , 所以学习曲线也相当陡峭 Flask 🍀 Flask 是一个轻量级 Web 应用框架 , 它基于 Werkzeug 实现的 WSGI 和 Jinjia2 模板引擎 Flask 的作者是 Armin Ronacher , 本来这只是作者愚人节开的一个玩笑 , 但是开源之后却受到 Python 程序员的喜爱 , 目前在 GitHub 上的 Star 数量已经超过了 Django . 设计上 , 它只保留核心 , 通过扩展机制来增加其他功能 , Flask 用到的依赖都是 Pocoo 团队开发的 , Pocoo 团队其他的项目还有 Pygments , Sphinx , 以及 lodgeit . Flask 的扩展环境非常繁荣 , 基本上 Web 应用的每个环节都有对应的扩展供选择 , 就算没有对应的扩展也能很方便的自己实现一个 Pyramid 🍀 Pyramid 在国内知名度并不高 , 主要原因是中文文档匮乏 , 其高级用法需要通过阅读源代码获取灵感 , 尽管被 Django 和 Flask 的光芒遮蔽 , 但是它的性能要比 Flask 高 . Pyramid 的灵感来源于 Zope , Pylons 1.0 和 Django . Pyramid 在努力朝着胜任不同级别应用的框架的方向在走 , 虽然它默认使用 Chameleon 和 Mako 模块 , 但很容易切换成 Jinja2 , 甚至可以让多种模板引擎共存 , 通过文件后缀名来识别 豆瓣赞赏和豆瓣钱包等产品就是基于此框架实现的 , 代码量级和 Flask 相同 , 性能比 Flask 要略高 Bottle 🍀 Bottle 也是一个轻量级的 Web 框架 , 它的特点是单文件 , 代码只使用了 Python 标准库 , 而不需要额外依赖其他的第三方库 , 它更符合微框架的定义 , 截止到今天只有 4100 多行的代码 Tornado 🍀 Tornado 全称 Tornado Web Server , 最初是由 FriendFeed 开发的非阻塞式 Web 服务器 , 现在我们看到的是被 Fackbook 收购后开源出来的版本 , 它和其他主流框架有个明显的区别 : 它是非阻塞式服务器 , 而且速度相当快 , 得益于其非阻塞的方式和对 epoll 的运用 , Tornado 每秒可以处理数以千计的连接 , 这意味着对于长轮询 , WebSocket 等实时 Web 服务来说 , Tornado 是一个理想的 Web 框架 Web.py 🍀 Web.py 也是一个微框架 , 由 Reddit 联合创始人 , RSS 规格合作创造者 , 著名计算机黑客 Aaron Swartz 开发 , Web.py 使用基于类的视图 , 简单易学却功能强大 Twisted 🍀 Twisted 是一个有着十多年历史的开源事件驱动框架 , 它适用于从传输层到自定义应用协议的所有类型的网络程序的开发 , 而不着眼于网络 HTTP 应用开发 , 并且它能在不同的操作系统上提供很高的运行效率 小众的Web框架 🍀 Quixote 🍀 Quixote 是由美国全国研究创新联合会的工程师 A.M.Kuchling , Neil Schemenauer 和 Greg Ward 开发的一个轻量级 Web 框架 , 它简单 , 高校 , 代码简洁 , 豆瓣的大部分用户产品都使用定制版的 Quixote 作为 Web 框架 它使用目录式的 URL 分发规则 , 用 Python 来写模板 , PTL 模板更适合程序员 , 但是并不适合美工参与前端代码的编写和修改 , 豆瓣在开发中使用了 Mako 替代 PTL ; 不建议在生产环境中选用 Quixote Klein 🍀 Klein 是 Twisted 组织开源出来的基于 werkzeug 和 twisted.web 的为框架 , Flask 很不错 , 但是不能使用异步非阻塞的方式编程 , 根本原因是它和 Django , Pyramid 一样 , 都基于 WSGI , 而WSGI 的接口是同步阻塞的 , Klein 用法非常像 Flask , 却可以使用异步的方式开发 Web 应用 选择 Web 框架时应遵循的原则 🍀 选择更主流的 Web 框架 , 因为他们文档齐全 , 技术积累更多 , 社区更繁盛 , 能得到更好的支持 关注框架的活跃情况 , 如果一个框架长时间没有更新 , 或者有一堆的问题需要解决但是没有得到回应 , 就不应该将这样的框架放在生产环境中 确认框架是否足够满足需求 , 没有最好的框架 , 只有最合适的框架 "},"05-Web框架/01-HTTP基础.html":{"url":"05-Web框架/01-HTTP基础.html","title":"HTTP基础","keywords":"","body":"HTTP基础 HTTP常用头字段 🍀 字段名 方向 解释 可能的值 Accept Request 接受什么介质类型 type/sub-type*/* 表示任何类型 , type/* 表示该类型下的所有子类型 Accept-Charset Request 接收的字符集 ISO-8859-1 Accept-Encoding Request 接收的编码方法 , 通常指定压缩方法 , 是否支持压缩 , 支持什么压缩方法 Gzip , deflate , UTF8 Accept-Language Request 接收的语言 En , cn Cache-Control Request 对服务器的缓存控制 no-cache : 不要从缓存中去取 , 要求现在从Web服务器中去取 Connection Request 连接状态通知 Close : 告诉Web服务器在完成本次请求的响应后 , 断开连接 , 不要等待本次连接的后续请求了Keepalive : 告诉Web服务器在完成本次请求的响应后 , 保持连接 , 等待本次连接的后续请求 Host Request 客户端指定自己想访问的Web服务器的域名 , IP 地址和端口号 IP : port Proxy-Authenticate Request 提供自己在代理服务器中的身份信息 Username : password range Request 需要获取对象的哪一部分内容 bytes=1024- : 获取从第1024个字节到最后的内容 Referer Request 浏览器向 Web 服务器表明自己是从哪个 URL 获得当前请求中的 URL 的 http://www.baidu.com User-Agent Request 指明浏览器的软件类型及版本 Mozilla/x.x: Windows浏览器Firefox/xx.x.x: Fiefox浏览器 Age Response 用该头部表示该实体从产生到现在经过多长时间 Authorization Response 当客户端接收到来 自 Web 服务器的WWW-Authenticate响应时 , 该头部回应自己的身份验证信息给Web服务器 Username : password Cache-Control Response 对客户端的缓存控制 Public : 可以用缓存内容回应任何用户Private : 只能用缓存内容回应先前请求该内容的那个用户 Connection Response 连接状态通知 Close : 连接已经关闭Keepalive : 连接保持 , 等待本次连接的后续请求 Expired Response Web服务器表明该实体将在什么时候过期 YYYY-MM-DD HH:MM:SS Location Response 访问的对象已经被转移到别的位置了 , 应该到本头字段指向的地址获取 http://mysite.com/another_url Proxy-Authenticate Response 代理服务器响应浏览器 , 要求其提供代理身份验证信息 Server Response 指明服务器的软件类型及版本 Nginx/1.14 Etag Both 内容唯一标识 , 服务端要把服务器传来的Etage保留 , 在下次请求相同的 URL 时提交给服务器 . 服务器用 Etag值判断同一个 URL 的内容是否有变化 , 如果有变化则发送更新的内容给客户端 任何值 Via Both 列出从客户端到服务器或者相反方向的响应经过了哪些代理服务器 , 它们用什么协议 (和版本) 发送的请求 HTTP常见错误代码 🍀 在每个 Response 的第1行中有一个整数状态码用于表达其对应 Request 的结果 , HTTP 除了约定该状态的表达方式 , 还约定了该状态的取值范围 , 约定的 5 类状态码如下 : 1xx : 信息 ; 表明服务器已经收到 Request , 但需要进一步处理 , 请客户端等待 2xx : 成功 ; 处理成功 3xx : 重定向 ; 请求的地址已被重定向 , 需要客户端重新发起请求 4xx : 客户端错误 ; 请求中提交的参数或内容有错误 5xx : 服务器错误 ; 服务器处理请求时出错 , 一般本类错误需要联系服务器管理员处理 注意 : 1xx~5xx 的错误为 HTTP 标准错误 , 在网站开发中如需要定义自己的错误代码 , 则需要避开该范围 常见 HTTP 错误代码表如下 : 代码 解释 代码 解释 代码 解释 100 继续等待 200 正常完成并返回 204 无内容 206 部分内容被返回 301 已移动 304 未修改 305 必须使用代理 400 语法错误 401 未授权 402 需要付费访问 403 禁止访问 500 服务器异常错误 501 未执行 502 上游的其他来源错误 503 临时过载或维护中 HTTP请求方法 🍀 HTTP 中常用的访问方式及其意义 访问方式的名称 意义 DELETE 从给定的地址中删除信息 GET 从访问的地址中获取信息 , 即获取信息头 , 也获取信息体 . 这是互联网上最主要的一种 HTTP 访问方式 HEAD 从访问的地址中获取信息 , 它与 GET 的区别是 : HEAD 只获取信息头 , 不获取信息体 . 在 Flask 路由中如果声明了 GET 访问方式 , 则无需显式地声明 HEAD 访问方式 OPTIONS 为客户端提供一种查询 \"本URL地址中有哪些可用的访问方式\" 的方法 POST 客户端通过 POST 方法向服务器提交新数据 , 服务器必须保证数据被完整地保存 , 并且服务器不允许出现重复的 POST 数据提交 , 这是 HTML 中通过表单提交数据所使用的 URL 访问方式 PUT 与 POST 访问方法类似 , POST 也是一种使客户端可以向服务器提交数据的方式 , 但是 PUT 允许客户端提交重复主键的数据 , 当通过 PUT 访问方式在服务器中发现重复主键的数据时 , 它会用新提交的数据覆盖服务器中已有的数据 "},"05-Web框架/02-REST.html":{"url":"05-Web框架/02-REST.html","title":"REST","keywords":"","body":"REST 介绍 🍀 REST 是 Representational State Transfer (表征状态转移) 的缩写 表征实际上指的是资源的表现特征 , 而资源指的是 Web 上一切可识别 , 可命名 , 可找到并被处理的实体 , 比如 HTML 页面 , 音频文件 , 图片等 用一个 URI (统一资源定位符) 指向资源 , 使用 HTTP 请求方法操作资源 , URL 可进一步划分为统一资源名 (URN , 代表资源的名字) 和统一资源定位符 (URL , 代表资源的地址) , 其中 URL 可以定位 HTTP 网址 , FTP 服务器和文件路径等 , 符合绝大多数场景 , 所以一般都可以用 URL 代替 URI REST架构约束 🍀 REST 架构风格最重要的架构约束有如下 5 个 : 客户端-服务端 , 这种 Client/Server 的架构形式提供了基本的分布式 , 客户端发起请求 , 服务端决定响应或者拒绝请求 , 如果出错则返回错误信息 , 由客户端处理异常 无状态 , 通信的会话状态应该全部由客户端负责维护 , 也就是请求中包含了全部必要的信息 . 如果使用基于服务端的会话 , 要么需要保证指定会话会使用同一个服务端响应所有请求 , 要么得创建一个可供所有服务器访问的公用的会话存储区 , 对每个请求都额外访问这个几种式的数据存储区获得会话状态 缓存 , 无状态就表示可能出现重复的请求 , 事实上这些请求只需要第一次真正的执行 , 其余的请求都可以享用这个已完成的记过而直接响应 , 所以缓存可以抵消一部分无状态带来的影响 统一接口 , 统一接口意味着每个 REST 应用都共享一种通用架构 , 那么熟悉这种架构的人一眼就能看明白接口的意义 , 并会继续延承下去 分层系统 , 将系统划分为几个部分 , 每个部分负责一部分相对单一的职责 , 然后通过上层对下层的依赖和调用组成一个完整的系统 , 通常可以划分为如下三层 : 应用层 : 负责返回 JSON 数据和其他业务逻辑 服务层 : 为应用层提供服务支持 , 如全站的账号系统 , 以及文件托管服务等 数据访问层 : 提供数据访问和存储的服务 , 如数据库 , 缓存系统 , 文件系统 , 搜索引擎等 如果一个架构符合 REST 原则 , 就称它为 RESTful 架构 RESTful API设计指南 🍀 使用名词表示资源 🍀 URI 不应该包含动词 , 动词应该通过不同的 HTTP 方法来体现 错误用法 : GET /getusers/1 POST /users/1/delete POST /users/1/create 正确用法 : GET /users/1 DELETE /users/1 PUT /users/1 关注请求头 🍀 一定要看请求头信息 , 并给予正确的状态码 , 例如 , 假设服务端只能返回 JSON 格式 , 如果客户端的头信息的 Accept 字段要求返回 application/xml , 这个时候就不应该返回 application/json 类型的数据 , 而应该返回 406 错误 合理使用请求方法和状态码 🍀 方法语义说明 方法 语义 OPTIONS 用于获取资源支持的所有 HTTP 方法 HEAD 用于只获取请求某个资源返回的头信息 GET 用于从服务器获取某个资源的信息 : 1. 完成请求后 , 返回状态码 200 OK2. 完成请求后 , 需要返回被请求的资源详细信息 POST 用于创建新资源 : 1. 创建完成后 , 返回状态码 201 Created2. 完成请求后 , 需要返回被创建的资源详细信息 PUT 用于完整的替换资源后者创建指定身份的资源 , 比如创建 id 为 123的某个资源 : 1. 如果是创建了资源 , 则返回 201 Created2. 如果是替换了资源 , 则返回 200 OK PATCH 用于局部更新资源 :1. 完成请求后 , 返回状态码 200 OK2. 完成请求后 , 需要返回被修改的资源详细信息 DELETE 用于删除某个资源 , 完成请求后返回状态码 204 NO Content 使用嵌套对象序列化 🍀 对象应该合理地嵌套 , 不应该都在一个层次上 , 如下的格式是不正确的 : { 'id': 1, 'post_id': '10001', 'post_name': 'Post1', 'post_content': 'this is a post' } 尽可能把相关联的资源信息内联在一起 , 应该把 post 作为一个键 : { 'id': 1, 'post': { 'id' : '10001', 'name': 'Post1', 'content': 'this is a post' } } 版本 🍀 常见的区分版本方法有三种 : 保存在 URI 中 , 比如 \"https://api.lyonyang.com/api/v2\" 放在请求头中 , 比如 GitHub 的用于 : \"Accept:application/vnd.github.v3+json\" 自定义请求头 , 比如 , \"X-Api-Version:1\" 推荐使用第一种方法 URI失效和迁移 🍀 随着业务发展 , 会出现一些 API 失效或者迁移 , 对失效的 API , 应该返回 \"404 not found\" 或 \"401 gone\" ; 对迁移的 API , 返回 301 重定向 速度限制 🍀 为了避免请求泛滥 , 给 API 设置速度限制很重要 , 为此 RFC 6585 引入了 HTTP 状态码 429 (too many requests) 加入限制功能后 , 应该提示用户 , 参照 GitHub 如下 : X-RateLimit-Limit : 当前时间段允许的并发请求书 X-RateLimit-Remaining : 当前时间段保留的请求数 X-RateLimit-Reset : 当前时间段剩余的秒数 我们使用 httpie 或者 curl 来访问 https://api.github.com/users/whatever : >curl -i https://api.github.com/users/whatever HTTP/1.1 200 OK Date: Mon, 01 Jun 2013 17:27:06 GMT Content-Type: application/json; charset=utf-8 Content-Length: 1350 Server: GitHub.com Status: 200 OK X-RateLimit-Limit: 60 X-RateLimit-Remaining: 59 X-RateLimit-Reset: 1530220396 ... 缓存 🍀 数据内容在一段时间不会变动 , 这个时候我们就可以合理地减少 HTTP 响应内容 , 应该在响应头中携带 Last-Modified , ETag , Vary , Date 等信息 , 客户端可以在随后请求这些资源时 , 在请求头中使用 If-Modified-Since , If-None-Match 等来确认资源是否进过修改 , 如果资源没有做过修改 , 那么就可以响应 \"304 Not Modified\" , 并且不在响应实体中返回任何内容 GitHub 用法如下 (隐藏了无关的自定义头) : >http https://api.github.com/users/lyonyang --headers HTTP/1.1 200 OK Access-Control-Allow-Origin: * Access-Control-Expose-Headers: ETag, Link, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval Cache-Control: public, max-age=60, s-maxage=60 Content-Encoding: gzip Content-Security-Policy: default-src 'none' Content-Type: application/json; charset=utf-8 Date: Thu, 04 Feb 2016 14:05:03 GMT ETag: W/\"02742979edf9240e6ea171ac41914d44\" Last-Modified: Mon, 25 Jun 2018 08:15:42 GMT Referrer-Policy: origin-when-cross-origin, strict-origin-when-cross-origin Server: GitHub.com Status: 200 OK Strict-Transport-Security: max-age=31536000; includeSubdomains; preload Transfer-Encoding: chunked Vary: Accept Vary: Accept-Encoding ... 通过 If-Modified-Since 实现缓存 : >http https://api.github.com/users/lyonyang \"If-Modified-Since: Thu, 04 Feb 2016 14:05:03 GMT\" --headers 并发控制 🍀 缺少并发控制的 PUT 和 PATCH 请求可能导致 \"更新丢失\" , 这个时候可以使用 Last-Modified 和 ETag 头来实现条件请求 , 具体原则如下 : 客户端发起的请求如果没有包含 If-Unmodified-Since 或者 If-Match 头 , 就返回 \"403 Forbidden\" , 在响应正文中解释为何返回该状态码 客户端发起的请求所提供的 If-Unmodified-Since 或者 If-Match 头与服务器记录的实际修改时间或 ETag 值不匹配时 , 返回状态码 \"412 Precondition Failed\" 客户端发起的请求所提供的 If-Unmodified-Since 或者 If-Match 头与服务器记录的实际修改时间或 ETag 的历史值匹配 , 但资源已经被修改过事 , 返回状态码 \"409 Conflict\" 客户端发起的请求所提供的条件服务实际值 就更新资源 , 响应 \"200 Ok\" 或者 \"204 No Content\" , 并且包含更新过的 Last-Modified 和 / 或 ETag 头 , 同时包含 Content-Location 头 , 其值为更新后的资源 URI "},"05-Web框架/03-ORM简介.html":{"url":"05-Web框架/03-ORM简介.html","title":"ORM简介","keywords":"","body":"ORM简介 介绍 🍀 ORM在开发者和数据库之间建立了一个中间层 , 把数据库中的数据转换成了 Python 中的对象实体 , 这样既屏蔽了不同数据库之间的差异性 , 又使开发者可以非常方便地操作数据库中的数据 , 而且可以使用面向对象的高级特性 ORM 为我们做了如下操作 : 将调用转换成 SQL 语句 通过数据库引擎发送给数据库执行 将数据库返回的结果记录用 ORM 映射技术转换成类对象 ORM 优点 : 向开发者屏蔽了数据库的细节 , 使开发者无需与 SQL 语句打交道 , 提高了开发小懒虫 便于数据库迁移 , 由于每种数据库的 SQL 语法有细微差别 , 所以基于 SQL 的数据访问层在更换数据库时通常需要花费大量的时间调试 SQL 语句 , 而 ORM 提供了独立于 SQL 的接口 , ORM 引擎会处理不同数据库之间的差异 , 所以迁移数据库时无须更改代码 应用缓存优化等技术有时可以提高数据库操作的效率 Python ORM 库介绍 🍀 Python 中提供 ORM 支持的组件有很多 , 每个组件的应用领域稍有区别 , 但是数据库操作的理论原理是相同的 , 下面对比较著名的 Python 数据库的 ORM 框架介绍 : SQLAlchemy : 是 Python 中最成熟的 ORM 框架 , 资源和文档都很丰富 , 大多数 Python Web 框架对其都有很好的支持 , 能够胜任大多数应用场合 ; SQLAlchemy 被认为是 Python 事实上的 ORM 标准 Django ORM : 是 Python 世界中大名鼎鼎的 Django Web 框架独用的 ORM 技术 , Django是一个大而全的框架 , 这使得其灵活性大大降低 , 其他 Python Web 框架可以随意更换 ORM , 但在 Django 中不能这样做 , 因为 Django 内置的很多 model 是用 Django 内置 ORM 实现的 Peewee : 小巧灵活 , 是一个轻量级的 ORM , Peewee 是基于 SQLAlchemy 内核开发的 , 整个框架只由一个文件构成 , Peewee 提供了对多种数据库的访问方式 , 如 SQLite , MySQL , PostgreSQL , 适用于功能简单的小型网站 Storm : 是一个中型的 ORM 库 ,比 SQLAlchemy 和 Django 等轻量 , 比 Peewee 的功能更丰富 , Storm 要求开发者编写数据表的 DDL 代码 , 而不能直接从数据表类定义中自动生成表定义 SQLObject : 与 SQLAlchemy 相似 , 也是一套大而全的 ORM , SQLObject 的特点是其设计借鉴了 Ruby on Rails 的 ActiveRecord 模型 , 是的熟悉 Ruby 的开发者上手非常容易 Peewee库使用 🍀 示例 : # 引入Peewee包的所有内容 from peewee import * # 建立一个SQLite数据库引擎对象,该引擎打开数据库文件sampleDB.db db = SqliteDatabase(\"sampleDB.db\") # 定义一个ORM的基类,在基类中指定本ORM所使用的数据库 # 这样在之后所有的子类中就不用重复声明数据库了 class BaseModel(Model): class Meta: database = db # 定义course表,继承自BaseModel class Course(BaseModel): id = PrimaryKeyField() title = CharField(null=false) period = IntegerField() description = CharField() class Meta: order_by = ('title',) db_table = 'course' # 定义teacher表,继承自BaseModel class Teacher(BaseModel): id = PrimaryKeyField() name = CharField(null=false) gender = BooleanField() description = CharField() course_id = ForeignKeyField(Course, to_field=\"id\", related_name=\"course\") class Meta: order_by = ('name',) db_table = 'course' 使用 ORM 映射对数据内容进行增 , 删 , 改 , 查 : # 建表,进需创建一次 Course.create_table() Teacher.craete_table() # 新增行 Course.create(id=1, title='经济学', period=320, description='文理科学生均可选修') Teacher.create(name='Lyon', gender=True, address='...', course_id=1) # 查询一行 record = Course.get(Course.title='经济学') print(\"课程:%s, 学时:%d\" % (record.titel, record.period)) # 更新 record.period = 200 record.save() # 删除 record.delete_instance() # 查询所有记录 courses = Course.select() # 带条件查询,并将结果按period字段倒序排序 courses = Course.select().where(Course.id100).execute() # 多表连接操作,Peewee会自动根据ForeignKeyField的外键定义进行连接 Record = Course.select().join(Teacher).where(Teacher.gender=True) "},"05-Web框架/Django/":{"url":"05-Web框架/Django/","title":"Django","keywords":"","body":"Django 介绍 本目录下为Django框架整理文章 , 内容概述如下 暂时内容不全 , 待后期更新修改 "},"05-Web框架/Django/01-Web框架简介.html":{"url":"05-Web框架/Django/01-Web框架简介.html","title":"Web框架简介","keywords":"","body":"Web框架简介 介绍 🍀 框架 (Framework) , 特指为解决一个开发性问题而设计的具有一定约束性的支撑结构 , 使用框架可以帮助我们快速开发特定的系统 , 简单说就是使用别人搭好的舞台 , 你来做表演 对于Web应用 , 本质其实就是一个socket服务端 , 而我们使用的浏览器就是一个socket客户端 , 如下 : # 服务端 import socket def handle_request(client): \"\"\"处理请求函数\"\"\" buf = client.recv(1024) print(buf) client.send(b\"HTTP/1.1 200 OK\\r\\n\\r\\n\") client.send(b\"Hello,Lyon\") def main(): \"\"\"主函数\"\"\" sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.bind(('localhost', 8000)) sock.listen(5) while True: connection, address = sock.accept() # 连接成功后直接处理请求 handle_request(connection) connection.close() if __name__ == '__main__': main() ''' 说明: 执行该脚本后,用浏览器(客户端)访问 127.0.0.1:8000 连接成功后服务端直接应答以及发送 Hello,Lyon 于是网页就直接显示出了Hello,Lyon的标题 ''' 上述通过socket实现了其本质 , 而对于真是开发中的Python Web程序来说 , 一般会分为两部分 : 服务器程序和应用程序 服务器程序负责对socket服务器进行封装 , 并在请求到来时 , 对请求的各种数据进行整理 , 如上述代码 应用程序负责具体的逻辑处理 为了方便应用程序的开发 , 就出现了众多的Web框架 , 常见的Python Web框架如下 : Django : 全能型Web框架 Flask : 一个轻量级的Web框架 web.py : 一个小巧的Web框架 Bottle : 和Flask类似的Web框架 Tornado : Facebook的开源异步Web框架 不同的框架有不同的开发方式 , 但是无论如何 , 开发出的应用程序都要和服务器程序配合 , 才能为用户提供服务 也就是说框架和Web服务器之间进行通信 , 那么首先就需要两者互相支持 , 所以为了使Web服务器能够匹配多个不同的Web框架 , 就需要设立一个标准 , 在Python中这个标准就是WSGI WSGI 🍀 WSGI : WSGI (Web Server Gateway Interface , Web服务器网关接口) 它定义了使用Python编写Web应用与Web服务端之间的接口格式 , 实现了Web应用与Web服务器间的解藕 Python标准库提供的独立WSGI服务器称为wsgiref , 所以我们可以使用wsgiref模块开发一个自己的Web框架 关于wsgiref模块的更多内容请可以阅读Python之路 - wsgiref模块 > , 这也是完成下面内容的前提 自定义Web框架 🍀 框架 🍀 通过Python标准库提供的wsgiref模块开发 # 从simple_server模块中导入make_server函数 from wsgiref.simple_server import make_server # 处理函数 def index(): return 'index' # 处理函数 def login(): return 'login' # 路由管理函数 def routers(): urlpatterns = ( ('/index/',index), ('/login/',login), ) return urlpatterns # 该函数就是simple_server模块中的demo_app函数,即创建一个app def RunServer(environ, start_response): \"\"\" 符合WSGI标准的一个HTTP处理函数 environ:一个包含所有HTTP请求信息的dict对象 start_response:一个发送HTTP响应的函数 注:该函数会在内部调用start_response函数 \"\"\" # start_response接收两个参数,一个是HTTP响应码,如下\"200 OK\";另一个是一组list表示的HTTP Header,每个Header用一个包含两个str的tuple表示 start_response('200 OK', [('Content-Type', 'text/html')]) # 获取路径信息,即url url = environ['PATH_INFO'] urlpatterns = routers() func = None # 查看路径是否存在 for item in urlpatterns: if item[0] == url: func = item[1] break if func: return func() else: return '404 not found' if __name__ == '__main__': # 创建一个WSGI服务器 httpd = make_server('', 8000, RunServer) print(\"Serving HTTP on port 8000...\") # 开始监听HTTP请求 httpd.serve_forever() 模板引擎 🍀 在上一步中 , 对于处理函数login以及index仅仅做了最简单的处理 , 在现实的Web请求中一般会返回一个复杂的符合HTML规则的字符串 , 所以我们一般会将要返回给用户HTML写在指定文件中 , 然后再返回 , 进一步优化如下 : # 通过文件处理发送HTML信息 def index(): # return 'index' f = open('index.html') data = f.read() return data def login(): # return 'login' f = open('login.html') data = f.read() return data 如上我们实现了静态的页面处理 , 要使其能够给用户返回动态内容 , 我们有两种方法 : 自定义一套特殊的语法 , 进行替换 使用开源工具jinja2 , 遵循其指定语法 Jinja2介绍 : Jinja2是Python下一个被广泛应用的模板引擎 , 他的设计思想来源于Django的模板引擎 , 并扩展了其语法和一系列强大的功能 , 他基于unicode并能在python2.4之后的版本运行 , 包括python3 于是我们可以再进一步 from jinja2 import Template def index(): # return 'index' # template = Template('Hello !') # result = template.render(name='John Doe') f = open('index.html') result = f.read() template = Template(result) data = template.render(name='John Doe', user_list=['Lyon', 'Kenneth']) return data.encode('utf-8') def login(): # return 'login' f = open('login.html') data = f.read() return data 以上就完成了一个最简单的Web框架 MVC和MTV 🍀 MVC MVC (Model View Controller , 模型-视图-控制器) 是一种Web架构的模式 , 它把业务逻辑 , 模型数据 , 用户界面分离开来 , 让开发者将数据与表现解藕 , 前端工程师可以只改页面效果部分而不用接触后端代码 , DBA(数据库管理员) 可以重新命名数据表并且只需要更改一个地方 , 无序从一大堆文件中进行查找和替换 MVC模式甚至还可以提高代码复用能力 , 现在MVC模式依然是主流 MVC三要素 : Model表示应用程序核心(比如数据库记录列表) , 是应用程序中用于处理应用程序数据逻辑的部分 , 通常模型对象负责在数据库中存取数据 View显示数据(数据库记录) , 是应用程序中处理数据显示的部分 , 通常视图是依据模型数据创建的 Controller处理输入(写入数据库记录) , 是应用程序中处理用户交互的部分 , 通常控制器负责从视图读取数据 , 控制用户输入 , 并向模型发送数据 MVC模式同时提供了对HTML , CSS和JavaScript的完全控制 MVC的特点是通信单向的 : 浏览器发送请求 Contorller和Model交互获取数据 Contorller调用View View渲染数据返回 MTV 在Python的世界中 , 基本都使用了MVC的变种MTV (Model Templates View , 模型-模板-视图) MTV三要素 : Model , 和MVC的Model一样 , 处理与数据相关的所有事务 : 如何存取 , 如何确认有效性 , 包含哪些行为以及数据之间的关系等 Template , 处理与表现相关的决定 : 如何在页面或其他类型文档中进行显示出来 View , 处理业务逻辑 , 视图就是一个特定URL的回调函数 , 回调函数中描述数据 : 从Model取出对应的数据 , 调用相关的模板 . 它就是Contorller要调用的那个用来做Model和View之间的沟通函数 , 从而完成控制 两者的区别在于 : MVC中的View的目的是「呈现哪一个数据」 , 而MTV的View的目的是「数据如何呈现」 下一篇就开始学习Django啦 "},"05-Web框架/Django/02-Django - Django初识.html":{"url":"05-Web框架/Django/02-Django - Django初识.html","title":"Django - Django初识","keywords":"","body":"Django - Django初识 介绍 🍀 通过上一篇整理 , 对于Web框架应该清晰了很多 , 当然上一篇仅仅是自定义了一个最low , 最底端的Web框架 , 基本仅能处理特定的HTTP请求 , 那么这一章就开始学习Python Web框架中的王牌——Django Django本身集成了ORM , 模型绑定 , 模板引擎 , 缓存 , Session等诸多功能 Django是一个基于MVC模式构造的框架 , 但是在Django中 , 控制器接受用户输入的部分由框架自行处理 , 所以 Django 里更关注的是模型 (Model) , 模板 (Template) 和视图 (Views) , 即MTV模式 , 它们各自的职责如下 层次 职责 模型 (Model) , 即数据存取层 处理与数据相关的所有事务 : 如何存取、如何验证有效性、包含哪些行为以及数据之间的关系等 视图 (View) , 即表现层 处理与表现相关的决定 : 如何在页面或其他类型文档中进行显示 模板 (Template) , 即业务逻辑层 存取模型及调取恰当模板的相关逻辑 , 模型与模板的桥梁 安装 $pip install django 获取版本 >>> import django >>> django.get_version() '1.10.8' 添加环境变量 C:\\Python3.5\\Scripts - 具体路径自行添加 创建一个Django项目 🍀 我们使用命令行来进行创建 , 命令如下 django-admin startproject mysite - 当前目录下创建 # IDE创建Django程序时本质上都是自动执行了该命令 创建成功后可见 mysite ├── mysite # 对整个程序进行配置 │ ├── __init__.py │ ├── settings.py # 配置文件 │ ├── urls.py # URL对应关系 │ └── wsgi.py # 一个WSGI兼容的Web服务器入口,以便运行你的项目,上线uwsgi + nginx ├── db.sqlite3 # 默认使用sqlite └── manage.py # 用户管理Django的工具 PS : 如果我们使用Pycharm 来完成这项操作 , 那么其还会为我们自动创建一个templates 文件夹 , 用于存放模板 启动Django项目 🍀 创建完成后我们就可以通过以下命令启动Django项目了 python manage.py runserver 0.0.0.0:8000 ''' 0.0.0.0:让其他电脑可连接到开发服务器 8000:端口,如果不说明,默认为8000 ''' 启动项目后我们就可以在浏览器输入服务器的IP及端口进行访问了 , 即在浏览器输入http://127.0.0.1:8000 创建一个Django模型 🍀 Django规定 , 如果要使用应用模型 , 必须要创建一个app 执行如下命令创建app python manage.py startapp blog - 同样在manage.py所在目录下执行 创建完成后就可以看到如下文件了 blog ├── migrations # 数据库相关目录 | └── __init__.py ├── __init__.py ├── admin.py # admin后台管理文件 ├── apps.py # 应用文件 ├── models.py # 模型文件 ├── tests.py # 测试文件 └── views.py # 对整个程序进行配置 PS : 我们每创建一个模型 , 都需要在settings.py 中添加配置 , 如下所示 # mysite\\settings.py \"\"\"截取文件中的片段\"\"\" # Application definition INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'blog', # 此行为添加的配置 ] 同步数据库 🍀 在同步数据库之前需要先生成同步数据库的脚本 , 一般我们创建一个模型时这一步都自动为我们生成了 , 如果没有 , 则可以使用如下命令生成同步数据库的脚本 python manage.py makemigrations 而后进行同步数据库 python manage.py migrate 访问admin 🍀 django admin是django提供的一个后台管理页面 , 如果我们使用django admin则需要提前创建好后台管理员 (超级用户) , 以及url的配置 创建后台管理员 python manage.py createsuperuser 配置后台管理url # mysite\\urls.py urlpatterns = [ url(r'^admin/', admin.site.urls), ] '''当然这一步实际上我们创建Django项目时就为我们自动完成了''' 于是我们就可以访问admin了 , 即浏览器中访问http://127.0.0.0:8000/admin/ 其他命令如下 python manage.py flush - 清空数据库 django-admin.py help startapp - 查询某个命令的详细信息 python manage.py shell - 启动交互界面 本篇仅仅对Django做一个简单的介绍 , 详细内容见后续文章 "},"05-Web框架/Django/03-Django - Settings.html":{"url":"05-Web框架/Django/03-Django - Settings.html","title":"Django - Settings","keywords":"","body":"Django - Settings 介绍 🍀 Django项目的配置信息在Django项目建立时就已经为我们创建完成 , 也就是目录下的settings.py 文件 由 python manage.py startproject 命令生成 , 每个设置都有默认值 , 这些默认值定义在django/conf/global_settings.py中 如需查看修改了哪些设置 , 可使用命令python manage.py diffsettings 显示当前settings文件与django默认设置的不同之处 本章配置文件根据Django 1.11x 描述 , 并仅为初始基本配置 SECRET_KEY 🍀 SECRET_KEY 为一个特定的Django安装密钥 , 用于提供加密签名 使用django-admin startproject 命令时为每一个项目随机生成 , 如果SECRET_KEY 没有设置 , Django将拒绝启动 DEBUG 🍀 DEBUG 配置默认为True , 在此状态下会暴露出一些错误信息或者配置信息以方便调试 , 但是上线后应该将其关掉 , 防止配置信息或者敏感错误信息泄漏 DEBUG = False ALLOWED_HOSTS 🍀 ALLOWED_HOSTS 是为了限定请求中的host值 , 以防止黑客构造包来发送请求 , 只有在列表中的host才能访问 , 建议不要使用通配符去配置 注意 : 当DEBUG设置为False时 , 该配置必须配置 , 否则会抛出异常 默认配置为空 , 配置模板如下 : ALLOWED_HOSTS = [ '.example.com', # 允许是域名或子域名 '.example.com.', # 也允许是FQDN或子域名 ] INSTALLED_APPS 🍀 INSTALLED_APPS 是一个列表 , 里面是应用中需要加载的自带或者自定义的app包路经列表 , 所以我们每创建一个应用都需要在这里进行添加 , 如下 : INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'blog', ] MIDDLEWARE 🍀 MIDDLEWARE 是一个列表 , 里面为要使用的中间件列表 , 默认如下 : MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', ] ROOT_URLCONF 🍀 ROOT_URLCONF 为一个字符串 , 代表你的根URLconf的模块名 , 如下 : ROOT_URLCONF = 'mydjango.urls' TEMPLATES 🍀 TEMPLATES 为一个包含所有模板引擎设置的列表 , 列表中的每一项都是一个包含单个引擎选项的字典 , 如下 : TEMPLATES = [ { # 要使用的模板后端,内置的模板后端如下,还有django.template.backends.jinja2.Jinja2, 'BACKEND': 'django.template.backends.django.DjangoTemplates', # 在搜索顺序中引擎应该寻找模板源文件的目录,即绝对路径 'DIRS': [os.path.join(BASE_DIR, 'templates')] , # 设置引擎是否应该在安装的应用程序中查找模板源文件 'APP_DIRS': True, # 额外的参数传递给模板后端,可用参数因模板后端而异 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] WSGI_APPLICATION 🍀 WSGI_APPLICATION 为Django内置服务器 (如runserver) 的应用程序对象的完整Python路径 django-admin startproject 执行会创建一个简单的wsgi.py 应用程序 , 并将此设置指向该应用程序 WSGI_APPLICATION = 'mydjango.wsgi.application' DATABASES 🍀 DATABASES 为一个包含所有数据库配置的字典 , 它是一个嵌套的字典 , 其内容将数据库别名映射到包含单个数据库选项的字典 数据库设置必须配置一个默认数据库 , 还可以指定任意数量的附加数据库 DATABASES = { 'default': { 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), 'USER': 'mydatabaseuser', 'PASSWORD': 'mypassword', 'HOST': '127.0.0.1', 'PORT': '5432', } } AUTH_PASSWORD_VALIDATORS 🍀 AUTH_PASSWORD_VALIDATORS 是一个列表 , 用于检查用户密码强度验证程序的列表 AUTH_PASSWORD_VALIDATORS = [ { 'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator', }, { 'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', }, { 'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator', }, { 'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator', }, ] LANGUAGE_CODE 🍀 LANGEUAGE_CODE 是一个字符串 , 默认为 'en-us' , 即一个表示安装的语言代码的字符串 , 为标准语言ID格式 , 美国英语是 'en-us' USE_I18N 必须激活此设置才能起作用 LANGUAGE_CODE = 'en-us' TIME_ZONE 🍀 TIME_ZONE 表示此安装时区的字符串 , 新版本默认为'UTC' , 这不一定是服务器的时区 , 因为一个服务器可以服务多个Django支持的站点 , 每个站点都有一个单独的时区设置 如果USE_TZ 是False , 这是Django存储所有日期时间的时区 , 如果为True , 这是Django会使用显示模板日期时间的默认时区 TIME_ZONE = 'UTC' USE_I18N 🍀 USE_I18N 为一个bool值 , 指定Django的翻译系统是否应该启动 , 如果设置为False , Django将进行一些优化 , 以避免加载翻译器 USE_I18N = True USE_L10N 🍀 USE_L10N 为一个bool值 , 它指定了默认的数据的本地化格式是否可以在默认情况下使用 , 如果为True , Django将用当前的语言环境来显示数字和日期 USE_L10N = True USE_TZ 🍀 USE_TZ 为一个bool值 , 指定日期时间默认情况下是否是时区感知的 , 如果设置为True , Django将在内部使用时区感知的日期时间 USE_TZ = True STATIC_URL 🍀 STATIC_URL 为一个引用名 , 引用位于STATIC_ROOT的静态文件 , STATIC_ROOT 如下 : STATICFILES_DIRS = ( os.path.join(BASE_DIR,'static'), # 实际名,即实际文件夹的名字 ) 注意 : 引用时 , 只能按照引用名找 , 不能按实际名 静态文件 (static) 主要指 , CSS , JavaScript , Images这样的文件 更多管理静态文件 : https://docs.djangoproject.com/en/1.11/howto/static-files/ 更多settings相关 : https://docs.djangoproject.com/en/1.11/ref/settings/ "},"05-Web框架/Django/04-Django - Urls.html":{"url":"05-Web框架/Django/04-Django - Urls.html","title":"Django - Urls","keywords":"","body":"Django - Urls 介绍 🍀 如settings.py 一样 , django-admin startproject 或者python manage.py startproject 执行创建时 , 会为我们自动创建其一个名为URLconf (URL配置) 的Python模块 , 即urls.py 通常把它称为路由系统 url.py 是纯Python代码 , 是一个简单的Python模式 (简单的正则表达式) 到Python函数 (你的视图) 之间的映射 如默认下已经有了admin这一条 from django.conf.urls import url urlpatterns = [ # 由正则表达是到urls视图函数之间的一个映射 url(r'^admin/', admin.site.urls), ] ''' urlpatterns是一个列表 列表中是一个个url()实例 ''' url参数介绍 def url(regex, view, kwargs=None, name=None): \"\"\" regex:一个正则表达式字符串 view:一个可调用对象,通常为一个视图函数或一个指定视图函数路径的字符串 kwargs:可选的要传给视图函数的默认参数,字典形式 name:可选参数name,具体可以查看源码 \"\"\" 请求处理 🍀 Django如何处理一个请求 当用户从Django支持的站点请求页面时 , 系统会遵循以下的算法来确定要执行的Python代码 : Django决定是否使用ROOT_URLCONF配置 ; 如果传入HttpRequest对象具有一个urlconf属性(由中间件设置) , 则将使用其值代替ROOT_URLCONF设置 Django加载Python模块并查找变量urlpatterns , 即urls.py中的django.conf.urls.url()实例列表 Django按顺序遍历每个URL模式 , 并停在与请求的URL匹配的第一个URL模式 , 这意味着找到一个后就不会继续往下找了 , 也就会出现覆盖现象(前面的pattern覆盖后面的pattern) 一旦一个正则表达式匹配 , Django就会导入并调用给定的视图 (视图函数) , 该函数参数如下 : 一个HttpRequest实例 如果匹配的正则表达式没有返回任何命名组 , 则将正则表达式的匹配作为位置参数提供 关键字参数由任何与正则表达式匹配的命名组组成 , 在可选的kwargs参数中指定的任何参数覆盖到django.url.urls.url() 如果没有正则表达式匹配 , 或者在这个过程中的任何一点引发异常 , Django就会调用一个合适的错误处理视图进行处理 基本示例 🍀 # 导入url函数 from django.conf.urls import url # 导入视图模块 from . import views # url()实例列表 urlpatterns = [ # r表示Python原生字符串 url(r'^articles/2003/$', views.special_case_2003), url(r'^articles/([0-9]{4})/$', views.year_archive), url(r'^articles/([0-9]{4})/([0-9]{2})/$', views.month_archive), url(r'^articles/([0-9]{4})/([0-9]{2})/([0-9]+)/$', views.article_detail), ] 说明 请求/articles/2005/03/匹配列表中的第三个条目 , Django会调用函数views.month_archive(request, '2005', '03') /articles/2005/3/ 不匹配任何URL模式 , 因为列表中的第三个条目需要两位数的月份 /articles/2003/将匹配列表中的第一个模式 , 而不是第二个模式 , 因为模式是按顺序测试的 , 而第一个模式是第一个要传递的测试 ; 你可以随意地使用这种排序来插入一些特殊的例子 , 在这里 , Django将调用函数views.special_case_2003(request) /articles/2003 将不匹配任何这些模式 , 因为每个模式都要求URL以斜杠 \"/\" 结尾 /articles/2003/03/03/将匹配最终模式 , Django会调用函数views.article_detail(request, '2003', '03', '03') 注意 : 捕获的值是作为位置参数 分组命名 🍀 # 导入url函数 from django.conf.urls import url # 导入视图函数 from . import views # url()实例列表 urlpatterns = [ url(r'^articles/2003/$', views.special_case_2003), url(r'^articles/(?P[0-9]{4})/$', views.year_archive), url(r'^articles/(?P[0-9]{4})/(?P[0-9]{2})/$', views.month_archive), url(r'^articles/(?P[0-9]{4})/(?P[0-9]{2})/(?P[0-9]{2})/$', views.article_detail), ] 说明 请求/articles/2005/03/将调用函数views.month_archive(request, year='2005',month='03') 而不是 views.month_archive(request, '2005', '03') 请求/articles/2003/03/03/调用函数 views.article_detail(request, year='2003', month='03',day='03') 注意 : 捕获的值将作为关键字参数传递 , 而不是位置参数 匹配/分组算法 如果有命名参数 , 则使用命名参数 , 忽略非命名参数 否则 , 它将传递所有非命名参数作为位置参数 额外参数 🍀 django.conf.urls.url() 函数可以接收一个可选的第三个参数 (kwargs) , 它应该是一个额外的关键字参数的字典 , 如下 : from django.conf.urls import url from . import views urlpatterns = [ # 传递额外的参数foo url(r'^blog/(?P[0-9]{4})/$', views.year_archive, {'foo': 'bar'}), ] 在上述例子中 , 对于请求/blog/2005/ , Django将调用函数views.year_archive(request, year='2005', foo='bar') 处理冲突 : 可能有一个URL模式捕获命名的关键字参数 , 并且还在其额外参数的字典中传递具有相同名称的参数 , 发生这种情况时 , 将使用字典中的参数 , 而不是在URL中捕获的参数 反向解析 🍀 django.conf.urls.url() 函数中的第四个可选参数name , 我们可以利用该参数进行反向解析 , 相当于为我们配置的第一个参数 (regex) 取一个别名 实例 from django.conf.urls import url from . import views urlpatterns = [ # 为home取别名为h1 url(r'^home', views.home, name='h1'), # 为index取别名为h2 url(r'^index/(\\d*)', views.index, name='h2'), ] 设置名称之后 , 可以在不同的地方进行反向解析 , 如 : 模板中使用生成URL 用户: 密码: 函数中使用生成URL , django.urls.reverse('h2', args=(2012,)) model中使用获取URL , 自定义get_absoulte_url() 方法 class NewType(models.Model): caption = models.CharField(max_length=16) def get_absolute_url(self): \"\"\" 为每个对象生成一个URL应用, 在对象列表中生成查看详细的URL,使用此方法即可 \"\"\" # return '/%s/%s' % (self._meta.db_table, self.id) # 或 from django.urls import reverse return reverse('NewType.Detail', kwargs={'nid': self.id}) 路由分发 🍀 如果所有应用的url都放在urls.py 这一个文件中 , 这无疑会对我们管理url造成麻烦 , Django中提供了一个django.conf.urls.include() 函数 , 可以为我们提供一个url之间的映射 , 我们把这叫做路由分发 , 如下 : myapp/urls.py # 导入url函数 from django.conf.urls import url # 从应用视图导入homepage函数 from myapp.views import homePage urlpatterns = [ url(r'homepage', homePage), ] mydjango/urls.py # 导入url函数 from django.conf.urls import url # 导入include函数 from django.conf.urls import include # 导入admin函数 from django.contrib import admin urlpatterns = [ url(r'^admin/', admin.site.urls), # 引用myapp下的urls.py url(r'^myapp/', include(\"myapp.urls\")) ] 命名空间 🍀 mydjango.urls.py from django.conf.urls import url,include urlpatterns = [ url(r'^a/', include('app01.urls', namespace='author-polls')), url(r'^b/', include('app01.urls', namespace='publisher-polls')), ] app01.urls.py from django.conf.urls import url from app01 import views app_name = 'app01' urlpatterns = [ url(r'^(?P\\d+)/$', views.detail, name='detail') ] app01.views.py def detail(request, pk): print(request.resolver_match) return HttpResponse(pk) 以上定义带命名空间的url之后 , 使用name参数生成URL时 , 应该如下 : v = reverse('app01:detail', kwargs={'pk':11}) {% url 'app01:detail' pk=12 pp=99 %} Django中的路由系统和其他语言的框架有所不同 , 在Django中每一个请求的url都要有一条路由映射 ; 其他大部分的Web框架则是对一类的url请求做一条路由映射 , 从而使路由系统变得简洁 更多URL调度相关 : https://docs.djangoproject.com/en/1.11/topics/http/urls/ "},"05-Web框架/Django/05-Django - Views.html":{"url":"05-Web框架/Django/05-Django - Views.html","title":"Django - Views","keywords":"","body":"Django - Views 介绍 🍀 在前面的文章中已经整理了关于URLconf 的相关内容 , 我们知道url() 的第二个位置参数是一个视图函数 , 简称视图 , 视图函数其实就是一个简单的Python函数 , 它的作用就是接收Web请求并且返回Web响应 URLconf 就像是Django所支撑网站的目录 , 它的本质是URL模式以及要为该URL模式调用的视图函数之间的映射表 , 也就是每一个URL都有相对应的视图进行处理 在Django中当我们创建一个应用时 , 也就是执行命令python manage.py startapp app_name , Django会自动创建一个views.py 文件 , 用来存放我们的视图函数 Django对于views.py 的文件命名没有特别的要求 , 不在乎这个文件叫什么 , 但是根据约定 , 把它命名成views.py 是个好主意 , 这样有利于其他开发者读懂你的代码 一个简单视图 🍀 我们编写一个简单的视图 , Hello视图 blog\\views.py from django.http import HttpResponse def hello(request): # 视图中必须实现响应 return HttpResponse(\"Hello Lyon\") mysite\\urls.py from blog import views urlpatterns = [ url(r'^hello/', views.hello), ] 编写完成后我们就可以通过浏览器进行访问了 , 访问http://127.0.0.1:8000/hello/ 获取结果 这样我们就完成了一个非常简单的视图 , 接下来分析一下上面的代码 : 每个视图至少要有一个参数 , 通常被叫做request , 这是一个触发这个视图 , 包含当前Web请求信息的对象 , 是django.http.HttpRequest 的一个实例 这个视图会返回一个HttpResponse 对象 , 其中包含生成的响应 . 每个视图函数都负责返回一个HttpResponse对象 在urls.py 中我们需要导入视图模块 , 并配置好URL模式 注意 : 当访问URL/hello/ 时 , Django根据settings.py中的ROOT_URLCONF 的设置装载URLconf , 然后按顺序逐个匹配URLconf里的URLpatterns , 直到找到一个匹配的 HttpRequest 🍀 Django使用请求和响应对象来通过系统传递状态 当请求页面时 , Django创建一个HttpRequest 包含关于请求的元数据的对象 , 然后Django加载响应的视图 , 将HttpRequest 作为视图函数的第一个参数传递 , 而视图则负责返回一个HttpResponse 对象 , 这点在上节中已经有说明 HttpRequest对象的属性如果没有特别说明 , 都被认为是只读的 , 下面对HttpRequest对象的属性进行说明 HttpRequest属性 🍀 class HttpRequest [source] 属性 说明 HttpRequest.scheme 表示请求方案的字符串(通常为http或https) HttpRequest.body 原始Http请求的字节字符串 , 这对于处理数据的方式与传统的HTML表单很有用 ; 对于处理传统表单数据 , 请使用HttpRequest.POST ; 可以利用HttpRequest.read() 进行查看 HttpRequest.path 表示请求页面的完整路径的字符串 , 不包括scheme和 domain HttpRequest.path_info 在某些Web服务器配置下 , 主机名后的URL部分被分成脚本前缀部分和路径信息部分 , 该path_info属性始终包含路径的路径信息部分 , 无论使用何种Web服务器 HttpRequest.method 表示请求中使用的HTTP方法的字符串 , 必须为大写 , 即('GET'或'POST') HttpRequest.encoding 表示当前编码 HttpRequest.content_type 表示请求的MIME类型的字符串 , 从CONTENT_TYPE头部解析 HttpRequest.content_params 包含在CONTENT_TYPE头部的键/值参数的字典 HttpRequest.GET 一个包含所有给定的HTTP GET 参数的字典对象 , 详情阅读 QueryDict HttpRequest.POST 一个包含所有给定HTTP POST参数的字典对象 , 前提是请求包含表单数据 , 详情阅读 QueryDict HttpRequest.COOKIES 包含所有cookies的标准Python字典对象 ; keys和values都是字符串 HttpRequest.FILES 包含所有上传文件的类字典对象 , FILES中的每一个Key都是标签中name属性的值 , FILES中的每一个value同时也是一个标准的Python字典对象 , 包含下面三个Keys : filename , 上传文件名，用字符串表示 content_type , 上传文件的Content Typecontent , 上传文件的原始内容 HttpRequest.META 包含所有可用HTTP标头的字典 HttpRequest.resolver_match 一个ResolverMatch的实例 , 它表示已解析的URL 这个属性只在URL解析发生之后才被设置 , 这意味着它在所有的视图中都可用 , 但在解析发生之前执行的中间件中是不可用的 应用程序设置的属性 🍀 Django本身没有设置这些属性 , 但是如果你的应用程序设置了这些属性 , 就可以使用它们 属性 说明 HttpRequest.current_app url模板标签将使用它的值作为reverse() 的current_app参数 HttpRequest.urlconf 这将用作当前请求的根URLconf , 会覆盖ROOT_URLCONF设置 urlconf可以设置为None恢复以前中间件所做的任何更改并返回到使用ROOT_URLCONF 中间件设置的属性 🍀 包含在Django的contrib应用程序中的一些中间件在请求中设置了属性 , 如果在请求中看不到该属性，请确保列出了相应的中间件类MIDDLEWARE 属性 说明 HttpRequest.session 来自 SessionMiddleware : 唯一可读写的属性 , 代表当前会话的字典对象 ; 只有激活Django中的session支持时该属性才可用 HttpRequest.site 来自CurrentSiteMiddleware : 代表当前网站的实例Site或 RequestSite通过 get_current_site() 获取 HttpRequest.user 来自AuthenticationMiddleware : AUTH_USER_MODEL代表当前登录用户的一个实例。如果用户当前没有登录，user将被设置为一个AnonymousUser 实例 , 可以用 is_authenticated 进行区分 HttpRequest对象的方法点这里 : Methods 常用 get_full_path() , 返回path , 加上一个附加的查询字符串(如果使用) \"/music/bands/the_beatles/?print=true\" HttpResponse 🍀 HttpRequest对象是由Django自动创建的 , 而HttpResponse 对象是由我们自己来创建的 , 我们写的视图负责实例化 , 填充和返回一个HttpResponse 对象 这个HttpResponse 类定义在 django.http模块中 用法 🍀 传递字符串 典型的用法是将页面内容作为字符串传递给HttpResponse构造函数 >>> from django.http import HttpResponse >>> response = HttpResponse(\"Here's the text of the Web page.\") >>> response = HttpResponse(\"Text only, please.\", content_type=\"text/plain\") 如果想增加内容 , 可以将其当做类似文件对象使用 >>> response = HttpResponse() >>> response.write(\"Here's the text of the Web page.\") >>> response.write(\"Here's another paragraph.\") 传递迭代器 除了传递字符串 , 还可以传递一个迭代器对象 ; 它会立即使用迭代器 , 将其内容存储为字符串 , 然后丢弃 如果需要将响应以迭代器对象传输到客户端 , 则必须使用StreamingHttpResponse 类 设置标题栏 要设置或删除响应中的标题字段 , 可以将其视为字典 >>> response = HttpResponse() >>> response['Age'] = 120 >>> del response['Age'] 注意 : 如果标题字段不存在 , del 不会引发KeyError , 这一点与字典不同 告诉浏览器对待响应作为文件附件 要告诉浏览器将响应当做文件附件处理 , 可以使用content_type 参数并设置Content-Disposition 标题 返回一个Excel电子表格 >>> response = HttpResponse(my_data, content_type='application/vnd.ms-excel') >>> response['Content-Disposition'] = 'attachment; filename=\"foo.xls\"' 属性 🍀 HttpResponse属性 属性 说明 HttpResponse.content 表示内容的字符串 HttpResponse.charset 表示响应将被编码的字符串的字符串 , 如果在HttpResponse实例化的时候没有给出 , 则会从中提取 content_type , 如果不成功 , DEFAULT_CHARSET将使用该 设置 HttpResponse.status_code 该响应的 HTTP状态码 , 除非reason_phrase明确设置 , 否则修改 status_code构造函数外部的值也会修改值 reason_phrase HttpResponse.reason_phrase 响应的HTTP原因短语 , 它使用的HTTP标准的默认原因短语。 除非明确规定 , reason_phrase由价值决定status_code HttpResponse.streaming 此属性存在使中间件可以以不同于常规响应的方式处理流响应 , 一直为False HttpResponse.closed 如果响应已经结束则为True HttpResponse对象方法可见 : Method HttpResponse子类 🍀 Django包含许多HttpResponse处理不同类型HTTP响应的子类 , 这些子类都在django.http 中 , 子类如下 : class HttpResponseRedirect [source] class HttpResponsePermanentRedirect [source] class HttpResponseNotModified [source] class HttpResponseBadRequest [source] class HttpResponseNotFound [source] class HttpResponseForbidden [source] class HttpResponseNotAllowed [source] class HttpResponseGone [source] class HttpResponseServerError [source] 更多子类相关 : HttpResponse相关 https://docs.djangoproject.com/en/1.11/_modules/django/http/response/ render 🍀 为了方便 , Django中有一个shortcuts模块 , 其中收集了跨越多个级别的MVC的帮助函数和类 这里介绍一下其中的render()和redirect() : render , 对html进行渲染 redirect , 从当前页面进行跳转 def render(request, template_name, context=None, content_type=None, status=None, using=None): \"\"\" Returns a HttpResponse whose content is filled with the result of calling django.template.loader.render_to_string() with the passed arguments. \"\"\" content = loader.render_to_string(template_name, context, request, using=using) return HttpResponse(content, content_type, status) 必要参数 : request : 用于生成此响应的请求对象 template_name : 要使用的模板的全名或模板名称的序列 , 如果给出了一个序列 , 将使用存在的第一个模板 可选参数 : context : 要添加到模板上下文的值的字典 content_type : 用于生成文档的MIME类型 , 默认为DEFAULT_CONTENT_TYPE设置的值 status : 响应的状态码 , 默认为200 using : settings.py中NAME配置的模板引擎使用加载的模板 实例 from django.shortcuts import render def my_view(request): # View code here... return render(request, 'myapp/index.html', {'foo': 'bar',}, content_type='application/xhtml+xml') 上述实例相当于 from django.http import HttpResponse from django.template import loader def my_view(request): # View code here... t = loader.get_template('myapp/index.html') c = {'foo': 'bar'} return HttpResponse(t.render(c, request), content_type='application/xhtml+xml') redirect 🍀 def redirect(to, *args, **kwargs): \"\"\" Returns an HttpResponseRedirect to the appropriate URL for the arguments passed. The arguments could be: * A model: the model's `get_absolute_url()` function will be called. * A view name, possibly with arguments: `urls.reverse()` will be used to reverse-resolve the name. * A URL, which will be used as-is for the redirect location. By default issues a temporary redirect; pass permanent=True to issue a permanent redirect \"\"\" if kwargs.pop('permanent', False): redirect_class = HttpResponsePermanentRedirect else: redirect_class = HttpResponseRedirect return redirect_class(resolve_url(to, *args, **kwargs)) 默认返回一个临时的重定向 ; 传递permanent=True 可以返回一个永久的重定向 参数 : A model: the model’s get_absolute_url() function will be called. A view name, possibly with arguments: reverse() will be used to reverse-resolve the name. An absolute or relative URL, which will be used as-is for the redirect location. 实例 通过传递一些对象 , 该对象的get_absolute_url() 方法将被调用来找出重定向URL from django.shortcuts import redirect def my_view(request): ... object = MyModel.objects.get(...) return redirect(object) 通过传递一个视图的名称和可选的一些参数 , 该URL将通过reverse()` 方法反向解析 def my_view(request): ... return redirect('some-view-name', foo='bar') 通过传递一个硬编码的URL进行重定向 def my_view(request): ... return redirect('/some/url/') 也适用于完整的网址 def my_view(request): ... return redirect('https://example.com/') 默认情况下，redirect()返回一个临时重定向 , 所有上述形式都接受permanent参数 ; 如果设置为True永久重定向将被返回 def my_view(request): ... object = MyModel.objects.get(...) return redirect(object, permanent=True) 更多shortcuts内容 : Django shortcut functions The Django Book : http://docs.30c.org/djangobook2/index.html "},"05-Web框架/Django/06-Django - Model.html":{"url":"05-Web框架/Django/06-Django - Model.html","title":"Django - Model","keywords":"","body":"Django - Model 介绍 🍀 在我们的Web应用中 , 与数据库的交互不可避免 ; 数据库驱动网站在后台链接数据库服务器 , 从中取出一些数据 , 然后在Web页面用漂亮的格式展示这些数据 , 这就是我们需要的模型(Model) 我们知道Python中的sqlalchemy 是一个第三方的ORM框架 , Django中的Model也是通过ORM来进行数据库管理的 特点 : 每个模型都是django.db.models.Molde 的一个Python子类 模型的每个属性都表示为数据库中的一个字段 一旦我们创建一个数据模型 , Django会自动为我们提供一个数据抽象API , Making queries 当我们执行python manage.py startapp app_name 命令时 , Django就已经为我们自动创建了一个models.py 文件 , 就是用来存放我们的模型定义的 注意 : 我们创建模型前 , 需要将数据库相关配置完成 , 方法见settings整理文章 简单示例 🍀 首先创建一个应用 python manage.py startapp myapp 安装应用 # settings.py INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'myapp', # 此行为添加项 ] 定义模型 # myapp/models.py # 导入models from django.db import models # 必须继承models.Model class Userinfo(models.Model): username = models.CharField(max_length=30) password = models.CharField(max_length=20) 生成模型 # Django 1.6.x 及以下 python manage.py syncdb # Django 1.7.x 及以上 python manage.py makemigrations # 生成同步记录 python manage.py migrate # 开始同步 这里我使用的是Django 1.7.x 及以上版本 , 可见信息如下 : # 生成同步记录 mysite>python manage.py makemigrations No changes detected # 开始同步 mysite>python manage.py migrate Operations to perform: Apply all migrations: admin, auth, contenttypes, sessions Running migrations: Applying contenttypes.0001_initial... OK Applying auth.0001_initial... OK Applying admin.0001_initial... OK Applying admin.0002_logentry_remove_auto_add... OK Applying contenttypes.0002_remove_content_type_name... OK Applying auth.0002_alter_permission_name_max_length... OK Applying auth.0003_alter_user_email_max_length... OK Applying auth.0004_alter_user_username_opts... OK Applying auth.0005_alter_user_last_login_null... OK Applying auth.0006_require_contenttypes_0002... OK Applying auth.0007_alter_validators_add_error_messages... OK Applying auth.0008_alter_user_username_max_length... OK Applying sessions.0001_initial... OK # 默认使用sqlite 以上的Person模型会在数据库中创建这样一张表 : CREATE TABLE myapp_userinfo ( \"id\" serial NOT NULL PRIMARY KEY, \"username\" varchar(30) NOT NULL, \"password\" varchar(20) NOT NULL ); 注意 : 表名称是根据模型中的某些元数据自动生成的 , 也可以重写 , Table names id字段是Django自动为我们添加的一个主键 , 也可以重写 , Automatic primary key fields Django会根据settings.py 中指定的数据库类型来使用相应的SQL语句 如果我们使用的数据库为MySQL : Django对于MySQL默认使用的是MySQLdb模块 , 而Python 3 中是没有MySQLdb的 , 使用的是pymsql , 所以我们需要添加以下内容 # mysite/__init__.py import pymysql pymysql.install_as_MySQLdb()　 # settings DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME':'dbname', 'USER': 'root', 'PASSWORD': 'xxx', 'HOST': '', 'PORT': '', } } 添加数据 🍀 添加数据需要先创建对象 , 然后再执行save函数 , 相当于SQL中的INSERT myapp/views.py # 一步完成:使用create函数 from django.http import HttpResponse from myapp import models def add_user(request): models.Userinfo.objects.create(username='Lyon', password='123456') # models.Userinfo.objects.create(**user1) user1={'username':'Lyon','password':'123465'} return HttpResponse('数据添加成功!') # 先创建对象后执行操作:使用save函数 from django.http import HttpResponse from myapp import models def add_user(request): user1 = models.Userinfo(username='Lyon', password='123456') user1.save() return HttpResponse('数据添加成功!') mysite/urls.py from myapp import views urlpatterns = [ url(r'^add_user/', views.add_user), ] 访问http://127.0.0.1:8000/testdb/ 完成添加 查询数据 🍀 myapp/views.py from myapp import models def select_user(request): # 通过objects这个模型管理器的all()获得所有数据行,相当于SQL中的SELECT * FROM result = models.Userinfo.objects.all() # filter相当于SQL中的WHERE,可设置条件过滤结果 response1 = models.Userinfo.objects.filter(id=1) # 获取单个对象 response2 = models.Userinfo.objects.get(id=1) # 限制返回的数据,相当于SQL中的OFFSET 0 LIMIT 2; models.Userinfo.objects.order_by('username')[0:2] # 数据排序 models.Userinfo.objects.order_by(\"id\") # 上面的方法可以连锁使用 models.Userinfo.objects.filter(username=\"Lyon\").order_by(\"id\") # all()返回的是一个QuerySet对象,即封装了一行数据的所有属性的对象 for var in result: print(var.id,var.username,var.password) print(response1) print(response2) return HttpResponse('查询成功!') mysite/urls.py from myapp import views urlpatterns = [ url(r'^select_user/', views.select_user), ] 删除数据 🍀 myapp/views.py from myapp import models def delete_user(request): models.Userinfo.objects.filter(username='Lyon').delete() models.Userinfo.objects.all().delete() return HttpResponse('删除成功!') mysite/urls.py from myapp import views urlpatterns = [ url(r'^delete_user/', views.delete_user), ] 更新数据 🍀 myapp/views.py from myapp import models def update_user(request): models.Userinfo.objects.filter(username='Lyon').update(username='Kenneth') models.Userinfo.objects.all().update(password='456789') return HttpResponse('更新成功!') mysite/urls.py from myapp import views urlpatterns = [ url(r'^update_user/', views.update_user), ] 本章仅对Model进行简单的操作介绍 详细参考 : Model instance reference "},"05-Web框架/Django/07-Django - Model Fields.html":{"url":"05-Web框架/Django/07-Django - Model Fields.html","title":"Django - Model Fields","keywords":"","body":"Django - Model Fields 介绍 🍀 对于一个模型来说 , 最重要的和不可或缺的是列出该模型在数据库中定义的字段 字段由属性指定 , 但是选择的字段名称不要和models API 冲突 , 比如 save , clean 或者 delete , 如下 : from django.db import models class Musician(models.Model): first_name = models.CharField(max_length=50) last_name = models.CharField(max_length=50) instrument = models.CharField(max_length=100) class Album(models.Model): artist = models.ForeignKey(Musician, on_delete=models.CASCADE) name = models.CharField(max_length=100) release_date = models.DateField() num_stars = models.IntegerField() 字段类型 🍀 模型中的每个字段都是 Field 子类的某个实例 , Djnaog根据字段的类型确定以下信息 : 列类型 , 它告知数据库要存储哪种数据 (如 , INTEGER , VARCHAR , TEXT) 渲染表单时使用的默认HTML widget , 例如 : , ). 即使用select小部件 最低限度的验证需求 , 它被用在Django管理站点和自动生成的表单中 Django拥有数十种内置的字段类型 ; 你可以在 model field reference 中找到完整列表 , 如果Django内置的字段不能满足我们的要求 , 那么我们可以进行自定义模型字段 : Writing custom model fields 字段选项 🍀 每个字段都接受一组与字段有关的参数 , 例如 , CharField (和它的派生类) 需要max_length 参数来指定VARCHAR 数据库字段的大小 而对于所有的字段类型 , 都有一组通用的参数可供使用 , 以下介绍一些最常用的 : 参数 说明 null 如果为True , Django将会把数据库中的空值保存为NULL ; 默认为False blank 如果为True , 该字段允许为空值 ; 默认为False choices 一个二元组组成的可迭代对象 (list或tuple) , 用来给字段提供选择项 ; 如果设置了choices , 默认的表单将是一个选择框而不是标准的文本框 , 而且这个选择框的选项就是choices中的选项choices实例见表格补充 default 默认值 , 可以是一个值或可调用对象 , 如果是可调用对象 , 那么每个新对象被创建它都会被调用 help_text 表单部件额外显示的帮助内容 , 即使字段不在表单中使用 , 它对生成文档也很有用 primary_key 如果为True , 那么这个字段就是模型的主键如果没有指定任何一个字段的primary_key=True , Django就会自动添加一个IntegerField字段作为主键 , 也就是字段 id主键字段是只读的 , 如果你在一个已存在的对象上面更改主键的值并且保存 , 那么就会在原有对象之外创建出一个新的对象 unique 如果True , 则这个字段在整张表中必须是唯一的 表格补充 : choices实例 YEAR_IN_SCHOOL_CHOICES = ( ('FR', 'Freshman'), ('SO', 'Sophomore'), ('JR', 'Junior'), ('SR', 'Senior'), ('GR', 'Graduate'), ) ''' 元组中的第一个元素是将被存储在数据库中的值,第二个元素将由默认窗体小部件或ModelChoiceField显示 给定一个模型实例,可以使用get_FOO_display()方法来访问选项字段的显示值 ''' 访问选项字段的显示值 from django.db import models class Person(models.Model): SHIRT_SIZES = ( ('S', 'Small'), ('M', 'Medium'), ('L', 'Large'), ) name = models.CharField(max_length=60) shirt_size = models.CharField(max_length=1, choices=SHIRT_SIZES) # 创建Person对象 >>> p = Person(name=\"Fred Flintstone\", shirt_size=\"L\") # 保存到数据库 >>> p.save() # 查看shirt_size属性 >>> p.shirt_size 'L' # 查看shirt_size显示值 >>> p.get_shirt_size_display() 'Large' 注 : 上述仅仅对最常见的字段选项进行说明 , 完整查看 common model field option reference 自增主键字段 🍀 默认 , Django给了每个模型一个主键字段 : id = models.AutoField(primary_key=True) 这是一个自增主键 当我们想指定一个自定义主键字段时 , 只需要在某个字段上指定primary_key=True 即可 , 因为Django看到你显示地设置了Field.primary_key 就不会自动添加id 列 每个模型只能有一个字段指定primary_key=True 字段的自述名 🍀 除了ForeignKey , ManyToManyField 和 OneToOneField 之外 , 每个字段类型都接受一个可选的位置参数 , 即字段的自述名 (在第一的位置) ; 如果没有给定自述名 , Django将根据字段的属性名称自动创建自述名 , 即将属性名称的下划线替换成空格 实例 # 自述名为person's first name first_name = models.CharField(\"person's first name\", max_length=30) # 自述名为first name first_name = models.CharField(max_length=30) ForeignKey , ManyToManyField 和OneToOneField都要求第一个参数是一个模型类 , 所以要使用verbose_name 关键字参数才能指定自述名 : poll = models.ForeignKey( Poll, on_delete=models.CASCADE, verbose_name=\"the related poll\", ) sites = models.ManyToManyField(Site, verbose_name=\"list of sites\") place = models.OneToOneField( Place, on_delete=models.CASCADE, verbose_name=\"related place\", ) 习惯上 , verbose_name 的首字母不用大写 , Djnaog在必要的时候会自动大写首字母 数据库关系 🍀 关系型数据库的威力体现在表之间的相互关联 , 而Django提供了三种最常见的数据库关系 : 多对一 , many - to - one 多对多 , many - to - many 一对一 , ont - to - one 多对一 🍀 Django使用django.db.models.ForeignKey 定义多对以关系 和使用其它字段类型一样 , 在模型当中把它作为一个类属性使用 class ForeignKey(to, on_delete, **options): \"\"\" ForeignKey需要两个位置参数,模型相关联的类和on_delete选项 更多详细内容:https://docs.djangoproject.com/en/1.11/ref/models/fields/#foreignkey \"\"\" 实例 from django.db import models # 制造商可以生产很多汽车 class Manufacturer(models.Model): # ... pass # 每一辆汽车只能有一个制造商 class Car(models.Model): manufacturer = models.ForeignKey(Manufacturer, on_delete=models.CASCADE) # ... 若要创建递归关联关系 , 即具有多对一关系的对象需要使用 models.ForeignKey('self' , on_delete=models.CASCADE) 如果需要在尚未定义的模型上创建关系 , 可以使用模型的名称 , 而不是模型对象本身 : from django.db import models class Car(models.Model): manufacturer = models.ForeignKey( 'Manufacturer', on_delete=models.CASCADE, ) # ... class Manufacturer(models.Model): # ... pass # 更多说明:https://docs.djangoproject.com/en/1.11/ref/models/fields/#ref-foreignkey ForeignKey字段还接受许多别的参数 , 更多 : the model field reference 多对一更多示例 : Many-to-one relationship model example 多对多 🍀 ManyToManyField 字段是用来定义多对多关系的 , 同使用其他字段类型一样 class ManyToManyField(to, **options): \"\"\" 多对多关系,需要一个位置参数:模型相关联的类 它与ForeignKey的工作方式完全相同,包括递归和延迟(未定义)关系 \"\"\" 实例 from django.db import models # 一种装饰可以在多个Pizza上 class Topping(models.Model): # ... pass # 一个Pizza上可以有多种装饰 class Pizza(models.Model): # ... toppings = models.ManyToManyField(Topping) 官方文档中建议以被关联模型名称的复数形式作为ManyToManyField 的名字 , 如上实例中为toppings 对于多对多关系 , 在哪个模型中设置ManyToManyField 都可以 , 但是不要两个模型都设置 完整示例 : Many-to-many relationship model example 中介模型 🍀 对与上一节中Pizza和Topping搭配这样简单的多对多关系时 , 使用标准的ManyToManyField 就可以了 , 但是有时我们可能需要关联数据到两个模型之间的上 例如 , 有这样一个应用 , 它记录音乐家所属的音乐小组 , 我们可以用一个ManyToManyField 表示小组和成员之间的多对多关系 , 但是 , 有时你可能想知道更多成员关系的细节 , 比如成员是何时加入小组的 , 对于这些情况 , Django允许你指定一个中介模型 来定义多对多关系 , 你可以将其他字段放在中介模型里面 源模型的ManyToManyField 字段将使用through参数指向中介模型 , 实例 : from django.db import models # 音乐家,目标模型 class Person(models.Model): name = models.CharField(max_length=128) def __str__(self): # __unicode__ on Python 2 return self.name # 音乐小组,源模型 class Group(models.Model): name = models.CharField(max_length=128) # 通过through参数指向中介模型Membership members = models.ManyToManyField(Person, through='Membership') def __str__(self): # __unicode__ on Python 2 return self.name # 中介模型 class Membership(models.Model): person = models.ForeignKey(Person, on_delete=models.CASCADE) group = models.ForeignKey(Group, on_delete=models.CASCADE) date_joined = models.DateField() invite_reason = models.CharField(max_length=64) 中介模型的一些限制 : 中介模型必须有且只有一个外键到源模型 , 或者必须使用ManyToManyField.through_fields 显示指定Django应该在关系中使用的外键 ; 如果你的模型中存在不止一个外键 , 并且through_fields 没有指定 , 将会触发一个无效的错误 . 对目标模型的外键有相同的限制(Person) 对于通过中介模型与自己进行多对多关联的模型 , 允许存在到同一个模型有两个外键 , 但他们将被当做多对多关联中一个关系的两边 ; 如果有超过两个外键 , 同样你必须像上面一样指定through_fields , 否则将引发一个验证错误 使用中介模型定义与自身的多对多关系时 , 你必须设置symmetrical=False , 详细见the model field reference 设置好中介模型(Membership)后 , 接下来要开始创建多对多关系 , 首先创建中介模型的实例 : >>> ringo = Person.objects.create(name=\"Ringo Starr\") >>> paul = Person.objects.create(name=\"Paul McCartney\") >>> beatles = Group.objects.create(name=\"The Beatles\") >>> m1 = Membership(person=ringo, group=beatles, ... date_joined=date(1962, 8, 16), ... invite_reason=\"Needed a new drummer.\") >>> m1.save() >>> beatles.members.all() ]> >>> ringo.group_set.all() ]> >>> m2 = Membership.objects.create(person=paul, group=beatles, ... date_joined=date(1960, 8, 1), ... invite_reason=\"Wanted to form a band.\") >>> beatles.members.all() , ]> 与普通的多对多字段不同 , 不能使用add() , create() 和 set() 来建立关系 : >>> # The following statements will not work >>> beatles.members.add(john) >>> beatles.members.create(name=\"George Harrison\") >>> beatles.members.set([john, paul, ringo, george]) 因为你不能只创建Membership 和Group 之间的关联关系 , 你还要指定Person模型中所需要的所有信息 , 而简单的add() , create() 和复制语句是做不到这一点的 ; 所以它们不能在使用中介模型的多对多关系中使用 , 唯一的办法就是创建中介实例 同上remove() 也被禁用 , 但是clear()方法是可以用的 , 因为它会清空某个实例所有的多对多关系 >>> # Beatles have broken up >>> beatles.members.clear() >>> # Note that this deletes the intermediate model instances >>> Membership.objects.all() 建立好关系之后 , 就可以执行查询了 '''直接使用被关联模型的属性进行查询''' # Find all the groups with a member whose name starts with 'Paul' >>> Group.objects.filter(members__name__startswith='Paul') ]> '''利用中介模型的属性进行查询''' # Find all the members of the Beatles that joined after 1 Jan 1961 >>> Person.objects.filter( ... group__name='The Beatles', ... membership__date_joined__gt=date(1961,1,1)) '''直接获取Membership模型''' >>> ringos_membership = Membership.objects.get(group=beatles, person=ringo) >>> ringos_membership.date_joined datetime.date(1962, 8, 16) >>> ringos_membership.invite_reason 'Needed a new drummer.' '''还有一种方法是在Person对象查询多对多反向关系''' # 详细见https://docs.djangoproject.com/en/1.11/topics/db/queries/#m2m-reverse-relationships >>> ringos_membership = ringo.membership_set.get(group=beatles) >>> ringos_membership.date_joined datetime.date(1962, 8, 16) >>> ringos_membership.invite_reason 'Needed a new drummer.' 一对一 🍀 OneToOneField 用来定义一对一关系 , 和使用其它字段类型一样 当某个对象想扩展自另一个对象时 , 最常用的方式就是这个对象的主键上添加一对一关系 class OneToOneField(to, on_delete, parent_link=False, **options): \"\"\" OneToOneField和ForeignKey以及ManyToManyField一样 它有两个位置参数:相关联的类以及on_delete选项 parent_link:该参数为True且使用从另一个具体模型继承的模型时,表明该字段应该作为返回到父类的链接,而 而不是通常由子类隐式创建的额外的OneToOneField \"\"\" 如果你正在建立一个\"places\" 的数据库 , 那么你将建立一个非常标准的地址 , 电话号码等 , 在数据库中; 接下来 , 如果你想在places数据库的基础上建立一个restaurant数据库 , 而不想将已有的字段复制到Restaurant模型 , 那么你可以在Restaurant添加一个OneToOneField字段 , 这个字段指向Place (因为Restaurant本身就是一个Place) , 事实上 , 在处理这个问题时 , 我们应该使用一个典型的inheritance , 因为它隐含一个一对一关系 示例 : https://docs.djangoproject.com/en/1.11/topics/db/examples/one_to_one/ 与ForeignKey一样 , 可以定义 recursive relationship 和 references to as-yet undefined models Meta选项 🍀 使用内部的class Meta 可以定义模型的元数据 , 如下 : from django.db import models class Ox(models.Model): horn_length = models.IntegerField() class Meta: ordering = [\"horn_length\"] verbose_name_plural = \"oxen\" 模型元数据是任何不是字段的数据 , 比如排序选项 (ordering) , 数据库表名 (db_table) 或者可读的单复数名称 (verbose_name和verbose_name_plural) 所有Meta选项的完整列表见 : model option reference 更多相关内容 : https://docs.djangoproject.com/en/1.11/topics/db/models/ "},"05-Web框架/Django/08-Django - Model Field Options.html":{"url":"05-Web框架/Django/08-Django - Model Field Options.html","title":"Django - Model Field Options","keywords":"","body":"Django - Model Field Options 介绍 🍀 本篇文章为\"工具\"笔记 , 适合用于翻阅查找 , 对于字段元选项的总结 以下字段选项对于所有字段类型都是可选的 null 🍀 说明 如果为 True , 则Django将在数据库中将空值存储为NULL , 也意味着该字段可填可不填 , 默认为 False 示例 class Student(models.Model): name = models.CharField(null=True) 特别的 : 应该避免在基于字符串的字段中字段中使用 null=True , 例 : CharField 和 TextField ; 因为空字符串值会存储为空字符串而不是NULL , 而如果使用该选项 , 则意味着 \"无数据\" 可能有两种状态 , NULL 和空字符串 , 并且在大多数情况下 , Django中惯例使用空字符串而不是 NULL 如果在 CharField 中同时拥有两个选项 , unique=True 与 blank=True , 那么需要设置 null=True 来防止在保存多个空白值 (blank=True的结果)时违反 unique=True 唯一约束 null 选项是纯粹的数据库范畴 , 针对数据库的选项 , 即数据库中 , 数据是否能为空值 如果我们需要在BooleanField 字段中设置 null , 我们应该使用 NullBooleanField 字段来代替 blank 🍀 说明 与上面的 null 不同 , blank 针对的是数据验证的范畴 , 如在使用 admin 录入数据时 , 默认不允许输入空值 , 通过设置 blank=True 即可输入空值 在设计表时 , 如果仅设置 null=True , 那么在使用 admin 录入数据时 , 是不可输入空值的 示例 class Student(models.Model): name = models.CharField(blank=True) choices 🍀 说明 设置 choices 后 , 该字段就为一个选择框 (设置 max_length 选项指定可选数) , 设置值为一个可迭代的结构 , 如上示例中 : 每个元组中的第一个元素 , 是存储在数据库中的值 ; 第二个元素是该选项的描述值 示例 class Student(models.Model): YEAR_IN_SCHOOL_CHOICES = ( ('FR', 'Freshman'), ('SO', 'Sophomore'), ('JR', 'Junior'), ('SR', 'Senior'), ) # default为默认选择项 year_in_school = models.CharField(max_length=2, choices=YEAR_IN_SCHOOL_CHOICES, default='FR') 除了单层的二元元组 , 还可以设置成多成二元元组 : MEDIA_CHOICES = ( ('Audio', ( ('vinyl', 'Vinyl'), ('cd', 'CD'), ) ), ('Video', ( ('vhs', 'VHS Tape'), ('dvd', 'DVD'), ) ), ('unknown', 'Unknown'), ) 同样的 , 第一个元素为组的名字 , 第二个元素为描述值 , 并且每个二元元组的第一个元素是不可选的 , 可选选项为最里层的二元元组的第二个元素 , 例如 : 'Vinyl' , 'CD' Django中 , 对于每一个选择框 , 都有一个 get_fieldname_display() 方法以获取描述值 (二元元组的第二个元素) , 如下 : >>> stu = Student(year_in_school=\"FR\") >>> stu.save() >>> stu.year_in_school 'FR' >>> stu.get_year_in_school_display() 'Freshman' PS : 如果不指定默认选项 , 那么选择菜单默认为 \"-----------\" , 如果我们要进行重写 , 只需在元组添加一项包含None的元组到 choices 中 , 如 : (None, 'Your Choices') db_column 🍀 说明 指定在数据库中的字段名 , 默认使用 Field 名 (即示例中的 name ) 示例 class Student(models.Model): name = models.CharField(max_length=32, db_column=\"姓名\") # 数据库表 +----+---------+ | id | 姓名 | +----+---------+ | 1 | lyon | +----+---------+ db_index 🍀 说明 为字段创建索引 , 默认为 False , 设置为 True Django会为该字段创建数据库索引 示例 class Student(models.Model): name = models.CharField(max_length=32, db_index=True) db_tablespace 🍀 待整理 default 🍀 说明 设置字段的默认值 , 该值可为一个值或一个可调用对象 , 但是不可为可变对象 (dict , list , ...) ; 如果为一个可调用对象 , 那么每次创建实例时都会被调用一次 示例 def contact_default(): return {\"email\": \"to1@example.com\"} class Student(models.Model): name = models.CharField(max_length=32, default=\"未知\") # 默认值为可调用对象 contact_info = JSONField(\"ContactInfo\", default=contact_default) PS : 可调用对象不可使用 lambdas 函数 , 因为这类参数无法被 migrations 命令进行序列化 editable 🍀 说明 默认为 True , 如果设置为 False , 那么这个字段将不会出现 admin (本质使用了ModelForm) 或者其他 ModelForm 中 示例 class Student(models.Model): role = models.CharField(max_length=32, editable=False) error_messages 🍀 说明 指定 error_messages 选项可以更改该字段将引发的默认错误信息 , error_messages 为一个字典 , 其中 key 包括 : null , blank , invalid , invalid_choice , unique 以及 unique_for_data (Django 1.7以上版本后添加) 示例 class Student(models.Model): name = models.CharField(max_length=32, db_column=\"姓名\", error_messages={ 'null':\"不能为空\", }) 它们的默认错误信息如下 : default_error_messages = { 'invalid_choice': _('Value %(value)r is not a valid choice.'), 'null': _('This field cannot be null.'), 'blank': _('This field cannot be blank.'), 'unique': _('%(model_name)s with this %(field_label)s ' 'already exists.'), # Translators: The 'lookup_type' is one of 'date', 'year' or 'month'. # Eg: \"Title must be unique for pub_date year\" 'unique_for_date': _(\"%(field_label)s must be unique for \" \"%(date_field_label)s %(lookup_type)s.\"), # 特殊的,在Field的派生类中定义,以DateField为例 'invalid': \"'%(value)s' value has an invalid date format. It must be \" \"in YYYY-MM-DD format.\" } help_text 🍀 说明 该选项会在表单控件form中 , 添加一些文档 , 文档会出现在 input 框下方 示例 class Student(models.Model): name = models.CharField(max_length=32, help_text='这里填名字') PS : 默认会对文档进行HTML转换 , 所以为了避免任何HTML特定的字符 , 我们可以使用简单文本或 django.utils.html.escape() 进行转换 , 以防止用户进行的跨站点脚本攻击 primary_key 🍀 说明 指定该字段为主键 , 默认情况下 , 如果在model中没有指定 primary_key=True , Django会自动添加一个 AutoField 来作为主键 , 即默认创建的 id 字段 示例 class Student(models.Model): sid = models.AutoField(primary_key=True) PS : 主键字段是只读的 , 如果你更改现有对象的主键的值 , 然后将其保存 , 该结果并不是更改原对象的值 , 而是创建一个新对象 unique 🍀 说明 唯一约束 , 如果为True , 该字段在表中必须是唯一的 示例 class Student(models.Model): name = models.CharField(max_length=32, unique=True) PS : 这个选项同 null 一样 , 是一个在数据库级别与Form验证级别的强制性动作 ; 该选项对于 ManyToManyField 和 OneToOneField 是无效的 只要设置 unique=True , 意味着创建唯一索引 , 所以不需要再次指定 db_index 选项 版本区别 : 在Django 1.11 中 , 为了版本支持 , unique=True 不可以使用 FileField unique_for_date 🍀 说明 用于设置时间相关字段的值唯一 , 即针对 DateField 和 DateTimeField 字段 示例 class Student(models.Model): opening_date = models.DateField(unique_for_date=True) PS : 该选项是在Form验证期间通过 Model.validate_unique() 强制执行的 , 而不是在数据库级别进行的 , 所以这就意味着 , 如果字段中有 editable=True , 那么 Model.validate_unique() 将忽略该约束 unique_for_month 🍀 类似于 unique_for_date , 只是要求字段对于月份是唯一的 unique_for_year 🍀 类似于 unique_for_date , 只是要求字段对于年份是唯一的 verbose_name 🍀 说明 为字段设置一个可读性更高的名称 , 如果未设置该选项 , 那么Django在Form中默认会使用字段名 , 并会将字段名中的 \"_\" 转换成空格显示以及自动首字母大写 示例 class Student(models.Model): name = models.CharField(max_length=32, verbose_name=\"学生姓名\") validators 🍀 说明 可以自定制验证器 , 即验证值是否符合要求 , 它的值为一个列表 , 列表中为可调用对象 示例 from django.core.exceptions import ValidationError from django.utils.translation import ugettext_lazy as _ def validate_even(value): if value % 2 != 0: raise ValidationError( _('%(value)s is not an even number'), params={'value': value}, ) from django.db import models class MyModel(models.Model): even_field = models.IntegerField(validators=[validate_even]) 除了我们自定制之外 , Django为我们提供了很多的内置验证器 , 我们可从 django.core.validators 中进行导入 本文参考 : https://docs.djangoproject.com/en/1.11/ref/models/fields "},"05-Web框架/Django/09-Django - Model QuerySet API.html":{"url":"05-Web框架/Django/09-Django - Model QuerySet API.html","title":"Django - Model QuerySet API","keywords":"","body":"Django - Model QuerySet API 介绍 🍀 我们知道Django中存在着大量的接口 , 而跟QuerySet 就是一个Model相关的接口 , 它建立在 model 和 database query 指南的基础上 , 而这两个指南已经在前面的文章整理完成了 , 但是对于QuerySet API的整理还不完全 本篇中依然会使用在上一篇中使用的例子 上一篇中我们已经知道 , 当我们不对QuerySet 进行求值时 , 它会像生成器一样 , 不做任何反应 QuerySet 求值有以下方法 : 迭代 , QuerySet 是可迭代的 , 它在首次迭代查询集时会对数据库进行查询 , 实例如下 : for e in Entry.objects.all(): print(e.headline) ''' 这两条语句虽然可以验证在数据库中是否至少存在一条记录,但是使用exists()方法会更高效 ''' 切片 , 如 Limiting QuerySets 中说的那样 , 可以使用Python的序列切片语法对一个QuerySet 进行切片 ; 一个未求值的QuerySet 进行切片通常返回另一个未求值的QuerySet , 但是如果使用\"step\"参数 , Django 将执行数据库查询并返回一个列表 ; 对一个已经求值的QuerySet 进行切片将返回一个列表 注意 : 虽然对未求值的QuerySet 进行切片返回另一个未求值的QuerySet , 但是却不可以进一步修改它 , 比如添加更多的filter , 或者修改排序的方式 , 因为这将不太好翻译成SQL而且含义也不清晰 Pickling/Caching , 序列化将读取数据库 , 下节介绍 repr() , 当对QuerySet 调用repr() 时 , 将对它求值 ; 这是为了在Python交互式解释器中方便显示结果 len() , 当对QuerySet 调用len() 时 , 将对它求值 , 返回一个查询集的长度 注意 : 如果确定集合中记录的数量 , 而不需要实际的数据对象 , 那么使用SQL语句的SELECT COUNT(*)效率会更高 , 为此Django提供了一个count() 方法 list() , 当对QuerySet 调用list() 将强制对它求值 bool() , 测试布尔值 , 例如使用bool() , and , or 或者if语句将导致查询集的执行 Pickling QuerySet 🍀 如果你pickle一个QuerySet , 它将在pickle之前强制将所有的结果加载到内存中 ; pickle通常用于缓存之前 , 并且当缓存的查询集重新加载时 , 你希望结果已经存在随时准备使用 ; 不过注意 , pickle的数据只是pickle时的 , 也就是说pickle的数据不是即时的 如果此后你只想pickle必要的信息来重新创建QuerySet , 可以使用如下方式 : import pickle query = pickle.loads(s) # Assuming 's' is the pickled string. qs = MyModel.objects.all() qs.query = query # Restore the original 'query'. query是一个不透明的对象 , 它表示查询的内部构造 , 不属于公开的API 注意 : QuerySet 的pickle在不同的Django版本中是不保证兼容的 , 所以pickle不可用于归档的长期策略 QuerySet API 🍀 class QuerySet(model=None, query=None, using=None): \"\"\" 通常,当你与QuerySet交互时,都是通过链接过滤器来使用它 为了实现这一功能,大多数QuerySet方法都返回新的QuerySet QuerySet类有两个公共属性: ordered:如果QuerySet是排好序的则为True,如有一个order_by()子句或者模型有默认的排序;否则为False db:如果现在执行,则返回将使用的数据库 \"\"\" QuerySet 存在query参数是为了让具有特殊查询用途的子类如GeoQuerySet 可以重新构造内部的查询状态 , 这个参数的值是查询状态的不透明的表示 , 不是一个公开的API QuerySet API 中有非常多的方法供我们使用 , 分为如下几种 : 返回新的QuerySet 不返回新的QuerySet field查找 聚合函数 返回QuerySet 🍀 Django提供了一系列的QuerySet筛选方法 , 用于改变QuerySet返回的结果类型或者SQL查询执行的方式 filter() filter(**kwargs): \"\"\" 返回一个新的QuerySet,它包含满足查询参数的对象 **kwargs:应该满足字段查询中的格式,在底层的SQL语句中,多个参数通过AND连接 \"\"\" exclude() exclude(**kwargs): \"\"\" 返回一个新的QuerySet,它包含不满足给定查询参数的对象 **kwargs:应该满足字段查询中的格式,在底层的SQL语句中,多个参数通过AND连接,然后所有的内容放入NOT()中 \"\"\" 实例 Entry.objects.exclude(pub_date__gt=datetime.date(2005, 1, 3)).exclude(headline='Hello') # 对应的SQL SELECT ... WHERE NOT pub_date > '2005-1-3' AND NOT headline = 'Hello' annotate() annotate(*args,**kwargs): \"\"\" 使用提供的查询表达式注释QuerySet中的每个对象,表达式可以是简单的值,对模型上字段的应用, 对与QuerySet中对象相关的对象进行计算的聚合表达式 *args,**kwargs:每个参数都是一个注释,它将添加到返回的QuerySet中的每个对象 \"\"\" 实例 '''我们正在操作一个Blog对象列表,你可能想知道每个Blog有多少Entry''' >>> from django.db.models import Count >>> q = Blog.objects.annotate(Count('entry')) # The name of the first blog >>> q[0].name 'Blogasaurus' # The number of entries on the first blog >>> q[0].entry__count # Blog模型本身没有定义entry__count属性 42 控制Annotation的名称 >>> q = Blog.objects.annotate(number_of_entries=Count('entry')) # The number of entries on the first blog, using the name provided >>> q[0].number_of_entries 42 order_by() order_by(*fields): \"\"\" 对QuerySet进行指定排序;默认情况下,QuerySet返回的结果是由模型的元数据中的排序选项指定的 *fields:指定排序的字段 \"\"\" 实例 # 先按照pub_date降序排列,然后再按照headline升序排序 Entry.objects.filter(pub_date__year=2005).order_by('-pub_date', 'headline') ''' \"-\",负号表示降序;升序是隐含的,随机可以使用\"?\" ''' # 这种方式查询可能耗费资源而且很慢,取决于使用的数据库 Entry.objects.order_by('?') 使用跨表 , 即双下划线 : Entry.objects.order_by('blog__name', 'headline') 如果排序的字段与另外一个模型关联 , Django将使用关联的模型的默认排序 , 或者如果没有指定Meta.ordering 将通过关联的模型的主键排序 , 如下 : Entry.objects.order_by('blog') # 与上面相同 Entry.objects.order_by('blog__id') # Blog设置ordering=['name'],第一个QuerySet等同于 Entry.objects.order_by('blog__name') reverse() reverse(): \"\"\"翻转,即反向排序\"\"\" 实例 # 获取QuerySet中最后五个元素 my_queryset.reverse()[:5] # 注意QuerySet应该已经定义排序,否则reverse将无效 distinct() distinct(*fields): \"\"\" 返回一个在SQL查询中使用SELECT DISTINCT的新QuerySet,它将去除查询结果中重复的行 \"\"\" 默认情况下 , QuerySet不会去除重复的行 ; 在实际应用中这一般不是个问题 , 但是如果查询跨越多张表 , 当对QuerySet求值时就可能得到重复的结果 , 这时候我们就应该使用distinct() 注意 : order_by() 调用中的任何字段都将包含在SQL的SELECT列中 , 与distinct() 一起使用时可能导致无法预料的后果 ; 总之使用distiinct() 时 , 一定要注意相关模型的排序 values() values(*fields,**expressinos): \"\"\" 返回一个QuerySet字典,每个字典表示一个对象,键对应于模型对象的属性名称 \"\"\" 实例 # This list contains a Blog object. >>> Blog.objects.filter(name__startswith='Beatles') ]> # This list contains a dictionary. >>> Blog.objects.filter(name__startswith='Beatles').values() SELECT接收可选的位置参数*fields , 它指定values()应该限制哪些字段 ; 如果指定字段 , 每个字典将只包含指定字典的键/值 , 如果没有指定字段 , 每个字典将包含数据库中所有字段的键和值 , 如下 : >>> Blog.objects.values() >>> Blog.objects.values('id', 'name') 采用关键字参数**expressions , 这些参数传递给annotate() : >>> from django.db.models.functions import Lower >>> Blog.objects.values(lower_name=Lower('name')) values_list() value_list(*fields,flat=False): \"\"\" 与values()类似,只是在迭代时返回的是元组而不是字典 flat:如果为True表示返回的结果是单个值而不是元组 如果有多个字段,传递flat将发生错误 \"\"\" 实例 >>> Entry.objects.values_list('id').order_by('id') >>> Entry.objects.values_list('id', flat=True).order_by('id') dates() dates(field,kind,order='ASC'): \"\"\" 返回一个QuerySet,其为一个包含datetime.date对象的列表;date对象在QuerySet中表示特定类型的所有可用时间 field:DateField名称 kind:应为year,month,day -year,返回对应该字段的所有不同年份值的list -month,返回字段的所有不同年/月值的list -day,返回字段的所有不同年/月/日值的list order:指定排序方式,默认为ASC,即升序还可设置为DESC,即为降序 \"\"\" 实例 >>> Entry.objects.dates('pub_date', 'year') [datetime.date(2005, 1, 1)] >>> Entry.objects.dates('pub_date', 'month') [datetime.date(2005, 2, 1), datetime.date(2005, 3, 1)] >>> Entry.objects.dates('pub_date', 'day') [datetime.date(2005, 2, 20), datetime.date(2005, 3, 20)] >>> Entry.objects.dates('pub_date', 'day', order='DESC') [datetime.date(2005, 3, 20), datetime.date(2005, 2, 20)] >>> Entry.objects.filter(headline__contains='Lennon').dates('pub_date', 'day') [datetime.date(2005, 3, 20)] datetimes() datetimes(field_name,kind,order='ASC',tzinfo=None): \"\"\" 与dates()相同 field_name:为DateField的名称 kind:应为hour,minute,month,year,second,day order:同dates() tzinfo:定义在阶段之前将数据时间转换到的时区 \"\"\" none() 调用none() 将创建一个从不返回任何对象的queryset , 并且在访问结果时不会执行任何查询 ; qs.name() 查询集是EmptyQuerySet 的一个实例 实例 >>> Entry.objects.none() >>> from django.db.models.query import EmptyQuerySet >>> isinstance(Entry.objects.none(), EmptyQuerySet) True all() 返回当前QuerySet 或QuerySet 子类的副本 , 它可以用于在你希望传递一个模型管理器或QuerySet 并对结果做进一步过滤的情况 当对QuerySet 进行求值时 , 会缓存其结果 ; 如果数据库中的数据在QuerySet 求值之后可能已经改变 , 你可以通过在以前求值过的all() 上调用相同的QuerySet 查询以获得更新后的结果 union() union(*other_qs,all=False): \"\"\" 使用SQL的UNION运算符组合两个或多个Queryset的结果 all:为False表示不允许重复值,True即允许重复值 \"\"\" 实例 >>> qs1.union(qs2, qs3) intersection() intersection(*other_qs): \"\"\"使用SQL的INTERSECT运算符返回两个或多个QuerySet的共享元素\"\"\" 实例 >>> qs1.intersection(qs2, qs3) difference() difference(*other_qs): \"\"\"使用SQL的EXCEPT运算符只保留QuerySet中的元素,而不是在其他QuerySet中保存\"\"\" 实例 >>> qs1.difference(qs2, qs3) select_related() select_related(*fields): \"\"\" 返回一个QuerySet,当执行它的查询时它沿着外键关系查询关联的对象的数据 它会生成一个复杂的查询并引起性能的损耗,但是在以后使用外键关系时将不需要数据库查询 简单说,在对QuerySet使用select_related()函数后,Django会获取相应外键对应的对象,从而在之后需要的时候不必再查询数据库了 \"\"\" 普通查询 # Hits the database. e = Entry.objects.get(id=5) # Hits the database again to get the related Blog object. b = e.blog select_related() 查询 # Hits the database. e = Entry.objects.select_related('blog').get(id=5) # Doesn't hit the database, because e.blog has been prepopulated # in the previous query. b = e.blog select_related() 可用于objects的查询集 from django.utils import timezone # Find all the blogs with entries scheduled to be published in the future. blogs = set() for e in Entry.objects.filter(pub_date__gt=timezone.now()).select_related('blog'): # Without select_related(), this would make a database query for each # loop iteration in order to fetch the related blog for each entry. blogs.add(e.blog) 注 : select_related('foo', 'bar') 等同 select_related('foo').select_related('bar') prefetch_related() prefetch_related(*lookups): \"\"\" 返回一个QuerySet,它将在单个批处理中自动检索每个指定查找的相关对象 \"\"\" 这具有与select_related类似的目的 , 两者都被设计为阻止由访问相关对象而导致的数据库查询的泛滥 , 但是策略是完全不同的 select_related通过创建SQL连接并在SELECT语句中包括相关对象的字段来工作 ; 因此 , select_related在同一数据库查询中获取相关对象 , 然而 , 为了避免由于跨越\"多个\"关系而导致的大得多的结果集 , select_related限于单值关系 - 外键和一对一关系 prefetch_related , 另一方面 , 为每个关系单独查找 , 并在Python中\"加入\" , 这允许它预取多对多和多对一对象 , 除了外键和一对一关系 , 它们不能使用select_related来完成 extra() extra(select=None,where=None,params=None,tables=None,order_by=None,select_params=None): \"\"\" 有些情况下,Django的查询语法难以简单的表达复杂的WHERE子句,对于这种情况, Django提供了extra()修改机制,它能在QuerySet生成的SQL从句中注入新子句 \"\"\" extra()可以指定一个或多个WHERE , 如下 : select , 该参数可以让你在SELECT从句中添加其他字段信息,它应该是一个字典,存放着属性名到SQL从句的映射 Entry.objects.extra(select={'is_recent': \"pub_date > '2006-01-01'\"}) # 对应的SQL SELECT blog_entry.*, (pub_date > '2006-01-01') AS is_recent FROM blog_entry; where/tables , 你可以使用WHERE定义显示SQL where子句 , 也许执行非显示连接 ; 你可以使用FROM手动将表添加到SQL tables子句 ; where和tables都接受字符串列表 , 所有where参数均为\"与\"任何其他搜索条件 Entry.objects.extra(where=[\"foo='a' OR bar = 'a'\", \"baz = 'a'\"]) # SQL如下 SELECT * FROM blog_entry WHERE (foo='a' OR bar='a') AND (baz='a') order_by , 如果你需要使用通过extra()包含的一些新字段或表来对结果查询进行排序 , 可以使用order_by参数传入一个字符串序列 , 这些字符串应该是模型字段 , 如下 : q = Entry.objects.extra(select={'is_recent': \"pub_date > '2006-01-01'\"}) q = q.extra(order_by = ['-is_recent']) ''' 同前面的order_by参数 ''' params , 上述where参数可以使用标准Python数据库字符串占位符%s , 来指示数据库引擎应自动引用的参数 ; params参数是要替换的任何额外参数的列表 # 引号被正确转义 Entry.objects.extra(where=['headline=%s'], params=['Lennon']) # 始终使用params而不是将值直接嵌入where,因为params会确保根据你的特定后端正确引用值 defer() defer(*fields): \"\"\"用于延迟字段的查询集\"\"\" 在一些复杂的数据建模情况下 , 你的模型可能包含大量字段 , 其中一些可能包含大量数据 (例如文本字段) , 或者需要昂贵的处理来将他们转换为Python对象 ; 当你最初获取数据时不知道是否需要这些特定字段的情况下 , 如果你正在使用查询集的结果 , 你可以告诉Django不要从数据库中检索它们 # 通过传递字段名称到defer()实现不加载 Entry.objects.defer(\"headline\", \"body\") 具有延迟字段的查询集仍将返回模型实例 , 每个延迟字段将在你访问该字段时从数据库中检索 , 并且每次只检索一个 , 而不是一次检索所有的延迟字段 还可以多次调用 # Defers both the body and headline fields. Entry.objects.defer(\"body\").filter(rating=5).defer(\"headline\") only() only(*fields): \"\"\"与defer相反,仅让这些字段立即加载,其余的被延迟\"\"\" 对only()的连续调用的结果是只有最后一次调用的字段被考虑 # This will defer all fields except the headline. Entry.objects.only(\"body\", \"rating\").only(\"headline\") 由于defer() 以递增方式动作 (想延迟列表中添加字段) , 因此你可以结合only() 和 defer() , 它们将合乎逻辑地工作 : # Final result is that everything except \"headline\" is deferred. Entry.objects.only(\"headline\", \"body\").defer(\"body\") # Final result loads headline and body immediately (only() replaces any # existing set of fields). Entry.objects.defer(\"body\").only(\"headline\", \"body\") using() using(alias): \"\"\" 控制QuerySet在哪个数据库上求值 alias:数据库的别名,定义在DATABASES \"\"\" 实例 # queries the database with the 'default' alias. >>> Entry.objects.all() # queries the database with the 'backup' alias >>> Entry.objects.using('backup') select_for_update() select_for_update(nowait=False,skip_locked=False): \"\"\" 返回一个锁住行直到事务结束的查询集,如果数据库支持, 它将生成一个SELECT ... FOR UPDATE语句 nowait:默认如果其他事务锁定了相关行,那么本查询将被阻塞,直到锁被释放;改为True使查询不阻塞 skip_locked:如果其他事务持有冲突的锁,可以改为Trye忽略锁定的行 nowait与skip_locked是互斥的,同时启用会导致ValueError \"\"\" raw() raw(raw_query,params=None,translations=None): \"\"\" 接收一个原始的SQL查询,执行并返回一个django.db.models.query.RawQuereySet实例 这个RawQuerySet实例可以迭代以提供实例对象,就像普通的QuerySet一样 \"\"\" raw() 永远触发一个新的查询 , 而与之前的filter无关 ; 因此 , 它通常应该从Manager或一个全新的QuerySet实例调用 不返回QuerySet 🍀 以下方法对QuerySet进行求值并返回 , 返回结果不是QuerySet 这些方法不使用高速缓存 , 并且每次被调用的时候都会查询数据库 get() 返回按照查询参数匹配到的对象 , 如果匹配到的对象个数不止一个 , get()将会触发MultipleObjectReturned异常 实例 entry = Entry.objects.filter(...).exclude(...).get() create() 快捷创建对象并保存 , 如下 : p = Person.objects.create(first_name=\"Bruce\", last_name=\"Springsteen\") # 等同如下 p = Person(first_name=\"Bruce\", last_name=\"Springsteen\") p.save(force_insert=True) get_or_create() get_or_create(defaults=None,**kwargs): \"\"\" 通过给出的kwargs来查询对象的便捷方法,需要的话创建一个对象, 返回一个由(object,created)组成的元组,object是一个查询到的或者是被创建的对象,created是一个表示是否创建了新的对象的布尔值 \"\"\" 实例 obj, created = Person.objects.get_or_create( first_name='John', last_name='Lennon', defaults={'birthday': date(1940, 10, 9)}, ) 任何传递给 get_or_create() 的关键字参数 , 除了一个可选的defaults, 都将传递给get() 调用 ; 如果查找到一个对象 , get_or_create() 返回一个包含匹配到的对象以及False 组成的元组 , 如果查找到的对象超过一个以上 , get_or_create 将引发MultipleObjectsReturned异常 , 如果查找不到对象 , get_or_create() 将会实例化并保存一个新的对象 , 返回一个由新的对象以及True 组成的元组 , 新的对象将会大概按照以下的逻辑创建 : params = {k: v for k, v in kwargs.items() if '__' not in k} params.update({k: v() if callable(v) else v for k, v in defaults.items()}) obj = self.model(**params) obj.save() update_or_create() update_or_create(defaults=None,**kwargs): \"\"\" 返回一个由(object,created)组成的元组, 元组中的object是一个创建的或者是被更新的对象, created是一个标识是否创建了心得额对象的布尔值 \"\"\" 实例 obj, created = Person.objects.update_or_create( first_name='John', last_name='Lennon', defaults={'first_name': 'Bob'}, ) bulk_create() bulk_create(objs,batch_size=None): \"\"\" 以高效的方式(通常只有1个查询,无论多少对象)将提供的对象列表插入到数据库中 obj:插入的对象 batch_size:控制在单个查询中创建的对象数;默认值是在一个批处理中创建所有的对象,除了SQLite,其中默认值为每个查询最多使用999个变量 \"\"\" 实例 >>> Entry.objects.bulk_create([ ... Entry(headline='This is a test'), ... Entry(headline='This is only a test'), ... ]) 该方法需要注意以下 : 将不会调用模型的sava()方法 , 并且不会发送pre_save 和 post_save 信号 它不适用于多表继承场景中的子模型 如果模型的主键是AutoField , 则不会像save() 那样检索并设置主键属性 , 除非数据库后端支持(当前是PostgreSQL) 它不适用于多对多关系 count() 返回在数据中对应的QuerySet 对象的个数 , count() 永远不会引发异常 # Returns the total number of entries in the database. Entry.objects.count() # Returns the number of entries whose headline contains 'Lennon' Entry.objects.filter(headline__contains='Lennon').count() in_bulk() in_bulk(id_list=None): \"\"\" 获取主键值的列表,并返回将每个主键值映射到具有给定ID的对象的实例的字典, 如果未提供列表,则会返回查询集中的所有对象 \"\"\" 实例 >>> Blog.objects.in_bulk([1]) {1: } >>> Blog.objects.in_bulk([1, 2]) {1: , 2: } >>> Blog.objects.in_bulk([]) {} >>> Blog.objects.in_bulk() {1: , 2: , 3: } iterator() 对QuerySet进行求值并返回一个迭代器 , 其不会在QuerySet级别执行任何缓存 (内部 , 默认迭代器调用iterator() 并高速缓存返回值) latest() latest(field_name=None): \"\"\" 按日期返回表中的最新对象 field_name:日期字段 \"\"\" 实例 Entry.objects.latest('pub_date') earliest() 除非方向更改 , 否则像latest() first() 返回结果集的第一个对象 , 当没有找到时返回None ; 如果QuerySet没有设置排序 , 则将会自动按主键进行排序 , 如下 : p = Article.objects.order_by('title', 'pub_date').first() last() 工作方式类似first() , 只是返回的是查询集中最后一个对象 aggregate() aggregate(*args,**kwargs): \"\"\" 返回汇总值的字典(平均值,总和等),通过QuerySet进行计算 每个参数指定返回的字典中将要包含的值 \"\"\" 实例 >>> from django.db.models import Count >>> q = Blog.objects.aggregate(Count('entry')) {'entry__count': 16} >>> q = Blog.objects.aggregate(number_of_entries=Count('entry')) {'number_of_entries': 16} exists() 如果QuerySet包含任何结果 , 则返回True , 否则返回False 它视图用最简单和最快的方法完成查询 , 但它执行的方法与普通的QuerySet查询几乎一样 , exists()用于搜寻对象是都在QuerySet中以及QuerySet受否存在任何对象 , 特别是QuerySet比较大的时候 entry = Entry.objects.get(pk=123) if some_queryset.filter(pk=entry.pk).exists(): print(\"Entry contained in queryset\") update() 对指定的字段执行SQL更新查询 , 并返回匹配的行数 (如果某些行已具有新值 , 则可能不等于已更新的行数) >>> Entry.objects.filter(pub_date__year=2010).update(comments_on=False, headline='This is old') delete() 对QuerySet中的所有行执行SQL删除查询 , 并返回删除的对象数和每个对象类型的删除次数的字典 >>> b = Blog.objects.get(pk=1) # Delete all the entries belonging to this Blog. >>> Entry.objects.filter(blog=b).delete() (4, {'weblog.Entry': 2, 'weblog.Entry_authors': 2}) as_manager() classmethod as_manager() 类方法 , 返回Manager的实例与QuerySet的方法的副本 Field查询 🍀 字段查询是指如何指定SQL WHERE子句的内容 , 它通过QuerySet的filter() , exclude() 和 get() 的关键字参数指定 , 即使用双下划线时后的参数 exact 精确匹配 , 如果为比较提供的值为NULL , 它将被解释SQL None Entry.objects.get(id__exact=14) Entry.objects.get(id__exact=None) # 等价的SQL SELECT ... WHERE id = 14; SELECT ... WHERE id IS NULL; iexact 不区分大小写的精确匹配 , 如果为比较提供的值为NULL , 它将被解释为SQL None Blog.objects.get(name__iexact='beatles blog') Blog.objects.get(name__iexact=None) # 等价的SQL SELECT ... WHERE name ILIKE 'beatles blog'; SELECT ... WHERE name IS NULL; contains 大小写敏感的包含关系测试 Entry.objects.get(headline__contains='Lennon') # 等价SQL SELECT ... WHERE headline LIKE '%Lennon%'; icontains 与contains相反 , 大小写不敏感的包含关系测试 Entry.objects.get(headline__icontains='Lennon') # 等价SQL SELECT ... WHERE headline ILIKE '%Lennon%'; in 在给定的列表 Entry.objects.filter(id__in=[1, 3, 4]) # 等价SQL SELECT ... WHERE id IN (1, 3, 4); gt 大于 Entry.objects.filter(id__gt=4) # 等价SQL SELECT ... WHERE id > 4; gte 大于或等于 lt 小于 lte 小于或等于 startswith 区分大小写 , 开始位置匹配 Entry.objects.filter(headline__startswith='Lennon') # 等价SQL SELECT ... WHERE headline LIKE 'Lennon%'; istartswith 不区分大小写 , 开始位置匹配 Entry.objects.filter(headline__istartswith='Lennon') # 等价SQL SELECT ... WHERE headline ILIKE 'Lennon%'; endswith 区分大小写 , 结尾位置匹配 Entry.objects.filter(headline__endswith='Lennon') # 等价SQL SELECT ... WHERE headline LIKE '%Lennon'; iendswith 不区分大小写 , 结尾位置匹配 Entry.objects.filter(headline__iendswith='Lennon') # 等价SQL SELECT ... WHERE headline ILIKE '%Lennon' range 范围测试 import datetime start_date = datetime.date(2005, 1, 1) end_date = datetime.date(2005, 3, 31) Entry.objects.filter(pub_date__range=(start_date, end_date)) # 等价SQL SELECT ... WHERE pub_date BETWEEN '2005-01-01' and '2005-03-31'; date 对于datetime字段 , 将值作为日期转换 ; 允许链接附加字段查找 , 获取日起值 Entry.objects.filter(pub_date__date=datetime.date(2005, 1, 1)) Entry.objects.filter(pub_date__date__gt=datetime.date(2005, 1, 1)) year 对于日期和时间字段 , 确切的年匹配 , 循序链接附加字段查找 Entry.objects.filter(pub_date__year=2005) Entry.objects.filter(pub_date__year__gte=2005) # 等价SQL SELECT ... WHERE pub_date BETWEEN '2005-01-01' AND '2005-12-31'; SELECT ... WHERE pub_date >= '2005-01-01'; month 对于日期和日期时间字段 , 确切的月份匹配 , 允许链接附加字段查找 Entry.objects.filter(pub_date__month=12) Entry.objects.filter(pub_date__month__gte=6) # 等价SQL SELECT ... WHERE EXTRACT('month' FROM pub_date) = '12'; SELECT ... WHERE EXTRACT('month' FROM pub_date) >= '6'; day 对于日期和日期时间字段 , 具体到某一天的匹配 , 允许链接附加字段查找 Entry.objects.filter(pub_date__day=3) Entry.objects.filter(pub_date__day__gte=3) # 等价SQL SELECT ... WHERE EXTRACT('day' FROM pub_date) = '3'; SELECT ... WHERE EXTRACT('day' FROM pub_date) >= '3'; week 对于日期和日期时间字段 , 根据 ISO-8601 返回周号 , 即星期一开始的星期 Entry.objects.filter(pub_date__week=52) Entry.objects.filter(pub_date__week__gte=32, pub_date__week__lte=38) week_day 对于日期和日期时间字段 , 周号匹配 , 允许链接附加字段查找 Entry.objects.filter(pub_date__week_day=2) Entry.objects.filter(pub_date__week_day__gte=2) time 对于datetime字段 , 将值转换为时间 , 允许链接附加字段查找 , 获取datetime.time的值 Entry.objects.filter(pub_date__time=datetime.time(14, 30)) Entry.objects.filter(pub_date__time__between=(datetime.time(8), datetime.time(17))) hour 对于日期时间和时间字段 , 确切的时间匹配 , 允许链接附加字段查找 Event.objects.filter(timestamp__hour=23) Event.objects.filter(time__hour=5) Event.objects.filter(timestamp__hour__gte=12) # 等价SQL SELECT ... WHERE EXTRACT('hour' FROM timestamp) = '23'; SELECT ... WHERE EXTRACT('hour' FROM time) = '5'; SELECT ... WHERE EXTRACT('hour' FROM timestamp) >= '12'; minute 对于日期时间和时间字段 , 确切的分钟匹配 , 允许链接附加字段查找 Event.objects.filter(timestamp__minute=29) Event.objects.filter(time__minute=46) Event.objects.filter(timestamp__minute__gte=29) # 等价SQL SELECT ... WHERE EXTRACT('minute' FROM timestamp) = '29'; SELECT ... WHERE EXTRACT('minute' FROM time) = '46'; SELECT ... WHERE EXTRACT('minute' FROM timestamp) >= '29'; second 对于日期时间和时间字段 , 确切的第二个匹配 , 允许链接附加字段查询 Event.objects.filter(timestamp__second=31) Event.objects.filter(time__second=2) Event.objects.filter(timestamp__second__gte=31) # 等价SQL SELECT ... WHERE EXTRACT('second' FROM timestamp) = '31'; SELECT ... WHERE EXTRACT('second' FROM time) = '2'; SELECT ... WHERE EXTRACT('second' FROM timestamp) >= '31'; isnull 值为False或True , 相当于SQL语句IS NULL 和IS NOT NULL Entry.objects.filter(pub_date__isnull=True) # 等价SQL SELECT ... WHERE pub_date IS NULL; search 一个Boolean类型的全文搜索 , 以全文搜索的优势 ; 这个很像contains , 但是由于全文索引的优势 , 以使它更显著的块 Entry.objects.filter(headline__search=\"+Django -jazz Python\") # 等价SQL SELECT ... WHERE MATCH(tablename, headline) AGAINST (+Django -jazz Python IN BOOLEAN MODE); regex 区分大小写的正则表达式匹配 Entry.objects.get(title__regex=r'^(An?|The) +') # 等价SQL SELECT ... WHERE title REGEXP BINARY '^(An?|The) +'; -- MySQL SELECT ... WHERE REGEXP_LIKE(title, '^(An?|The) +', 'c'); -- Oracle SELECT ... WHERE title ~ '^(An?|The) +'; -- PostgreSQL SELECT ... WHERE title REGEXP '^(An?|The) +'; -- SQLite iregex 不区分大小写的正则表达式匹配 Entry.objects.get(title__iregex=r'^(an?|the) +') # 等价SQL SELECT ... WHERE title REGEXP '^(an?|the) +'; -- MySQL SELECT ... WHERE REGEXP_LIKE(title, '^(an?|the) +', 'i'); -- Oracle SELECT ... WHERE title ~* '^(an?|the) +'; -- PostgreSQL SELECT ... WHERE title REGEXP '(?i)^(an?|the) +'; -- SQLite 聚合函数 🍀 Django的django.db.models 模块提供以下聚合函数 , 关于如果使用这些聚合函数的细节 , 参考 the topic guide on aggregation , 如何创建聚合函数 , 参考 Aggregate 所有聚合函数都具有以下共同的参数 : expression 引用模型字段的一个字符串 , 或者一个查询表达式 output_field 用来表示返回的模型字段 , 它是一个可选的参数 **extra 这些关键字参数可以给聚合函数生成的SQL提供额外的信息 Avg class Avg(expression, output_field=FloatField(), **extra*) 返回给定表达式的平均值 , 它必须是数值 , 除非你指定不同的output_field 默认别名为 : __avg 返回类型 : float(或指定output_field的类型) Count 返回表达式相关的对象的个数 默认别名 :__count 返回类型 : int 有一个可选的参数 : distinct , 如果为True , Count将只计算唯一的实例 , 它等同于CONUT(DISTINCT ) SQL语句 , 默认为False Max class Max(expression,output_field=None,**extra) 返回expression 的最大值。 默认的别名 : __max 返回类型 : 与输入字段的类型相同 , 如果提供则为 output_field 类型 Min class Min(expression,output_field=None, **extra) 返回expression 的最小值 默认的别名 : __min 返回类型 : 与输入字段的类型相同 , 如果提供则为 output_field 类型 StdDev class StdDev(expression, sample=False, **extra) 返回expression 的标准差 默认的别名 : __stddev 返回类型 : float 有一个可选的参数 : sample , 默认情况下 , StdDev 返回群体的标准差 ; 但是 , 如果sample=True , 返回的值将是样本的标准差 Sum class Sum(expression, output_field=None, **extra) 计算expression 的所有值的和。 默认的别名 : __sum 返回类型 : 与输入字段的类型相同 , 如果提供则为 output_field 类型 Variance class Variance(expression,sample=False, **extra*) 返回expression 的方差。 默认的别名 : __variance 返回类型 : float 有一个可选的参数 : sample , 默认情况下 , Variance 返回群体的方差 ; 但是 , 如果sample=True , 返回的值将是样本的方差 在QuerySet API中还有几个查询相关的工具 : Q() 对象 Prefetch() 对象 prefetch_related_objects() 详细内容见 : Query-related tools 更多QuerySet API 详细内容 : https://docs.djangoproject.com/en/1.11/ref/models/querysets/ "},"05-Web框架/Django/10-Django - Model Making queries.html":{"url":"05-Web框架/Django/10-Django - Model Making queries.html","title":"Django - Model Making queries","keywords":"","body":"Django - Model Making queries 介绍 🍀 一旦我们建立好模型 , Django就会自动为我们生成一套数据库抽象的API , 可以让我们进行创建 , 检索 , 更新和删除对象 , 这篇文章主要阐述怎么去使用这些API 关于模型接口的完整细节见 , data model reference 我们首先建立好模型 , 该模型构成一个博客应用 : from django.db import models class Blog(models.Model): name = models.CharField(max_length=100) tagline = models.TextField() def __str__(self): # __unicode__ on Python 2 return self.name class Author(models.Model): name = models.CharField(max_length=200) email = models.EmailField() def __str__(self): # __unicode__ on Python 2 return self.name class Entry(models.Model): blog = models.ForeignKey(Blog) headline = models.CharField(max_length=255) body_text = models.TextField() pub_date = models.DateField() mod_date = models.DateField() authors = models.ManyToManyField(Author) n_comments = models.IntegerField() n_pingbacks = models.IntegerField() rating = models.IntegerField() def __str__(self): # __unicode__ on Python 2 return self.headline 获取对象 🍀 从数据库获取对象 , 是通过模型中的Manager 构造出一个QuerySet ; QuerySet表示从数据库中取出来的对象的集合 , 从SQL的角度来讲 , QuerySet与WHERE语句等价 Manager则是Django模型提供数据库查询操作的接口 , 在一个Django应用中 , 每个模型至少存在一个Manager , 默认情况下命名为objects , 也就是说我们的模型Manager就是我们所使用的objects , 例如 : models.Blog.objects.all() ''' models.Blog:model class objects:Manager all:QuerySet API ''' 注意 : Manager只能通过模型类访问 , 而不能通过模型的实例访问 , 目的是为了强制区分\"表级别\" 的操作和\"记录级别\" 的操作 ; 对于一个模型来说 , Manager是QuerySet的主要来源 获取所有对象 🍀 可以使用Manager的all() 方法将一个表中的所有对象全部获取 # all()方法返回一个包含数据库表中的所有对象的QuerySet all_entries = Entry.objects.all() 过滤器获取对象 🍀 all() 方法返回了一个包含数据库表中的所有记录的QuerySet , 但是通常情况下我们往往需要获取完整数据集的一个子集 , 即我们需要使用过滤器 filter(**kwargs): \"\"\"返回一个新的QuerySet,它包含满足查询参数的对象\"\"\" exclude(**kwargs): \"\"\"返回一个新的QuerySet,它包含不满足查询参数的对象\"\"\" 实例 # 获取年份为2006的所有文章的QuerySet Entry.objects.filter(pub_date_year=2006) # 相当于 Entry.objects.all().filter(pub_date_year=2006) 链接过滤器 🍀 QuerySet的筛选结果本身还是QuerySet , 所以可以将筛选语句链接在一起 , 如下 : Entry.objects.filter( headline__startswith='What' ).exclude( pub_date__gte=datetime.date.today() ).filter( pub_date__gte=datetime(2005, 1, 30) ) QuerySet唯一性 🍀 每次筛选一个QuerySet , 得到的都是全新的另一个QuerySet , 它和之前的QuerySet没有任何绑定关系 , 每次筛选都会创建一个独立的QuerySet , 它可以被存储及反复使用 , 如下 : # 所有标题包含\"what\"开头的记录 q1 = Entry.objects.filter(headline__startswith=\"What\") # q1的子集,排除pub_date为今天的记录 q2 = q1.exclude(pub_date__gte=datetime.date.today()) # q1的子集,只选择pub_date为今天的记录 q3 = q1.filter(pub_date__gte=datetime.date.today()) ''' q1不会受筛选过程的影响 ''' QuerySet惰性 🍀 QuerySet是惰性执行的 , 创建QuerySet不会带来任何数据的访问 , 类似于Python中的生成器 , 你可以将过滤器保持一整天 , 知道QuerySet需要取值时 , Django才会真正执行这个查询 , 如下 : q = Entry.objects.filter(headline__startswith=\"What\") q = q.filter(pub_date__lte=datetime.date.today()) q = q.exclude(body_text__icontains=\"food\") # 此时才真正访问数据库 print(q) QuerySet切片 🍀 QuerySet是支持切片的 , 它等同于SQL的OFFSET和LIMIT语句 # LIMIT 5,返回前5个对象 Entry.objects.all()[:5] # OFFSET 5 LIMIT 5,返回第6至第10个 Entry.objects.all()[5:10] 但是QuerySet不支持负的索引 , 如 : Entry.objects.all()[-1] 通常QuerySet的切片返回一个新的QuerySet , 它不会执行查询 , 但是如果使用Python切片语法中的step 参数 , 即步长 , 那么它将执行查询 获取单个对象 🍀 如需获取单个对象 , 可以使用Manager的get() 方法 one_entry = Entry.objects.get(pk=1) 字段查询 🍀 字段查询是指如何指定SQL WHERE 子句的内容 , 通常使用过滤器的关键字参数指定 查询的关键字参数的基本形式是field__lookuptype=value , 如下 : Entry.objects.filter(pub_date__lte='2006-01-01') # 翻译成SQL大体如下 SELECT * FROM blog_entry WHERE pub_date 查询条件中指定的字段必须是模型字段的名称 ; 有一个例外 , 对于ForeignKey 你可以使用字段名加上_id 后缀 , 在这种情况下 , 该参数的值应该是外键的原始值 , 如下 : Entry.objects.filter(blog_id=4) 数据库API支持大约二十种lookuptype , 完整参考见 : field lookup reference 下面介绍几个常用的 : exact , 精确匹配 Entry.objects.get(headline__exact=\"Cat bites dog\") # 对应SQL SELECT ... WHERE headline = 'Cat bites dog'; # 如果关键字参数不包含双下划线,默认假定查询类型是exact,以下两条语句相等 Blog.objects.get(id__exact=14) # Explicit form Blog.objects.get(id=14) # __exact is implied iexact , 大小写不敏感的匹配 # 将匹配标题为\"Beatles Blog\",\"beatles blog\"甚至\"BeAtlES blOG\"的Blog Blog.objects.get(name__iexact=\"beatles blog\") contains , 大小写敏感的包含关系测试 Entry.objects.get(headline__contains='Lennon') # 翻译成SQL SELECT ... WHERE headline LIKE '%Lennon%'; ''' 这将匹配'Today Lennon honored',但不能匹配'today lennon honored' 还有一个大小写不敏感版本,icontains ''' startswith , endswith , 以 ... 开头 , 以 ... 结尾 更多详细字段类型会在下一篇文章中整理 跨表查询 🍀 Django提供了一种强大又直观的方式来\"处理\"查询中的关联关系 , 它在后台自动帮你处理JOIN 如果要进行跨表查询 , 只需要使用关联的模型字段的名称 , 并使用双下划线分隔 , 直至你想要的字段 , 如下 : # 获取所有Blog表中name为'Beatles Blog'的Entry对象 Entry.objects.filter(blog__name='Beatles Blog') 这种跨越可以是任意深度的 , 它还可以反向工作 , 若要引用一个\"反向\"的关系 , 只需要使用该模型的小写的名称 , 如下 : # 获取所有Entry表中headline包含'Lennon'的Blog对象 Blog.objects.filter(entry__headline__contains='Lennon') 如果在多个关联关系过滤而且其中某个中介模型没有满足过滤条件的值 , Django将把它当做一个空的(所有的值都为NULL) 但是合法的对象 , 这意味着不会有错误引发 Blog.objects.filter(entry__authors__name='Lennon') 更多实例 models.Tb1.objects.filter(id__lt=10, id__gt=1) # 获取id大于1 且 小于10的值 models.Tb1.objects.filter(id__in=[11, 22, 33]) # 获取id等于11、22、33的数据 models.Tb1.objects.exclude(id__in=[11, 22, 33]) # not in models.Tb1.objects.filter(id__range=[1, 2]) # 范围bettwen and F expressions 🍀 如果我们想将模型的一个字段与同一个模型的另外一个字段进行比较 , 可以使用F() 表达式 F() 返回的实例用作查询内部对模型字段的引用 , 然后可以在过滤器中使用这些引用来比较同一模型实例中两个不同字段的值 , 如下 : from django.db.models import F # 获取所有comments数目大于pingbacks的Entry对象,gt表示大于,将在下一篇QuerySet API中详细介绍 Entry.objects.filter(n_comments__gt=F('n_pingbacks')) Django支持对F()对象使用加法 , 减法 , 乘法 , 除法 , 取模以及幂运算等算数操作 , 两个操作数可以都是常数和其他F() 对象 , 如下 : # 查找comments数目比pingbacks两倍还多的Entry对象 Entry.objects.filter(n_comments__gt=F('n_pingbacks') * 2) F() 对象同样可以使用双下划线进行跨表 # 获取author的名字与blog名字相同的Entry对象 Entry.objects.filter(authors_name=F('blog__name')) 对于date和date/time字段 , 可以给它们加上或减去一个timedalta对象 # 获取发布超过3天后被修改的所有Entry对象 from datetime import timedelta Entry.objects.filter(mod_date__gt=F('pub_date') + timedelta(days=3)) F() 对象支持位操作bitand() , bitor() , bitrightshift() 和 bitleftshift() F('somefield').bitand(16) 更多F表达式相关 : F expressions 缓存与QuerySet 🍀 每个QuerySet都包含一个缓存来最小化对数据的访问 , 在一个新创建的QuerySet中 , 缓存为空 ; 首次对QuerySet进行求值 , 同时发生数据库查询 , Django将保存查询的结果到QuerySet的缓存中并返回明确请求的结果 , 我们需要考虑的是对QuerySet重用缓存的问题 如果QuerySet使用不当 , 它是会坑你的 , 如下 : # 查询后就释放缓存 print([e.headline for e in Entry.objects.all()]) # 查询后就释放缓存 print([e.pub_date for e in Entry.objects.all()]) ''' 这两条语句意味着相同的数据库查询将执行两次, 显然倍增了数据库负载, 同时,还有可能两个结果列表并不一样, 因为两次请求期间有可能有新的Entry对象被添加进来或删除掉 ''' 为了避免这个问题 , 我们只需保存QuerySet并重新使用它 queryset = Entry.objects.all() print([p.headline for p in queryset]) # Evaluate the query set. print([p.pub_date for p in queryset]) # Re-use the cache from the evaluation. QuerySet不缓存 QuerySet并不总是缓存它们的结果 , 当只对查询集的部分进行求值时会检查缓存 , 但是如果这部分不在缓存中 , 那么接下来查询返回的记录都将不会被缓存 ; 具体的说 , 这意味着使用切片或索引来约束查询集将不会进行缓存 实例1 # 重复获取查询集对象中一个特定的索引将每次都查询数据库,不会进行缓存 queryset = Entry.objects.all() print(queryset[5]) # Queries the database print(queryset[5]) # Queries the database again 实例2 # 已经对全部查询集求值过,则将检查缓存 queryset = Entry.objects.all() [entry for entry in queryset] # Queries the database print(queryset[5]) # Uses cache print(queryset[5]) # Uses cache 下面是一些其它的例子 , 它们会使得全部的查询集被求值并填充到缓存中 >>> [entry for entry in queryset] >>> bool(queryset) >>> entry in queryset >>> list(queryset) 注意 : 简单地打印查询集不会填充缓存 , 因为__repr__() 调用只会返回全部查询集的一个切片 更多 : OR lookups examples Manager更多详细资料 : https://docs.djangoproject.com/en/1.11/topics/db/managers/ 更多Making queries : https://docs.djangoproject.com/en/1.11/topics/db/queries/ Model API reference : https://docs.djangoproject.com/en/1.11/ref/models/ 下一章将会对本章的一些内容进行说明 , 及QuerySet API "},"05-Web框架/Django/11-Django - Forms.html":{"url":"05-Web框架/Django/11-Django - Forms.html","title":"Django - Forms","keywords":"","body":"Django - Forms 介绍 🍀 表单在网页中主要负责数据采集功能 , 比如我们可以利用表单来采集访问者的某些信息 , 例如 : 名字 , email地址 , 留言簿等等 Django提供了大量的工具和库来帮助我们构建表单来接收网站访问者的输入 , 然后处理以及响应输入 HTML表单 在HTML中 , 表单的作用是收集标签中的内容 , 即form标签中间可以由访问者添加类似于文本 , 选择 , 或者一些控制模块等等 , 然后这些内容将会被送到服务端 与 元素一样 , 一个表单必须指定两样东西 : where : 响应用户输入数据的URL how : 发送数据所使用的HTTP方式 例如 , Django Admin站点的登录表单包含如下几个元素 : type=\"text\" , 用户名 type=\"password\" , 密码 type=\"submit\" , 用于\"Log in\" 按钮 它还包含一些用户看不到的隐藏的文本字段 , Django使用他们来决定下一步的行为 它还告诉浏览器表单数据应该发往元素的action属性指定的URL , 如 : /admin/ 而且应该使用method属性指定的HTTP机制来发送 , 即是使用GET还是POST GET和POST 处理表单时只会用到POST和GET方法 Django的登录表单使用POST方法 , 在这个方法中浏览器组合表单数据 , 对它们进行编码以用于传输 , 将它们发送到服务器然后接收它的响应 mysite/myapp/views.py from django.shortcuts import render from django.views.decorators import csrf # 接收POST请求数据 def search_post(request): ctx ={} if request.POST: ctx['rlt'] = request.POST['q'] return render(request, \"post.html\", ctx) mysite/templates/post.html Lyon's Blog {% csrf_token %} {{ rlt }} 相反 , GET组合提交的数据为一个字符串 , 然后是用它来生成一个URL ; 这个URL将包含数据发送的地址以及数据的键和值 , 比如你在Django documentation中进行一次搜索 , 其将会生成一个URL https://docs.djangoproject.com/search/? mysite/myapp/views.py from django.http import HttpResponse from django.shortcuts import render_to_response # 表单 def search_form(request): return render_to_response('search_form.html') # 接收请求数据 def search(request): request.encoding='utf-8' if 'q' in request.GET: message = 'The content of your search is' + request.GET['q'] else: message = 'The form is empty' return HttpResponse(message) mysite/templates/get.html Lyon's Blog POST和GET用于实现不同的目的 如果需要改变系统状态的请求应该使用POST , 如果不影响系统状态的请求则应该使用GET GET还不适合密码表单 , 因为这意味着你的密码将出现在URL中, 以及浏览器的历史和服务器的日志中 , 而且都是以普通的文本格式 ; 它还不适合数据量大的表单和二进制数据 , 例如一张图片 ; 使用GET请求作为管理站点的表单具有安全隐患 : 攻击者很容易模拟表单请求来取得系统的敏感数据 不过GET方式适合网页搜索这样的表单 , 因为这种表示一个GET请求的URL可以很容易地作为书签 , 分享和重新提交 Forms In Django 🍀 Django的表单功能可以简化自动化大部分工作 , 而且还可以比大部分程序员自己所编写的代码更安全 Django会处理表单工作中的三个显著不同的部分 : 准备数据 , 重构数据 , 以便下一步提交 为数据创建HTML表单 接收并处理客户端提交的表单和数据 这些我们可以自己手工编写来实现 , 但是Django可以帮你完成这些所有的工作 在一个Web应用中 , 表单可能指HTML中的 , 或者生成它的Django的Form , 或者提交时发送的结构化数据、或者这些部分的总和 Django 的Form类 表单系统的核心部分是Django的Form类 , Django的模型描述一个对象的逻辑结构 , 行为以及展现给我们的方式 , 与此类似 , Form类描述一个表单并决定它如何工作和展现 就将Model中的ORM一样 , 表单类的字段会映射到HTML的 表单元素 (ModelForm通过一个Form映射模型类的字段到HTML表单的元素 ; Django的Admin站点就是基于这个的) 表单的字段本身就是一个个的类 ; 它们管理表单数据 , 并在提交表单时执行验证 表单字段在浏览器中呈现给用户的是一个HTML的\"小部件\" , 即用户界面的一个片段 每个字段类型都有一个合适的默认 Widget class , 在我们需要时可以将其覆盖 实例化、处理和渲染 🍀 在Django中渲染一个对象时 , 我们通常 : 在视图中获得它 (例如 , 通过视图函数从数据库中获取) 将它传递给模板上下文 使用模板变量将它扩展为HTML标记 在模板中渲染表单和渲染其他类型的对象几乎一样 , 除了有几个比较例外 在模型实例不包含数据的情况下 , 在模板中对它做处理很少有什么用处 ; 但是渲染一个未填充的表单却是非常有意义的 , 我们希望用户去填充它 ; 所以当我们的视图中处理模型实例时 , 我们一般从数据库中获取它 , 当我们处理表单时 , 我们一般在视图中实例化它 当我们实例化表单时 , 我们可以选择让它为空还是预先填充它 构建一个表单 🍀 HTML Form 如果我们想在自己的网站上创建一个简单的表单 , 以获取用户的名字 , 模板如下 : Your name: 这告诉浏览器以POST方式发送表单的数据到URL /your-name/ , 它将显示一个\"Your name\"文本字段 , 以及一个\"OK\"按钮 , 如果模板上下文包含一个your_name变量 , 它将用于预填充current_name字段 你将需要一个视图来渲染这个包含HTML表单的模板 , 并提供合适的current_name字段 当表单提交时 , 发往服务器的POST请求将包含表单数据 这是一个非常简单的表单 , 实际应用中 , 一个表单可能包含几十上百个字段 , 其中大部分需要预填充 , 而且我们预料到用户将来回编辑 , 提交几次才能完成操作 ; 即使在提交表单之前 , 我们也可能需要在浏览器中进行一些验证 , 我们可能想要使用更复杂的字段 , 这样可以让用户做一些事情 , 例如从日历中选择日期等等 , 那么这个时候 , 让Django来为我们完成大部分工作是很容易的 Django Form 🍀 上面我们已经构建好了一个HTML表单 , 那么现在我们就需要来构建一个Django表单了 from django import forms class NameForm(forms.Form): your_name = forms.CharField(label='Your name', max_length=100) 这个Form类仅仅定义了一个字段your_name , 字段允许的最大长度通过max_length 定义 , 它完成两件事情 ; 首先 , 它在HTML 上放置一个maxlength=100 , 这样浏览器将在第一时间阻止输入多于这个数目的字符 , 它还意味着当Django收到浏览器发送过来的表单时 , 它将验证数据的长度 Form的实例具有一个is_valid() 方法 , 它会为所有的字段执行验证程序 ; 如果所有的字段都包含有效的数据它将会返回True并且将表单的数据放到cleaned_data属性中 完整的表单 , 第一次渲染时 , 看上去将如下 : Your name: 注意 : 它不包含 标签 , 以及提交按钮 , 所以我们必须自己在模板中提供它们 视图 🍀 发送回Django网站的表单数据由视图处理 , 通常是发布表单的相同视图 , 这允许我们重用一些相同的逻辑 views.py from django.shortcuts import render from django.http import HttpResponseRedirect from .forms import NameForm def get_name(request): # if this is a POST request we need to process the form data if request.method == 'POST': # create a form instance and populate it with data from the request: form = NameForm(request.POST) # check whether it's valid: if form.is_valid(): # process the data in form.cleaned_data as required # ... # redirect to a new URL: return HttpResponseRedirect('/thanks/') # if a GET (or any other method) we'll create a blank form else: form = NameForm() return render(request, 'name.html', {'form': form}) 如上 , 如果访问视图的是一个GET请求 , 它将创建一个空的表单实例并将它放置到要渲染的模板的上下文中 , 这是我们在第一次访问该URL时发生的事情 如果使用POST请求提交表单 , 该视图将再次创建一个表单实例 , 并使用请求中的数据填充表单 , 即form = NameForm(request.POST) 这种方式称为绑定 , 即将数据绑定到表单 我们调用is_valid()方法 ; 如果不是True , 也就是说我们的所有数据中存在不合法的项 , 那么它就会返回到HTML Form , 也就是我们提交之前的位置 , 但是这个时候这个实例不是空的 (这一步是未绑定的) , HTML Form将使用前面提交合法的数据进行填充 , 而对于不合法的我们就可以对其进行编辑和修改 如果is_valid() 返回True , 我们就可以在cleaned_data属性中找到所有合法的表单数据 , 我们就可以使用这些数据去更新数据库或进行其他操作 , 然后将HTTP重定向发送给浏览器 , 告诉它下一步怎么走 模板 🍀 我们不需要在模板中做很多的工作 , 最简单的例子 : {% csrf_token %} {{ form }} 这样我们就可以通过模板语言中的form变量将所有的form字段和属性都会打包为HTML中的form 跨站请求伪造的防护 Django原生支持一个简单易用的 protection against Cross Site Request Forgeries , 当提交一个启用CSRF防护的POST表单时 , 你必须使用上面例子中的csrf_token 模板标签 HTML 5 输入类型和浏览器验证 如果你的表单包含URLField , EmailField 或其它整数字段类型 , Djanog将使用url , email 和 number这样的HTML 5 输入类型 ; 默认情况下 , 浏览器可能会对这些字段进行它们自身的验证 , 这些验证可能比Django的验证更严格 , 如果你想禁用这个行为 , 可以通过设置form标签的novalidate 属性 , 或者指定一个不同的字段 , 如TextInput More about Django Form classes 🍀 所有的表单类都被创建为django.forms.Form 的子类 , 包括Django Admin中的 ModelForm 实际上 , 如果你的表单打算直接用来添加和编辑Django的模型 , ModelForm 可以节省你的许多时间 , 精力和代码 , 因为它将根据Model类中构建一个表单以及相应的字段和属性 绑定和未绑定 🍀 绑定和未绑定的表单实例之间的区别如下 : 未绑定的表单没有关联的数据 , 当渲染给用户时 , 它将为空或包含默认的值 绑定的表单具有提交的数据 , 因此可以用来检验数据是否合法 ,如果渲染一个不合法的绑定的表单 , 它将包含内联的错误信息 , 告诉用户如何纠正数据 表单的is_bound 属性将告诉你一个表示是否具有绑定的数据 字段 🍀 前面的例子中 , 我们仅使用了一个字段 , 当然字段还有很多 , 我们可以在 Form fields 中找到完整的列表 窗口小部件 🍀 每个表单字段都有一个对应的 Widget class , 它对应一个HTML表单小部件 , 例如 : 在大部分情况下 , 字段都具有一个合理的默认Widget , 例如 , 默认情况下 , CharField具有一个TextInput Widget , 它在HTML中生成一个 >>> from django import forms >>> name = forms.TextInput(attrs={'size': 10, 'title': 'Your name',}) >>> name.render('name', 'A name') '' 字段数据 🍀 不管提交的是什么数据 , 一旦通过调用is_valid() 成功验证后 , 验证后的表单数据将位于form.cleaned_data 字典中 , 并且这些数据已经为你转换好Python类型 此时你依然可以从request.POST 中直接访问到未验证的数据 , 但是访问验证后的数据更好一些 from django.core.mail import send_mail if form.is_valid(): subject = form.cleaned_data['subject'] message = form.cleaned_data['message'] sender = form.cleaned_data['sender'] cc_myself = form.cleaned_data['cc_myself'] recipients = ['info@example.com'] if cc_myself: recipients.append(sender) send_mail(subject, message, sender, recipients) return HttpResponseRedirect('/thanks/') 有些字段类型需要一些额外的处理 , 例如 , 使用表单上传的文件需要不同地处理 (它们可以从request.POST获取 , 而不是request.FILES ) , 图和使用表单处理文件上传的更多细节 , 见Binding uploaded files to a form 关于Django中如何发送邮件的更多信息 , 见Sending email 使用表单模板 🍀 表单渲染选项 🍀 对于/ 来说 , 还有其他几个输出选项 : form.as_table , 以表格的形式将他们渲染在标签中 form.as_p , 将它们渲染在标签中 form.as_ul , 将它们渲染在标签中 注意 , 我们必须自己提供或 实例 # 实例输出{{ form.as_p }} Subject: Message: Sender: Cc myself: 手动渲染字段 🍀 我们不必让Django打开表单的字段 , 如果我们想要 , 我们也可以手动执行 (例如 , 允许我们重新排序字段) , 每个字段都是表单的一个属性 , 可以使用 {{ form.name_of_field }} 来访问 , 并将在Django模板中正确地渲染 , 如下 : {{ form.non_field_errors }} {{ form.subject.errors }} Email subject: {{ form.subject }} {{ form.message.errors }} Your message: {{ form.message }} {{ form.sender.errors }} Your email address: {{ form.sender }} {{ form.cc_myself.errors }} CC yourself? {{ form.cc_myself }} 完整的 元素还可以使用label_tag() 生成 {{ form.subject.errors }} {{ form.subject.label_tag }} {{ form.subject }} 渲染表单错误信息 🍀 对于错误信息 , Django已经帮我们处理好了 , 如下我们将自己处理每个字段的错误和表单整体的各种错误 Sender is required. 这个ul有一个errorlist CSS class , 你可以用它来定义外观 field属性 🍀 {{ field.label }} {{ field.label_tag }} tag. This includes the form’s label_suffix. For example, the default label_suffix is a colon: Email address: -> {{ field.id_for_label }} {{ field.value }} {{ field.html_name }} {{ field.help_text }} {{ field.errors }} containing any validation errors corresponding to this field. You can customize the presentation of the errors with a {% for error in field.errors %} loop. In this case, each object in the loop is a simple string containing the error message. -> {{ field.is_hidden }} {{ field.field }} 完整的属性和方法列表 , 见BoundField 循环隐藏和可见的字段 🍀 如果你在模板中手动布局一个表单 , 而不是依赖Django的默认表单布局 , 你可能会想要用不同于非隐藏的字段来处理不同的字段 ; 例如 , 因为隐藏的字段不显示任何内容 , 将错误信息放在 \"旁边\" 可能会引起用户的混淆 , 因此这些字段的错误应该以不同的方式处理 Django提供了两种方法 , 让你可以独立地遍历隐藏和可见字段 : visible_fields()和hidden_fields() , 如下 : {# Include the hidden fields #} {% for hidden in form.hidden_fields %} {{ hidden }} {% endfor %} {# Include the visible fields #} {% for field in form.visible_fields %} {{ field.errors }} {{ field.label_tag }} {{ field }} {% endfor %} 这个示例没有处理隐藏字段中的任何错误信息 , 通常 , 隐藏字段中的错误意味着表单被篡改 , 因为正常的表单填写不会改变他们 可重用表单模板 🍀 如果你的网站在多个地方对表单使用相同的渲染逻辑 , 你可以保存表单的循环到一个单独的模板来减少重复 , 然后在其他模板中使用include 标签来重用它 : # In your form template: {% include \"form_snippet.html\" %} # In form_snippet.html: {% for field in form %} {{ field.errors }} {{ field.label_tag }} {{ field }} {% endfor %} 如果传递到模板上写问中的表单对象具有一个不同的名称 , 你可以使用include标签的with参数来给它起个别名 : {% include \"form_snippet.html\" with form=comment_form %} 上面只是一些基础 , 表单还可以完成更多的工作 , 更多内容见 : The Forms Reference "},"05-Web框架/Django/12-Django - Template.html":{"url":"05-Web框架/Django/12-Django - Template.html","title":"Django - Template","keywords":"","body":"Django - Template 介绍 🍀 本篇相当于一个引子 , 了解即可 , 我们一般不这样去使用 上一篇中我们了解了视图 , 即处理请求返回响应 ; 通常我们都是返回一个字符串 , 一个以HTML规则编写的字符串 , 使其在浏览器上能够很好的显示 那么最初 , 我们都是直接返回一堆字符串 , 如下 : # 原始视图函数 from djang.shortcuts import HttpResponse def index(request): # 所有的视图函数都必须返回响应 return HttpResponse(\"\"\" Title %s \"\"\" % \"Hello World!\") Django为我们提供了另一种写法 : from django.template import Template,Context from django.shortcuts import HttpResponse,render def index(request): # 创建模板对象,其中{{ message }}如同%s一样占位 tem_obj = Template(\"\"\" Title {{ message }} \"\"\") # 创建上下文对象,用于渲染模板 con_obj = Context({'message':'Hello World'}) # 进行渲染 html = tem_obj.render(con_obj) # 将渲染完成的html返回给前端 return HttpResponse(html) 这种写法看起来并没有比原生视图函数有什么好 , 我们一般不这么写 , 而是使用如下写法 : from django.shortcuts import HttpResponse,render def index(request): message = \"Hello World!\" # render为我们进行了封装,我们只需直接传递模板文件名和上下文就可直接完成 return render(request,'index.html',{'message':message}) 我们知道 , 我们的页面 (也就是我们的模板) 经常有很多相同的代码 , 这是非常不好的 , 所以 , 这是Web框架需要解决的问题 所以作为一个Web框架 , Django需要一个方便的方式来动态生成HTML , 最常见的方法就是依赖于模板 Django项目可以配置一个或多个模板引擎 , Django为其自己的模板系统提供内置后端 , 也就是我们所说的Djanog模板语言 , 目前最流行的就是Jinja2 由于历史原因 , 对模板引擎的一般支持和Django模板语言的实现都存在于django.template 命名空间中 , 对于模板语言会放在下一篇中进行整理 Warning : 模板系统对于不受信任的模板作者是不安全的 , 如 : 一个站点不应允许其用户提供自己的模板 , 因为模板作者可以执行诸如XSS攻击和访问可能包含敏感信息的模板变量的属性 当我们使用Pycharm 创建一个项目时 , 会自动会我们创建一个templates 文件夹 , 就是用来存放我们的模板文件的 Python使用模板系统是一个三步过程 : 配置一个Engine 将模板代码编译成一个 Template对象 利用Context对象对模板进行渲染 注意 : 这一篇主要对于模板引擎的配置 , Template对象 , Context对象进行描述 ; 但是我们一般不会自己创建Template和Context对象 , 因为Django已经帮我们做了这些工作 , 所以我们主要还是直接使用render() Engine 🍀 模板引擎使用TEMPLATES 设置进行配置 , 位于settings.py 中 , 如下 : TEMPLATES = [ { # 实现Django模板后端API的模板引擎类的Python路径,内置后端是 # django.template.bakcends.django.DjangoTemplates # django.template.backends.jinja2.Jinja2 'BACKEND': 'django.template.backends.django.DjangoTemplates', # 根据搜索顺序定义引擎应该查找模板源文件的目录列表 'DIRS': [os.path.join(BASE_DIR, 'templates')] , # 告诉引擎是否应该在已安装的应用程序中查找模板,每个后端都为其模板应存储在其中的应用程序内的子目录定义一个常规名称 'APP_DIRS': True, # 包含后端特定的设置 'OPTIONS': { # 当模板被请求呈现时,可用于填充上下文的可调用Python路经列表 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] 用法 🍀 在django.template.loader 模块中定义了两个函数来加载模板 def get_template(template_name, using=None): \"\"\" Loads and returns a template for the given name. Raises TemplateDoesNotExist if no such template exists. \"\"\" # return:The exact type of the return value depends on the backend that loaded the template. Each backend has its own Template class. # How templates are searched and loaded depends on each engine’s backend and configuration. # using:engine's NAME,restrict search to a specific template engine def select_template(template_name_list, using=None): \"\"\" Loads and returns a template for one of the given names. Tries names in order and returns the first template found. Raises TemplateDoesNotExist if no such template exists. \"\"\" # select_template() is just like get_template(), except it takes a list of template names. It tries each name in order and returns the first template that exists. Template对象通过调用get_template() 和select_template() 生成 , 但是必须提供一个render() 方法 一个搜索算法🌰 TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [ '/home/html/example.com', '/home/html/default', ], }, { 'BACKEND': 'django.template.backends.jinja2.Jinja2', 'DIRS': [ '/home/html/jinja2', ], }, ] 如果我们调用get_template('story_detail.html') , 那么Django将会寻找以下文件 : /home/html/example.com/story_detail.html ('django' engine) /home/html/default/story_detail.html ('django' engine) /home/html/jinja2/story_detail.html ('jinja2' engine) 如果调用select_template(['story_253_detail.html','story_detail.html']) , 则Django将会寻找如下文件 : /home/html/example.com/story_253_detail.html ('django' engine) /home/html/default/story_253_detail.html ('django' engine) /home/html/jinja2/story_253_detail.html ('jinja2' engine) /home/html/example.com/story_detail.html ('django' engine) /home/html/default/story_detail.html ('django' engine) /home/html/jinja2/story_detail.html ('jinja2' engine) 当Django找到一个存在的模板时 , 就会停止寻找 为了减少加载和渲染模板的重复性 , Django提供了一个快捷方式来自动化这个过程 , 即django.template.loader 中的render_to_string() , 具体如下 : def render_to_string(template_name, context=None, request=None, using=None): \"\"\" Loads a template and renders it with a context. Returns a string. template_name may be a string or a list of strings. template_name:要加载和渲染的模板名称,如果传入模板名称列表,则Django将会使用select_template() context:一个用于呈现模板上下文的dict request:一个可选的HttpRequest using:一个可选的模板引擎名称,搜索模板将被限制在该引擎 \"\"\" # render_to_string()相当于调用get_template()后并调用其render()方法 用法示例 : from django.template.loader import render_to_string rendered = render_to_string('my_template.html', {'foo': 'bar'}) engines 模板引擎可以使用django.template.engines : from django.template import engines # The lookup key — 'django' in this example — is the engine’s NAME. django_engine = engines['django'] template = django_engine.from_string(\"Hello {{ name }}!\") 内置后端 🍀 DjangoTemplates TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [], # DjangoTemplate引擎会在已安装应用程序的模板子目录中寻找模板,这个通用名称是为了向后兼容而保留的 'APP_DIRS': True, # 可选配置 'OPTIONS': { # 控制是否启动HTML自动转义 'autoescape':True # 当模板被请求呈现时,用于填充上下文的可调用Python路径列表.这些可调用对象将请求对象作为参数,并返回一个合并到上下文的列表 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], # 打开模板调试模式,默认为DEBUG设置的值 'debug':True # 模板加载器的Python路径列表 'loaders':[ ( 'django.template.loaders.filesystem.Loader', [os.path.join(BASE_DIR, 'templates')], ), ], # 如果变量不存在,模板系统会默认插入引擎string_if_invalid的值 'string_if_invalid':'' # 用于读取磁盘上的模板文件的字符集,默认utf-8 'file_charset':'utf-8' # 一个标签和模板标签模块路径的字典,用于注册模板引擎 'libraries':{ 'myapp_tags':'path.to.myapp.tags', 'admin.urls':'django.contrib.admin.templatetags.admin_urls', }, # 一个要添加到内置插件的模板标签模块 'builtins': ['myapp.builtins'], }, }, ] Jinja2 安装 pip install Jinja2 将BACKEND 设置为django.template.backends.jinja2.Jinja2 最重要的OPTIONS条目是environment , 这是一个Python路径 , 可以返回一个Jinja2环境 , 默认为jinja2.Environment Django添加了几个与Jinja2不同的默认值 : 'autoescape' : True 'loader' : a loader configured for DIRS and APP_DIRS 'auto_reload' : settings.DEBUG 'undefined' : DebugUndefined if settings.DEBUG else Undefined Jinja2也接受如DjangoTemplate的OPTIONS : TEMPLATES = [ { 'BACKEND': 'django.template.backends.jinja2.Jinja2', 'DIRS': [] 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] Jinja2 : http://jinja.pocoo.org/docs/2.10/ 自定义后端 : https://docs.djangoproject.com/en/1.11/topics/templates/#custom-backends origin API : https://docs.djangoproject.com/en/1.11/topics/templates/#origin-api-and-3rd-party-integration Template 🍀 通过上文我们就可以配置好一个Engine了 , 那么接下来就是将模板代码编译成Template对象了 推荐创建Template对象的方法是调用Engine中的get_template() , select_template() 和工厂方法from_string() 同样django.template.backends.django.Template 也适用django.template.Template 通用的模板API , 也就是说无论DjangoTemplate 还是Jinja2 , 都可以通过django.template.Tempalte 来进行创建 class Template[source] 这个类存在于django.template.Template 中 , 构造函数如下 : def __init__(self, template_string, origin=None, name=None, engine=None): 实例 from django.template import Template template = Template(\"My name is {{ my_name }}.\") 注意 : 创建Template对象时 , 系统只解析一次原始模板代码 , 从那时起 , 它就被存储在内部 , 作为一个树形结构来提高性能 Context 🍀 一旦我们拥有了一个Template对象 , 我们就可以用它来渲染一个上下文 ; 并且可以重复使用相同的模板 , 使用不同的上下文多次渲染它 django.template.Context 除了上下文数据之外 , 还保存一些元数据 , 它被传递给Template.render() 来呈现模板 django.template.RequestContext 是Context存储当前HttpRequest并运行模板上下文处理器的子类 class Context(dict_ = None) [source] 构造函数如下 def __init__(self, dict_=None, autoescape=True, use_l10n=None, use_tz=None): 实例 >>> from django.template import Context, Template >>> template = Template(\"My name is {{ my_name }}.\") >>> context = Context({\"my_name\": \"Adrian\"}) >>> template.render(context) \"My name is Adrian.\" >>> context = Context({\"my_name\": \"Dolores\"}) >>> template.render(context) \"My name is Dolores.\" 大多数情况下 , 我们将Context通过传入完全填充的字典来实例化对象Context() , 但是Context , 使用标准字典语法 , 也可以在实例化对象后添加和删除项目 , 如下 : >>> from django.template import Context >>> c = Context({\"foo\": \"bar\"}) >>> c['foo'] 'bar' >>> del c['foo'] >>> c['foo'] Traceback (most recent call last): ... KeyError: 'foo' >>> c['newvariable'] = 'hello' >>> c['newvariable'] 'hello' 更多 : https://docs.djangoproject.com/en/1.11/ref/templates/api/ https://docs.djangoproject.com/en/1.11/topics/templates/ "},"05-Web框架/Django/13-Django - Template Language.html":{"url":"05-Web框架/Django/13-Django - Template Language.html","title":"Django - Template Language","keywords":"","body":"Django - Template Language 介绍 🍀 模板只是一个文本文件 , 它能够生成以下文本格式的文件 , 如 : HTML , XML , CSV , etv等 下面是一个简单的基本模板 , 每个元素将在本文后面解释 {% extends \"base_generic.html\" %} {% block title %}{{ section.title }}{% endblock %} {% block content %} {{ section.title }} {% for story in story_list %} {{ story.headline|upper }} {{ story.tease|truncatewords:\"100\" }} {% endfor %} {% endblock %} 模板包括在使用时会被替换掉的变量 , 以及控制模板逻辑的标签 变量 🍀 定义变量 变量名称由字母数字和下划线组成 , \".\"和\"_\" 也可以出现在变量部分 ; 变量名中不能有空格或标点符号 {{ variable }} 实例 My first name is {{ first_name }}. My last name is {{ last_name }}. 在上下文中 , {'first_name': 'John', 'last_name': 'Doe'} , 将呈现如下效果 : My first name is John. My last name is Doe. 字典查找 , 属性查找和列表索引查找用 \".\" 号实现 : {{ my_dict.key }} {{ my_object.attribute }} {{ my_list.0 }} 当模板系统遇到 \".\" 时 , 会按照以下顺序进行查找 : Dictionary lookup Attribute or method lookup Numeric index lookup 如果结果值是可调用的 , 它将不使用参数进行调用 , 而调用的结果则成为模板值 这个查找顺序可能会导致一些无法预料的行为 , 这些对象覆盖了字典查找 , 如下 : {% for k, v in defaultdict.iteritems %} Do something with k and v here... {% endfor %} # 由于字典查找首先发生,该行为将启动并提供一个默认值,而不是使用预期的iteritems()方法,在这种情况下,我们应该考虑先转换成字典 过滤器 🍀 可以通过过滤器来修改变量的显示 , 使用 \"|\" 来应用过滤器 单个过滤器 # 将文本转换为小写 {{ name|lower}} 链接过滤器 # 转义文本内容,将换行符转换为p标签 {{ text|escape|linebreaks}} 带参数过滤器 # 显示bio变量的前30个单词 {{ bio|truncatewords:30 }} # 过滤器参数中包含空格的必须被引用,用逗号和空格连接列表 {{ list|join:\", \"}} Django中提供了大约60个内置的模板过滤器 , 以下是一些常用的模板过滤器 : default : 如果一个变量是错误或者为空 , 则使用默认给定的 , 否则就使用变量的值 {{ value|default:\"nothing\"}} length : 返回值的长度 , 适用于字符串和列表 {{ value|length}} filesizeformat : 格式化值为一个人们可读的文件大小 , 如 : '13KB' , '4.1MB' , '102bytes'等 {{ value|filesizeformat}} # value = 123456789 # output : 117.7MB 更多内置过滤器 : built-in filter reference 标签 🍀 标签比变量更复杂 : 标签可以在输出中创建文本 ; 执行循环或逻辑控制 ; 将外部信息加载到模板中以供以后的变量使用 定义标签 {% tag %} Django附带大约二十个内置模板标签 , 以下是一些常用标签 : for : 遍历数组中的每个项目 # 展示运动员名单 {% for athlete in athlete_list %} {{ athlete.name }} {% endfor %} for ... empty : 当给出的组为空或者没有被找到时 , 所执行的操作 {% for person in person_list %} {{ person.name }} {% empty %} sorry,no person here {% endfor %} if , elif 和else : 流程控制 {% if athlete_list %} Number of athletes: {{ athlete_list|length }} {% elif athlete_in_locker_room_list %} Athletes should be out of the locker room soon! {% else %} No athletes. {% endif %} 使用过滤器和各种操作符 {% if athlete_list|length > 1 %} Team: {% for athlete in athlete_list %} ... {% endfor %} {% else %} Athlete: {{ athlete_list.0.name }} {% endif %} 注意 : 大多数模板的过滤器返回字符串 , 所以使用过滤器在数学上通常不会像所期望的那样工作 , length是一个列外 block和extends 这两个标签用户设置模板继承 , 这是一种在模板中减少 \"样板\" 的强大方法 , 见下文 内置标签参考 : built-in tag reference 自定义模板标签和过滤器 : Custom template tags and filters 注释 🍀 要在模板中注释行的一部分 , 可以使用注释语法 : # 单行注释 {# ... #} # 多行注释 {% comment %} ... {% endcomment %} 模板继承 🍀 Django模板引擎中最强大 , 也是最复杂的部分是模板继承 , 模板继承允许你创建一个基本 \"框架\" 模板 , 其中包含网站所有常用元素 , 并定义子模板可以覆盖的块 base.html {% block title %}My amazing site{% endblock %} {% block sidebar %} Home Blog {% endblock %} {% block content %}{% endblock %} 上面定义了一个简单的HTML框架文档 , 在这个例子中 , block标签定义了三个子模板可以填充的块 , 所有的block 标签都告诉模板引擎一个子模板可以覆盖模板的哪些部分 子模板可能如下所示 : {% extends \"base.html\" %} {% block title %}My amazing blog{% endblock %} {% block content %} {% for entry in blog_entries %} {{ entry.title }} {{ entry.body }} {% endfor %} {% endblock %} extends 标签用于告诉模板引擎 , 该模板扩展了另一个模板 , 当模板系统执行这个模板时 , 首先会找到父模板 , 也就是这里的base.html 于是 , 模板引擎就将block标签中的内容替换base.html 中block标签中的内容 , 根据blog_entries 的值 , 输出可能如下 : My amazing blog Home Blog Entry one This is my first entry. Entry two This is my second entry. 在子模板中为定义的块 , 会使用父模板中的块 , 也就是说 , 没有定义则以父模板作为备用 为了增加可读性 , 可以给标签进行命名 , 如下 : {% block content %} ... {% endblock content %} 自动HTML转义 🍀 为了避免变量值中带有的HTML字符被解析 , 我们有两种方式 : 将潜在有害的HTML字符转换为无害的字符 , 这种方式会把责任放在我们身上 , 需要我们自己来逃避数据 , 所以明显这不够安全 ; 这也是Django头几年的默认解决方案 利用Django的自动HTML转义 默认情况下 , 在Django中 , 每个模板都会自动转义每个变量标签的输出 , 具体来说 , 以下五个字符是会被转义的 : &lt; > 被转换成&gt; ' (单引号) 转换为&#39; \" (双引号) 转换为&quot; & 被转换为&amp; 注意 : 这种行为默认是开启的 关闭自动HTML转义 对于个人变量 使用safe过滤器 : # 会被转义 This will be escaped: {{ data }} # 不会被转义 This will not be escaped: {{ data|safe }} data中包含 , 结果如下 : This will be escaped: &lt;b&gt; This will not be escaped: 对于模板块 使用autoscape标签 : {% autoescape off %} Hello {{ name }} {% endautoescape %} autoscape标签接收两个参数 , on和off , 如下 : Auto-escaping is on by default. Hello {{ name }} {% autoescape off %} This will not be auto-escaped: {{ data }}. Nor this: {{ other_data }} {% autoescape on %} Auto-escaping applies again: {{ name }} {% endautoescape %} {% endautoescape %} 自动转义标记将其影响传递到扩展当前的模板和包含通过include标记的模板 , 如下 : base.html {% autoescape off %} {% block title %}{% endblock %} {% block content %} {% endblock %} {% endautoescape %} child.html {% extends \"base.html\" %} {% block title %}This &amp; that{% endblock %} {% block content %}{{ greeting }}{% endblock %} 呈现如下 : This &amp; that Hello! 自定义标签和过滤器 : https://docs.djangoproject.com/en/1.10/howto/custom-template-tags/ https://docs.djangoproject.com/en/1.10/ref/templates/language/#custom-tag-and-filter-libraries 更多Template Language相关 : https://docs.djangoproject.com/en/1.10/ref/templates/language/ "},"05-Web框架/Django/14-Django - Middleware.html":{"url":"05-Web框架/Django/14-Django - Middleware.html","title":"Django - Middleware","keywords":"","body":"Django - Middleware 介绍 🍀 在Django中 , 中间件本质上就是一个类 , 我们可以使用中间件来对请求和响应进行批量处理 , 中间件所在的层次介于WSGI协议与Django URL系统之间 , 它类似一个一个的盒子 , 所有的请求和响应到来时 , 都必须穿过一个一个的盒子 (中间件) , 如下 : 请求 → → 中间件1 → → 中间件2 → ... → 响应 ↓ ↓ 请求 ← ← 中间件1 ← ← 中间件2 ← ... ← 响应 它是一个轻量级 , 底层的 \"插件\" 系统 , 用于在全局修改Django的输入或输出 每个中间件负责完成某个特定的功能 , 如下为默认Django激活的中间件如下 : MIDDLEWARE = [ # 安全保护中间件 'django.middleware.security.SecurityMiddleware', # 会话中间件 'django.contrib.sessions.middleware.SessionMiddleware', # 通用中间件 'django.middleware.common.CommonMiddleware', # 跨站请求伪造保护中间件 'django.middleware.csrf.CsrfViewMiddleware', # 认证中间件 'django.contrib.auth.middleware.AuthenticationMiddleware', # 消息中间件 'django.contrib.messages.middleware.MessageMiddleware', # 防止点击劫持中间件 'django.middleware.clickjacking.XFrameOptionsMiddleware', ] 我们可以看到MIDDLEWARE是一个列表 , 也就是说中间件是有顺序的 , 我们在使用中间件时需要注意中间件的排序 , 因为有的中间件可能需要依赖某一中间件 , 所以其应该放在依赖的中间件之后 对于请求和响应的少量或部分处理 , 我们可以使用装饰器来实现 中间件排序 : Middleware ordering 内置中间件 : built-in middleware reference CSRF 🍀 CSRF 即Cross Site Request Forgery protection , 中文意思为跨站请求伪造 , 也被称为\"One Click Attack\"或者 Session Riding , 通常缩写为CSRF或XSRF , 是一种对网站的恶意利用 攻击通过在授权用户访问的页面中包含链接或者脚本的方式工作 , 例如 : 一个网站用户Bob可能正在浏览聊天论坛 , 而同时另一个用户Alice也在此论坛中 , 并且后者刚刚发布了一个具有Bob银行链接的图片消息 ; 设想一下 , Alice编写了一个在Bob的银行站点上进行取款的form提交的链接 , 并将此链接作为图片src ; 如果Bob的银行在cookie中保存他的授权信息 , 并且此cookie没有过期 , 那么当Bob的浏览器尝试装载图片时将提交这个取款form和他的cookie , 这样在没经Bob同意的情况下便授权了这次事务 所以为了防止CSRF的发生 , Django为我们提供了中间件django.middleware.csrf.CsrfViewMiddleware CSRF中间件使用 🍀 如果要在我们的视图中使用CSRF保护 , 我们需要进行如下操作 : CSRF中间件默认在MIDDLEWARE设置中被激活 , CSRF中间件应该在任何视图中间件之前 , 以确保CSRF攻击已被处理 在任何使用POST的表单模板中 , 如果表单用于内部URL , 则需要使用csrf_token标记form标签 , 如下 : {% csrf_token %} 在相应的视图函数中 , 确保使用RequestContext 来渲染响应 , 以便csrf_token能够正常使用 , render()函数 , 或者contrib应用以及通用视图都是用RequestContext 在Jinja2模板中用csrf_input代替了csrf_token 注意 : 如果传入的请求未能通过CsrfViewMiddleware执行的检查 , 则会向用户发送403 Forbidden 响应 ; 这也就是如果我们激活了CsrfViewMiddleware 中间件 , 而没添加csrf_token 时为什么会出现403 Forbidden错误 更多CSRF中间件使用参考 : Cross Site Request Forgery protection documentation 激活中间件 🍀 我们如果要使用中间件 , 就需要在Django配置中的MIDDLEWARE添加中间件组件 默认时 , Django已经为我们配置好了一些内置的中间件 , 如果我们想要使用自定义的中间件 , 那么我们就需要在该配置中进行添加了 , 如下 : MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', 'RbacMiddleware', # 激活Rbac中间件 ] 自定义中间件 🍀 有时候我们需要自定义中间件来达到我们的实际要求 , 其有两种方式 , 即通过类或者函数 通常我们使用类 , 如下 : class SimpleMiddleware(object): def __init__(self, get_response): self.get_response = get_response # One-time configuration and initialization. def __call__(self, request): # Code to be executed for each request before # the view (and later middleware) are called. response = self.get_response(request) # Code to be executed for each request/response after # the view is called. return response 在我们自定义中间件时需要注意如下 : __init__() 必须接受get_response参数 , 旧版中__init__()不接受任何参数 , 所以为了兼容性 , 我们应该这样写def __init__(self, get_response): 每个请求都会调用一次__call__()方法 当Web服务器启动时 , __init__()仅会被调用一次 MiddlewareMixin 🍀 上面的写法只适用于Django 1.9及之前的写法 , 在1.10的版本中 , Django为我们提供了django.utils.deprecation.MiddlewareMixin以简化MIDDLEWARE和旧的MIDDLEWARE_CLASSES兼容的中间件类 ; Django 1.10之后的版本使用MIDDLEWARE代替MIDDLEWARE_CLASSES , Django中包含的所有中间件类都兼容这两种设置 class MiddlewareMixin # 自定义中间件类需要继承该类 class MiddlewareMixin(object): def __init__(self, get_response=None): self.get_response = get_response super(MiddlewareMixin, self).__init__() def __call__(self, request): response = None if hasattr(self, 'process_request'): response = self.process_request(request) if not response: response = self.get_response(request) if hasattr(self, 'process_response'): response = self.process_response(request, response) return response ''' 注意: 为了解决版本的兼容问题, 我们不应该由django.utils.deprecation中来导入MiddlewareMixin, 因为在之后的版本MiddlewareMixin将会被剔除 ''' 如果与MIDDLEWARE_CLASSES一起使用 , 在不会使用__call__()方法 ; Django会直接调用process_request()和process_response() 在大多数情况下 , 继承这种混合将足以使旧式中间件与新系统兼容 , 具有足够的向后兼容性 钩子函数 🍀 在请求阶段中 , 调用视图之前 , Django会按照MIDDLEWARE中定义的顺序自顶向下应用中间件 , 我们需要用到以下两个钩子函数 : process_request() process_view() 在响应阶段中 , 调用视图之后 , 中间件会按照相反的顺序应用 , 自底向上 , 我们需要用到以下三个钩子函数 : process_exception() , 仅当视图抛出异常时使用 process_template_response() , 仅用于模板响应 process_response() 如下图 : 我们可以将这些中间件比作为一个洋葱 , 每个中间件类都是一个\"洋葱层\" 如果请求通过洋葱的所有层 , 一直到核心的视图 , 随后响应会按照相反的顺序原路返回 如果其中某一层短路并返回响应 , 那么将不能到达视图 , 而是直接在短路层就返回响应 process_request() 🍀 process_request(request): \"\"\" request:是一个HttpRequest对象 \"\"\" 在Django决定执行哪个视图之前 , 将会先调用process_request() process_request()应该返回一个None或者一个HttpResponse对象 , 返回说明如下 : 如果返回None , Django会继续处理这个请求 , 执行其它中间件的process_request() , 然后执行中间件的process_view() , 最后执行对应的视图 如果返回一个HttpResponse对象 , Django就不会去调用其他的中间件的request_view或request_exception或对应的视图 , 而是直接转变到响应阶段 , 按照原路返回 process_view() 🍀 process_view(request, view_func, view_args, view_kwargs): \"\"\" request:一个HttpRequest对象,与我们在前面视图函数中的request一样 view_func:是Django会调用的一个Python函数 view_args:一个会传递到视图的位置参数列表 view_kwargs:一个会传递到视图的关键字参数字典 view_args和view_kwargs都不包含第一个视图函数request \"\"\" process_view()会在Django调用视图之前被调用 , 它返回一个None或一个HttpResponse对象 , 返回说明如下 : 如果返回None , Django将会继续处理这个请求 , 执行其它的process_view()中间件然后调用对应的视图 如果返回一个HttpResponse对象 , Django就不会去调用其它中间件的process_view()或process_exception()或对应的视图 , 它将转变至响应阶段 , 并返回结果 Note Accessing request.POST inside middleware before the view runs or in process_view() will prevent any view running after the middleware from being able to modify the upload handlers for the request, and should normally be avoided. The CsrfViewMiddleware class can be considered an exception, as it provides the csrf_exempt() and csrf_protect() decorators which allow views to explicitly control at what point the CSRF validation should occur. process_exception() 🍀 process_exception(request, exception): \"\"\" request:一个HttpRequest对象 exception:是一个被视图中的方法抛出来的exception对象 \"\"\" 当一个视图抛出异常时 , Django会调用process_exception()来处理 ; process_exception()应该返回None或者HttpResponse对象 , 如果返回HttpResponse对象 , 则将应用模板响应和响应中间件 , 并将生成的响应返回给浏览器 , 否则Django会使用默认异常处理方式进行处理 注意 : 在处理响应期间 , 中间件的执行顺序是倒序执行的 , 所以如果异常中间件返回响应 , 那么下一层中间件的process_exception方法将不会调用 , 因为在上一层已经捕捉完成 process_template_response() 🍀 process_template_response(request, response): \"\"\" request:是一个HttpRequest对象 response:是一个TemplateResponse对象(或等价的对象), 由Django视图或者中间件返回 \"\"\" 这个方法必须返回一个实现了render方法的响应对象 , 它可以修改给定的response对象 , 通过修改response.tmplate_name和response.context_data或者它可以创建一个全新的TemplateResponse对象(或等价的对象) 并且一旦所有的模板响应中间件被调用 , 响应会自动被渲染 process_response() 🍀 process_response(request,response): \"\"\" request:一个HttpRequest对象 response:Django视图或者中间件返回的HttpResponse或者StreamingHttpResponse对象 \"\"\" process_response()在所有响应返回浏览器之前被调用 这个方法必须返回HttpResponse或者StreamingHttpResponse对象 , 它可以改变已有的response , 或者创建并返回新的HttpResponse或StreamingHttpResponse对象 process_response不像process_request和process_view那样会因为前一个中间件返回的HttpResponse而被跳过 , process_response方法总是会被调用 , 这意味着你的process_response方法不能依赖于process_request方法中的设置 处理流响应 🍀 不像HttpResponse , StreamingHttpResponse并没有content属性 , 所以 , 中间件再也不能假设所有响应都带有content属性 , 如果它们需要访问内容 , 他们必须测试是否为流式响应 , 并相应地调整自己的行为 , 如下 : if response.streaming: response.streaming_content = wrap_streaming_content(response.streaming_content) else: response.content = alter_content(response.content) 注意 : 我们需要假设streaming_content可能会大到在内存中无法容纳 , 响应中间件可能会把它封装在新的生成器中 , 但是一定不要销毁它 , 封装一般如下 : def wrap_streaming_content(content): for chunk in content: yield alter_content(chunk) RBAC案例 🍀 rbac即Role-Based Access Control , 基于角色的权限访问控制 , 这种控制极大地简化了权限的管理 , 下面为rbac中我们自定义使用的中间件案例 : import re from django.shortcuts import redirect,HttpResponse from django.conf import settings class MiddlewareMixin(object): def __init__(self, get_response=None): self.get_response = get_response super(MiddlewareMixin, self).__init__() def __call__(self, request): response = None if hasattr(self, 'process_request'): response = self.process_request(request) if not response: response = self.get_response(request) if hasattr(self, 'process_response'): response = self.process_response(request, response) return response # 继承MiddlewareMixin类 class RbacMiddleware(MiddlewareMixin): def process_request(self,request): # 获取当前请求的URL current_url = request.path_info # 当前请求不需要执行权限验证 for url in settings.VALID_URL: if re.match(url,current_url): return None # 获取Session中保存当前用户的权限 permission_list = request.session.get(\"permission_url_list\") if not permission_list: return redirect('/login/') # 判断是否具有权限并设置标志位 flag = False for db_url in permission_list: regax = \"^{0}$\".format(db_url) if re.match(regax, current_url): flag = True break # 最后如果具有权限那么继续走向下一个中间件或者视图 # 否则,返回响应 if not flag: return HttpResponse('无权访问') ''' 注意: 对于钩子函数是否定义在于我们自己,但是要注意中间件的工作原理, 比如在这里我们没有定义process_response方法, 但是在MiddlewareMixin类的__call__方法中, 使用了get_response方法 ''' 更多中间件相关 : middleware usage guide "},"05-Web框架/Django/15-Django - Sessions.html":{"url":"05-Web框架/Django/15-Django - Sessions.html","title":"Django - Sessions","keywords":"","body":"Django - Cookie与Sessions 介绍 🍀 基于 Internet的各种服务系统应运而生 , 建立商业站点或者功能比较完善的个人站点 , 常常需要记录访问者的一些信息 ; 论坛作为 Internet发展的产物之一 , 在 Internet 中发挥着越来越重要的作用 , 是用户获取、交流、传递信息的主要场所之一 , 论坛常常也需要记录访问者的一些基本信息 (如身份识别号码、密码、用户在 Web 站点购物的方式或用户访问该站点的次数) ; 目前公认的是 , 通过 Cookie 和 Session 技术来实现记录访问者的一些基本信息 Cookie Cookie 是在 HTTP 协议下 , 服务器或脚本可以维护客户工作站上信息的一种方式 Cookie 是由 Web 服务器保存在用户浏览器 (客户端) 上的小文本文件 , 它可以包含有关用户的信息 ; 无论何时用户链接到服务器 , Web 站点都可以访问 Cookie 信息 目前Cookie是临时的 , 有些则是持续的 , 临时的Cookie只在浏览器上保存一段规定的时间 , 一旦超过规定的时间 , 该Cookie就会被系统清除 持续的 Cookie 则保存在用户的 Cookie 文件中 , 下一次用户返回时 , 仍然可以对它进行调用 ; 在 Cookie 文件中保存 Cookie , 有些用户担心 Cookie 中的用户信息被一些别有用心的人窃取 , 而造成一定的损害 ; 其实 , 网站以外的用户无法跨过网站来获得 Cookie 信息 ; 如果因为这种担心而屏蔽 Cookie , 肯定会因此拒绝访问许多站点页面 , 因为 , 当今有许多 Web 站点开发人员使用 Cookie 技术 , 例如 Session 对象的使用就离不开 Cookie 的支持 Cookie可以弥补HTTP协议无状态的不足 , 在Session出现之前 , 基本上所有的网站都采用Cookie来跟踪会话 Django实现的Cookie 获取Cookie request.COOKIES['key'] requset.get_signed_cookie(self, key, default=RAISE_ERROR, salt='', max_age=None) \"\"\" get_signed_cookie() Attempts to return a signed cookie. If the signature fails or the cookie has expired, raises an exception... unless you provide the default argument in which case that value will be returned instead. \"\"\" 设置Cookie # 响应对象 rep = HttpResponse(...) rep ＝ render(request, ...) rep ＝ redirect() # 设置Cookie rep.set_cookie(self, key, value='', max_age=None, expires=None, path='/',domain=None, secure=False, httponly=False) \"\"\" set_cookie() Sets a cookie. ``expires`` can be: - a string in the correct format, - a naive ``datetime.datetime`` object in UTC, - an aware ``datetime.datetime`` object in any time zone. If it is a ``datetime.datetime`` object then ``max_age`` will be calculated. \"\"\" rep.set_signed_cookie(self, key, value, salt='', **kwargs) 由于cookie保存在客户端的电脑上 , 所以 , JavaScript和jquery也可以操作Cookie Session Cookie虽然一定程度上解决了\"保持状态\" 的需求 , 但是由于Cookie本身最大支持4096字节 , 以及Cookie本身保存在客户端 , 可能被拦截或窃取 , 因此就需要一种新的东西 . 它能支持更多的字节 , 并且他保存在服务器 , 有较高的安全性 在计算机中 , 尤其是在网络应用中 , 我们将Session称为“会话控制” Session 对象存储特定用户会话所需的属性及配置信息 , 这样 , 当用户在应用程序的 Web 页之间跳转时 , 存储在 Session 对象中的变量将不会丢失 , 而是在整个用户会话中一直存在下去 ; 当用户请求来自应用程序的 Web 页时 , 如果该用户还没有会话 , 则 Web 服务器将自动创建一个 Session 对象 , 当会话过期或被放弃后 , 服务器将终止该会话 Session 对象最常见的一个用法就是存储用户的首选项 , 例如 , 如果用户指明不喜欢查看图形 , 就可以将该信息存储在 Session 对象中 ; 注意会话状态仅在支持Cookie的浏览器中保留 Django支持所有的匿名会话 , 简单说就是使用跨网页之间可以进行通讯 , 比如显示用户名 , 用户是否已经发表评论 ; 这个Session框架让你存储和获取每个站点访客的任意数据 它将数据存储在服务端 , 并以Cookies的形式进行发送和接收数据 , Cookies包含一个Session ID , 而不是数据本身 (除非你使用的基于Cookie的后端) 启用Sessions 🍀 Django中的Sessions是通过中间件实现的 如果我们要启用Session功能 , 需要执行以下操作 : 编辑settings.py中的MIDDLEWARE , 确保其中包含django.contrib.sessions.middleware.SessionMiddleware ; 默认在我们使用django-admin startproject 命令时 , settings.py中已经启用该中间件 如果你不想使用Sessions , 你可以将MIDDLEWARE中的SessionMiddleware移除以及将INSTALLED_APPS中的django.contrib.sessions移除 , 它能够为你节省一点开销 配置Session引擎 🍀 默认情况下 , Django存储会话到你的数据库中 (使用django.contrib.sessions.models.Session) , 尽管这很方便 , 但是在某些情况下 , 在其他地方存储会话数据的速度更快 , 因此Django可以配置为在文件系统或缓存中存储会话数据 数据库 🍀 如果你想使用数据库支持的会话 , 你需要添加django.contrib.sessions 到你settings.py中的INSTALLED_APP设置中 , 默认就是使用的数据库 在配置完成之后 , 需要执行manage.py migrate 来安装村粗会话数据的一张数据库表 缓存 🍀 为了更好的性能 , 我们可以使用一个基于缓存的会话后端 使用Django的缓存系统存储会话数据之前 , 我们需要确保已经配置好了我们的缓存 , 关于缓存的配置见 : cache documentation 注意 : 如果你使用的是Memcached缓存后端 , 那么你应该只使用基于缓存的会话 ; 本地内存缓存后端不会保留足够长的数据 , 它会更快地使用文件或数据库会话 , 而不是通过文件或数据库缓存的后端发送所有数据 , 此外 , 本地内存缓存后端不是多进程安全的 , 因此对于生产环境来说可能不是一个好的选择 如果在 CACHES 中定义了多个缓存 , Django将使用默认的缓存 , 要使用另外的缓存 , 需要将SESSION_CACHE_ALIAS 设置为该缓存的名称 配置好缓存之后 , 对于如何在缓存中存储数据你有两个选择 : 对于简单的缓存会话存储 , 可以设置SESSION_ENGINE为django.contrib.sessions.backends.cache , 此时会话数据将直接存储在你的缓存中 , 然而缓存数据可能不会持久 : 如果缓存填满或者缓存服务器重启 , 缓存数据可能会被清理掉 若要持久的缓存数据 , 可以设置SESSION_ENGINE为django.contrib.sessions.backends.cached_db , 这使用直接写缓存 , 每次写入高速缓存也将写入数据库 , 会话读取仅在数据不在缓存中时才使用数据库 两种会话的存储都非常快 , 但是简单的缓存更快 , 因为它放弃了持久性 , 大部分情况下 , cached_db后端已经足够快 , 但是如果你需要榨干最后一点性能 , 并且接收会话数据丢失的风险 , 那么你可以使用cache 后端 注意使用cached_db会话后端 , 需要遵循 using database-backed sessions 文件 🍀 要使用基于文件的会话 , 将SESSION_ENGINE为django.contrib.sessions.backends.file 你可以设置SESSION_FILE_PATH来控制Djanog存储会话文件的地址 , 默认来自tempfile.gettempdir() ,大多数情况下为/tmp , 当然请确保你的Web服务器具有读取和写入这个位置的权限 Cookie 🍀 同样 , 使用基于Cookie的会话 , 设置SESSION_ENGINE为django.contrib.sessions.backends.signed_cookies , 此时 , 会话数据的存储将使用Django的工具进行cryptographic signing 和 SECRET_KEY 设置 建议保留SESSION_COOKIE_HTTPONLY设置为True , 以防止从JavaScript中访问存储的数据 注意 : 如果SECRET_KEY不保密 , 而你正在使用 PickleSerializer 这可能导致任意远程执行代码 拥有SECRET_KEY的攻击者不仅可以生成你的站点新人的伪造会话数据 , 而且还可以远程执行任意代码 , 因为数据是用pickle序列化的 如果你使用基于Cookie的会话 , 需要格外注意安全密钥对于任何可以远程访问的系统都是永远完全保密的 会话数据已签名但未加密 如果使用基于Cookie的会话 , 则会话数据可以被客户端读取 MAC(消息认证码)被用来保护数据不被客户端修改 , 这样会话数据在被篡改时就会失效 ; 如果保存Cookie的客户端 (例如你的浏览器) 不能保存所有的会话Cookie或丢失数据 , 会话同样会变得不合法 ; 尽管Django对数据进行压缩 , 仍然完全有可能超过每个Cookie常见的4096个字节的限制 没有实时保证 MAC可以保证数据的权威性 (由你的站点生成 , 而不是任何其他人) 和完整性 (包含全部的数据并且是正确的) , 但是它不能保证是最新的 ; 这意味着对于某些会话数据的使用 , 基于Cookie可能让你受到 replay attacks 其它方式的会话后端在服务器端保存每个会话并在用户注销时使它无效 , 而基于Cookie的会话不会在用户注销时失效 , 因此 , 如果攻击者窃取了用户的Cookie , 那么即使用户注销了 , 他们也可以使用该Cookie进行登录 ; Cookies只能被当做是\"过期的\" , 如果它们比你的SESSION_COOKIE_AGE 还要旧 性能 最后 , Cookie的大小对你网站的速度有影响 视图中使用Session 🍀 当SessionMiddleware激活时 , 每个HttpRequest对象 , 也就是传递给Django视图函数的第一个参数 , 将会具有一个session属性 , 它是一个类似字典对象 你可以在你的视图中任何地方读取并写入request.session , 并且可以多次编辑它 class backends.base.SessionBase \"\"\"这是所有会话对象的基类,它具有以下标准字典方法\"\"\" __getitem__(key) '''Example: fav_color = request.session['fav_color']''' __setitem__(key, value) '''Example: request.session['fav_color'] = 'blue'''' __delitem__(key) '''Example: del request.session['fav_color']. This raises KeyError if the given key isn’t already in the session.''' __contains__(key) '''Example: 'fav_color' in request.session''' get(key, default=None) '''Example: fav_color = request.session.get('fav_color', 'red')''' pop(key, default=__not_given) '''Example: fav_color = request.session.pop('fav_color', 'blue')''' keys() items() setdefault() clear() '''It also has these methods:''' flush() ''' Deletes the current session data from the session and deletes the session cookie. This is used if you want to ensure that the previous session data can’t be accessed again from the user’s browser (for example, the django.contrib.auth.logout() function calls it). ''' set_test_cookie() ''' Sets a test cookie to determine whether the user’s browser supports cookies. Due to the way cookies work, you won’t be able to test this until the user’s next page request. See Setting test cookies below for more information. ''' test_cookie_worked() ''' Returns either True or False, depending on whether the user’s browser accepted the test cookie. Due to the way cookies work, you’ll have to call set_test_cookie() on a previous, separate page request. See Setting test cookies below for more information. ''' delete_test_cookie() ''' Deletes the test cookie. Use this to clean up after yourself. ''' set_expiry(value) ''' Sets the expiration time for the session. You can pass a number of different values: If value is an integer, the session will expire after that many seconds of inactivity. For example, calling request.session.set_expiry(300) would make the session expire in 5 minutes. If value is a datetime or timedelta object, the session will expire at that specific date/time. Note that datetime and timedelta values are only serializable if you are using the PickleSerializer. If value is 0, the user’s session cookie will expire when the user’s Web browser is closed. If value is None, the session reverts to using the global session expiry policy. Reading a session is not considered activity for expiration purposes. Session expiration is computed from the last time the session was modified. ''' get_expiry_age() ''' Returns the number of seconds until this session expires. For sessions with no custom expiration (or those set to expire at browser close), this will equal SESSION_COOKIE_AGE. This function accepts two optional keyword arguments: modification: last modification of the session, as a datetime object. Defaults to the current time. expiry: expiry information for the session, as a datetime object, an int (in seconds), or None. Defaults to the value stored in the session by set_expiry(), if there is one, or None. ''' get_expiry_date() ''' Returns the date this session will expire. For sessions with no custom expiration (or those set to expire at browser close), this will equal the date SESSION_COOKIE_AGE seconds from now. This function accepts the same keyword arguments as get_expiry_age(). ''' get_expire_at_browser_close() ''' Returns either True or False, depending on whether the user’s session cookie will expire when the user’s Web browser is closed. ''' clear_expired() ''' Removes expired sessions from the session store. This class method is called by clearsessions. ''' cycle_key() ''' Creates a new session key while retaining the current session data. django.contrib.auth.login() calls this method to mitigate against session fixation. ''' 序列化 🍀 默认情况下 , Django使用JSON序列化会话数据 , 你额可以使用SESSION_SERIALIZER设置顶顶亿会话序列化格式 , 即使是使用我们写自己的序列化器 , 同样强烈建议使用JSON , 特别是在使用Cookie后端时 ; 自定义序列化器 : Write your own serializer 例如 , 如果使用pickle序列化会话数据 , 则会出现攻击情况 ; 如果你使用的是签署Cookie会话后端 , 并且SECRET_KEY被攻击者知道 (Django本身没有漏洞会导致它被泄漏) , 攻击者就可以在会话中插入一个字符串 , 在unpickle之后可以在服务器上执行任何代码 在因特网上这个攻击技术很简单并且很容易差到 , 尽管Cookie会话的存储对Cookie保存的数据进行了签名以防止篡改 , SECRET_KEY的泄漏会立即使得可以执行远程代码 捆绑序列化器 🍀 class serializers.JSONSerializer: \"\"\" 对django.core.signing中的JSON序列化方法的一个包装, 只能序列化基本数据类型 \"\"\" 另外 , 因为JSON只支持字符串作为键 , 注意使用非字符串作为request.session的键将不工作 >>> # initial assignment >>> request.session[0] = 'bar' >>> # subsequent requests following serialization & deserialization >>> # of session data >>> request.session[0] # KeyError >>> request.session['0'] 'bar' 类似地 , 无法在JSON中编码的数据 , 如非UTF-8字节 , 如\\xd9将不能被存储 class serializers.PickleSerializer: \"\"\"支持任意Python对象,但是可能导致远程执行代码的漏洞,如果攻击者知道了SECRET_KEY\"\"\" 自定义序列化器 🍀 对于PickleSerializer与JSONSerializer两者的差别 , 这是常见的情况 , 需要我们在便利性和安全性之间权衡 但是如果我们希望在JSON格式的会话中存储更高级的数据类型比如request.session和datatime , 我们需要自己编写一个序列化器 (或者在保存它们到Decimal中之前转换这些值使其成为一个可JSON序列化的对象) ; 虽然序列化这些值是相当简单的 (比如我们可以使用DjangoJSONEncoder) , 但是编写一个能够可靠地返回您所输入的相同内容的解码器是更加脆弱的 , 例如 , 返回一个datatime时 , 它可能实际上是与datatime格式碰巧相同的一个字符串 我们自定义序列化器时 , 必须实现两个方法 , dumps(self, obj)和loads(self,data) 来分别序列化和反序列化会话数据的字典 会话对象 🍀 在request.session上使用普通的Python字符串作为字典的键 , 这主要是为了方便而不是一条必须遵守的规则 以一个下划线开始的会话字典的键被Django保留作为内部使用 不要用新的对象覆盖request.session , 且不要访问或设置它的属性 , 要像Python中的字典一样使用它 设置Cookie测试 🍀 为了方便 , Django提供了一个简单的方法来测试用户的浏览器是否支持Cookie ; 只需要在一个视图中调用request.session中的set_cookie_worked() , 并在下一个视图中调用test_cookie_worked() , 注意不是在同一个视图中调用 由于Cookie的工作方式 , 在set_test_cookie()和test_cookie_worked()之间这种笨拙的分离是必要的 , 因为我们设置一个Cookie , 在浏览器的下一个请求之前 , 你不可能知道浏览器是否接受它 验证Cookie测试之后 , 我们可以使用delete_test_cookie()来进行清除操作 实例 from django.http import HttpResponse from django.shortcuts import render def login(request): if request.method == 'POST': if request.session.test_cookie_worked(): request.session.delete_test_cookie() return HttpResponse(\"You're logged in.\") else: return HttpResponse(\"Please enable cookies and try again.\") request.session.set_test_cookie() return render(request, 'foo/login_form.html') 视图外使用Sessions 🍀 这一节中的示例直接从django.contrib.session.backends.db导入SessionStore , 在我们的代码中应该从SESSION_ENGINE中导入SessionStore , 如下 : >>> from importlib import import_module >>> from django.conf import settings >>> SessionStore = import_module(settings.SESSION_ENGINE).SessionStore 可以使用一个API来操作视图之外的会话数据 : >>> from django.contrib.sessions.backends.db import SessionStore >>> s = SessionStore() >>> # stored as seconds since epoch since datetimes are not serializable in JSON. >>> s['last_login'] = 1376587691 >>> s.create() >>> s.session_key '2b1189a188b44ad18c35e113ac6ceead' >>> s = SessionStore(session_key='2b1189a188b44ad18c35e113ac6ceead') >>> s['last_login'] 1376587691 上述中SessionStore.create()旨在创建一个新的会话 (即一个没有从会话存储中加载 , 并且使用session_key=None) save()旨在保存现有会话 (即从存储中加载的会话) , 在新会话中调用save()也可以正常工作 , 但可能生成与现有的会话相冲突的session_key ; create()会调用save()和循环 , 知道生成一个未使用的session_key 如果你使用的是django.contrib.sessions.backends.db后端 , 每个会话都只是一个普通的Django模型 , Session模型定义在django/contrib/sessions/models.py中 ; 因为它是一个普通的模型 , 你可以使用普通的Django数据库API访问会话 : >>> from django.contrib.sessions.models import Session >>> s = Session.objects.get(pk='2b1189a188b44ad18c35e113ac6ceead') >>> s.expire_date datetime.datetime(2005, 8, 20, 13, 35, 12) 注意 , 我们需要调用get_decoded()以获得会话的字典 , 这是必须的 , 因为字典是以编码后的格式保存的 : >>> s.session_data 'KGRwMQpTJ19hdXRoX3VzZXJfaWQnCnAyCkkxCnMuMTExY2ZjODI2Yj...' >>> s.get_decoded() {'user_id': 42} 会话保存 🍀 默认情况下 , Django只有在会话被修改时才会保存会话到数据库中 , 即它的字典中的任何值被修改时 # Session is modified. request.session['foo'] = 'bar' # Session is modified. del request.session['foo'] # Session is modified. request.session['foo'] = {} # Gotcha: Session is NOT modified, because this alters # request.session['foo'] instead of request.session. request.session['foo']['bar'] = 'baz' 上面例子的最后一种情况 , 我们可以通过设置会话对象的modified属性显示地告诉会话对象已经被修改过 : request.session.modified = True 若修改这个默认的行为 , 可以设置SESSION_SAVE_EVERY_REQUEST为True , 当设置为True时 , Django将对每个请求都保存会话到数据库中 注意会话的Cookie只有在一个会话被创建或修改后才会发送 , 如果SESSION_SAVE_EVERY_REQUEST为True , 会话的Cookie将在每个请求中发送 类似地 , 每次会话Cookie发送时 , 会话Cookie的过期部分都会被更新 如果响应的状态是500 , 则会话不会保存 会话时长 🍀 你可以通过SESSION_EXPIRE_AT_BROWSER_CLOSE配置来控制会话框架使用浏览器时长的会话 , 还是持久的会话 默认情况下 , SESSION_EXPIRE_AT_BROWSER_CLOSE设置为False , 表示会话的Cookie保存在用户的浏览器中 , 时间为SESSION_COOKIE_AGE ; 如果我们不想让别人每次打开浏览器都需要登录时 , 可以这样做 如果SESSION_EXPIRE_AT_BROWSER_CLOSE设置为True , Django将使用浏览器时长的Cookie , 即如果用户关闭浏览器 , 那么Cookie就会立即过期 , 如果你想让别人每次打开浏览器时都要登录 , 那么就使用这个 这个设置时一个全局的默认值 , 我们可以通过调用request.session()的set_expiry()方法来进行覆盖 注意 : 某些浏览器 (如Chrome) 提供一种设置 , 允许用户在关闭并重新打开浏览器后继续使用会话 , 在某些情况下 , 这可能干扰SESSION_EXPIRE_AT_BROWSER_CLOSE设置并导致会话在浏览器关闭后不过期 , 在测试启用SESSION_EXPIRE_AT_BROWSER_CLOSE设置的Django应用时需要特别注意这一点 清除会话存储 🍀 随着用户在你的网站上创建新的会话 , 会话数据可能会在你的会话存储仓库中积累 , 如果你正在使用数据库作为后端 , django_session数据库表将持续增长 ; 如果你正在使用文件作为后端 , 你的临时目录包含的文件数量将持续增长 Django不提供自动清除过期会话的功能 , 因此 , 定期地清除会话时我们自己的任务 , Django提供一个清除用的管理命令来满足这个目的 : clearsessions , 建议定期调用这个命令 , 例如作为一个日常运行的cron任务 但是 , 以缓存为后端不存在这个问题 , 因为缓存会自动删除过期的数据 ; 以Cookie为后端也不存在这个问题 , 因为会话数据通过用户的浏览器保存 对于会话的行为有很多控制配置 , 详细见 : Django settings 更多Sessions相关 : How to use sessions "},"05-Web框架/Django/16-Django - Authentication System.html":{"url":"05-Web框架/Django/16-Django - Authentication System.html","title":"Django - Authentication System","keywords":"","body":"Django - Authentication System 介绍 🍀 Django为我们提供了一个认证系统 , 它提供了认证 (authentiaction) 和授权功能 (authorization) , 这两种功能在某些地方时耦合的 User对象 🍀 User对象是认证系统的核心 , 它们通常表示与你的站点进行交互的用户 , 并用于启用限制访问 , 注册用户信息和给创建者关联内容等 在Django的认证框架中只存在一种类型的用户 , 因此诸如superusers或管理员staff 用户只是具有特殊属性集的User对象 , 而不是不同类型的User对象 默认User的基本属性有 : uesrname password email first_name last_name 完整参考见 : full API documentation 创建 users 🍀 创建users最直接的方法时使用create_user()函数 , 如下 : >>> from django.contrib.auth.models import User >>> user = User.objects.create_user('john', 'lennon@thebeatles.com', 'johnpassword') # At this point, user is a User object that has already been saved # to the database. You can continue to change its attributes # if you want to change other fields. >>> user.last_name = 'Lennon' >>> user.save() 如果我们安装了admin , 我们可以交互式地创建users , 见 : create users interactively 创建 superusers 🍀 我们可以使用如下命令创建一个超级用户 : $ python manage.py createsuperuser --username=joe --email=joe@example.com 修改密码 🍀 Django不会在user模型上存储原始的 (明文) 密码 , 而只是一个哈希值 (完整见 : documentation of how passwords are managed ) , 因此 , 不要试图直接操作用户的密码属性 , 这就是为什么创建用户时使用帮助函数的原因 所以修改密码我们可以通过以下方式 : manage.py changepassword *username* 提供了一种从命令行更改用户密码的方法 , 它提示你修改一个给定的user密码 , 你必须输入两次 , 如果两次输入匹配 , 密码就会立即被修改 , 如果你没有提供user , 命令行将尝试修改与当前系统用户匹配的用户名的密码 通过set_password() >>> from django.contrib.auth.models import User >>> u = User.objects.get(username='john') >>> u.set_password('new password') >>> u.save() 如果你安装了Django admin , 可以在身份验证系统的管理页面上更改用户的密码 Django还提供视图和表单允许用户修改他们自己的密码 注意 : 更改用户密码将会注销所有会话 , 详见 : Session invalidation on password change 用户认证 🍀 使用authenticate()来验证一组凭证 , 它接收关键字参数credentials , 默认为username 和password authenticate(request=None, **credentials): \"\"\" request:HttpRequest对象 credentials:默认username和password \"\"\" 根据每个认证的后端进行验证 , 如果某个后端凭证通过则返回一个User对象 , 如果凭证对任何后端都无效 , 则主动触发PermissionDenied , 并返回None , 如下 : from django.contrib.auth import authenticate user = authenticate(username='john', password='secret') if user is not None: # A backend authenticated the credentials else: # No backend authenticated the credentials 权限和授权 🍀 Django本身提供了一个简单的权限系统 , 它提供了一种为特定用户和用户组分配权限的方法 Django中的admin站点也使用了该权限系统 , 使用的权限如下 : 查看\"add\"表单和添加对象仅限于具有\"add\"权限的用户类型对象 查看\"change\"表单和更改对象仅限于具有\"change\"权限的用户类型对象 删除一个对象仅限于具有“delete”权限的用户类型对象 权限不但可以根据每个对象的类型 , 而且可以根据特定的对象实例设置 , 通过ModelAdmin 提供的has_add_permission() , has_change_permission()和has_delete_permission()方法 , 可以针对相同类型的不同对象实例自定义权限 User对象具有两个多对多字段 : groups和user_permissions User对象可以使用和其他Django模型一样的方式访问他们相关联的对象 , 如下 : myuser.groups.set([group_list]) myuser.groups.add(group, group, ...) myuser.groups.remove(group, group, ...) myuser.groups.clear() myuser.user_permissions.set([permission_list]) myuser.user_permissions.add(permission, permission, ...) myuser.user_permissions.remove(permission, permission, ...) myuser.user_permissions.clear() 默认权限 🍀 当django.contrib.auth在你的INSTALLED_APPS配置中列出时 , 它将确保为你安装的应用中的每个Django模型创建3个默认的权限 , 即add , change和delete 当你运行manage.py migrate 时, 将创建这些权限 ; 在django.contrib.auth添加INSTALLED_APPS之后 , 首次运行migrate时 , 将为所有先前安装的模型创建默认权限 , 以及当时安装的任何新模型 ; 之后 , 每次运行manage.py migrate , 它将为新的模型创建默认权限(创建权限的函数与post_migrate信号连接) Groups 🍀 django.contrib.auth.models.Group模型是一种对用户进行分类的通用方式 , 通过这种方式你可以引用权限或其他标签都这些用户 ; 一个用户可以属于任意多个组 组中每个用户自动具有该组的权限 , 例如 , 如果 Site editors组具有can_edit_home_page权限 , 那么该组中的任何用户都具有该权限 除了权限之外 , 组还是给分类用户分配标签，添加功能的便捷方法 ; 例如 , 你可以创建一个组Special users , 只有在该组中的用户才能够访问会员的页面 编程方式创建权限 🍀 虽然我们可以在模型的Meta类中自定义权限 , 但是你也可以直接创建权限 , 例如 , 你可以在myapp中为BlogPost模型创建can_publish权限 : from myapp.models import BlogPost from django.contrib.auth.models import Permission from django.contrib.contenttypes.models import ContentType content_type = ContentType.objects.get_for_model(BlogPost) permission = Permission.objects.create( codename='can_publish', name='Can Publish Posts', content_type=content_type, ) 然后该权限可以通过user_permissions属性或者通过Group的permissions属性分配给用户 权限缓存 🍀 ModelBackend在第一次需要访问User对象的权限时会对权限进行缓存 , 由于对新添加的权限并不会立即检查 , 所以这种做法对request-response循环是非常有利的 (例如在admin中) , 如果你想要在添加新的权限后马上在测试或视图检查他们 , 最简单的解决办法是从数据库中重新获取User , 如下 : from django.contrib.auth.models import Permission, User from django.contrib.contenttypes.models import ContentType from django.shortcuts import get_object_or_404 from myapp.models import BlogPost def user_gains_perms(request, user_id): user = get_object_or_404(User, pk=user_id) # any permission check will cache the current set of permissions user.has_perm('myapp.change_blogpost') content_type = ContentType.objects.get_for_model(BlogPost) permission = Permission.objects.get( codename='change_blogpost', content_type=content_type, ) user.user_permissions.add(permission) # Checking the cached permission set user.has_perm('myapp.change_blogpost') # False # Request new instance of User # Be aware that user.refresh_from_db() won't clear the cache. user = get_object_or_404(User, pk=user_id) # Permission cache is repopulated from the database user.has_perm('myapp.change_blogpost') # True ... Web请求中的认证 🍀 Django使用Sessions和Middleware来拦截request objects到认证系统中 认证系统为每个请求提供一个request.user属性来代表当前的用户 , 如果当前的用户仍未登录 , 该属性将会被设置为一个AonnymousUser实例 , 否则该属性将会是一个User实例 我们可以使用is_authenticated属性来进行区分 : if request.user.is_authenticated: # Do something for authenticated users. ... else: # Do something for anonymous users. ... 登录用户 🍀 如果你有一个经过身份验证的用户 , 你想把它附带到当前的会话中 , 可以通过login()函数完成 login(request, user, backend=None): \"\"\" request:HttpRequest对象 user:User对象 backend:后端 \"\"\" login()使用Django的Session框架来将用户的ID保存在session中 注意 , 匿名会话期间的任何数据集在用户登录后都会保留在会话中 from django.contrib.auth import authenticate, login def my_view(request): username = request.POST['username'] password = request.POST['password'] user = authenticate(request, username=username, password=password) if user is not None: login(request, user) # Redirect to a success page. ... else: # Return an 'invalid login' error message. ... 选择验证后端 🍀 用户登录时 , 用户的ID和用于身份验证的后端保存在用户的会话中 , 这允许相同的身份验证后端在将来的请求中获取用户的详细信息 . 保存会话中的认证后端选择如下 : 使用可选的backend参数的值 (如果提供) 使用user.backend属性的值 (如果存在) 如果只有一个 , 则使用AUTHENTICATION_BACKENDS中的后端 否则 , 触发异常 登出用户 🍀 要登出一个已经通过django.contrib.auth.login()登入的用户 , 可以在视图中使用django.contrib.auth.logout() logout(request): \"\"\" request:HttpRequest对象 没有返回值 \"\"\" 实例 from django.contrib.auth import logout def logout_view(request): logout(request) # Redirect to a success page. 调用logou()时 , 当前请求的会话数据将被彻底清楚 , 这是为了防止另外一个人使用相同的Web浏览器登入并访问前一个用户的会话数据 , 如果你想在用户登出之后可以立即访问放入会话中的数据 , 则需要在调用django.conruib.auth.logout()之后放入 限制访问页面 🍀 原始方式 🍀 限制访问页面的简单原始方法时检查request.user.is_authenticated , 并重定向到登录页面 : from django.conf import settings from django.shortcuts import redirect def my_view(request): if not request.user.is_authenticated: return redirect('%s?next=%s' % (settings.LOGIN_URL, request.path)) # ... 或者显示错误信息 from django.shortcuts import render def my_view(request): if not request.user.is_authenticated: return render(request, 'myapp/login_error.html') # ... login_required 🍀 一个比较快捷的方式 , 可以使用login_required()装饰器 login_required(redirect_field_name='next', login_url=None): \"\"\" Decorator for views that checks that the user is logged in, redirecting to the log-in page if necessary. redirect_filed_name:重定向路径,设置为None可以从URL中移除 login_url:指定没有通过检查的用户的重定向向登录页面,默认为settings.LOGIN_URL \"\"\" 实例 from django.contrib.auth.decorators import login_required @login_required def my_view(request): ... login_required()完成下面的事情 : 如果用户没有登录 , 则重定向到settings.LOGIN_URL , 并且把当前请求的绝对路径作为查询参数传递到登陆页面 , 例如 : /accounts/login/?next=/polls/3/ 如果用户已经登入 , 则正常执行视图 , 视图的代码可以安全地假设用户已经登入 修改密码实例 @login_required def set_password(request): user = request.user state = None if request.method == 'POST': old_password = request.POST.get('old_password', '') new_password = request.POST.get('new_password', '') repeat_password = request.POST.get('repeat_password', '') if user.check_password(old_password): if not new_password: state = 'empty' elif new_password != repeat_password: state = 'repeat_error' else: user.set_password(new_password) user.save() return redirect(\"/log_in/\") else: state = 'password_error' content = { 'user': user, 'state': state, } return render(request, 'set_password.html', content) LoginRequiredMixin 🍀 当你使用基于类的视图时 , 可以使用LoginRequireMixin实现与login_required相同的行为 , 这个mixin应该位于继承列表中最左侧的位置 如果一个视图使用这个mixin，那么所有未经身份验证的用户的请求将被重定向到登录页面，或者显示HTTP 403 Forbidden错误，这取决于 raise_exception参数 您可以设置AccessMixin的任何参数来定制未授权用户的处理 : from django.contrib.auth.mixins import LoginRequiredMixin class MyView(LoginRequiredMixin, View): login_url = '/login/' redirect_field_name = 'redirect_to' user_passes_test 🍀 为了快捷 , 你可以使用user_passes_test装饰器 , 返回False时执行重定向 user_passes_test(test_func,login_url=None,redirect_field_name='next): \"\"\" Decorator for views that checks that the user passes the given test, redirecting to the log-in page if necessary. The test should be a callable that takes the user object and returns True if the user passes. test_func:一个以User对象为参数的回调函数 login_url:指定没有通过检查的用户的重定向向登录页面,默认为settings.LOGIN_URL redirect_field_name:重定向路径,设置为None可以从URL中移除 \"\"\" 实例 from django.contrib.auth.decorators import user_passes_test def email_check(user): return user.email.endswith('@example.com') @user_passes_test(email_check) def my_view(request): ... UserPassesTestMixin 🍀 当使用基于类的视图时 , 可以使用UserPassesTestMixin , 与user_passes_test类似 test_func() 你必须在你的类中覆盖test_func()方法来提供执行的测试 , 此外 , 你可以设置 AccessMixin 的任何参数来定制未授权用户的处理: from django.contrib.auth.mixins import UserPassesTestMixin class MyView(UserPassesTestMixin, View): def test_func(self): return self.request.user.email.endswith('@example.com') get_test_func() 你也可以覆盖get_test_func()方法以使mixin使用不同命名的函数来进行检查 (而不是test_func()) 由于UserPassesTestMixin的实现方式 , 你不能将它们放在继承列表中 , 以下内容不起作用 : class TestMixin1(UserPassesTestMixin): def test_func(self): return self.request.user.email.endswith('@example.com') class TestMixin2(UserPassesTestMixin): def test_func(self): return self.request.user.username.startswith('django') class MyView(TestMixin1, TestMixin2, View): ... permission_required 🍀 permission_required(perm, login_url=None, raise_exception=False): \"\"\" 检查一个用户是否有指定的权限 perm:权限名称,形式app.permission login_url:指定没有通过检查的用户的重定向登录页面,默认为settings.LOGIN_URL raise_exception:如果提供了该参数,装饰器会抛出PermissionDenied异常,从而导致403(HTTP Forbidden)视图替代重定向的登录页面 \"\"\" 实例 from django.contrib.auth.decorators import permission_required @permission_required('polls.can_vote') def my_view(request): ... 如class models.User中的has_perm()方法一样 , 权限名称采用\".\"的形式 , 例如 , polls.can_vote表示polls应用中一个模型的权限 装饰器也可以采取可迭代的权限 , 这种情况下 , 用户必须具有所有权限才能访问视图 PermissionRequiredMixin 🍀 对基于类的视图应用权限进行检查 , 可以使用PermissionRequiredMixin 这个mixin , 就相当于permission_required装饰器 , 如下 : from django.contrib.auth.mixins import PermissionRequiredMixin class MyView(PermissionRequiredMixin, View): permission_required = 'polls.can_vote' # Or multiple of permissions: permission_required = ('polls.can_open', 'polls.can_edit') 你可以设置 AccessMixin 的任何参数来定制未授权用户的处理 你还可以覆盖以下方法 : get_permission_required(): ''' 返回由mixin使用的许可名称的可迭代, 默认为permission_required属性,如有必要,转换为元组 ''' has_permission(): ''' 返回一个布尔值,表示当前用户是否具有执行装饰视图的权限 默认情况下,返回使用get_permission_required()返回的权限列表调用has_perms()的结果 ''' 在admin中管理用户 🍀 如果django.contrib.auth和django.contrib.admin这两个你都安装了 , 将可以通过admin方便地查看和管理用户 , 组和权限 ; 可以像其他任何Django模型一样创建和删除用户 , 可以创建组 , 并分配权限给用户和组 , admin中还会保存和显示对用户模型编辑的日志 admin更多相关见下一篇整理 认证视图 : Authentication Views 内置表单 : Built-in forms 更多相关内容 : https://docs.djangoproject.com/en/1.11/topics/auth/default/ 认证系统 : User authentication in Django "},"05-Web框架/Django/50-Django - 源码之startproject.html":{"url":"05-Web框架/Django/50-Django - 源码之startproject.html","title":"Django - 源码之startproject","keywords":"","body":"Django - 源码之startproject 介绍 🍀 django是Python中的一个Web框架 , 它的本质其实就是一个别人已经为我们写好了的 , Python第三方库 而我们使用它也是通过Python中的导入语句 , 将其导入后使用 我们看看django的文件系统 >>> import django >>> help(django) Help on package django: NAME django PACKAGE CONTENTS __main__ apps (package) conf (package) contrib (package) core (package) db (package) dispatch (package) forms (package) http (package) middleware (package) shortcuts template (package) templatetags (package) test (package) urls (package) utils (package) views (package) 文档树如下 django ├── apps ├── bin ├── conf ├── contrib ├── core ├── db ├── dispatch ├── forms ├── http ├── middleware ├── templatetages ├── test ├── urls ├── utils ├── views ├── __init__.py ├── __main__.py └── shortcuts.py 分析时 , 源码省略部分以pass带过 开始 🍀 在我们使用命令行安装django时 , 通常都会自动为我们添加一个环境变量 , 也就是django/bin/django-admin.py这个文件 , 我们可是在命令行输入django-admin命令来测试是否已经添加了环境变量 $ django-admin Type 'django-admin help ' for help on a specific subcommand. Available subcommands: [django] check compilemessages createcachetable dbshell diffsettings dumpdata flush inspectdb loaddata makemessages makemigrations migrate runserver sendtestemail shell showmigrations sqlflush sqlmigrate sqlsequencereset squashmigrations startapp startproject test testserver Note that only Django core commands are listed as settings are not properly configured (error: Requested setting INSTALLED_APPS, but settings are not configured. You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings.configure() before accessing settings.). 如果命令行输出以上信息 , 那么就说明环境变量已经添加 , 如果没有 , 那么你就得自己添加了 , 如何添加环境变量可以访问如下链接操作 : www.baidu.com 现在我们已经知道了django的入口 , 就是django-admin.py , 根据命令行的提示我们就可以开始创建我们的Django项目了 # 用法如下 $ django-admin startproject [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS] [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--template TEMPLATE] [--extension EXTENSIONS] [--name FILES] name [directory] 实际上我们使用PyCharm时 , 创建django项目 , 其内部也是帮我们调用了这条命令 , 接下来我们应该看看django-admin.py中包含了什么信息了 django/bin/django-admin.py #!/usr/bin/env python from django.core import management if __name__ == \"__main__\": management.execute_from_command_line() 在这个入口中 , 我们看到了django真正的入口 , 也就是在这个management里面 , 接下来我们看看我们在命令行输出的命令django是如何解析的 startproject 🍀 以django-admin startprojec为例 , 首先我们切换到要存放项目的目录 , 然后在命令行输入一下命令 $ django-admin startproject lyonyangproject 我们来看看这条命令到底是怎么执行的 毋庸置疑 , 创建我们的Django项目 , 将进行一些列复杂的加载工作 , 也就是整个项目所需要的配置等等的导入工作 , 我们将一层层的来观察这些动作 首先django-admin.py被执行 , 随后进入了django/core/management/__init__.py , 我们简单提取关键步骤 , 如下图 : 有了这个图 , 那么文字说明就好说了 , 在management.execute_from_command_line() 执行之前 , 我们看看django做了些什么 导入apps , 执行了其中的apps = Apps(installed_apps=None) , 这个apps实例暂时并没有真实的数据 , 它管理着一个存储安装应用的注册表 , 以及维护着一个与models的通道 导入settings , 执行settings = LazySettings() , 这一步至关重要 , 它所做的事情 , 都隐藏在其基类LazyObject中 , 因为LazySettings()是没有构造函数的 , 所以只能向它的父亲拿了 empty = object() class LazyObject(object): # 这个类的作用就是为了延迟实例化 # Avoid infinite recursion when tracing __init__ (#19456). _wrapped = None def __init__(self): # Note: if a subclass overrides __init__(), it will likely need to # override __copy__() and __deepcopy__() as well. # 这里并未进行真正意义上的初始化,因为empty是一个空对象 self._wrapped = empty ...... class LazySettings(LazyObject): def _setup(self, name=None): \"\"\" Load the settings module pointed to by the environment variable. This is used the first time we need any settings at all, if the user has not previously configured the settings manually. \"\"\" # ENVIRONMENT_VARIABLE = \"DJANGO_SETTINGS_MODULE\" # 这个环境变量会在execute函数执行时设置成\"[project].settings\" # 我们的项目就是lyonyangproject.settings settings_module = os.environ.get(ENVIRONMENT_VARIABLE) if not settings_module: desc = (\"setting %s\" % name) if name else \"settings\" raise ImproperlyConfigured( \"Requested %s, but settings are not configured. \" \"You must either define the environment variable %s \" \"or call settings.configure() before accessing settings.\" % (desc, ENVIRONMENT_VARIABLE)) # 此时一切就绪,完成真正的实例化 self._wrapped = Settings(settings_module) def __getattr__(self, name): # 这一步会在execute()中通过settings.INSTALLED_APPS激活 \"\"\" Return the value of a setting and cache it in self.__dict__. \"\"\" if self._wrapped is empty: self._setup(name) val = getattr(self._wrapped, name) self.__dict__[name] = val return val 导入工作差不多是完成了 , 但是此时这个settings却没有真正初始化 , 我们继续往下观察 导入工作完成后 , 那么就开始执行了 , 也就是调用execute_from_command_line()函数了 management/__init__.py def execute_from_command_line(argv=None): \"\"\" A simple method that runs a ManagementUtility. \"\"\" # 实例化ManagementUtility utility = ManagementUtility(argv) # 调用其execute方法 utility.execute() class ManagementUtility(object): \"\"\" Encapsulates the logic of the django-admin and manage.py utilities. \"\"\" def __init__(self, argv=None): # 获取命令行参数 self.argv = argv or sys.argv[:] # 执行命令的文件,django-admin.py self.prog_name = os.path.basename(self.argv[0]) self.settings_exception = None pass execute()方法源码如下 : def execute(self): \"\"\" Given the command-line arguments, this figures out which subcommand is being run, creates a parser appropriate to that command, and runs it. \"\"\" try: # 提取子命令名称,如:startproject subcommand = self.argv[1] except IndexError: subcommand = 'help' # Display help if no arguments were given. # Preprocess options to extract --settings and --pythonpath. # These options could affect the commands that are available, so they # must be processed early. # 实例化一个特定的参数解析器 parser = CommandParser(None, usage=\"%(prog)s subcommand [options] [args]\", add_help=False) # 预处理工作,添加参数 parser.add_argument('--settings') parser.add_argument('--pythonpath') parser.add_argument('args', nargs='*') # catch-all try: options, args = parser.parse_known_args(self.argv[2:]) # 设置settings和pythonpath参数 handle_default_options(options) except CommandError: pass # Ignore any option errors at this point. try: # 正常情况下,我们看到这样的语句差不多可以直接掠过了,因为似乎没有意义 # 但是settings却是一个特殊的情况,此时它并没有INSTALLED_APPS # 所以Python是找不到的,于是就会执行它的__getattr__方法来查找了 # 于是,在这里就完成了初始化工作 settings.INSTALLED_APPS except ImproperlyConfigured as exc: self.settings_exception = exc if settings.configured: # Start the auto-reloading dev server even if the code is broken. # The hardcoded condition is a code smell but we can't rely on a # flag on the command class because we haven't located it yet. # 下面跟Django的启动有关,暂且不说 if subcommand == 'runserver' and '--noreload' not in self.argv: try: autoreload.check_errors(django.setup)() except Exception: # The exception will be raised later in the child process # started by the autoreloader. Pretend it didn't happen by # loading an empty list of applications. apps.all_models = defaultdict(OrderedDict) apps.app_configs = OrderedDict() apps.apps_ready = apps.models_ready = apps.ready = True # Remove options not compatible with the built-in runserver # (e.g. options for the contrib.staticfiles' runserver). # Changes here require manually testing as described in # #27522. _parser = self.fetch_command('runserver').create_parser('django', 'runserver') _options, _args = _parser.parse_known_args(self.argv[2:]) for _arg in _args: self.argv.remove(_arg) # In all other cases, django.setup() is required to succeed. else: # 配置settings,logging,urlresolvers,以及注册应用 django.setup() self.autocomplete() if subcommand == 'help': if '--commands' in args: sys.stdout.write(self.main_help_text(commands_only=True) + '\\n') elif len(options.args) 接下来我们看看最后的这个fetch_command(subcommand) 和 run_from_argv(self.argv) fetch_command() 🍀 management/__init__.py def fetch_command(self, subcommand): \"\"\" Tries to fetch the given subcommand, printing a message with the appropriate command called from the command line (usually \"django-admin\" or \"manage.py\") if it can't be found. \"\"\" # Get commands outside of try block to prevent swallowing exceptions commands = get_commands() try: # 获取子命令模块对应的前缀,这里是django.core app_name = commands[subcommand] except KeyError: if os.environ.get('DJANGO_SETTINGS_MODULE'): # If `subcommand` is missing due to misconfigured settings, the # following line will retrigger an ImproperlyConfigured exception # (get_commands() swallows the original one) so the user is # informed about it. # 防止未初始化 settings.INSTALLED_APPS else: sys.stderr.write(\"No Django settings specified.\\n\") sys.stderr.write( \"Unknown command: %r\\nType '%s help' for usage.\\n\" % (subcommand, self.prog_name) ) sys.exit(1) if isinstance(app_name, BaseCommand): # If the command is already loaded, use it directly. klass = app_name else: # load_command_class会返回指定子命令相对应的Command类的实例 # load_command_class见下一小结 klass = load_command_class(app_name, subcommand) return klass 下面详细的解释一下load_command_class() load_command_class( ) 🍀 management/__init__.py def load_command_class(app_name, name): \"\"\" Given a command name and an application name, returns the Command class instance. All errors raised by the import process (ImportError, AttributeError) are allowed to propagate. \"\"\" # app_name:子模块对应的前缀名 # name:子命令 # 加载django.core.management.commands.startproject module = import_module('%s.management.commands.%s' % (app_name, name)) # 返回startproject中的Command类的实例 return module.Command() run_from_argv( ) 🍀 那么到这里 , 准备工作已经全部完成 , 现在就是真正的执行时刻了 , 在开始之前还有点事需要说明 , 这个方法存在的位置并不是子命令对应的Command中 , 而是在其最高基类BaseCommand中 , 所有的Command类都必须直接或者间接的继承BaseCommand类 对于run_from_argv源码就不贴了 , 因为实际上 , 它也不是正主 , run_from_argv主要就是设置环境 (比如Python路径和Django配置) , 随后它会调用BaseCommand类的execute()方法 , 但这不是绝对 , 因为有的派生类中重写了execute() : def execute(self, *args, **options): \"\"\" Try to execute this command, performing system checks if needed (as controlled by the ``requires_system_checks`` attribute, except if force-skipped). \"\"\" # 在需要是进行系统检查 ... # 最后会调用Command类中的handle output = self.handle(*args, **options) ... 上述中的handle()方法 , 必须在子类中实现 , 原因如下 : 在django/core/management/base.py , BaseCommand类中的handle() def handle(self, *args, **options): \"\"\" The actual logic of the command. Subclasses must implement this method. \"\"\" # 执行我?对不起我要给你抛个NotImplementedError raise NotImplementedError('subclasses of BaseCommand must provide a handle() method') 接下来 , 我们看看startproject.py中Command类的handle() def handle(self, **options): project_name, target = options.pop('name'), options.pop('directory') self.validate_name(project_name, \"project\") # Check that the project_name cannot be imported. try: import_module(project_name) except ImportError: pass else: raise CommandError( \"%r conflicts with the name of an existing Python module and \" \"cannot be used as a project name. Please try another name.\" % project_name ) # Create a random SECRET_KEY to put it in the main settings. options['secret_key'] = get_random_secret_key() # 加载其基类TemplateCommand中的handle方法 # PS:TempalteCommand的基类为BaseCommand super(Command, self).handle('project', project_name, target, **options) 最后TemplateCommand类中的handle()会为我们进行Django项目的布局到指定目录 , 至此 , 命令执行完毕 , 我们所看到的所有默认目录也已经创建完毕 小结 🍀 到这里对于django项目的开始已经有了基本的了解了 : 在命令行执行django-admin.py相关命令 执行management中的execute_from_command_line()函数 实例化ManagementUtility类并调用execute()方法 随后就是获取命令行输入的参数实例化相应的Command类 调用类中的handle()方法 注意 : django-admin命令并不仅仅根据django/core/management/commands来加载 , 而是会将所有的Application下的management/commands 都加载进入commands字典中 该commands字典是通过 , management/__init__.py 中的get_commands()函数生成的 @lru_cache.lru_cache(maxsize=None) def get_commands(): \"\"\" Returns a dictionary mapping command names to their callback applications. ... \"\"\" pass 到这里 , 我们可以想 , 既然django会到各个应用中去寻找management/commands目录 , 再寻找subcommand , 那么如果在自己的应用下创建一个mycommand , 然后定义一个Command类 , 重写handle()方法 , 是不是就自定制django-admin命令了 没错 , 这一点在django的官方文档中已经提供相关教程了 , 想要自定制命令就点击下面的教程链接吧 : Writing custom django-admin commands "},"05-Web框架/Django/51-Django - 源码之runserver.html":{"url":"05-Web框架/Django/51-Django - 源码之runserver.html","title":"Django - 源码之runserver","keywords":"","body":"Django - 源码之runserver 介绍 🍀 上一篇中 , 我们分析了Django项目从无到有的过程 , 也就是django-admin startproject命令 , 随后我们要做的就是启动这个项目 , 也就是执行django-admin runserver命令 实际上 , 我们在创建项目时 , 就已经见过这个命令字眼了 , 截取部分代码如下 : management/__init__ def execute(self): \"\"\" Given the command-line arguments, this figures out which subcommand is being run, creates a parser appropriate to that command, and runs it. \"\"\" ...... # 截取部分片段,为了便于查找,删去部分注释 if settings.configured: if subcommand == 'runserver' and '--noreload' not in self.argv: try: autoreload.check_errors(django.setup)() except Exception: apps.all_models = defaultdict(OrderedDict) apps.app_configs = OrderedDict() apps.apps_ready = apps.models_ready = apps.ready = True _parser = self.fetch_command('runserver').create_parser('django', 'runserver') _options, _args = _parser.parse_known_args(self.argv[2:]) for _arg in _args: self.argv.remove(_arg) # In all other cases, django.setup() is required to succeed. else: django.setup() ...... 在我们第一次创建Django项目时 , 会直接执行django.setup() , 在上一篇中 , 关于导入工作中忽略了一条语句 : from django.utils import autoreload, lru_cache, six , 这其中的autoreload是一个自动加载的触发器 , 它会加载一个配置RUN_RELOADER = True , 这也是我们自动重启服务端的一个开关 , 默认它是开着的 实际上执行runserver与startproject是一样的 , 只不过在配置时 , runserver会进行一些错误检查 , 主要是为了检查其兼容性 , 随后像其他命令一样 , 实例化各自模块中的Command类 , 随后调用execute() → handle() , 我们直接从handle()开始 handle 🍀 django/core/management/commands/runserver.py def handle(self, *args, **options): # 导入Django配置文件 from django.conf import settings if not settings.DEBUG and not settings.ALLOWED_HOSTS: raise CommandError('You must set settings.ALLOWED_HOSTS if DEBUG is False.') # 判断ipv6是否可用,该方式需要通过add_arguments来添加 self.use_ipv6 = options['use_ipv6'] if self.use_ipv6 and not socket.has_ipv6: raise CommandError('Your Python does not support IPv6.') self._raw_ipv6 = False if not options['addrport']: # 默认使用本地ip地址,端口为8000 self.addr = '' self.port = self.default_port else: # 获取addrport中的相关参数 m = re.match(naiveip_re, options['addrport']) if m is None: raise CommandError('\"%s\" is not a valid port number ' 'or address:port pair.' % options['addrport']) self.addr, _ipv4, _ipv6, _fqdn, self.port = m.groups() if not self.port.isdigit(): raise CommandError(\"%r is not a valid port number.\" % self.port) if self.addr: if _ipv6: self.addr = self.addr[1:-1] self.use_ipv6 = True self._raw_ipv6 = True elif self.use_ipv6 and not _fqdn: raise CommandError('\"%s\" is not a valid IPv6 address.' % self.addr) if not self.addr: self.addr = '::1' if self.use_ipv6 else '127.0.0.1' self._raw_ipv6 = self.use_ipv6 # 启动服务器 self.run(**options) 再随后 , 在run中会调用inner_run , 具体源码如下 : def inner_run(self, *args, **options): # If an exception was silenced in ManagementUtility.execute in order # to be raised in the child process, raise it now. autoreload.raise_last_exception() threading = options['use_threading'] # 'shutdown_message' is a stealth option. shutdown_message = options.get('shutdown_message', '') quit_command = 'CTRL-BREAK' if sys.platform == 'win32' else 'CONTROL-C' self.stdout.write(\"Performing system checks...\\n\\n\") self.check(display_num_errors=True) # Need to check migrations here, so can't use the # requires_migrations_check attribute. self.check_migrations() now = datetime.now().strftime('%B %d, %Y - %X') if six.PY2: now = now.decode(get_system_encoding()) self.stdout.write(now) self.stdout.write(( \"Django version %(version)s, using settings %(settings)r\\n\" \"Starting development server at %(protocol)s://%(addr)s:%(port)s/\\n\" \"Quit the server with %(quit_command)s.\\n\" ) % { \"version\": self.get_version(), \"settings\": settings.SETTINGS_MODULE, \"protocol\": self.protocol, \"addr\": '[%s]' % self.addr if self._raw_ipv6 else self.addr, \"port\": self.port, \"quit_command\": quit_command, }) try: # 这里最终获取的是WSGIHandler的实例,并且中间件的加载也是在这一步中完成的 handler = self.get_handler(*args, **options) # 我们把实参补全来观察 # run('127.0.0.1', 8000, WSGIHandler(), False, threading, WSGIServer) run(self.addr, int(self.port), handler, ipv6=self.use_ipv6, threading=threading, server_cls=self.server_cls) except socket.error as e:: pass # 省略我们直接看看真正的run吧 def run(addr, port, wsgi_handler, ipv6=False, threading=False, server_cls=WSGIServer): server_address = (addr, port) if threading: # 重新构造WSGIServer,并继承了socketserver.ThreadingMixIn和原WSGIServer httpd_cls = type(str('WSGIServer'), (socketserver.ThreadingMixIn, server_cls), {}) else: httpd_cls = server_cls # 实例化新的WSGIServer httpd = httpd_cls(server_address, WSGIRequestHandler, ipv6=ipv6) if threading: # 加快自动重启以及防止线程不正常终止 httpd.daemon_threads = True # wsgi_handler=WSGIHandler(),set_app结果:self.application = WSGIHandler() httpd.set_app(wsgi_handler) # socketserver.serve_forever,至此Web服务器启动 httpd.serve_forever() 处理请求 🍀 调用serve_forver()之后 , 我们的服务端就已经做好随时 \"接客\" (HTTP请求) 的准备了 def serve_forever(self, poll_interval=0.5): \"\"\"Handle one request at a time until shutdown. Polls for shutdown every poll_interval seconds. Ignores self.timeout. If you need to do periodic tasks, do them in another thread. \"\"\" self.__is_shut_down.clear() try: # XXX: Consider using another file descriptor or connecting to the # socket to wake this up instead of polling. Polling reduces our # responsiveness to a shutdown request and wastes cpu at all other # times. with _ServerSelector() as selector: selector.register(self, selectors.EVENT_READ) while not self.__shutdown_request: ready = selector.select(poll_interval) if ready: # 处理请求 self._handle_request_noblock() self.service_actions() finally: self.__shutdown_request = False self.__is_shut_down.set() 在分析如何处理请求之前 , 我们需要理清由type构造的这个新的WSGIServer 到底继承了基类的哪些东西 , 因为这关系到在请求处理过程 , 到底会执行该实例的哪一个方法 由于这个构造出来的新类其继承体系有点庞大 , 所以这里将直接观察其真正的执行方法 self._handle_reqeust_noblock() 具体内容如下 : def _handle_request_noblock(self): \"\"\"Handle one request, without blocking. I assume that selector.select() has returned that the socket is readable before this function was called, so there should be no risk of blocking in get_request(). \"\"\" try: # 获取请求和客户端地址 request, client_address = self.get_request() except OSError: return if self.verify_request(request, client_address): try: # 调用finish_request self.process_request(request, client_address) except: self.handle_error(request, client_address) self.shutdown_request(request) else: self.shutdown_request(request) self.process_request(request, client_address) 具体内容如下 : def process_request(self, request, client_address): self.finish_request(request, client_address) self.shutdown_request(request) self.finish_request(request, client_address) 具体内容如下 : def finish_request(self, request, client_address): \"\"\"Finish one request by instantiating RequestHandlerClass.\"\"\" # self.RequestHandlerClass = WSGIRequestHandler self.RequestHandlerClass(request, client_address, self) 就像源码注释中所说 , 它将通过实例化一个RequestHandleClass类 , 来处理一个请求 , 这个类就是WSGIRequestHandler , 这个类的构造函数在其最高基类BaseRequestHandler 中 , 如下 : def __init__(self, request, client_address, server): self.request = request self.client_address = client_address self.server = server self.setup() try: self.handle() finally: self.finish() 也就是说 , 在实例化时会调用类中的handler方法 , 这个方法在WSGIRequestHandler中被重写了 , 如下 : def handle(self): \"\"\"Copy of WSGIRequestHandler, but with different ServerHandler\"\"\" self.raw_requestline = self.rfile.readline(65537) if len(self.raw_requestline) > 65536: self.requestline = '' self.request_version = '' self.command = '' self.send_error(414) return if not self.parse_request(): # An error code has been sent, just exit return # 实例化并设置HTTP环境变量 handler = ServerHandler( self.rfile, self.wfile, self.get_stderr(), self.get_environ() ) handler.request_handler = self # backpointer for logging # self.server = httpd = httpd_cls(server_address, WSGIRequestHandler, ipv6=ipv6) # self.server.get_app() = WSGIHandler() handler.run(self.server.get_app()) 最后执行run , 该方法来自于ServerHandler的最高基类BaseHandler , 源码如下 : def run(self, application): \"\"\"Invoke the application\"\"\" # Note to self: don't move the close()! Asynchronous servers shouldn't # call close() from finish_response(), so if you close() anywhere but # the double-error branch here, you'll break asynchronous servers by # prematurely closing. Async servers must return from 'run()' without # closing if there might still be output to iterate over. try: # 设置WSGi环境变量 self.setup_environ() # self.result = WSGIHandler()(self.environ, self.start_response) # 调用__call__方法,返回结果 self.result = application(self.environ, self.start_response) self.finish_response() except: try: self.handle_error() except: # If we get an error handling an error, just give up already! self.close() raise # ...and let the actual server figure it out. WSGIHandler().__call__(self.environ, self.start_response) 如下 : def __call__(self, environ, start_response): set_script_prefix(get_script_name(environ)) signals.request_started.send(sender=self.__class__, environ=environ) # 生成请求对象 request = self.request_class(environ) # 根据请求获取响应对象 response = self.get_response(request) response._handler_class = self.__class__ # 状态码 status = '%d %s' % (response.status_code, response.reason_phrase) # 响应头 response_headers = [(str(k), str(v)) for k, v in response.items()] for c in response.cookies.values(): response_headers.append((str('Set-Cookie'), str(c.output(header='')))) start_response(force_str(status), response_headers) if getattr(response, 'file_to_stream', None) is not None and environ.get('wsgi.file_wrapper'): response = environ['wsgi.file_wrapper'](response.file_to_stream) return response 最后finish_response() , 返回响应 , 关闭套接字 ; 当然 , 服务器还是继续等待 \"客人\" 来光临 ! def finish_response(self): \"\"\"Send any iterable data, then close self and the iterable Subclasses intended for use in asynchronous servers will want to redefine this method, such that it sets up callbacks in the event loop to iterate over the data, and to call 'self.close()' once the response is finished. \"\"\" try: if not self.result_is_file() or not self.sendfile(): for data in self.result: self.write(data) self.finish_content() finally: self.close() 小结 🍀 分析过程中 , 为了避免派生类重写了基类中的方法而导致分析出错 , 不妨将所有方法整合到一个类中 , 虽然这个工作也不好做 , 但是却是不会出错 我们通过一条 \"执行线\" 来完成本次小结 : django-admin runserver → Command() → handle() → run() → → inner_run() → self.get_handler(*args, **options) → → basehttp.run() → httpd → httpd.serve_forever() → → self.RequestHandlerClass(request, client_address, self) → self.handle() → → ServerHandler(self.rfile, self.wfile, self.get_stderr(), self.get_environ()) → → handler.run(self.server.get_app()) → → WSGIHandler()(self.environ, self.start_response) → __call__ → self.finish_response() 处理请求过程 : 当一个HTTP请求到达服务器 , WSGIServer类会通过调用WSGIRequestHandler类的handle()方法来处理HTTP请求 , 在处理请求时 , 会先创建一个WSGI应用程序(WSGIHandler)接口的实例 , 随后作为参数传给ServerHandler类 , 最后对其进行处理 "},"05-Web框架/Django/52-Django - 源码之middleware.html":{"url":"05-Web框架/Django/52-Django - 源码之middleware.html","title":"Django - 源码之middleware","keywords":"","body":"Django - 源码之middleware 介绍 🍀 在关于上一篇runserver命令的分析中 , 实际上我们跳过了一步 , 那就是关于中间件加载的 当执行django-admin runsercer命令时 , 我们知道会执行runserver.Command类的handle()方法 , 那么加载中间件的这一步在哪里呢 它就藏在WSGIHandler中 , 并且在实例化时就会执行 , 而我们获取这个实例是通过调用runserver.Command类的get_handler()方法获取的 , 它在inner_run中被调用 , 如下 : runserver.Command.inner_run() def inner_run(self, *args, **options): pass # 截取如下内容 try: # 调用get_handler(),返回一个WSGIHandler实例 # 注意该实例实现了__call__方法... handler = self.get_handler(*args, **options) run(self.addr, int(self.port), handler, ipv6=self.use_ipv6, threading=threading, server_cls=self.server_cls) except socket.error as e: pass get_handler() def get_handler(self, *args, **options): \"\"\" Returns the default WSGI handler for the runner. \"\"\" return get_internal_wsgi_application() 随后调用get_internal_wsgi_application() → get_wsgi_application() → WSGIHandler() 其构造函数如下 : def __init__(self, *args, **kwargs): super(WSGIHandler, self).__init__(*args, **kwargs) self.load_middleware() load_middleware 🍀 def load_middleware(self): \"\"\" Populate middleware lists from settings.MIDDLEWARE (or the deprecated MIDDLEWARE_CLASSES). Must be called after the environment is fixed (see __call__ in subclasses). \"\"\" # 请求中间件 self._request_middleware = [] # 视图中间件 self._view_middleware = [] # 模板中间件 self._template_response_middleware = [] # 响应中间件 self._response_middleware = [] # 异常中间件 self._exception_middleware = [] if settings.MIDDLEWARE is None: # 在django比较老的版本(1.10之前)中使用的是settings.MIDDLEWARE_CLASS # 这一部分主要是为了解决兼容性问题 warnings.warn( \"Old-style middleware using settings.MIDDLEWARE_CLASSES is \" \"deprecated. Update your middleware and use settings.MIDDLEWARE \" \"instead.\", RemovedInDjango20Warning ) # convert_exception_to_response会将返回的异常转换成响应,它是一个装饰器 # self._legacy_get_response函数作用为应用请求中间件 handler = convert_exception_to_response(self._legacy_get_response) # 从settings中加载中间件 for middleware_path in settings.MIDDLEWARE_CLASSES: # 导入中间件所在模块 mw_class = import_string(middleware_path) try: # 实例化中间件 mw_instance = mw_class() except MiddlewareNotUsed as exc: if settings.DEBUG: if six.text_type(exc): logger.debug('MiddlewareNotUsed(%r): %s', middleware_path, exc) else: logger.debug('MiddlewareNotUsed: %r', middleware_path) continue # 根据中间件类型,将其process_xxxx方法添加如中间件列表 if hasattr(mw_instance, 'process_request'): self._request_middleware.append(mw_instance.process_request) if hasattr(mw_instance, 'process_view'): self._view_middleware.append(mw_instance.process_view) # 后三种加载顺序为后来者居上 if hasattr(mw_instance, 'process_template_response'): self._template_response_middleware.insert(0, mw_instance.process_template_response) if hasattr(mw_instance, 'process_response'): self._response_middleware.insert(0, mw_instance.process_response) if hasattr(mw_instance, 'process_exception'): self._exception_middleware.insert(0, mw_instance.process_exception) else: # 默认我们创建Django项目时,settings.MIDDLEWARE不为None # self._get_response()解析请求并调用响应的视图,最后返回响应 handler = convert_exception_to_response(self._get_response) for middleware_path in reversed(settings.MIDDLEWARE): # 导入中间件所在模块 middleware = import_string(middleware_path) try: # 实例化中间件 # 中间件接收一个get_handler参数,这里为self._get_response mw_instance = middleware(handler) except MiddlewareNotUsed as exc: if settings.DEBUG: if six.text_type(exc): logger.debug('MiddlewareNotUsed(%r): %s', middleware_path, exc) else: logger.debug('MiddlewareNotUsed: %r', middleware_path) continue if mw_instance is None: raise ImproperlyConfigured( 'Middleware factory %s returned None.' % middleware_path ) if hasattr(mw_instance, 'process_view'): self._view_middleware.insert(0, mw_instance.process_view) if hasattr(mw_instance, 'process_template_response'): self._template_response_middleware.append(mw_instance.process_template_response) if hasattr(mw_instance, 'process_exception'): self._exception_middleware.append(mw_instance.process_exception) # 执行中间件实例中的__call__方法,process_request()与process_response()正藏在这里 # 该__call__方法是现在MiddlewareMixin中 \"\"\" def __call__(self, request): response = None if hasattr(self, 'process_request'): response = self.process_request(request) if not response: # response = self._get_request(request) response = self.get_response(request) if hasattr(self, 'process_response'): response = self.process_response(request, response) return response \"\"\" handler = convert_exception_to_response(mw_instance) # We only assign to this when initialization is complete as it is used # as a flag for initialization being complete. # 最后完成中间件链的加载 self._middleware_chain = handler 在上面的代码清单中 , 我们分成两部分来说 , 因为对于中间件的加载 , 在django1.11之前的版本 , 肯定是不会有这么长的 , 原因在于 , 在django1.11中 , 为了解决与老版本之间的兼容性问题 , 作出了一些调整 在老版本中 , Django会直接应用请求中间件和响应中间件 , 也就是process_request , process_response , 在早起中间件中 , process_response总是会被调用 , 即使在这之前process_request 已经发生短路并返回了一个响应 而在新版本中 , 请求经过的中间件和响应经过的中间件层数是一样的 , 也就是说 , 一旦发生短路 , 响应就会按照相反的顺序返回 , 根本不会到达后面的中间件 , 所以这也是为什么在下部分代码中 , 只有视图中间件 , 模板中间件和异常中间件相关的内容了 self._get_response 🍀 那么接下来我们就需要看看关于这后部分三个中间件的细节内容了 , 它们在self._get_response()中 , 该方法在初始化时就已经被传入各个中间件中 : def _get_response(self, request): \"\"\" Resolve and call the view, then apply view, exception, and template_response middleware. This method is everything that happens inside the request/response middleware. \"\"\" response = None if hasattr(request, 'urlconf'): urlconf = request.urlconf set_urlconf(urlconf) resolver = get_resolver(urlconf) else: resolver = get_resolver() # 进行url匹配,查找相关视图 resolver_match = resolver.resolve(request.path_info) callback, callback_args, callback_kwargs = resolver_match request.resolver_match = resolver_match # Apply view middleware for middleware_method in self._view_middleware: # self._view_middleware中为process_view # 即各个视图中间件中的process_view方法 response = middleware_method(request, callback, callback_args, callback_kwargs) if response: # 如果响应不为空,终止处理 break if response is None: # 返回视图函数 wrapped_callback = self.make_view_atomic(callback) try: # 调用视图函数 response = wrapped_callback(request, *callback_args, **callback_kwargs) except Exception as e: # 应用异常中间件 response = self.process_exception_by_middleware(e, request) # Complain if the view returned None (a common error). if response is None: if isinstance(callback, types.FunctionType): # FBV view_name = callback.__name__ else: # CBV view_name = callback.__class__.__name__ + '.__call__' raise ValueError( \"The view %s.%s didn't return an HttpResponse object. It \" \"returned None instead.\" % (callback.__module__, view_name) ) # If the response supports deferred rendering, apply template # response middleware and then render the response elif hasattr(response, 'render') and callable(response.render): # 如果响应支持延迟渲染,就应用模板中间件 for middleware_method in self._template_response_middleware: response = middleware_method(request, response) # Complain if the template response middleware returned None (a common error). if response is None: raise ValueError( \"%s.process_template_response didn't return an \" \"HttpResponse object. It returned None instead.\" % (middleware_method.__self__.__class__.__name__) ) try: response = response.render() except Exception as e: response = self.process_exception_by_middleware(e, request) return response 到这里 , 加载工作就差不多了 , 而在上一篇关于runserver的分析中 , 我们也已经知道 , 当请求来到时 , 会调用ServerHandler的最高基类BaseHandler中的run()方法 , 并且已经将WSGIHandler()设置为了application , 如下 , 我们再次将run()源码贴出来 : def run(self, application): \"\"\"Invoke the application\"\"\" # Note to self: don't move the close()! Asynchronous servers shouldn't # call close() from finish_response(), so if you close() anywhere but # the double-error branch here, you'll break asynchronous servers by # prematurely closing. Async servers must return from 'run()' without # closing if there might still be output to iterate over. try: self.setup_environ() # self.result = WSGIHandler()(self.environ, self.start_response) # 即调用WSGIHandler类的__call__方法 self.result = application(self.environ, self.start_response) self.finish_response() except: try: self.handle_error() except: # If we get an error handling an error, just give up already! self.close() raise # ...and let the actual server figure it out. 现在 , 该亮出这个__call__了 : def __call__(self, environ, start_response): set_script_prefix(get_script_name(environ)) signals.request_started.send(sender=self.__class__, environ=environ) # 实例化请求 request = self.request_class(environ) # 处理请求,返回响应对象 response = self.get_response(request) response._handler_class = self.__class__ status = '%d %s' % (response.status_code, response.reason_phrase) response_headers = [(str(k), str(v)) for k, v in response.items()] for c in response.cookies.values(): response_headers.append((str('Set-Cookie'), str(c.output(header='')))) start_response(force_str(status), response_headers) if getattr(response, 'file_to_stream', None) is not None and environ.get('wsgi.file_wrapper'): response = environ['wsgi.file_wrapper'](response.file_to_stream) return response 最后 , 我们来看看这个处理请求的函数get_response() : def get_response(self, request): \"\"\"Return an HttpResponse object for the given HttpRequest.\"\"\" # Setup default url resolver for this thread set_urlconf(settings.ROOT_URLCONF) # self._middleware_chain在加载时已经完成 # 被装饰后的self._get_response response = self._middleware_chain(request) # This block is only needed for legacy MIDDLEWARE_CLASSES; if # MIDDLEWARE is used, self._response_middleware will be empty. try: # Apply response middleware, regardless of the response for middleware_method in self._response_middleware: # 调用中间件 response = middleware_method(request, response) # Complain if the response middleware returned None (a common error). if response is None: raise ValueError( \"%s.process_response didn't return an \" \"HttpResponse object. It returned None instead.\" % (middleware_method.__self__.__class__.__name__)) except Exception: # Any exception should be gathered and handled signals.got_request_exception.send(sender=self.__class__, request=request) response = self.handle_uncaught_exception(request, get_resolver(get_urlconf()), sys.exc_info()) response._closable_objects.append(request) # If the exception handler returns a TemplateResponse that has not # been rendered, force it to be rendered. if not getattr(response, 'is_rendered', True) and callable(getattr(response, 'render', None)): # 渲染模板响应 response = response.render() if response.status_code == 404: logger.warning( 'Not Found: %s', request.path, extra={'status_code': 404, 'request': request}, ) return response 到这里 , 对于中间件的分析就差不多了 , 是时候理一理了 小结 🍀 首先 , 中间件的加载是在启动时就已经加载完成 , 也就是还没有执行httpd.serve_forever()之前就已经完成 每一个中间件都是一个类 , 在 Django1.10 以后 通过继承 MiddlewareMixin 来保证Django版本之间的兼容 , 这是一个过渡 ; 在每一个中间件中会实现 process_reqeust() , process_view() , process_template_response() , process_response() , process_exception() 中间件处理步骤 : 请求通过请求中间件 , 如果process_request返回的response为空 , 则继续下一步 , 否则应用响应中间件处理传入请求与中间件返回的response 应用视图中间件 (应用之前会进行url匹配查找视图函数) , 如果process_view返回的response为空 , 则继续下一步 , 否则应用响应中间件处理传入请求与中间件返回的response 调用视图函数进行处理 如果视图抛出异常 , 应用异常中间件 , 应用响应中间件处理传入请求与中间件返回的response 如果返回的response支持延迟渲染 , 应用模板中间件 应用响应中间件 , 处理传入请求与中间件返回 "},"05-Web框架/Django/53-Django - 源码之url.html":{"url":"05-Web框架/Django/53-Django - 源码之url.html","title":"Django - 源码之url","keywords":"","body":"Django - 源码之url 介绍 🍀 上一篇对中间件的源码进行了阅读 , 我们知道了 , 当请求到来时 , 首先会应用请求中间件 , 随后就是进行URL匹配 , 去寻找对应的视图了 在上一篇的代码中 , 实际上我们已经见过了 , 那么这一篇就要对其进行详细的分析了 说先引出其入口 def _get_response(self, request): \"\"\" Resolve and call the view, then apply view, exception, and template_response middleware. This method is everything that happens inside the request/response middleware. \"\"\" response = None # 加载settings.ROOT_URLCONF,即根目录中的urls.py # 并返回一个 **RegexURLResolver** 对象 if hasattr(request, 'urlconf'): # 一般情况下,在默认request中没有urlconf属性 urlconf = request.urlconf set_urlconf(urlconf) resolver = get_resolver(urlconf) else: resolver = get_resolver() # **进行url匹配,查找相关视图** resolver_match = resolver.resolve(request.path_info) # 分解视图函数以及参数 callback, callback_args, callback_kwargs = resolver_match request.resolver_match = resolver_match # Apply view middleware for middleware_method in self._view_middleware: # self._view_middleware中为process_view # 即各个视图中间件中的process_view方法 response = middleware_method(request, callback, callback_args, callback_kwargs) if response: # 如果响应不为空,终止处理 break 由上可知 , 在_get_response() 中构造了RegexURLResolver对象 , 随后调用该对象的 resolve() 方法 , 最后从返回值中获取视图函数与参数 resolve 🍀 def resolve(self, path): path = force_text(path) # path may be a reverse_lazy object tried = [] # regex是一个类变量 # regex = LocaleRegexDescriptor() # LocaleRegexDescriptor是一个描述器类,当执行self.regex时会执行__get__方法 # self.regex返回一个经过编译的正则表达式 match = self.regex.search(path) if match: # 去除前缀mysite.url new_path = path[match.end():] # 遍历urlpatterns for pattern in self.url_patterns: try: # pattern为url()返回的RegexURLResolver(For include)或RegexURLPattern sub_match = pattern.resolve(new_path) except Resolver404 as e: sub_tried = e.args[0].get('tried') if sub_tried is not None: tried.extend([pattern] + t for t in sub_tried) else: tried.append([pattern]) else: # 未出现异常执行 if sub_match: # Merge captured arguments in match with submatch # 获取对应参数 sub_match_dict = dict(match.groupdict(), **self.default_kwargs) sub_match_dict.update(sub_match.kwargs) # If there are *any* named groups, ignore all non-named groups. # Otherwise, pass all non-named arguments as positional arguments. sub_match_args = sub_match.args if not sub_match_dict: sub_match_args = match.groups() + sub_match.args return ResolverMatch( sub_match.func, sub_match_args, sub_match_dict, sub_match.url_name, [self.app_name] + sub_match.app_names, [self.namespace] + sub_match.namespaces, ) tried.append([pattern]) raise Resolver404({'tried': tried, 'path': new_path}) raise Resolver404({'path': path}) 获取的 urlpatterns 中 , url() 函数会返回两种对象 , 源码可见 : def url(regex, view, kwargs=None, name=None): if isinstance(view, (list, tuple)): # 第一种,view为一个元组,元组中包含一个列表和元组 # 这是在进行路由分发时发生的,也就是使用include # For include(...) processing. urlconf_module, app_name, namespace = view return RegexURLResolver(regex, urlconf_module, kwargs, app_name=app_name, namespace=namespace) elif callable(view): # 第二种,view为一个函数 return RegexURLPattern(regex, view, kwargs, name) else: raise TypeError('view must be a callable or a list/tuple in the case of include().') 首先我们来看看第二种 , 也就是view为函数的情况 , 其返回一个RegexURLPattern对象 RegexURLPattern 🍀 class RegexURLPattern(LocaleRegexProvider): def __init__(self, regex, callback, default_args=None, name=None): LocaleRegexProvider.__init__(self, regex) self.callback = callback # the view self.default_args = default_args or {} self.name = name # 摘取如下部分 def resolve(self, path): # 完成url解析 match = self.regex.search(path) if match: # If there are any named groups, use those as kwargs, ignoring # non-named groups. Otherwise, pass all non-named arguments as # positional arguments. kwargs = match.groupdict() args = () if kwargs else match.groups() # In both cases, pass any extra_kwargs as **kwargs. kwargs.update(self.default_args) # ResolverMatch类中定义了__gititem__方法 return ResolverMatch(self.callback, args, kwargs, self.name) 看看这个返回的ResolverMatch , 源码如下 : class ResolverMatch(object): def __init__(self, func, args, kwargs, url_name=None, app_names=None, namespaces=None): # 分解出的视图函数 self.func = func # 视图函数参数 self.args = args self.kwargs = kwargs self.url_name = url_name # If a URLRegexResolver doesn't have a namespace or app_name, it passes # in an empty value. self.app_names = [x for x in app_names if x] if app_names else [] self.app_name = ':'.join(self.app_names) self.namespaces = [x for x in namespaces if x] if namespaces else [] self.namespace = ':'.join(self.namespaces) if not hasattr(func, '__name__'): # CBV # A class-based view self._func_path = '.'.join([func.__class__.__module__, func.__class__.__name__]) else: # FBV # A function-based view self._func_path = '.'.join([func.__module__, func.__name__]) view_path = url_name or self._func_path self.view_name = ':'.join(self.namespaces + [view_path]) def __getitem__(self, index): # 通过解包即可依次获得视图函数与各参数 return (self.func, self.args, self.kwargs)[index] 这就是最后在_get_response()中获取的结果了 , 对ResolverMatch 进行解包 接下来就是第二种情况了 , 也就是view为include() 返回的结果 , 它是一个元组 , 形式 : (list,tuple) include 🍀 首先我们需要弄清楚 , include返回的元组中到底有什么内容 , 其源码如下 : def include(arg, namespace=None, app_name=None): # arg: # - six.string_types (PY3 : str) # - tuple # - list if app_name and not namespace: raise ValueError('Must specify a namespace if specifying app_name.') if app_name: warnings.warn( 'The app_name argument to django.conf.urls.include() is deprecated. ' 'Set the app_name in the included URLconf instead.', RemovedInDjango20Warning, stacklevel=2 ) if isinstance(arg, tuple): # callable returning a namespace hint try: urlconf_module, app_name = arg except ValueError: if namespace: raise ImproperlyConfigured( 'Cannot override the namespace for a dynamic module that provides a namespace' ) warnings.warn( 'Passing a 3-tuple to django.conf.urls.include() is deprecated. ' 'Pass a 2-tuple containing the list of patterns and app_name, ' 'and provide the namespace argument to include() instead.', RemovedInDjango20Warning, stacklevel=2 ) # 获取urlconf模块,应用名,命名空间 urlconf_module, app_name, namespace = arg else: # No namespace hint - use manually provided namespace urlconf_module = arg if isinstance(urlconf_module, six.string_types): # urlconf_module为字符串,导入该url模块 urlconf_module = import_module(urlconf_module) # 获取内部的urlpatterns patterns = getattr(urlconf_module, 'urlpatterns', urlconf_module) # 获取app_name app_name = getattr(urlconf_module, 'app_name', app_name) if namespace and not app_name: warnings.warn( 'Specifying a namespace in django.conf.urls.include() without ' 'providing an app_name is deprecated. Set the app_name attribute ' 'in the included module, or pass a 2-tuple containing the list of ' 'patterns and app_name instead.', RemovedInDjango20Warning, stacklevel=2 ) namespace = namespace or app_name # Make sure we can iterate through the patterns (without this, some # testcases will break). if isinstance(patterns, (list, tuple)): # list为分发的url for url_pattern in patterns: # Test if the LocaleRegexURLResolver is used within the include; # this should throw an error since this is not allowed! if isinstance(url_pattern, LocaleRegexURLResolver): raise ImproperlyConfigured( 'Using i18n_patterns in an included URLconf is not allowed.') # 返回一个元组 return (urlconf_module, app_name, namespace) include实现了路由分发 , 但是实际上 , 它就是在 url() 中又套了一层 url() , 在每一个urls中都必须有一个urlpatterns , 通过一层一层的urlpatterns , 就可以实现路由分发了 最后 , 如果我们不使用include() , 实际上也是可以实现路由分发的 : # 版本一 urlpatterns = [ url(r'^home/', ( # 该列表即urlpatterns,可用函数替换掉 [ url(r'^index/', views.index), url(r'^login/', views.login), url(r'^register/', views.register), ], None, # app_name None # namespace )), ] # 版本二 def get_urlpatterns(): urlpatterns = [ url(r'^index/', views.index), url(r'^login/', views.login), url(r'^register/', views.register), ] return urlpatterns urlpatterns = [ url(r'^home/', ( get_urlpatterns, None, # app_name None # namespace )), ] 小结 🍀 在这一章关于URL的源码中 , 印象最深刻的还属 __get__ 的使用 , 构造描述器类 ( LocaleRegexDescriptor )来完成需求 , 以及以解包的方式获取 __gitItem__ 中的相关值 __get__ 示例 class Descriptor: def __get__(self): print(\"Visit me and execute me.\") class Onwer: desc = Descriptor() o = Onwer() o.d \"\"\" 执行结果: Visit me and execute me. \"\"\" __getitem__ 示例 class Foo: def __getitem__(self, item): return ('lyon', 'kenneth', 'even')[item] f = Foo() lyon, kenneth, even = f \"\"\" 执行结果: lyon kenneth even \"\"\" "},"05-Web框架/Django/54-Django - 源码之admin.html":{"url":"05-Web框架/Django/54-Django - 源码之admin.html","title":"Django - 源码之admin","keywords":"","body":"Django - 源码之Admin 介绍 🍀 Django为我们提供了一个强大的后台管理页面 , 也就是admin 在我们创建应用时 , 默认的 urls.py 中第一条就进行了admin分发 , 如下 : url(r'^admin/', admin.site.urls) 其应用增删改查路由规则如下 : 这些url都是admin为我们自动生成的 , 这也是我们需要了解的重点 # application:应用名 # table:表名 - /admin/application/table/ # 查询数据 - /admin/application/table/add/ # 添加数据 - /admin/application/table/1/change/ # 修改数据 - /admin/application/table/1/delete/ # 删除数据 admin.site.urls 🍀 在源码目录中 , admin 下的 sites.py 中使用了一个全局对象 # This global object represents the default admin site, for the common case. # You can instantiate AdminSite in your own code to create a custom admin site. site = AdminSite() 随后访问该实例的urls属性 , 如下 : @property def urls(self): return self.get_urls(), 'admin', self.name 该属性返回了一个元组 , 并且形式如 : (list,tuple) , 在上一篇URL的分析中我们已经知道 , 这正是进行路由分发 , 元组中的第一个元素为分发的子路由 self.get_urls()源码如下 : def get_urls(self): from django.conf.urls import url, include # Since this module gets imported in the application's root package, # it cannot import models from other applications at the module level, # and django.contrib.contenttypes.views imports ContentType. from django.contrib.contenttypes import views as contenttype_views def wrap(view, cacheable=False): def wrapper(*args, **kwargs): return self.admin_view(view, cacheable)(*args, **kwargs) wrapper.admin_site = self return update_wrapper(wrapper, view) # Admin-site-wide views. # admin本身视图 urlpatterns = [ url(r'^$', wrap(self.index), name='index'), url(r'^login/$', self.login, name='login'), url(r'^logout/$', wrap(self.logout), name='logout'), url(r'^password_change/$', wrap(self.password_change, cacheable=True), name='password_change'), url(r'^password_change/done/$', wrap(self.password_change_done, cacheable=True), name='password_change_done'), url(r'^jsi18n/$', wrap(self.i18n_javascript, cacheable=True), name='jsi18n'), url(r'^r/(?P\\d+)/(?P.+)/$', wrap(contenttype_views.shortcut), name='view_on_site'), ] # Add in each model's views, and create a list of valid URLS for the # app_index valid_app_labels = [] for model, model_admin in self._registry.items(): # model:models.UserInfo # model_admin:ModelAdmin(models.UserInfo, admin.site) # 为self._registry中的model生成增删改查url # self._registry默认为空,当我们在应用的admin.py中注册时会进行添加 # model_admin为ModelAdmin实例 urlpatterns += [ url(r'^%s/%s/' % (model._meta.app_label, model._meta.model_name), include(model_admin.urls)), ] if model._meta.app_label not in valid_app_labels: valid_app_labels.append(model._meta.app_label) # If there were ModelAdmins registered, we should have a list of app # labels for which we need to allow access to the app_index view, if valid_app_labels: regex = r'^(?P' + '|'.join(valid_app_labels) + ')/$' urlpatterns += [ url(regex, wrap(self.app_index), name='app_list'), ] return urlpatterns register 🍀 注册model到self._registry , 是通过register()实现的 , 如 : admin.site.register(models.UserInfo) def register(self, model_or_iterable, admin_class=None, **options): \"\"\" Registers the given model(s) with the given admin class. The model(s) should be Model classes, not instances. If an admin class isn't given, it will use ModelAdmin (the default admin options). If keyword arguments are given -- e.g., list_display -- they'll be applied as options to the admin class. If a model is already registered, this will raise AlreadyRegistered. If a model is abstract, this will raise ImproperlyConfigured. \"\"\" if not admin_class: # 默认使用ModelAdmin admin_class = ModelAdmin if isinstance(model_or_iterable, ModelBase): # 将model class放入列表中 model_or_iterable = [model_or_iterable] for model in model_or_iterable: if model._meta.abstract: raise ImproperlyConfigured( 'The model %s is abstract, so it cannot be registered with admin.' % model.__name__ ) if model in self._registry: raise AlreadyRegistered('The model %s is already registered' % model.__name__) # Ignore the registration if the model has been # swapped out. if not model._meta.swapped: # If we got **options then dynamically construct a subclass of # admin_class with those **options. if options: # For reasons I don't quite understand, without a __module__ # the created class appears to \"live\" in the wrong place, # which causes issues later on. options['__module__'] = __name__ admin_class = type(\"%sAdmin\" % model.__name__, (admin_class,), options) # Instantiate the admin class to save in the registry # 注册后示例:models.UserInfo : ModelAdmin(models.UserInfo, admin.site) self._registry[model] = admin_class(model, self) self._registry 是一个字典 , 最后注册完成后 , 字典的元素的key 为 modes.UserInfo , 而value则是一个ModelAdmin对象 , 对于model的url生成 , 就藏在ModelAdmin中 ModelAdmin 🍀 提取ModelAdmin中的部分内容 因为在分发的过程中 , 执行了include(model_admin.urls) @property def urls(self): return self.get_urls() self.get_urls() def get_urls(self): from django.conf.urls import url def wrap(view): def wrapper(*args, **kwargs): return self.admin_site.admin_view(view)(*args, **kwargs) wrapper.model_admin = self return update_wrapper(wrapper, view) info = self.model._meta.app_label, self.model._meta.model_name # 自动生成url列表 urlpatterns = [ url(r'^$', wrap(self.changelist_view), name='%s_%s_changelist' % info), url(r'^add/$', wrap(self.add_view), name='%s_%s_add' % info), url(r'^(.+)/history/$', wrap(self.history_view), name='%s_%s_history' % info), url(r'^(.+)/delete/$', wrap(self.delete_view), name='%s_%s_delete' % info), url(r'^(.+)/change/$', wrap(self.change_view), name='%s_%s_change' % info), # For backwards compatibility (was the change url before 1.9) url(r'^(.+)/$', wrap(RedirectView.as_view( pattern_name='%s:%s_%s_change' % ((self.admin_site.name,) + info) ))), ] return urlpatterns 上面自动生成的url对应视图如下 : class ModelAdmin(BaseModelAdmin): def __init__(self, model, admin_site): self.model = model self.opts = model._meta self.admin_site = admin_site super(ModelAdmin, self).__init__() @csrf_protect_m def changelist_view(self, request, extra_context=None): \"\"\" 查看model列表 \"\"\" pass def add_view(self, request, form_url='', extra_context=None): \"\"\" 添加数据 \"\"\" return self.changeform_view(request, None, form_url, extra_context) def change_view(self, request, object_id, form_url='', extra_context=None): \"\"\" 编辑数据 \"\"\" return self.changeform_view(request, object_id, form_url, extra_context) @csrf_protect_m def delete_view(self, request, object_id, extra_context=None): \"\"\" 删除数据 \"\"\" with transaction.atomic(using=router.db_for_write(self.model)): return self._delete_view(request, object_id, extra_context) def history_view(self, request, object_id, extra_context=None): \"\"\" 查看历史记录 \"\"\" pass 自定制Admin 🍀 当我们在admin.py 中间注册我们的model时 , 一般我们只需要传入一个参数 , 就是我们的model , 因为django为我们默认是用了ModelAdmin , 也就是admin_class参数 , 当然我们可以自己传入这个admin_class , 这样我们就可以完全定制属于我们自己admin了 定制之前 , 我们需要知道在ModelAdmin中的定制配置 @python_2_unicode_compatible class ModelAdmin(BaseModelAdmin): \"Encapsulates all admin options and functionality for a given model.\" # 定制显示的列 list_display = ('__str__',) # 定制可跳转的列 list_display_links = () # 定制右侧快速筛选 list_filter = () # 定制连表查询自动select_related list_select_related = False # 定制页面显示条数 list_per_page = 100 # 定制显示全部条数大小,只有当真实数据小于该值才应用 list_max_show_all = 200 # 定制可编辑的列 list_editable = () # 定制模糊搜索功能 search_fields = () # 对Date和DateTime类型进行搜索 date_hierarchy = None # 详细页面,按钮为\"Sava as new\"或\"Sava and add another\" save_as = False # 点击保存并继续编辑 save_as_continue = True # 详细页面,在页面上方是否也显示保存删除等按钮 save_on_top = False # 分页插件 paginator = Paginator # preserve_filters = True # 详细页面,如果有其他表和当前表做外键关联,那么详细页面可以进行动态增加和删除 inlines = [] # Custom templates (designed to be over-ridden in subclasses) add_form_template = None change_form_template = None change_list_template = None delete_confirmation_template = None delete_selected_confirmation_template = None object_history_template = None popup_response_template = None # Actions # 定制action中的操作 actions = [] action_form = helpers.ActionForm # Action选项在页面上方显示 actions_on_top = True # Action选项在页面下方显示 actions_on_bottom = False # 显示选择个数 actions_selection_counter = True checks_class = ModelAdminChecks 后期补充 ... "},"05-Web框架/Django/Django - Django命令整理.html":{"url":"05-Web框架/Django/Django - Django命令整理.html","title":"Django - Django命令整理","keywords":"","body":"Django - Django命令整理 介绍 🍀 django-admin是用于管理Django的命令行工具集 , 此外在每个Django项目中会自动为我们生成一个manage.py , 它与django-admin相同 , 但是会帮我们处理以下几件事情 : 它将为你的项目包添加环境变量 它用于设置DJANGO_SETTINGS_MODULE环境变量 , 因此它指向项目的settings.py文件 在我们编写项目时 , 通常使用manage.py会比django-admin方便些 , 但是如果我们需要在多个Django项目的settings文件中切换 , 可以使用django-admin加上DJANGO_SETTINGS_MODULE或者--settings参数 用法 $ django-admin [options] $ manage.py [options] $ python -m django [options] 对于以上三种方式的命令格式 , 其command与options都是一致的 , 哪一种格式都能达到我们的要求 , 通常我们使用manage.py格式是最多的 , 所以下面就以manage.py为示例了 $ python manage.py [options] 后续省略开头的python进行示例 基础命令 🍀 # 显示使用信息和每个应用的命令列表 $ manage.py help # 显示包含所有可用命令的列表 $ manage.py help --comands # 显示某一个命令的描述及可用的命令列表 $ manage.py help # 获取django版本 $ manage.py version # 创建Django项目 $ django-admin startproject name [directory] # 运行所有已安装的测试程序 $ manage.py test # 启动本地Web服务器 $ manage.py runserver [addrport] # 将迁移添加到migrations目录 $ manage.py makemigrations # 迁移数据库 $ manage.py migrate # 进行数据迁移并返回所执行的SQL语句 $ manage.py sqlmigrate # 刷新数据库 $ manage.py flush # 检查项目中的任何问题,而不进行迁移和访问数据库 $ manage.py check [app_label [app_label ...]] # 创建缓存表 $ manage.py createcachetable # 运行ENGINE设置中指定的数据库引擎命令行客户端 $ manage.py dbshell # 显示当前setting文件与Django默认settings文件之间的差异 $ manage.py diffsettings # 显示结果中\"###\"表示默认设置中没有定义的设置 # 发送测试电子邮件 $ manage.py sendtestemial [email] [email ...]] # 启动Python交互式解释器 $ manage.py shell -i {ipython,bpython,python} # 创建应用 $ manage.py startapp name [directory] # 创建超级用户 $ manage.py createsuperuser # 指定控制台打印的通知和调试信息量,以migrate为例 $ manage.py migrate --verbosity 2 # --verbosity {0,1,2,3}, -v {0,1,2,3} ''' 0,无输出 1,正常输出(默认) 2,详细输出 3,非常详细输出 ''' 代码执行命令 🍀 # 不带参数 from django.core import management from django.core.management.commands import loaddata management.call_command('flush', verbosity=0, interactive=False) management.call_command('loaddata', 'test_data', verbosity=0) management.call_command(loaddata.Command(), 'test_data', verbosity=0) # 带参数 # Similar to the command line management.call_command('dumpdata', '--natural-foreign') # Named argument similar to the command line minus the initial dashes and # with internal dashes replaced by underscores management.call_command('dumpdata', natural_foreign=True) # `use_natural_foreign_keys` is the option destination variable management.call_command('dumpdata', use_natural_foreign_keys=True) 更多详细内容 : django-admin and manage.py "},"05-Web框架/Django-Rest-Framework/":{"url":"05-Web框架/Django-Rest-Framework/","title":"Django-Rest-Framework","keywords":"","body":"Django Rest Framework 教程 本教程是一个以官方文档为模板的中文教程 , 但在内容上会增加一些注释已经说明 , 帮助您简单的完成 django-rest-framework 的学习 官网 : http://www.django-rest-framework.org/ "},"05-Web框架/Django-Rest-Framework/Quickstart.html":{"url":"05-Web框架/Django-Rest-Framework/Quickstart.html","title":"Quickstart","keywords":"","body":"快速开始 我们将创建一个简单的API , 允许管理员用户查看和编辑系统中的用户和组 创建项目 🍀 # 创建项目目录 mkdir tutorial cd tutorial # 创建虚拟环境 virtualenv env source env/bin/activate # Windows上使用`env\\Scripts\\activate` # 安装Django和Django REST framework into the virtualenv pip install django pip install djangorestframework # 创建Django项目和应用 django-admin.py startproject tutorial . # Note the trailing '.' character cd tutorial django-admin.py startapp quickstart cd .. 该项目目录如下 : $ pwd /tutorial $ find . . ./manage.py ./tutorial ./tutorial/__init__.py ./tutorial/quickstart ./tutorial/quickstart/__init__.py ./tutorial/quickstart/admin.py ./tutorial/quickstart/apps.py ./tutorial/quickstart/migrations ./tutorial/quickstart/migrations/__init__.py ./tutorial/quickstart/models.py ./tutorial/quickstart/tests.py ./tutorial/quickstart/views.py ./tutorial/settings.py ./tutorial/urls.py ./tutorial/wsgi.py 同步数据库 python manage.py migrate 模型 🍀 我们使用Django默认的两个模型 , 用户(User)和组 (Group) , 你可以使用 Admin 来进行管理 创建两个超级用户 python manage.py createsuperuser --emaill Lyon@example.com --username Lyon python manage.py createsuperuser --emaill Kenneth@example.com --username Kenneth 序列化器 🍀 我们需要定制一些序列化器来决定我们数据的表现形式 在 quickstart 应用下新建 serializers.py : from django.contrib.auth.models import User, Group from rest_framework import serializers class UserSerializer(serializers.HyperlinkedModelSerializer): \"\"\" 用户序列化器 \"\"\" class Meta: model = User fields = ('url', 'username', 'email', 'groups') class GroupSerializer(serializers.HyperlinkedModelSerializer): \"\"\" 组序列化器 \"\"\" class Meta: model = Group fields = ('url', 'name') 这里使用超链接模型序列化器 , 因为超链接是非常好的RESTful设计 , 当然你也可以使用其他的序列化器 视图 🍀 tutorial/quickstart/views.py from django.contrib.auth.models import User, Group from rest_framework import viewsets from quickstart.serializers import UserSerializer, GroupSerializer class UserViewSet(viewsets.ModelViewSet): \"\"\" 允许查看或编辑用户的API \"\"\" queryset = User.objects.all().order_by('-date_joined') serializer_class = UserSerializer class GroupViewSet(viewsets.ModelViewSet): \"\"\" 允许查看或编辑组的API \"\"\" queryset = Group.objects.all() serializer_class = GroupSerializer 路由 🍀 tutorial/urls.py from django.conf.urls import url, include from rest_framework import routers from quickstart import views # 因为使用的是viewsets,所以使用路由器进行注册 router = routers.DefaultRouter() router.register(r'users', views.UserViewSet) router.register(r'groups', views.GroupViewSet) urlpatterns = [ url(r'^', include(router.urls)), # rest_framework自带的url,login与logout: # - url(r'^login/$', login, login_kwargs, name='login'), # - url(r'^logout/$', logout, name='logout'), url(r'^api-auth/', include('rest_framework.urls', namespace='rest_framework')) ] 配置 🍀 在 tutorial/settings.py 的 INSTALLED_APPS 中 , 添加 rest_framework INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'quickstart', 'rest_framework', # 此处为添加项 ] 测试API 🍀 首先我们运行我们的项目程序 , 随后使用浏览器访问项目服务器地址 , 或者使用命令行工具 , 如 : httpie 等 查看我们的API , http://127.0.0.1:8000/ , 结果如下 : 随后我们可以通过API来获取用户数据 , 访问 http://127.0.0.1:8000/users/ , 结果如下 : 到这里 , django-rest-framework的简单使用就结束了 , 后续内容请看下一篇 "},"05-Web框架/Django-Rest-Framework/Tutorial 1 Serialization.html":{"url":"05-Web框架/Django-Rest-Framework/Tutorial 1 Serialization.html","title":"Tutorial 1 Serialization","keywords":"","body":"Tutorial 1: Serialization 介绍 🍀 本教程将会通过一些简单的代码来实现 Web API. 这个过程将会介绍 REST framework 的各个组件, 带你深入理解各个组件是如何一起工作 创建一个新的环境 🍀 为了确保我们的包配置与我们正在进行的其他项目保持良好的隔离 , 我们将使用 virtualenv 来创建一个新的虚拟环境 virtualenv env source env/bin/activate 在新的虚拟环境中安装 django 与 django rest framework pip install django pip install djangorestframework pip install pygments # 我们将使用这个模块来提高代码高亮 友情链接 : virtualenv , pygments 开始 🍀 首先我们创建一个新的项目 cd ~ django-admin.py startproject tutorial cd tutorial 创建应用 snippets python manage.py startapp snippets 配置应用 INSTALLED_APPS = [ ... 'rest_framework', 'snippets.apps.SnippetsConfig', ] 创建一个Model 🍀 我们首先创建一个简单的模型 from django.db import models from pygments.lexers import get_all_lexers from pygments.styles import get_all_styles # get_all_lexers() 返回所有的词法分析器 LEXERS = [item for item in get_all_lexers() if item[1]] # 语言类别 LANGUAGE_CHOICES = sorted([(item[1][0], item[0]) for item in LEXERS]) # get_all_styles() 返回所有样式的名称 STYLE_CHOICES = sorted((item, item) for item in get_all_styles()) class Snippet(models.Model): created = models.DateTimeField(auto_now_add=True) title = models.CharField(max_length=100, blank=True, default='') code = models.TextField() linenos = models.BooleanField(default=False) language = models.CharField(choices=LANGUAGE_CHOICES, default='python', max_length=100) style = models.CharField(choices=STYLE_CHOICES, default='friendly', max_length=100) class Meta: ordering = ('created',) 我们还需要为我们的代码片段模型创建一个初始迁移 , 并第一次同步数据库 python manage.py makemigrations snippets python manage.py migrate 创建一个序列化类 🍀 我们在Web API上首先需要做的一件事是提供一种将 Snippet 实例序列化和反序列化的方法 , 使之成为诸如json之类的表示形式的方式 我们可以通过声明与Django的表单非常相似的序列化器 (serializer) 来做到这一点 在 snippets 目录中创建 serializers.py , 内容如下 : from rest_framework import serializers from snippets.models import Snippet, LANGUAGE_CHOICES, STYLE_CHOICES class SnippetSerializer(serializers.Serializer): id = serializers.IntegerField(read_only=True) title = serializers.CharField(required=False, allow_blank=True, max_length=100) code = serializers.CharField(style={'base_template': 'textarea.html'}) linenos = serializers.BooleanField(required=False) language = serializers.ChoiceField(choices=LANGUAGE_CHOICES, default='python') style = serializers.ChoiceField(choices=STYLE_CHOICES, default='friendly') def create(self, validated_data): \"\"\" 根据已验证的数据,创建并返回一个新的 Snippet 实例 \"\"\" return Snippet.objects.create(**validated_data) def update(self, instance, validated_data): \"\"\" 根据已验证的数据,更新并返回一个新 Snippet 实例 \"\"\" instance.title = validated_data.get('title', instance.title) instance.code = validated_data.get('code', instance.code) instance.linenos = validated_data.get('linenos', instance.linenos) instance.language = validated_data.get('language', instance.language) instance.style = validated_data.get('style', instance.style) instance.save() return instance 序列化器类分为两个部分 : 第一部分定义了序列化/反序列化的字段 第二部分 create() 和 update() 方法定义了当调用 serializer.save() 时如何创建或修改实例 序列化器类与Django Form类非常相似 , 并且在各个字段中包含相似的验证标志 , 例如 required , max_length , default 等 另外 , 在某些情况下字段标志还可以控制序列化器如何显示 , 比如渲染到HTML时 , {'base_template': 'textarea.html'} 等同于Django Form 类中的 widget=widgets.Textarea , 这对于控制如何显示可浏览的API特别有用 , 我们将在后面的教程中看到 我们还可以通过使用 ModelSerializer (相当于ModelForm) 类来节省一些时间 , 稍后我们将会看到 , 但是现在我们将显式的定义序列化器 使用序列化器 🍀 在我们进一步讨论之前 , 我们将熟悉使用新的序列化器类 , 让我们先进入 Django shell python manage.py shell 那么接下来我们将在 shell 中创建几个 Snippet 实例一起工作 # 导入相关依赖 from snippets.models import Snippet from snippets.serializers import SnippetSerializer from rest_framework.renderers import JSONRenderer from rest_framework.parsers import JSONParser snippet = Snippet(code='foo = \"bar\"\\n') snippet.save() snippet = Snippet(code='print \"hello, world\"\\n') snippet.save() 现在我们已经有一些实例了 , 让我们看一看如何将实例序列化 注 : Model → Serialiezer serializer = SnippetSerializer(snippet) serializer.data # {'id': 2, 'title': '', 'code': 'print \"hello, world\"\\n', 'linenos': False, 'language': 'python', 'style': 'friendly'} 现在我们将模型实例 (model instance) 转化成Python原生数据类型 , 为了完成实例化过程 , 我们将数据渲染成 json 注 : Serialiezer → JSON content = JSONRenderer().render(serializer.data) content # '{\"id\":2,\"title\":\"\",\"code\":\"print \\\\\"hello, world\\\\\"\\\\n\",\"linenos\":false,\"language\":\"python\",\"style\":\"friendly\"}' 反序列化是相似的 , 首先我们将流 (stream) 解析成Python原生数据类型… from django.utils.six import BytesIO stream = BytesIO(content) data = JSONParser().parse(stream) 然后, 我们将Python原生数据恢复成正常的对象实例 注 : JSON → Serialiezer serializer = SnippetSerializer(data=data) serializer.is_valid() # True serializer.validated_data # OrderedDict([('title', ''), ('code', 'print \"hello, world\"\\n'), ('linenos', False), ('language', 'python'), ('style', 'friendly')]) serializer.save() # 请注意 , API和表单很相似 . 当我们用我们的序列器 (serializer) 写视图的时候 , 相似性会更明显. 除了将模型模型实例 (model instance) 序列化外 , 我们也能序列化查询集 (querysets) , 只需要添加一个序列化参数 many=True serializer = SnippetSerializer(Snippet.objects.all(), many=True) serializer.data # [OrderedDict([('id', 1), ('title', u''), ('code', u'foo = \"bar\"\\n'), ('linenos', False), ('language', 'python'), ('style', 'friendly')]), OrderedDict([('id', 2), ('title', u''), ('code', u'print \"hello, world\"\\n'), ('linenos', False), ('language', 'python'), ('style', 'friendly')]), OrderedDict([('id', 3), ('title', u''), ('code', u'print \"hello, world\"'), ('linenos', False), ('language', 'python'), ('style', 'friendly')])] 使用ModelSerializers 🍀 我们的 SnippetSerializer 类复制了 Snippet 模型中的许多信息 , 如果我们能让代码更简洁一些 , 那就太好了 就像Django提供了 Form 类和 ModelForm 类一样 , REST 框架也有 Serializer 类和 ModelSerializer 类 让我们使用 ModelSerializer 类重构我们的序列化器 class SnippetSerializer(serializers.ModelSerializer): class Meta: model = Snippet fields = ('id', 'title', 'code', 'linenos', 'language', 'style') 序列化器有一个很好的特性 , 就是你可以通过打印序列实例的结构 (representation) 查看它的所有字段 . 输入 python manage.py shell 打开Django shell , 尝试如下代码 : from snippets.serializers import SnippetSerializer serializer = SnippetSerializer() print(repr(serializer)) # SnippetSerializer(): # id = IntegerField(label='ID', read_only=True) # title = CharField(allow_blank=True, max_length=100, required=False) # code = CharField(style={'base_template': 'textarea.html'}) # linenos = BooleanField(required=False) # language = ChoiceField(choices=[('Clipper', 'FoxPro'), ('Cucumber', 'Gherkin'), ('RobotFramework', 'RobotFramework'), ('abap', 'ABAP'), ('ada', 'Ada')... # style = ChoiceField(choices=[('autumn', 'autumn'), ('borland', 'borland'), ('bw', 'bw'), ('colorful', 'colorful')... 要记住 , ModelSerializer 类没有做任何神奇的事情 , 它们只是创建序列化器类的快捷方式 : 一组自动确定的字段 简单默认实现的 create() 和 update() 方法 用序列化器写常规的Django视图 🍀 让我们看看如何使用我们的序列化器类来编写一些API视图 , 目前 , 我们不会使用REST框架的其他特性 , 只是写一些常规的Django视图 编辑 snippets/views.py : from django.http import HttpResponse, JsonResponse from django.views.decorators.csrf import csrf_exempt from rest_framework.renderers import JSONRenderer from rest_framework.parsers import JSONParser from snippets.models import Snippet from snippets.serializers import SnippetSerializer 我们的根API将会是一个视图 , 它支持列出所有现有的 snippets , 或者创建一个新的 snippets @csrf_exempt def snippet_list(request): \"\"\" 列出所有的 snippets,或者创建一个新的 snippet. \"\"\" if request.method == 'GET': snippets = Snippet.objects.all() serializer = SnippetSerializer(snippets, many=True) return JsonResponse(serializer.data, safe=False) elif request.method == 'POST': data = JSONParser().parse(request) serializer = SnippetSerializer(data=data) if serializer.is_valid(): serializer.save() return JsonResponse(serializer.data, status=201) return JsonResponse(serializer.errors, status=400) 注意 , 因为我们希望能够从没有CSRF令牌的客户端 POST 数据到这个视图 , 所以我们需要将视图标记为 csrf_exempt , 通常我们是不会这样做 , REST 框架视图使用了比这更合理的方式 , 不过那不是我们现在的目的 我们还需要一个视图对应单个 snippet , 并且我们可以使用这个视图进行检索 , 更新或删除 snippet @csrf_exempt def snippet_detail(request, pk): \"\"\" 检索、更新或删除 snippet. \"\"\" try: snippet = Snippet.objects.get(pk=pk) except Snippet.DoesNotExist: return HttpResponse(status=404) if request.method == 'GET': serializer = SnippetSerializer(snippet) return JsonResponse(serializer.data) elif request.method == 'PUT': data = JSONParser().parse(request) serializer = SnippetSerializer(snippet, data=data) if serializer.is_valid(): serializer.save() return JsonResponse(serializer.data) return JsonResponse(serializer.errors, status=400) elif request.method == 'DELETE': snippet.delete() return HttpResponse(status=204) 最后 , 我们需要使用路由将这些视图对应起来 , 创建 snippets/urls.py 文件 from django.conf.urls import url from snippets import views urlpatterns = [ url(r'^snippets/$', views.snippet_list), url(r'^snippets/(?P[0-9]+)/$', views.snippet_detail), ] 同时需要配置 tutorial/urls.py , 添加我们的 snippet 应用的 URLs from django.conf.urls import url, include urlpatterns = [ url(r'^', include('snippets.urls')), ] 值得注意的是 , 我们目前还有一些边界情况没有进行处理 , 如果我们发送错误的 json 数据 , 或者一个请求是用一个视图无法处理的方法进行的 , 那么我们将得到一个500的错误 , “Server Error” 测试我们的API 🍀 首先退出 shell quit() 随后启动Django开发服务器 python manage.py runserver Validating models... 0 errors found Django version 1.11, using settings 'tutorial.settings' Development server is running at http://127.0.0.1:8000/ Quit the server with CONTROL-C. 在另一个终端窗口中 , 我们可以来测试这个服务器 我们可以使用 curl , 或者 httpie , Httpie 是一个使用Python编写的对用户友好的http客户端 , 让我们安装它 (虽然上一章节中已经说明过) pip install httpie 我们可以获取 snippets 列表 http http://127.0.0.1:8000/snippets/ HTTP/1.0 200 OK ... [ { \"code\": \"foo = \\\"bar\\\"\\n\", \"id\": 1, \"language\": \"python\", \"linenos\": false, \"style\": \"friendly\", \"title\": \"\" }, { \"code\": \"print \\\"hello, world\\\"\\n\", \"id\": 2, \"language\": \"python\", \"linenos\": false, \"style\": \"friendly\", \"title\": \"\" }, ] 我们也可以指定 id 获取响应 snippet http http://127.0.0.1:8000/snippets/2/ HTTP/1.0 200 OK ... { \"code\": \"print \\\"hello, world\\\"\\n\", \"id\": 2, \"language\": \"python\", \"linenos\": false, \"style\": \"friendly\", \"title\": \"\" } 类似地 , 我们也可以使用浏览器访问获得相同的 json 数据 到目前为止 , 我们做得很好 , 我们编写的序列化 API 和 Django's Forms API 比较相似 , 同时编写了一些常规的Django视图. 我们的 API 没有做什么特殊的事情 , 除了作出json响应外 , 还有一些边缘事件没有处理 , 但至少是一个还有点功能的 Web API. 在教程的第2部分 , 我们将介绍如何对我们的 API 进行改进. "},"05-Web框架/Django-Rest-Framework/Tutorial 2 Requests and Responses.html":{"url":"05-Web框架/Django-Rest-Framework/Tutorial 2 Requests and Responses.html","title":"Tutorial 2 Requests and Responses","keywords":"","body":"Tutorial 2: Requests and Responses 从这节开始, 我们会接触到 REST 框架的核心. 让我们介绍一些基本构建组件 Request对象 🍀 REST framework 引入了一个 Request 对象 , 它扩展了常规的 HttpRequest , 并提供了灵活的请求解析 . Request 对象的核心功能是 request.data 属性 , 它和 request.POST 属性很相似 , 但是它对 Web APIs 更加有用 request.POST # 只处理表单数据,仅用于 'POST' 方法. request.data # 处理任意数据,可以用于 'POST', 'PUT' 和 'PATCH' 方法. Response对象 🍀 REST framework 也引入了 Response 对象 , 它是一类用为渲染和使用内容协商来决定返回给客户端的正确内容类型的 TemplateResponse return Response(data) # 按照客户的需求类型来渲染内容 状态码 🍀 在视图中使用HTTP状态码并不总是易读的 , 错误代码很容易被忽略 . REST framework 为每一个状态码提供了更明确的标识符，例如状态模块中的 HTTP_400_BAD_REQUEST , 使用这些标识符代替纯数字标识符是一个不错的注意 装饰API视图 🍀 REST framework 提供了两个装饰器来编写API视图 @api_view , 用于 FBV (function based view) APIView , 用于 CBV (class-based view) 这些装饰器提供了一些功能 , 例如确保从你的视图中获取 Request 对象 , 以及在 Response 对象中添加上下文 同时还提供一些行为 , 例如在合适的时候返回 405 Method Not Allowed 响应 , 例如处理在访问错误输入的 request.data 时出现的 ParseError 异常 协同工作 🍀 好了 , 让我们开始使用这些新组件来写一些视图吧 在 views.py 中我们不再需要 JSONResponse 类 , 现在删除它们 , 我们可以轻微地重构我们的视图 from rest_framework import status from rest_framework.decorators import api_view from rest_framework.response import Response from snippets.models import Snippet from snippets.serializers import SnippetSerializer @api_view(['GET', 'POST']) def snippet_list(request): \"\"\" 列出所有的 snippets,或者创建一个新的 snippet. \"\"\" if request.method == 'GET': snippets = Snippet.objects.all() serializer = SnippetSerializer(snippets, many=True) return Response(serializer.data) elif request.method == 'POST': serializer = SnippetSerializer(data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data, status=status.HTTP_201_CREATED) return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) 我们的实例视图是前面的修改版 , 更简洁 , 和我们使用的 Form API 很相似 , 同时使用了命名状态码 , 让响应代码意义更明显 下面是 views.py 中单个 snippet 的视图 @api_view(['GET', 'PUT', 'DELETE']) def snippet_detail(request, pk): \"\"\" 检索、更新或删除 snippet. \"\"\" try: snippet = Snippet.objects.get(pk=pk) except Snippet.DoesNotExist: return Response(status=status.HTTP_404_NOT_FOUND) if request.method == 'GET': serializer = SnippetSerializer(snippet) return Response(serializer.data) elif request.method == 'PUT': serializer = SnippetSerializer(snippet, data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data) return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) elif request.method == 'DELETE': snippet.delete() return Response(status=status.HTTP_204_NO_CONTENT) 这应该是非常熟悉的 , 与常规的Django视图并没有太多的不同 请注意 , 我们不再明确指定请求或响应的上下文类型 , request.data 可以处理的 json 格式的请求 , 同样也可以处理其他格式 . 同样的 , 我们允许 REST framework 将响应对象的数据渲染成正确的内容类型返回给客户端 在URLs中添加可选格式后缀 🍀 为了充分利用我们的响应不再是单一内容类型的事实 , 我们可以在API尾部添加格式后缀 , 格式后缀为我们提供了一个参考的格式 , 这意味着我们的API将能够处理诸如 http://example.com/api/items/4.json 之类的url 在视图中添加一个 format 关键字参数 , 像这样 def snippet_list(request, format=None): 和 def snippet_detail(request, pk, format=None): 现在更新 snippets/urls.py , 在已经存在的URL中添加一个 format_suffix_patterns 集合 from django.conf.urls import url from rest_framework.urlpatterns import format_suffix_patterns from snippets import views urlpatterns = [ url(r'^snippets/$', views.snippet_list), url(r'^snippets/(?P[0-9]+)$', views.snippet_detail), ] urlpatterns = format_suffix_patterns(urlpatterns) 我们并不需要添加这些额外的url , 但是它为我们提供了一种简单明了的方式来指定特定的格式 进行测试 🍀 继续像我们在教程第1部分中所做的那样 , 通过命令行测试API , 所有的工作都是类似的 , 同时我们可以很好地处无效请求产生的错误 我们可以像之前一样 , 获得 snippets 列表 http http://127.0.0.1:8000/snippets/ HTTP/1.1 200 OK ... [ { \"id\": 1, \"title\": \"\", \"code\": \"foo = \\\"bar\\\"\\n\", \"linenos\": false, \"language\": \"python\", \"style\": \"friendly\" }, { \"id\": 2, \"title\": \"\", \"code\": \"print \\\"hello, world\\\"\\n\", \"linenos\": false, \"language\": \"python\", \"style\": \"friendly\" } ] 我们可以通过使用 Accept 响应头控制返回响应的格式 : http http://127.0.0.1:8000/snippets/ Accept:application/json # Request JSON http http://127.0.0.1:8000/snippets/ Accept:text/html # Request HTML 或者在URL后添加格式后缀 : http http://127.0.0.1:8000/snippets.json # JSON 后缀 http http://127.0.0.1:8000/snippets.api # 可浏览的 API 后缀 同样的 , 我们可以使用 Content-Type 头控制我们请求的格式 # POST using form data http --form POST http://127.0.0.1:8000/snippets/ code=\"print 123\" { \"id\": 3, \"title\": \"\", \"code\": \"print 123\", \"linenos\": false, \"language\": \"python\", \"style\": \"friendly\" } # POST using JSON http --json POST http://127.0.0.1:8000/snippets/ code=\"print 456\" { \"id\": 4, \"title\": \"\", \"code\": \"print 456\", \"linenos\": false, \"language\": \"python\", \"style\": \"friendly\" } 如果你使用 --debug 参数 , 那么你可以看到请求头中的请求类型 使用浏览器打开 http://127.0.0.1:8000/snippets/ 浏览可视化 🍀 由于 API 响应类型是根据客户端的请求进行选择的 , 因此 , 当使用 web 浏览器请求的时候 , 默认会使用 HTML 格式来表示资源 , 这允许 API 返回一个完整的浏览器可视的 HTML 表示 拥有一个浏览器可视化的 API 是非常有用的 , 这会使得开发和使用 API 变的极为简单 , 这也让其他开发者更容易查看和使用你的 API 查看 browsable api 主题获取更更多关于 browsable API 的信息 , 比如 特性 , 定制 下一步 🍀 在教程的第3部分, 我们将开始使用基于类的视图(CBV) , 并介绍如何使用通用的视图来减少代码量 "},"05-Web框架/Django-Rest-Framework/Tutorial 3 Class-based Views.html":{"url":"05-Web框架/Django-Rest-Framework/Tutorial 3 Class-based Views.html","title":"Tutorial 3 Class-based Views","keywords":"","body":"Tutorial 3: Class-based Views 我们也可以使用基于类的视图编写我们的 API , 如我们所见 , 这是一个有利的模式 , 允许我们重用共同的功能 , 使3我们的代码不重复 用基于类的视图重构我们的API 🍀 重构 views.py from snippets.models import Snippet from snippets.serializers import SnippetSerializer from django.http import Http404 from rest_framework.views import APIView from rest_framework.response import Response from rest_framework import status class SnippetList(APIView): \"\"\" 列出所有的 snippets,或者创建一个新的 snippet. \"\"\" def get(self, request, format=None): snippets = Snippet.objects.all() serializer = SnippetSerializer(snippets, many=True) return Response(serializer.data) def post(self, request, format=None): serializer = SnippetSerializer(data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data, status=status.HTTP_201_CREATED) return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) 到目前为止 , 一起都很好 , 它和之前的情况非常类似 , 但我们可以更好的区分不同的 HTTP 方法 , 我们需要继续更新 views.py 中的实例视图 class SnippetDetail(APIView): \"\"\" 检索、更新或删除 snippet. \"\"\" def get_object(self, pk): try: return Snippet.objects.get(pk=pk) except Snippet.DoesNotExist: raise Http404 def get(self, request, pk, format=None): snippet = self.get_object(pk) serializer = SnippetSerializer(snippet) return Response(serializer.data) def put(self, request, pk, format=None): snippet = self.get_object(pk) serializer = SnippetSerializer(snippet, data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data) return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) def delete(self, request, pk, format=None): snippet = self.get_object(pk) snippet.delete() return Response(status=status.HTTP_204_NO_CONTENT) 接下来 , 我们还需要用基于类的视图的方式 , 重构 snippets/urls.py from django.conf.urls import url from rest_framework.urlpatterns import format_suffix_patterns from snippets import views urlpatterns = [ url(r'^snippets/$', views.SnippetList.as_view()), url(r'^snippets/(?P[0-9]+)/$', views.SnippetDetail.as_view()), ] urlpatterns = format_suffix_patterns(urlpatterns) 至此 , 如果你运行你的开发服务器 , 那么一切还是和之前的一样 使用mixins 🍀 使用基于类的视图的一大优势是 , 它允许我们轻松地创建可重用的行为 到目前为止 , 我们所使用的 create/retriev/update/delete 操作和我们所创建的任何模型API视图都是非常相似 , 这些常见的行为是在 REST 框架的 mixin 类中实现的 让我们看看如何使用 mixin 类来编写 views.py 模块 from snippets.models import Snippet from snippets.serializers import SnippetSerializer from rest_framework import mixins from rest_framework import generics class SnippetList(mixins.ListModelMixin, mixins.CreateModelMixin, generics.GenericAPIView): queryset = Snippet.objects.all() serializer_class = SnippetSerializer def get(self, request, *args, **kwargs): return self.list(request, *args, **kwargs) def post(self, request, *args, **kwargs): return self.create(request, *args, **kwargs) 我们将花一点时间来看看这里到底发生了什么 , 我们使用 GenericAPIView 来构建我们的视图 , 并添加 ListModelMixin 和 CreateModelMixin 基类提供了核心功能 , mixin 类提供了.list() 和 .create() , 然后我们绑定 get 和 post 方法到合适的动作, 到目前为止 , 已经变得足够简单 class SnippetDetail(mixins.RetrieveModelMixin, mixins.UpdateModelMixin, mixins.DestroyModelMixin, generics.GenericAPIView): queryset = Snippet.objects.all() serializer_class = SnippetSerializer def get(self, request, *args, **kwargs): return self.retrieve(request, *args, **kwargs) def put(self, request, *args, **kwargs): return self.update(request, *args, **kwargs) def delete(self, request, *args, **kwargs): return self.destroy(request, *args, **kwargs) 类似地 , 我们使用 GenericAPIView 类提供核心功能 , 添加 mixins 提供 .retrieve() , .update() and .destroy() 动作 使用基于generic类的视图 🍀 我们使用 mixin 类使用比之前较少的代码编写视图 , 但我们可以更进一步 , REST framework 提供了一组已经混合的 generics 视图 , 我们可以使用它来进一步缩减 views.py 模块 from snippets.models import Snippet from snippets.serializers import SnippetSerializer from rest_framework import generics class SnippetList(generics.ListCreateAPIView): queryset = Snippet.objects.all() serializer_class = SnippetSerializer class SnippetDetail(generics.RetrieveUpdateDestroyAPIView): queryset = Snippet.objects.all() serializer_class = SnippetSerializer 现在我们的代码已经越来越像Django的使用了 接下来 , 我们将介绍本教程的第4部分 , 我们将了解如何处理 API 的认证和权限 "},"05-Web框架/Django-Rest-Framework/Tutorial 4 Authentication & Permissions.html":{"url":"05-Web框架/Django-Rest-Framework/Tutorial 4 Authentication & Permissions.html","title":"Tutorial 4 Authentication & Permissions","keywords":"","body":"Tutorial 4: Authentication & Permissions 当前 , 我们的 API 没有限制, 谁都可以编辑或删除 snippets , 我们需要一些更高级的行为来确保 : 代码片段总是与创建者联系在一起 只有授权用户才能创建 snippets 只有 snippet 创建者可以更新或者删除它 未授权的请求只有只读权限 添加信息到模型中 🍀 我们需要对我们的 Snippet 模型类做一些修改 , 首先 , 添加两个字段 , 一个用来代表代码片段的创建者 , 另一个用来存储高亮显示的HTML代码 修改 models.py 添加字段到 Snippet 模型中 owner = models.ForeignKey('auth.User', related_name='snippets', on_delete=models.CASCADE) highlighted = models.TextField() 同时 , 我们需要确保 , 模型在保存的时候 , 使用 pyments 代码高亮库填充 highlighted 字段 我们需要额外导入 : from pygments.lexers import get_lexer_by_name from pygments.formatters.html import HtmlFormatter from pygments import highlight 现在我们可以模型类中添加 .save() 方法 : def save(self, *args, **kwargs): \"\"\" 使用 `pygments` 库来创建代码高亮的HTML代替 snippet \"\"\" lexer = get_lexer_by_name(self.language) linenos = 'table' if self.linenos else False options = {'title': self.title} if self.title else {} formatter = HtmlFormatter(style=self.style, linenos=linenos, full=True, **options) self.highlighted = highlight(self.code, lexer, formatter) super(Snippet, self).save(*args, **kwargs) 完成这些工作后 , 我们需要更新数据库表 , 通常我们会创建一个数据库迁移来完成这个任务 , 但是为了本教程的目的 , 我们只需要删除原来的数据库 , 然后重新创建 rm -f db.sqlite3 rm -r snippets/migrations python manage.py makemigrations snippets python manage.py migrate 你需要创建一些不同的用户 , 用来测试 API , 最快的方式是使用 createsuperuser 命令 python manage.py createsuperuser 为我们的模型添加端口 🍀 现在我们已经创建了一些用户 , 我们最好将用户添加到我们的 API , 我们很容易创建一个新的序列 , 在serializers.py 文件中添加 : from django.contrib.auth.models import User class UserSerializer(serializers.ModelSerializer): snippets = serializers.PrimaryKeyRelatedField(many=True, queryset=Snippet.objects.all()) class Meta: model = User fields = ('id', 'username', 'snippets') 因为 snippets 与 User 模型是反向关系 , 使用 ModelSerializer 类 , 默认不会包含它 , 所以我们需要手动为用户序列添加这个字段 我们还需要添加两个视图到 views.py 中 , 我们为用户添加只读视图, 因此我们使用基于视图的一般类 ListAPIView 和 RetrieveAPIView from django.contrib.auth.models import User class UserList(generics.ListAPIView): queryset = User.objects.all() serializer_class = UserSerializer class UserDetail(generics.RetrieveAPIView): queryset = User.objects.all() serializer_class = UserSerializer 确保导入了 UserSerializer 类 from snippets.serializers import UserSerializer 最后我们需要修改 URL 配置 , 添加这些视图到 API 中 , 添加以下内容到 urls.py 中 url(r'^users/$', views.UserList.as_view()), url(r'^users/(?P[0-9]+)/$', views.UserDetail.as_view()), 将Snippets与Users关联 🍀 现在 , 如果我们创建一个 snippet , 我们没法将用户和创建的 snippet 实例联系起来 , 虽然用户不是序列表示的部分 , 但是它代表传入请求的一个属性 我们通过重写 snippet 视图的 .perform_create() 方法来处理这个问题 , 它允许我们修改如何保存实例 , 并且处理传入请求或请求的URL中隐含的任何信息 在 SnippetList 类中 , 添加以下方法 : def perform_create(self, serializer): serializer.save(owner=self.request.user) 现在 , 我们序列的 create() 方法将会传入一个有效请求数据的 owner 字段 更新我们的serializer 🍀 现在 , snippets 和创建他们的用户已经建立了联系 , 更新我们的 SnippetSerializer 来表示用户 , 在 serializers.py 中添加字段 owner = serializers.ReadOnlyField(source='owner.username') PS : 确保你在 Meta 的字段列表中也添加了 owner 这个字段会做一些有趣的事情 , source 参数控制哪个属性被作用于一个字段 , 并且可以指向 serialized 实例上的任何属性 , 它也能像上面一样使用点标记 (.) , 在这种情况下 , 它将遍历给定的属性 , 就像Django的模板语言一样 我们添加的字段是无类型的 ReadOnlyField 类 , 与其他类型字段 , 如 CharField , BooleanField 等相比 , 无类型的 ReadOnlyfField 总是只读的 , 它用于序列化表示 , 但是不能用于数据反序列化时更新模型实例 , 在这里我们也可以使用 CharField(read_only=True) 为视图添加权限 🍀 现在 , snippets 与 users 已经相关联 , 我们希望确保只有经过身份验证的用户才能创建 , 更新和删除 snippets REST framework 中包含了许多权限类 , 我们可以使用这些类来限制谁可以访问给定的视图 , 在这种情况下 , 我们需要 使用IsAuthenticatedOrReadOnly , 它将确保经过身份验证的请求获得读写访问 , 而未经身份验证的请求则只有只读权限 首先 , 在 views.py 中导入如下代码 : from rest_framework import permissions 然后在 SnippetList 和 SnippetDetail 视图类中添加如下属性 permission_classes = (permissions.IsAuthenticatedOrReadOnly,) 在可浏览API中添加登录 🍀 如果你打开浏览器并操控可浏览的API , 你将发现你不再有创建新的 snippets 的权限 , 为了做到这一点 , 我们需要以用户的身份登录 我们可以在我们的项目级别的 URLconf : urls.py 文件中添加一个登录视图 添加入到语句 from django.conf.urls import include 并添加一个包含登录和注销视图的 url urlpatterns += [ url(r'^api-auth/', include('rest_framework.urls')), ] r'^api-auth/' 可以使用你想要的URL 现在 , 如果再次打开浏览器 , 刷新页面 , 你将可以看到一个 Login 链接在页面的右上角 , 现在可以使用已经创建的用户登录 , 创建 snippets 一旦你创建了一些 snippets , 访问 '/user/' 端 , 你会注意到在每个用户的 snippets 字段 , 会显示跟用户有关的 snippets id 对象级别权限 🍀 虽然我们希望所有 snippets 都能被任何人看到 , 但是也要确保只有创建该 snippets 的用户才能更新或删除它 要做到这一点 , 我们需要创建一个用户权限 在 snippets 应用下 , 创建一个新文件 , permissions.py from rest_framework import permissions class IsOwnerOrReadOnly(permissions.BasePermission): \"\"\" 自定义仅允许对象的owners可修改的权限 \"\"\" def has_object_permission(self, request, view, obj): # 读权限允许任何请求, # 因此我们总是允许GET,HEAD或者OPTIONS请求 if request.method in permissions.SAFE_METHODS: return True # 写权限仅允许该snippet对戏那个的所有者,owner return obj.owner == request.user 现在 , 通过编辑 SnipetDetail 视图类中的 permission_classes 属性 ,我们可以添加自定义权限到我们的 snippet 实例端 permission_classes = (permissions.IsAuthenticatedOrReadOnly, IsOwnerOrReadOnly,) 确保导入 IsOwnerOrReadOnly 类 from snippets.permissions import IsOwnerOrReadOnly 现在 , 如果你再使用浏览器 , 你会发现只有你登录与创建 snippets 一致的用户 , 你才有权限使用 DELETE 和 PUT 动作 验证API 🍀 由于现在 API 有权限集合 , 在我们需要编辑任何 snippets 的时候 , 需要认证我们的请求 , 我们没有设置他任何认证类 ( authentication classes ) , 默认情况下只有 SessionAuthentication 和 BasicAuthentication 当我们通过浏览器进行交互时 , 我们可以登录 , 浏览器会话 (session) 将为请求提供认证 如果我们以编程的方式使用API , 我们需要为每个请求提供明确的 认证凭证 如果我们尝试在没有认证的情况下创建 snippet , 我们会获得一个 error http POST http://127.0.0.1:8000/snippets/ code=\"print 123\" { \"detail\": \"Authentication credentials were not provided.\" } 我们可以通过提供之前创建的用户的用户名和密码 , 来创建 snippet http -a admin:password123 POST http://127.0.0.1:8000/snippets/ code=\"print 789\" { \"id\": 1, \"owner\": \"admin\", \"title\": \"foo\", \"code\": \"print 789\", \"linenos\": false, \"language\": \"python\", \"style\": \"friendly\" } 概要 🍀 我们的 API 已经具有一个相当精细的权限集合 , 同时为系统用户和他们创建的 snippets 提供了端点 在教程的第5部分 , 我们将介绍如何为高亮的 snippets 创建一个HTML端点 , 将所有内容联系起来 , 同时为系统中的关系使用超链接提高我们 API 的凝聚力 "},"05-Web框架/Django-Rest-Framework/Tutorial 5 Relationships & Hyperlinked APIs.html":{"url":"05-Web框架/Django-Rest-Framework/Tutorial 5 Relationships & Hyperlinked APIs.html","title":"Tutorial 5 Relationships & Hyperlinked APIs","keywords":"","body":"Tutorial 5: Relationships & Hyperlinked APIs 目前 , 我们的API中的关系是通过使用主键来表示的 . 在本教程的这一部分中 , 我们将改进我们的API的内聚性和可见性 , 通过使用超链接来实现关系 为我们的根API创建一个端点 🍀 现在我们有了 'snippets' 和 'users' 的端点 , 但是没有为我们的API设置单独的入口 . 为了创建一个单独的入口 , 我们将使用一个常规的基于函数的视图以及 @api_view 装饰器创建一个入口端点 . 在文件 snippets/views.py 中添加 : from rest_framework.decorators import api_view from rest_framework.response import Response from rest_framework.reverse import reverse @api_view(['GET']) def api_root(request, format=None): return Response({ 'users': reverse('user-list', request=request, format=format), 'snippets': reverse('snippet-list', request=request, format=format) }) 在这里我们需要注意两点 , 首先我们使用 REST framework 的 reverse 方法限定返回的 URLs , 其次 , URL 格式使用方便的名字作为标识符 , 稍后会在 snippets/urls.py 中声明 创建一个高亮的snippets断点 🍀 还有一个明显的事情就是我们的 pastebin API 缺乏代码高亮的端点 与我们其他的API端点不同 , 我们不想使用 JSON , 而只使用 HTML 显示 . REST framework 提供了两种渲染方式 , 一种是用模板渲染 , 另一种是用预渲染 HTML , 在这个端点 , 我们使用第二种渲染方式 另一个需要我们思考的是 , 在创建高亮代码视图的时候 , 高亮视图在通用视图中是不存在的 , 我们不会返回一个对象实例 , 而是返回对象的一个属性 我们不使用 generic 视图 , 而是通过基础类 , 在 snippets/views.py 中创建我们自己的 .get() 方法 from rest_framework import renderers from rest_framework.response import Response class SnippetHighlight(generics.GenericAPIView): queryset = Snippet.objects.all() renderer_classes = (renderers.StaticHTMLRenderer,) def get(self, request, *args, **kwargs): snippet = self.get_object() return Response(snippet.highlighted) 像往常一样 , 我们需要添加新的视图到 URL 配置中 , 文件snippets/urls.py url(r'^$', views.api_root), 然后为高亮 snippet 添加一个url样式 url(r'^snippets/(?P[0-9]+)/highlight/$', views.SnippetHighlight.as_view()), 为我们的API添加超链接 🍀 在Web API设计中 , 处理实体之间的关系是一项非常有挑战的事情 , 代表一种关系可以有很多种方式 使用主键 在实体间使用超链接 在相关的实体上使用唯一的 slug 字段 使用相关实体的默认字符串 在父表示中使用嵌套的实体 其他自定义的表示 REST framework 支持以上所有的方式 , 正向或反向关系均可以使用 , 或者像使用一般外键一样使用自定义的管理方式 在这种情况下 , 我们在实体间使用超链接方法 , 为了达到目的 , 我们将修改我们的序列 ( serializers ) , 扩展 HyperlinkedModelSerializer 代替 ModelSerializer HyperlinkedModelSerializer 和 ModelSerializer 有以下几点不同 : 默认不包含 id 字段 它包含一个 url 字段 , 使用 HyperlinkedIdentityField 关系使用 HyperlinkedRelatedField 代替 PrimaryKeyRelatedField 我们可以快速的将存在的序列重写成超链接的方式 , 文件 snippets/serializers.py class SnippetSerializer(serializers.HyperlinkedModelSerializer): owner = serializers.ReadOnlyField(source='owner.username') highlight = serializers.HyperlinkedIdentityField(view_name='snippet-highlight', format='html') class Meta: model = Snippet fields = ('url', 'id', 'highlight', 'owner', 'title', 'code', 'linenos', 'language', 'style') class UserSerializer(serializers.HyperlinkedModelSerializer): snippets = serializers.HyperlinkedRelatedField(many=True, view_name='snippet-detail', read_only=True) class Meta: model = User fields = ('url', 'id', 'username', 'snippets') 注意 , 我们还新添加了一个 'highlighted' 字段 , 这个字段的类型和 url 字段类型一致 , 只是它指向 'snippet-highlight' 端点 , 而不是 'snippet-detail' 因为我们已经配置了 URLs 后缀 , 比如 '.json' , 同时我们需要在 highlight 字段中指明后缀 , .html 确保我们的URL模式均已命名 🍀 如果我们要使用超链接 API , 我们必须确保对 URL 模型进行命名 , 让我们看看哪些链接需要命名 : 根API指向 user-list 和 snippet-list snippet 序列包含一个指向 snippet-highlight 字段 user 序列包含一个指向 snippet-detail 字段 我们的 snippet 和 user 序列包含 'url' 字段默认指向 '{model_name}-detail' , 当前情况指向 'snippet-detail' 和 'user-detail' 命名加入 URL 配置之后 , snippets/urls.py 应该是下面这样子 : from django.conf.urls import url, include from rest_framework.urlpatterns import format_suffix_patterns from snippets import views # API endpoints urlpatterns = format_suffix_patterns([ url(r'^$', views.api_root), url(r'^snippets/$', views.SnippetList.as_view(), name='snippet-list'), url(r'^snippets/(?P[0-9]+)/$', views.SnippetDetail.as_view(), name='snippet-detail'), url(r'^snippets/(?P[0-9]+)/highlight/$', views.SnippetHighlight.as_view(), name='snippet-highlight'), url(r'^users/$', views.UserList.as_view(), name='user-list'), url(r'^users/(?P[0-9]+)/$', views.UserDetail.as_view(), name='user-detail') ]) 添加分页 🍀 users 和 snippets 的列表视图可能会返回大量的实例 , 所以我们要对返回的结果进行分页 , 并允许客户端访问每个页面 我们可以改变默认的列表样式来使用分页 , 在 tutorial/settings.py 文件 , 添加如下配置 REST_FRAMEWORK = { 'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination', 'PAGE_SIZE': 10 } REST framework 的所有设置都是在 settings 中 REST_FRAMEWORK 字典中的 , 它可以帮我们区分项目中的其他配置 同时 , 我们也可以自定义分页的样式 , 在这里 , 我们使用默认方式 浏览API 🍀 如果我们打开浏览器 , 并访问可浏览的 API , 你会发现你可以使用下面的链接使用 API 你也可以看到 snippet 实例的 'highlight' 链接 , 这些链接会返回高亮的 HTML 代码 在教程的第6部分 , 我们会介绍怎么使用 ViewSets 和 Routers 通过更少的代码 , 实现我们的 API "},"05-Web框架/Django-Rest-Framework/Tutorial 6 ViewSets & Routers.html":{"url":"05-Web框架/Django-Rest-Framework/Tutorial 6 ViewSets & Routers.html","title":"Tutorial 6 ViewSets & Routers","keywords":"","body":"Tutorial 6: ViewSets & Routers REST framework 中有一个 ViewSets 的抽象 , 它可以让开发者将精力集中在API的状态和交互上 , 同时帮助开发者 , 基于共同约定 , 自动处理 URL 构建 ViewSet 类几乎和 View 类一样 , 除了它提供的 read 或者 update 操作 , 而不是像 get 或 put 一样的方法 一个 ViewSet 类在它被实例化成一个视图集合的最后时刻 , 通过一个处理复杂 URL 配置的 Router 类绑定 , 且只绑定一个方法集合 使用ViewSets重构 🍀 首先使用单个 UserViewSet 视图重构 UserList 和 UserDetail 视图 文件 snippets/views.py from rest_framework import viewsets class UserViewSet(viewsets.ReadOnlyModelViewSet): \"\"\" 该viewset自动提供 list 和 detail 功能 \"\"\" queryset = User.objects.all() serializer_class = UserSerializer 这里我们使用 ReadOnlyModelViewSet 类自动提供默认的 'read-only' 操作 . 我们需要像使用常规视图一样 , 设置 queryset 和 serializer_class 属性 , 但是我们不再需要为两个分开的类提供相同的信息 接下来替换 SnippetList , SnippetDetail 和 SnippetHighlight 视图类 from rest_framework.decorators import detail_route from rest_framework.response import Response class SnippetViewSet(viewsets.ModelViewSet): \"\"\" 这个viewset自动提供 list,create,retrieve,update,destroy功能 此外,我们还提供了一个额外的\"高亮显示\"功能 \"\"\" queryset = Snippet.objects.all() serializer_class = SnippetSerializer permission_classes = (permissions.IsAuthenticatedOrReadOnly, IsOwnerOrReadOnly,) @detail_route(renderer_classes=[renderers.StaticHTMLRenderer]) def highlight(self, request, *args, **kwargs): snippet = self.get_object() return Response(snippet.highlighted) def perform_create(self, serializer): serializer.save(owner=self.request.user) 这一次我们使用了 ModelViewSet 类获得默认的完整的读写操作 注意 , 我们同时使用了 @detail_route 装饰器 , 用于创建一个自定义动作 , 即 highlight , 这个装饰器可以用于添加任何不适合 create/update/delete 方式的自定义端点 使用@detail_route 装饰器自定义的动作默认会响应 GET 请求 , 如果我们需要响应 POST 请求 , 我们可以使用 methods 参数 自定义动作的默认 URLs 取决于它们的名字 , 如果你想改变 url 构建方法 , 你可以在使用装饰器的时候传入 url_path 关键字参数 将ViewSets绑定到URLs 🍀 视图的处理方法仅会按照我们的 URL conf 对相应方法进行绑定 , 现在我们为我们为您的 ViewSets 显示地创建一个视图集合 , 来看看发生了什么 snippets/urls.py 文件 , 我们绑定我们的 ViewSet 类到一组具体的视图 from snippets.views import SnippetViewSet, UserViewSet, api_root from rest_framework import renderers snippet_list = SnippetViewSet.as_view({ 'get': 'list', 'post': 'create' }) snippet_detail = SnippetViewSet.as_view({ 'get': 'retrieve', 'put': 'update', 'patch': 'partial_update', 'delete': 'destroy' }) snippet_highlight = SnippetViewSet.as_view({ 'get': 'highlight' }, renderer_classes=[renderers.StaticHTMLRenderer]) user_list = UserViewSet.as_view({ 'get': 'list' }) user_detail = UserViewSet.as_view({ 'get': 'retrieve' }) 注意我们如何从每个 ViewSet 类 , 通过绑定 http 方法到响应的动作来创建多个视图 现在 , 我们将我们的资源绑定到了具体的视图 , 我们可以像往常一样将我们的视图注册到 url 配置中 urlpatterns = format_suffix_patterns([ url(r'^$', api_root), url(r'^snippets/$', snippet_list, name='snippet-list'), url(r'^snippets/(?P[0-9]+)/$', snippet_detail, name='snippet-detail'), url(r'^snippets/(?P[0-9]+)/highlight/$', snippet_highlight, name='snippet-highlight'), url(r'^users/$', user_list, name='user-list'), url(r'^users/(?P[0-9]+)/$', user_detail, name='user-detail') ]) 使用Routers 🍀 因为我们使用 ViewSet 代替 View , 实际上我们不需要自己设计 URL 配置 , 我们可以通过 Router 类 , 将资源 (resources) , 视图 (views) , urls自动联系起来 , 我们只需要使用 Router 类注册合适的视图集合 重写 snippets/urls.py from django.conf.urls import url, include from rest_framework.routers import DefaultRouter from snippets import views # 创建路由器并注册视图 router = DefaultRouter() router.register(r'snippets', views.SnippetViewSet) router.register(r'users', views.UserViewSet) # API URLs 由路由器自动确定 urlpatterns = [ url(r'^', include(router.urls)) ] 使用 router 注册的视图集合提供一个 urlpattern , 包括两个参数 - 视图的 URL 前缀和视图集合本身 我们使用默认 DefaultRouter 类也会自动为我们创建API根视图 , 现在我们可以从 views 模块中删除 api_root 方法 权衡使用views和viewsets 🍀 viewsets 是一个非常有用的抽象 , 它可以确保 URL 原型和你的 API 保持一致 , 最大限度的减少代码量 , 允许你将精力放在 API 的交互和表示上 , 而不是放在编写 URL conf 上 这并不意味在所有地方都要使用 viewsets , 在使用基于类的视图和基于函数的视图时 , 需要进行权衡 , 使用 viewsets 没有单独构建 views 明确 在教程第7部分, 我们将介绍 , 如何添加一个 APP schema , 并使用客户端库或命令行工具与我们的 API进行交互 "},"05-Web框架/Django-Rest-Framework/Tutorial 7 Schemas & client libraries.html":{"url":"05-Web框架/Django-Rest-Framework/Tutorial 7 Schemas & client libraries.html","title":"Tutorial 7 Schemas & client libraries","keywords":"","body":"Tutorial 7: Schemas & client libraries schema 是一种机器可读的文档 , 用于描述可用的API端点 , URLS , 以及他们支持的操作 schema 可以用于自动生成文档 , 也可以用于驱动可以与API交互的动态客户端库 Core API 🍀 为了提供 schema 支持, REST 框架使用 Core API Core API 是用于描述 APIs 的文档规范 . 它可以用来提供内部可用端点内部表示格式和API暴露的可能的交互 . 它可以用于服务端或客户端 当用于服务端时 , Core API 允许API支持呈现 schema 或渲染超媒体格式 当用于客户端 , Core API 允许动态驱动的客户端库与任何支持 schema 或超媒体格式的 API 交互 添加一个schema 🍀 REST framework 支持明确定义的 schema 视图 , 或自动生成的 schemas. 由于我们使用 ViewSets 和Routers , 我们可以很简单的自动生成 schema 你需要安装 coreapi pip install coreapi 在 URL 配置中包含一个自动生成的 schema 视图 from rest_framework.schemas import get_schema_view schema_view = get_schema_view(title='Pastebin API') urlpatterns = [ url(r'^schema/$', schema_view), ... ] 如果你使用浏览器访问 API 根节点 , 在选项中 , 你可以看到 corejson 选项变成可用的状态 我们也可以使用命令行 , 通过在 Accept 请求头中指定期望的内容类型 , 请求 schema $ http http://127.0.0.1:8000/schema/ Accept:application/coreapi+json HTTP/1.0 200 OK Allow: GET, HEAD, OPTIONS Content-Type: application/coreapi+json { \"_meta\": { \"title\": \"Pastebin API\" }, \"_type\": \"document\", ... 默认的输出风格使用的是Core JSON编码 其他的 schema 格式 , 比如 Open API (以前称作Swagger) , 同样支持 使用命令行客户端 🍀 现在 , 我们的 API 暴露了一个 schema 端点 , 我们可以使用动态客户端库与 API 交互 , 为了证明这点 , 我们是用 Core API 命令行客户端 命令行客户端需要使用 coreapi-cli 包 pip install coreapi-cli 通过命令行检查 , coreapi-cli 是否可用 $ coreapi Usage: coreapi [OPTIONS] COMMAND [ARGS]... Command line client for interacting with CoreAPI services. Visit http://www.coreapi.org for more information. Options: --version Display the package version number. --help Show this message and exit. Commands: ... 首先我们使用命令行客户端加载 API schema $ coreapi get http://127.0.0.1:8000/schema/ snippets: { highlight(id) list() read(id) } users: { list() read(id) } 我们还没有认证 , 所以我们只能看到只读端点 , 符合我们设计的 API 权限 让我们尝试使用命令行客户端 , 列出已经存在的 snippets $ coreapi action snippets list [ { \"url\": \"http://127.0.0.1:8000/snippets/1/\", \"id\": 1, \"highlight\": \"http://127.0.0.1:8000/snippets/1/highlight/\", \"owner\": \"lucy\", \"title\": \"Example\", \"code\": \"print('hello, world!')\", \"linenos\": true, \"language\": \"python\", \"style\": \"friendly\" }, ... 有些 API 端点依赖命名参数 , 比如 , 我们要获取指定 snippet 的高亮 HTML , 需要提供一个 id $ coreapi action snippets highlight --param id=1 Example ... 认证我们的客户端 🍀 如果我们想要创建 , 编辑 , 删除 snippets , 我们需要认证一个有效的用户 , 这种情况下 , 我们只使用基本身份验证 将 和 替换成真实的用户名和密码 $ coreapi credentials add 127.0.0.1 : --auth basic Added credentials 127.0.0.1 \"Basic \" 现在 , 如果我们重新获取 schema , 我们可以看到所有的可用交互的集合 $ coreapi reload Pastebin API \"http://127.0.0.1:8000/schema/\"> snippets: { create(code, [title], [linenos], [language], [style]) delete(id) highlight(id) list() partial_update(id, [title], [code], [linenos], [language], [style]) read(id) update(id, code, [title], [linenos], [language], [style]) } users: { list() read(id) } 现在我们可以 和这些端点交互 , 比如 , 创建一个新的 snippet $ coreapi action snippets create --param title=\"Example\" --param code=\"print('hello, world')\" { \"url\": \"http://127.0.0.1:8000/snippets/7/\", \"id\": 7, \"highlight\": \"http://127.0.0.1:8000/snippets/7/highlight/\", \"owner\": \"lucy\", \"title\": \"Example\", \"code\": \"print('hello, world')\", \"linenos\": false, \"language\": \"python\", \"style\": \"friendly\" } 删除 snippet coreapi action snippets delete --param id=7 除了使用命令行客户端 , 开发者也可以使用客户端库与你的 API 进行交互 , Python客户端第一个可用的库 Javascript , 将在不久之后发布 有关定义 schema 生成和使用 Core API 客户端库 , 你可以参考完整的文档 回到我们的工作 🍀 我们使用很少的代码 , 拥有了一个完整的可浏览的 pastebin Web API , 它包含一个 schema-driven 客户端库 , 完整的身份认证 , 对象级权限和多格式渲染器 我们走过了设计过程的每一步, 看到了如何使用常规的Django视图进行定制. 你可以在GitHub上查阅最终的代码 tutorial code , 或者在 the sandbox 中进行尝试 到这里 , 我们已经完成了教程 , 如果你想跟多的参与到 REST framework 项目 , 你可以使用以下几种方式 : 在 GitHub 上进行审查 , 提交问题 , 发出 pull requests 加入 REST framework discussion group , 帮助构建社区 在Twitter上关注 作者 , 并发送 hi "},"05-Web框架/Flask/":{"url":"05-Web框架/Flask/","title":"Flask","keywords":"","body":"Flask初识 介绍 🍀 Flask是一个微型框架 , 它基于Python , 并且依赖着两个外部库 : Jinja2模板引擎和Werkzeug WSGI工具集 Flask的 \"微\" (Micro) 并不是意味着把整个Web应用放入到一个Python文件 , 尽管确实可以这么做 , 其主要时候指 , Flask旨在保持代码简介且易于扩展 , 所以Flaks不会为你做太多的选择 , 比如选择什么样的数据库 , 选择什么样的模板引擎 , 这些都取决你 默认情况下 , Flask并不包含数据库抽象层 , 表单验证或者任何其他现有库 (Django) 能处理的 , 相反 , Flask支持扩展 , 这些扩展能够添加功能到你的应用 , 就像是Flask本身实现的一样 . 众多的扩展提供了数据库集成 , 表单验证 , 上传处理 , 多种开放的认证技术等功能 Flask可能是 \"微\" 型的 , 但是它已经能够在各种各样的需求中生产使用 配置和约定 🍀 Flask有许多带有合理默认值的配置项 , 也遵循一些惯例 , 比如 : 按惯例 , 模板和静态文件存储在应用Python源代码树下的子目录中 , 模板文件夹名称与Django一样 \"templates\" , 静态文件夹名称 \"static\" 等等 总之 , Flask由于它的 \"微\" , 当然也可以说 \"轻\" , 使它得到的广泛的应用 , 便于扩展 , 而不像Django那样 , 强大却有时会显得笨重 "},"05-Web框架/Flask/01-Flask - 源码简要说明.html":{"url":"05-Web框架/Flask/01-Flask - 源码简要说明.html","title":"Flask - 源码简要说明","keywords":"","body":"Flask - 源码简要说明 介绍 🍀 内容概况 flask ├── json │ ├── __init__.py │ └── tag.py ├── __init__.py ├── __main__.py ├── _compat.py # 实现关于Python版本兼容性的配置 ├── app.py # 实现WSGI应用程序对象,即Flask ├── blueprints.py # 蓝图 ├── cli.py # 实现简单的命令行应用程序 ├── config.py # 实现与配置相关的对象 ├── ctx.py # 实现上下文相关的对象 ├── debughelpers.py # debug模式相关 ├── globals.py # 定义了所有的全局对象 ├── helpers.py ├── logging.py ├── sessions.py ├── signals.py # 基于blinker的信号 ├── templating.py ├── testing.py ├── views.py # 提供了类似于Djando的CBV └── wrappers.py # 实现了WSGI包装器,即request和response 阅读指引 🍀 本目录下为 Flask 源码阅读相关 , 读者应该对 WSGI 和 socketserver 有一定的了解 , 因为在某些部分这可能成为阻碍 , 相对而言 , Flask 的源码比 Django 要简单得多 , 因为 Django 过于庞大 , 并且耦合度高 如果您有一定的 Web 框架基础 , 对于 Flask 框架的学习可能会变得简单 , 您可以通过阅读源码来获得更多的灵感 , 该目录文章为本人学习交流所撰 , 欢迎交流 如果您没有 Web 框架基础 , 那么您不妨通过 Flask 官方的文档来进行学习 , 链接在此 : Flask官方文档 ; 中文文档 , 该文档翻译不一定准确 , 但是可以借鉴 更多 Flask 相关中文翻译 : http://www.pythondoc.com/ "},"05-Web框架/Flask/02-Flask - 源码之开始.html":{"url":"05-Web框架/Flask/02-Flask - 源码之开始.html","title":"Flask - 源码之开始","keywords":"","body":"Flask - 源码之开始 介绍 🍀 Flask 中文文档比较齐全 , 阅读源码最好的方式应该是从最简单的应用开始 直接使用官方快速入门中的最小的应用开始 from flask import Flask # 实例化Flask对象 # __name__ 为模块名,当前为 __main__ app = Flask(__name__) # 绑定路由 @app.route('/') def hello_world(): return 'Hello World!' if __name__ == '__main__': app.run() 绑定路由 🍀 我们首先实例化了 Flask 对象 , 我们传入了一个参数 import_name , 该参数的概念是给 Flask 一个属于你的应用程序的概念 , 它用于查找文件系统上的资源 , 可以由扩展名用于改进调试信息等 随后写了一个视图函数 hello_world 以及给这个视图函数添加了一个装饰器 ; 当然 , 我们已经知道这个装饰器的作用是添加路由规则 , 我们看看它具体是怎么做的 flask.app.Flask.route() : def route(self, rule, **options): \"\"\" rule:URL规则的字符串 **options:该参数将在实例化Rule对象时作为参数传入 - options参数: - defaults=None,默认值,当URL中无参数,函数需要参数时,使用defaults={'page': 1} - subdomain=None,子域名访问,默认由Map对象提供,即为 '' - methods=None,允许的请求方式 - build_only=False,如果为True,则URL无法匹配但是仍然会构建 - endpoint=None,端点,用于反向生成URL,即:url_for() - strict_slashes=None,对URL最后的 / 符号是否严格要求,默认由Map对象提供,即为True - redirect_to=None,重定向到指定地址 - alias=False,如果为True,则作为另一条端点和参数相同的规则的别名 - host=None,为整个主机提供匹配规则 \"\"\" def decorator(f): # 获取endpoint参数,默认是为None的 # endpoint参数用于反向生成url时使用 endpoint = options.pop('endpoint', None) self.add_url_rule(rule, endpoint, f, **options) return f return decorator app.route('/') == decorator →→ @app.route('/') == @decorator , 于是在此装饰器执行时 , 将执行 self.add_url_rule() , 源码如下 : flask.app.Flask.add_url_rule() : @setupmethod # 该装饰器与调试模式有关,我们可以跳过它 def add_url_rule(self, rule, endpoint=None, view_func=None, provide_automatic_options=None, **options): if endpoint is None: # 返回视图函数的函数名 endpoint = _endpoint_from_view_func(view_func) options['endpoint'] = endpoint # 获取options中的methods参数,它是一个列表 methods = options.pop('methods', None) if methods is None: # 如果view_func为CBV方式,并且有methods参数,否则使用('GET) methods = getattr(view_func, 'methods', None) or ('GET',) if isinstance(methods, string_types): raise TypeError('Allowed methods have to be iterables of strings, ' 'for example: @app.route(..., methods=[\"POST\"])') # 转换成大写 methods = set(item.upper() for item in methods) # 必要属性 required_methods = set(getattr(view_func, 'required_methods', ())) # 强制启用自动选项处理 if provide_automatic_options is None: provide_automatic_options = getattr(view_func, 'provide_automatic_options', None) if provide_automatic_options is None: if 'OPTIONS' not in methods: provide_automatic_options = True required_methods.add('OPTIONS') else: provide_automatic_options = False # 添加必要属性到methods methods |= required_methods # 实例化Rule对象 rule = self.url_rule_class(rule, methods=methods, **options) rule.provide_automatic_options = provide_automatic_options # 生成url并完成绑定 self.url_map.add(rule) if view_func is not None: # view_func为视图函数 old_func = self.view_functions.get(endpoint) if old_func is not None and old_func != view_func: raise AssertionError('View function mapping is overwriting an ' 'existing endpoint function: %s' % endpoint) # 将视图函数加入Flask对象的view_functions属性中,以端点为key # self.view_functions = {} self.view_functions[endpoint] = view_func 启动WSGI服务器 🍀 到这里 , 好像路由与视图函数都准备好了 , 那么接下来就是启动该应用了 , app.run() def run(self, host=None, port=None, debug=None, load_dotenv=True, **options): ...... from werkzeug.serving import run_simple try: # 启动WSGI应用 run_simple(host, port, self, **options) finally: self._got_first_request = False 接下来将会执行 werkzeug.serving.run_simple 中的 inner def inner(): try: fd = int(os.environ['WERKZEUG_SERVER_FD']) except (LookupError, ValueError): fd = None # 创建一个服务器实例 # BaseWSGIServer srv = make_server(hostname, port, application, threaded, processes, request_handler, passthrough_errors, ssl_context, fd=fd) if fd is None: log_startup(srv.socket) # 启动服务器 # BaseWSGIServer.serve_forever() srv.serve_forever() BaseWSGIServer 继承了http.server.HTTPServer , BaseWSGIServer.serve_forever() 如下 : def serve_forever(self): self.shutdown_signal = False try: # 一次处理一个请求直到服务器关闭 # 将BaseWSGIServer对象作为参数传入 HTTPServer.serve_forever(self) except KeyboardInterrupt: pass finally: self.server_close() 由于 HTTPServer 继承自 socketserver.TCPServer , 并且它并没有重写 serve_forever , 所以 , 源代码如下 : def serve_forever(self, poll_interval=0.5): \"\"\"Handle one request at a time until shutdown. Polls for shutdown every poll_interval seconds. Ignores self.timeout. If you need to do periodic tasks, do them in another thread. \"\"\" self.__is_shut_down.clear() try: # XXX: Consider using another file descriptor or connecting to the # socket to wake this up instead of polling. Polling reduces our # responsiveness to a shutdown request and wastes cpu at all other # times. with _ServerSelector() as selector: selector.register(self, selectors.EVENT_READ) while not self.__shutdown_request: ready = selector.select(poll_interval) if ready: self._handle_request_noblock() self.service_actions() finally: self.__shutdown_request = False self.__is_shut_down.set() 到这里 , 我们不妨来对比一下 , 在 Django 中 , 创建服务器实例是由 wsgiref.simple_server.WSGIServer 衍生而来 , 而 WSGIServer 继承了http.server.HTTPServer 而在 Flask 中 , 如上 , 也继承了 http.server.HTTPServer 创建服务器实例 🍀 在 BaseWSGIServer 进行实例化时 , 与 Django 一样 , 都传入了一个 RequestHandlerClass 参数 , 这个参数听名字就知道 , 它是处理请求的核心 , 源码如下 : class BaseWSGIServer(HTTPServer, object): \"\"\"Simple single-threaded, single-process WSGI server.\"\"\" # 省略部分代码 def __init__(self, host, port, app, handler=None, passthrough_errors=False, ssl_context=None, fd=None): if handler is None: # werkzeug.serving.WSGIRequestHandler # Django:django.core.servers.basehttp.WSGIRequestHandler handler = WSGIRequestHandler # RequestHandlerClass为出去self后第二个参数,handler HTTPServer.__init__(self, get_sockaddr(host, int(port), self.address_family), handler) 回到 serve_forever 中 , 调用 self._handle_request_noblock() , 随后再调用 self.process_request(request, client_address) def process_request(self, request, client_address): \"\"\"Call finish_request. Overridden by ForkingMixIn and ThreadingMixIn. \"\"\" self.finish_request(request, client_address) self.shutdown_request(request) 最后调用了 self.finish_request(request, client_address) def finish_request(self, request, client_address): \"\"\"Finish one request by instantiating RequestHandlerClass.\"\"\" self.RequestHandlerClass(request, client_address, self) 实例化WSGI请求处理类 🍀 至此 , 在创建服务器实例 , 也就是 BaseWSGIServer 时 , 已经将该类传入 , 即 WSGIRequestHandler ; 重点来了 , WSGIRequestHandler 自身根本没有构造函数 , 它的基类 socketserver.TCPServer 还是没有 , 继续往上发现 , 关于 RequestHandlerClass 初始化的函数在继承链的顶部 socketserver.BaseServer 中 , 其构造函数如下 : def __init__(self, request, client_address, server): self.request = request self.client_address = client_address self.server = server self.setup() try: # 该函数在WSGIRequestHandler中进行了重构 self.handle() finally: self.finish() WSGIRequestHandler.handler() 如下 : def handle(self): \"\"\"Handles a request ignoring dropped connections.\"\"\" rv = None try: # 其基类的handler rv = BaseHTTPRequestHandler.handle(self) except (socket.error, socket.timeout) as e: self.connection_dropped(e) except Exception: if self.server.ssl_context is None or not is_ssl_error(): raise if self.server.shutdown_signal: self.initiate_shutdown() return rv def handle(self): \"\"\"Handle multiple requests if necessary.\"\"\" self.close_connection = True # 该方法在WSGIRequestHandler被重写 self.handle_one_request() while not self.close_connection: self.handle_one_request() 重写源码如下 : def handle_one_request(self): \"\"\"Handle a single HTTP request.\"\"\" self.raw_requestline = self.rfile.readline() if not self.raw_requestline: self.close_connection = 1 elif self.parse_request(): # 运行WSGI应用服务器 return self.run_wsgi() 视图调度 🍀 摘取 run_wsgi() 部分代码如下 : def run_wsgi(self): if self.headers.get('Expect', '').lower().strip() == '100-continue': self.wfile.write(b'HTTP/1.1 100 Continue\\r\\n\\r\\n') # 环境处理 self.environ = environ = self.make_environ() headers_set = [] headers_sent = [] def write(data): pass def start_response(status, response_headers, exc_info=None): pass def execute(app): # 调用Flask应用实例的__call__方法 # Flask(__name__)(environ, start_response) application_iter = app(environ, start_response) try: for data in application_iter: write(data) if not headers_sent: write(b'') finally: if hasattr(application_iter, 'close'): application_iter.close() application_iter = None try: # self.server = BaseWSGIServer # BaseWSGIServer.app即Flask应用实例 execute(self.server.app) pass Flask.__call__ 源码如下 : def __call__(self, environ, start_response): \"\"\"The WSGI server calls the Flask application object as the WSGI application. This calls :meth:`wsgi_app` which can be wrapped to applying middleware.\"\"\" # WSGI服务器调用Flask应用对象作为WSGI应用 return self.wsgi_app(environ, start_response) def wsgi_app(self, environ, start_response): # 请求上下文对象 ctx = self.request_context(environ) error = None try: try: # 将请求上下文对象推入栈中 # _request_ctx_stack.push(self) # self = RequestContext(environ) ctx.push() # 分派请求 response = self.full_dispatch_request() except Exception as e: error = e response = self.handle_exception(e) except: error = sys.exc_info()[1] raise # 执行Response对象的__call__方法 # 这里将由werkzeug来进行处理 # werkzeug.wrappers.BaseResponse.__call__ # start_response在run_wsgi函数中 # def __call__(self, environ, start_response): \"\"\"Process this response as WSGI application. :param environ: the WSGI environment. :param start_response: the response callable provided by the WSGI server. :return: an application iterator \"\"\" # app_iter, status, headers = self.get_wsgi_response(environ) # start_response(status, headers) # return app_iter return response(environ, start_response) finally: if self.should_ignore_error(error): error = None ctx.auto_pop(error) self.full_dispatch_request() def full_dispatch_request(self): \"\"\"Dispatches the request and on top of that performs request pre and postprocessing as well as HTTP exception catching and error handling. .. versionadded:: 0.7 \"\"\" self.try_trigger_before_first_request_functions() try: request_started.send(self) # 请求预处理 rv = self.preprocess_request() if rv is None: # 进行视图调度 rv = self.dispatch_request() except Exception as e: rv = self.handle_user_exception(e) # 完成请求 return self.finalize_request(rv) 预处理过程如下 : def preprocess_request(self): \"\"\"Called before the request is dispatched. Calls :attr:`url_value_preprocessors` registered with the app and the current blueprint (if any). Then calls :attr:`before_request_funcs` registered with the app and the blueprint. If any :meth:`before_request` handler returns a non-None value, the value is handled as if it was the return value from the view, and further request handling is stopped. \"\"\" # 从请求上下文栈中获取蓝图 bp = _request_ctx_stack.top.request.blueprint # 预处理函数 # self.url_value_preprocessors = {} funcs = self.url_value_preprocessors.get(None, ()) if bp is not None and bp in self.url_value_preprocessors: funcs = chain(funcs, self.url_value_preprocessors[bp]) for func in funcs: # 执行预处理 func(request.endpoint, request.view_args) # 在处理请求之前调用的函数 # self.before_request_funcs = {} funcs = self.before_request_funcs.get(None, ()) if bp is not None and bp in self.before_request_funcs: funcs = chain(funcs, self.before_request_funcs[bp]) for func in funcs: rv = func() if rv is not None: # 有返回值请求处理结束 return rv 视图调度 , self.dispatch_request() def dispatch_request(self): \"\"\"Does the request dispatching. Matches the URL and returns the return value of the view or error handler. This does not have to be a response object. In order to convert the return value to a proper response object, call :func:`make_response`. .. versionchanged:: 0.7 This no longer does the exception handling, this code was moved to the new :meth:`full_dispatch_request`. \"\"\" # 从栈中获取请求上下文对象 req = _request_ctx_stack.top.request if req.routing_exception is not None: self.raise_routing_exception(req) rule = req.url_rule # if we provide automatic options for this URL and the # request came with the OPTIONS method, reply automatically if getattr(rule, 'provide_automatic_options', False) \\ and req.method == 'OPTIONS': return self.make_default_options_response() # otherwise dispatch to the handler for that endpoint # 通过endpoint参数获取视图函数并调用,视图函数参数从获取的请求上下文对象中获取 return self.view_functions[rule.endpoint](**req.view_args) 视图调度完成后 , 接下来就是完成请求了 , 从 dispatch_request 或者 preprocess_request 中已经获取了需要的 rv , 最后就是执行 full_dispatch_request() 中的最后一行 return self.finalize_request(rv) finalize_request() 源码如下 : def finalize_request(self, rv, from_error_handler=False): \"\"\"Given the return value from a view function this finalizes the request by converting it into a response and invoking the postprocessing functions. This is invoked for both normal request dispatching as well as error handlers. Because this means that it might be called as a result of a failure a special safe mode is available which can be enabled with the `from_error_handler` flag. If enabled, failures in response processing will be logged and otherwise ignored. :internal: \"\"\" # 将视图返回值转换成response对象 response = self.make_response(rv) try: # 请求结束后处理 response = self.process_response(response) request_finished.send(self, response=response) except Exception: if not from_error_handler: raise self.logger.exception('Request finalizing failed with an ' 'error while handling an error') # 返回响应 return response 到这里其实我们已经分析的差不多了 , 当 response 对象获取之后 , 一步步往外返回 , 将会返回到 werkzeug 层次 , 当然 werkzeug 也是使用 socketserver 的一部分功能来完成这些操作 , 这一部分就不再分析了 小结 🍀 首先 , 从整体来讲 , 我们可以通过与 Django 进行对比 : Django , 使用 wsgiref + socketserver + http Flask , 使用 werkzeug + socketserver + http 在传输层 , 两者都是使用的 socketserver , 当然它们在 socketserver 的基础上做了一些不同的改变 那么对于 Flask 运行到处理请求流程如下 : 创建 Flask 应用对象 添加路由映射 启动服务器 请求来临 , 创建请求上下文对象 , 并压入请求上下文栈 处理请求 , 从栈中获取对象 , 调用视图 (dispatch_request) 将视图返回值打包成响应对象 完成响应 "},"05-Web框架/Flask/03-Flask - 源码之配置.html":{"url":"05-Web框架/Flask/03-Flask - 源码之配置.html","title":"Flask - 源码之配置","keywords":"","body":"Flask - 源码之配置 介绍 🍀 Flask 中的配置主要使用 flask/config.py 中的两个对象 : Config , ConfigAttribute Config 🍀 Config 是 dict 的子类 , 所以它的一些行为跟 dict 一样 # 为了方便阅读,删除部分代码 # 删除部分以 ... 代表 class Config(dict): def __init__(self, root_path, defaults=None): dict.__init__(self, defaults or {}) self.root_path = root_path def from_envvar(self, variable_name, silent=False): \"\"\" 更新配置,从环境变量中获取,等价于如下: app.config.from_pyfile(os.environ['YOURAPPLICATION_SETTINGS']) \"\"\" rv = os.environ.get(variable_name) ... return self.from_pyfile(rv, silent=silent) def from_pyfile(self, filename, silent=False): \"\"\" 更新配置,从文件中获取配置 \"\"\" filename = os.path.join(self.root_path, filename) # 获取一个module对象 d = types.ModuleType('config') d.__file__ = filename try: with open(filename, mode='rb') as config_file: # 加载配置到,module对象的命名空间中 exec(compile(config_file.read(), filename, 'exec'), d.__dict__) ... self.from_object(d) return True def from_object(self, obj): \"\"\" 更新配置,从对象中获取配置,官方实例: app.config.from_object('yourapplication.default_config') from yourapplication import default_config app.config.from_object(default_config) \"\"\" if isinstance(obj, string_types): obj = import_string(obj) for key in dir(obj): # 获取配置模块的属性列表 if key.isupper(): self[key] = getattr(obj, key) def from_json(self, filename, silent=False): \"\"\" 更新配置,从json文件中获取配置 类似于from_pyfile,但是返回方式不同 \"\"\" filename = os.path.join(self.root_path, filename) try: with open(filename) as json_file: obj = json.loads(json_file.read()) ... return self.from_mapping(obj) def from_mapping(self, *mapping, **kwargs): \"\"\" 更新配置, \"\"\" mappings = [] if len(mapping) == 1: if hasattr(mapping[0], 'items'): mappings.append(mapping[0].items()) else: mappings.append(mapping[0]) ... mappings.append(kwargs.items()) for mapping in mappings: for (key, value) in mapping: if key.isupper(): self[key] = value return True def get_namespace(self, namespace, lowercase=True, trim_namespace=True): \"\"\" 返回包含自己配置选项的字典 namespace:配置的命名空间 \"\"\" def __repr__(self): return '' % (self.__class__.__name__, dict.__repr__(self)) 因为它是字典的子类 , 所以你可以使用 update() 一次性更新多个配置 app.config.update( DEBUG=True, SECRET_KEY='...' ) 关于通过 from_object() 方法来配置 , 你可以利用继承实现如下 : # configmodule.py class Config(object): DEBUG = False TESTING = False DATABASE_URI = 'sqlite://:memory:' class ProductionConfig(Config): DATABASE_URI = 'mysql://user@localhost/foo' class DevelopmentConfig(Config): DEBUG = True class TestingConfig(Config): TESTING = True # demo.py app.config.from_object('configmodule.ProductionConfig') ConfigAttribute 🍀 该类的作用就是为config设置一些属性 class ConfigAttribute(object): \"\"\" 将一个属性转接到Config, 该类是一个描述器类,即实现了__get__方法的类, __get__的作用是,当通过另一个类的实例来访问该对象时, 将会执行__get__方法,在Flask中如下: class Flask(_PackageBoundObject): testing = ConfigAttribute('TESTING') self.config = self.make_config(instance_relative_config) application = Flask(__name__) # 执行ConfigAttribute中的__get__方法 # 该配置在self.config中已被设置 application.testing \"\"\" def __init__(self, name, get_converter=None): self.__name__ = name self.get_converter = get_converter def __get__(self, obj, type=None): \"\"\" obj:即Flask对象 \"\"\" if obj is None: return self rv = obj.config[self.__name__] if self.get_converter is not None: rv = self.get_converter(rv) return rv def __set__(self, obj, value): obj.config[self.__name__] = value defaults_config 🍀 接下来我们看看关于 Flask 的一些默认的设置 , 首先是 Flask 对象中的 defaults_config , 它是一个 ImmutableDict 对象 ImmutableDict 是 werkzeug 中特意构造的一个数据类型 , 不可变字典 default_config = ImmutableDict({ 'ENV': None, # 由helpers.get_env()获取,默认为production # 是否启用调试模式 'DEBUG': None, # 由helpers.get_debug_flag()获取,默认为True # 是否启用测试模式 'TESTING': False, # 是否显示启用异常传播 'PROPAGATE_EXCEPTIONS': None, # 当 TESTING 或 DEBUG 为真时,总是开启的 'PRESERVE_CONTEXT_ON_EXCEPTION': None, # 缺省情况下,如果应用在调试模式下运行, # 那么请求环境在发生异常时不会被弹出, # 以方便调试器内省数据, # 可以通过这个配置来禁止这样做, # 还可以使用这个配置强制不执行调试, # 这样可能有助于调试生产应用(风险大) # 密钥 'SECRET_KEY': None, # 持久化会话的存活时间 'PERMANENT_SESSION_LIFETIME': timedelta(days=31), # 开关x-sendfile 'USE_X_SENDFILE': False, # 服务器的名称和端口,用于支持子域 'SERVER_NAME': None, # 应用的路径 'APPLICATION_ROOT': '/', # 会话cookie的名称 'SESSION_COOKIE_NAME': 'session', # 会话cookie的域 'SESSION_COOKIE_DOMAIN': None, # 会话cookie的路径 'SESSION_COOKIE_PATH': None, # cookie的httponly标志 'SESSION_COOKIE_HTTPONLY': True, # 设置cookie的安全标志 'SESSION_COOKIE_SECURE': False, # cookie是否使用SameSite属性 'SESSION_COOKIE_SAMESITE': None, # 是否刷新请求 'SESSION_REFRESH_EACH_REQUEST': True, # 拒绝内容长度超过该值的请求,单位为字节 'MAX_CONTENT_LENGTH': None, # 默认缓存控制的最大期限 'SEND_FILE_MAX_AGE_DEFAULT': timedelta(hours=12), # 反馈坏请求异常 'TRAP_BAD_REQUEST_ERRORS': None, # 是否执行HTTP异常处理 'TRAP_HTTP_EXCEPTIONS': False, 'EXPLAIN_TEMPLATE_LOADING': False, # url模式方案 'PREFERRED_URL_SCHEME': 'http', # 是否把对象编码为ASCII 'JSON_AS_ASCII': True, # 是否按键值排序JSON对象 'JSON_SORT_KEYS': True, # jsonify响应是否完美打印 'JSONIFY_PRETTYPRINT_REGULAR': False, # json类型 'JSONIFY_MIMETYPE': 'application/json', # 自动重载模板 'TEMPLATES_AUTO_RELOAD': None, # 最大cookie尺寸 'MAX_COOKIE_SIZE': 4093, }) Flask 对象默认配置 : # 静态文静路径 static_url_path=None, # 静态文件夹 static_folder='static', static_host=None, host_matching=False, subdomain_matching=False, template_folder='templates', instance_path=None, instance_relative_config=False, root_path=None 配置加载 🍀 那么了解了 config.py 中的内容后 , 我们应该想的是 , Flask 到底是如何去加载的 首先在我们实例化 Flask 对象 , 有这么一个操作 : # instance_relative_config默认为False self.config = self.make_config(instance_relative_config) 这个 make_config 就是加载配置了 def make_config(self, instance_relative=False): # root_path在Flask的基类中已经被设置,就是当前的根目录 # root_path = get_root_path(self.import_name) root_path = self.root_path if instance_relative: root_path = self.instance_path # 这里self.defaults_config在上一小节已经详细说明了 defaults = dict(self.default_config) defaults['ENV'] = get_env() defaults['DEBUG'] = get_debug_flag() # 返回一个Config对象,即Config(root_path, defaults) return self.config_class(root_path, defaults) 更多的配置需要在相关应用上介绍 "},"05-Web框架/Flask/04-Flask - 源码之路由.html":{"url":"05-Web框架/Flask/04-Flask - 源码之路由.html","title":"Flask - 源码之路由","keywords":"","body":"Flask - 源码之路由 介绍 🍀 在分析 Flask 的请求处理流程中 , 已经碰到了一些路由相关的代码了 , 但是并未深入 , 现在就来看看吧 添加路由 🍀 通常我们使用 Flask 中的装饰器 route 来完成路由注册 , 实际上 , 它也仅仅是一个中介 , 方便我们使用 ; 本质上它只是帮我们调用了 Flask 对象中的 add_url_rule 方法 , 如下 : def route(self, rule, **options): def decorator(f): endpoint = options.pop('endpoint', None) self.add_url_rule(rule, endpoint, f, **options) return f return decorator 也就是说我们可以这样来注册路由 : app = Flask(__name__) def index(): pass app.add_url_rule('/', 'index', index) 而 add_url_rule 中 , 我们截取重要部分来分析 : def add_url_rule(self, rule, endpoint=None, view_func=None, provide_automatic_options=None, **options): if endpoint is None: endpoint = _endpoint_from_view_func(view_func) options['endpoint'] = endpoint methods = options.pop('methods', None) # if the methods are not given and the view_func object knows its # methods we can use that instead. If neither exists, we go with # a tuple of only ``GET`` as default. if methods is None: # 此处添加GET,在后续会加入OPTIONS和HEAD methods = getattr(view_func, 'methods', None) or ('GET',) if isinstance(methods, string_types): raise TypeError('Allowed methods have to be iterables of strings, ' 'for example: @app.route(..., methods=[\"POST\"])') methods = set(item.upper() for item in methods) # Methods that should always be added required_methods = set(getattr(view_func, 'required_methods', ())) # starting with Flask 0.8 the view_func object can disable and # force-enable the automatic options handling. if provide_automatic_options is None: provide_automatic_options = getattr(view_func, 'provide_automatic_options', None) if provide_automatic_options is None: if 'OPTIONS' not in methods: provide_automatic_options = True required_methods.add('OPTIONS') else: provide_automatic_options = False # Add the required methods now. # 此处会将required_methods中的OPTIONS加入methods中 methods |= required_methods # 实例化Rule对象,并不会对rule做出改变 # 实例化过程中会将 HEAD 加入到methods中 rule = self.url_rule_class(rule, methods=methods, **options) rule.provide_automatic_options = provide_automatic_options # self.url_map = Map() # 所有的路由将全部存储在Map对象中,并且路由的转换工作也将在该对象中完成 self.url_map.add(rule) if view_func is not None: old_func = self.view_functions.get(endpoint) if old_func is not None and old_func != view_func: raise AssertionError('View function mapping is overwriting an ' 'existing endpoint function: %s' % endpoint) # 将视图函数加入view_functions中,默认为{} self.view_functions[endpoint] = view_func 绑定路由 🍀 我们已经知道 , 路由的转换是利用Map 对象来实现的 , self.url_map.add(rule) 源码如下 : def add(self, rulefactory): # Rule.get_rules()是一个生成器,返回Rule实例 for rule in rulefactory.get_rules(self): # Rule.bind()会将url绑定到Map对象,也就是这里self # 并且会创建一个正则表达式,其规则将会从Map对象中获取 rule.bind(self) # 将Rule对象加入_rules列表中 self._rules.append(rule) # 加入_rules_by_endpoint中 self._rules_by_endpoint.setdefault(rule.endpoint, []).append(rule) self._remap = True 在上面的 add 方法中 , 最重要的就是 rule.bind(self) , 我们看看细节部分 : def bind(self, map, rebind=False): if self.map is not None and not rebind: raise RuntimeError('url rule %r already bound to map %r' % (self, self.map)) # 为Rule对象添加map属性 self.map = map if self.strict_slashes is None: # map.strict_slashes = True self.strict_slashes = map.strict_slashes if self.subdomain is None: # map.default_subdomain = '' self.subdomain = map.default_subdomain # 完成正则表达式并存储它 self.compile() self.compile() 如下 : def compile(self): \"\"\"Compiles the regular expression and stores it.\"\"\" assert self.map is not None, 'rule not bound' if self.map.host_matching: domain_rule = self.host or '' else: domain_rule = self.subdomain or '' self._trace = [] self._converters = {} self._static_weights = [] self._argument_weights = [] regex_parts = [] def _build_regex(rule): \"\"\" 构建正则表达式,并放入regex_parts中 \"\"\" index = 0 # parse_rule(rule)会返回一个生成器,迭代时返回(converter, arguments, variable) for converter, arguments, variable in parse_rule(rule): # 只有使用静态规则时,converter才为None if converter is None: regex_parts.append(re.escape(variable)) self._trace.append((False, variable)) for part in variable.split('/'): if part: self._static_weights.append((index, -len(part))) else: if arguments: c_args, c_kwargs = parse_converter_args(arguments) else: c_args = () c_kwargs = {} # 获取转换器 convobj = self.get_converter( variable, converter, c_args, c_kwargs) regex_parts.append('(?P%s)' % (variable, convobj.regex)) self._converters[variable] = convobj self._trace.append((True, variable)) self._argument_weights.append(convobj.weight) self.arguments.add(str(variable)) index = index + 1 _build_regex(domain_rule) regex_parts.append('\\\\|') self._trace.append((False, '|')) _build_regex(self.is_leaf and self.rule or self.rule.rstrip('/')) if not self.is_leaf: self._trace.append((False, '/')) if self.build_only: return regex = r'^%s%s$' % ( u''.join(regex_parts), (not self.is_leaf or not self.strict_slashes) and '(?/?)' or '' ) self._regex = re.compile(regex, re.UNICODE) 转换器 🍀 在上面这段代码中 , 主要流程为 : 根据 url 中的信息 , 解析使用什么转换器 , 以及含有的变量等 , 再利用转换器的规则 , 生成路由放入 regex_parts 中 ; Map 对象中提供了一些默认的转换器 : # Map对象构造函数上方 default_converters = ImmutableDict(DEFAULT_CONVERTERS) DEFAULT_CONVERTERS = { 'default': UnicodeConverter, 'string': UnicodeConverter, 'any': AnyConverter, 'path': PathConverter, 'int': IntegerConverter, 'float': FloatConverter, 'uuid': UUIDConverter, } 这些转换器都是通过继承 BaseConverter 类实现的 class BaseConverter(object): \"\"\"Base class for all converters.\"\"\" regex = '[^/]+' weight = 100 def __init__(self, map): self.map = map def to_python(self, value): \"\"\" 路由匹配时,匹配成功后传递给视图函数中参数的值 \"\"\" return value def to_url(self, value): \"\"\" 使用url_for反向生成URL时,传递的参数经过该方法处理,返回值用于生成URL的参数 \"\"\" # 用指定的编码对value进行编码 return url_quote(value, charset=self.map.charset) 以 UnicodeConverter 为例 : class UnicodeConverter(BaseConverter): def __init__(self, map, minlength=1, maxlength=None, length=None): BaseConverter.__init__(self, map) if length is not None: length = '{%d}' % int(length) else: if maxlength is None: maxlength = '' else: maxlength = int(maxlength) length = '{%s,%s}' % ( int(minlength), maxlength ) self.regex = '[^/]' + length # Rule('/pages/'), # Rule('/') @app.route('/') def string_view(lang_code): return lang_code 自定义转换器 🍀 在 Flask 默认的转换器中 , 并没有支持正则的 , 如果我们需要像 Django 一样 , 支持正则 , 那就需要我们自己来增加这个规则了 , 也就是自定义一个转换器 from flask import Flask,url_for from werkzeug.routing import BaseConverter app = Flask(__name__) class RegexConverter(BaseConverter): \"\"\" 正则转换器 \"\"\" def __init__(self, map, regex): super(RegexConverter, self).__init__(map) self.regex = regex def to_python(self, value): return int(value) def to_url(self, value): return super(RegexConverter, self).to_url(value) # 添加到Map对象的converters中 app.url_map.converters['regex'] = RegexConverter @app.route('/index/') def index(nid): return \"Index\" 反向生成URL 🍀 在 Django 中 , 利用 reverse() 来反向生成 URL , 而在 Flask 中 , 也提供了一个函数可以反向生成 URL : url_for() def url_for(endpoint, **values): # 获取应用上下文 appctx = _app_ctx_stack.top # 获取请求上下文 reqctx = _request_ctx_stack.top if appctx is None: raise RuntimeError( 'Attempted to generate a URL without the application context being' ' pushed. This has to be executed when application context is' ' available.' ) # If request specific information is available we have some extra # features that support \"relative\" URLs. if reqctx is not None: # 请求上下文对象的url适配器 url_adapter = reqctx.url_adapter blueprint_name = request.blueprint if endpoint[:1] == '.': if blueprint_name is not None: endpoint = blueprint_name + endpoint else: endpoint = endpoint[1:] external = values.pop('_external', False) # Otherwise go with the url adapter from the appctx and make # the URLs external by default. else: # 应用上下文适配器 url_adapter = appctx.url_adapter if url_adapter is None: raise RuntimeError( 'Application was not able to create a URL adapter for request' ' independent URL generation. You might be able to fix this by' ' setting the SERVER_NAME config variable.' ) external = values.pop('_external', True) anchor = values.pop('_anchor', None) method = values.pop('_method', None) scheme = values.pop('_scheme', None) appctx.app.inject_url_defaults(endpoint, values) # This is not the best way to deal with this but currently the # underlying Werkzeug router does not support overriding the scheme on # a per build call basis. old_scheme = None if scheme is not None: if not external: raise ValueError('When specifying _scheme, _external must be True') old_scheme = url_adapter.url_scheme url_adapter.url_scheme = scheme try: try: # 通过适配器构建url rv = url_adapter.build(endpoint, values, method=method, force_external=external) finally: if old_scheme is not None: url_adapter.url_scheme = old_scheme except BuildError as error: # We need to inject the values again so that the app callback can # deal with that sort of stuff. values['_external'] = external values['_anchor'] = anchor values['_method'] = method values['_scheme'] = scheme return appctx.app.handle_url_build_error(error, endpoint, values) if anchor is not None: rv += '#' + url_quote(anchor) return rv 补充 🍀 关于路由默认的 methods 属性 , Rule 对象实例化时 , 会完成3次添加 : 执行 add_url_rule , 添加 GET 属性 通过 required_methods 与 methods 并集的结果 , 添加 OPTIONS 实例化 Rule 对象时 , 添加 HEAD 属性 在 Flask 对象进行实例化时 , 会首先将静态文件路由进行添加 if self.has_static_folder: assert bool(static_host) == host_matching, 'Invalid static_host/host_matching combination' self.add_url_rule( self.static_url_path + '/', endpoint='static', host=static_host, view_func=self.send_static_file ) 自动添加的静态文件规则如下 : ' (HEAD, OPTIONS, GET) -> static>, "},"05-Web框架/Flask/05-Flask - 源码之视图.html":{"url":"05-Web框架/Flask/05-Flask - 源码之视图.html","title":"Flask - 源码之视图","keywords":"","body":"Flask - 源码之视图 介绍 🍀 在 Flask 请求处理流程中 , 视图的调用是由 dispatch_request() 方法来控制的 如果我们使用 FBV 的方式 , 由于请求方法是由我们定义条件语句控制的 , 所以 dispatch_request() 可以直接使用 , 但是如果我们像 Django 那样来使用 , 那么我们就需要重写 dispatch_request() 了 , 不过在 Flask 中 , 已经有相关的类帮我们实现了 在 flask.views.py 中已经帮我们实现了 CBV , 我们可以继承他们来使用 class View(object): \"\"\" 使用该类必须重写dispatch_reqeust方法, 该类中有一个decorators属性,可以放入一些装饰器, 装饰器将会自动装饰在视图上 \"\"\" pass class MethodViewType(type): \"\"\" 该类为MethodView类的元类,它决定了视图的methods属性 \"\"\" pass class MethodView(with_metaclass(MethodViewType, View)): \"\"\" 为我们已经实现了dispatch_request方法,我们可以直接使用 \"\"\" pass View 🍀 class View(object): methods = None provide_automatic_options = None # 视图装饰器有关 decorators = () def dispatch_request(self): \"\"\" 子类必须继承 \"\"\" raise NotImplementedError() @classmethod def as_view(cls, name, *class_args, **class_kwargs): \"\"\" 将类转换成视图 \"\"\" def view(*args, **kwargs): # 实例化视图类 self = view.view_class(*class_args, **class_kwargs) # 通过dispatch_request方法来控制类中方法的调用 return self.dispatch_request(*args, **kwargs) if cls.decorators: view.__name__ = name view.__module__ = cls.__module__ for decorator in cls.decorators: view = decorator(view) view.view_class = cls view.__name__ = name view.__doc__ = cls.__doc__ view.__module__ = cls.__module__ view.methods = cls.methods view.provide_automatic_options = cls.provide_automatic_options return view 实例 from flask import Flask, request, render_template from flask.views import View app = Flask(__name__) class BaseView(View): def get_template_name(self): raise NotImplementedError def render_template(self, context): return render_template(self.get_template_name(), **context) def dispatch_request(self): if request.method != \"GET\": return \"UNSUPPORTED!\" context = {'users': self.get_users()} return self.render_template(context) class UserView(BaseView): def get_template_name(self): return 'user.html' def get_users(self): return [{ 'username': 'Lyon', 'avatar': 'https://github.com/lyonyang/blogs/blob/master/assets/avatar/one.jpg' }] app.add_url_rule('/users', view_func=UserView.as_view('userview')) if __name__ == '__main__': app.run() MethodViewType 🍀 class MethodViewType(type): \"\"\"Metaclass for :class:`MethodView` that determines what methods the view defines. \"\"\" def __init__(cls, name, bases, d): super(MethodViewType, cls).__init__(name, bases, d) # d为视图参数的字典 # MethodView使用该元类时,会将空字典传入d if 'methods' not in d: methods = set() # http_method_funcs = frozenset(['get', 'post', 'head', # 'options','delete', 'put', 'trace', 'patch']) for key in http_method_funcs: # 如果视图中定义了属性方法,那么就在methods中添加对应属性 if hasattr(cls, key): methods.add(key.upper()) if methods: cls.methods = methods MethodView 🍀 class MethodView(with_metaclass(MethodViewType, View)): def dispatch_request(self, *args, **kwargs): meth = getattr(self, request.method.lower(), None) if meth is None and request.method == 'HEAD': meth = getattr(self, 'get', None) assert meth is not None, 'Unimplemented method %r' % request.method return meth(*args, **kwargs) 实例 from flask import Flask, jsonify, abort, g from flask.views import MethodView app = Flask(__name__) def user_required(f): def decorator(*args, **kwargs): if not g.user: abort(401) return f(*args, **kwargs) return decorator class UserAPI(MethodView): decorators = [user_required,] def get(self): return jsonify({ 'username': 'Lyon', 'avatar': 'https://github.com/lyonyang/blogs/blob/master/assets/avatar/one.jpg' }) def post(self): return \"UNSUPPORTED!\" app.add_url_rule('/user', view_func=UserAPI.as_view('userview')) if __name__ == '__main__': app.run() "},"05-Web框架/Flask/06-Flask - 源码之蓝图.html":{"url":"05-Web框架/Flask/06-Flask - 源码之蓝图.html","title":"Flask - 源码之蓝图","keywords":"","body":"Flask - 源码之蓝图 介绍 🍀 首先 , 我们得说说蓝图的作用 蓝图 (Blueprint) 实现了应用的模块化 , 使用蓝图让应用层次清晰 , 开发者可以更容易的开发和维护项目 , 蓝图通常作用于相同的 URL 前缀 , 比如/user/:id , /user/profile 这样的地址都以 /user 开头 , 那么他们就可以放在一个模块中 构建蓝图 🍀 我们从示例开始 : myapplication/simple_page.py from flask import Blueprint, render_template, abort from jinja2 import TemplateNotFound # Blueprint类与Flask类一样,都继承了_PackageBoundObject # Blueprint相当于Flask的子应用 simple_page = Blueprint('simple_page', __name__, template_folder='templates') @simple_page.route('/', defaults={'page': 'index'}) @simple_page.route('/') def show(page): try: return render_template('pages/%s.html' % page) except TemplateNotFound: abort(404) Blueprint 类与 Flask 类非常相似 , 它就像是一个应用 , 上面代码中 , 首先实例化一个应用对象 , 虽然绑定路由与视图函数 , 然而它与 Flask 的不同之处就在于 , route 完成的操作并没有真的完成路由的添加 , 而是完成了一个函数的添加 , 我们看看源码 : def route(self, rule, **options): def decorator(f): # 与Flask类中的route方法不同的是,蓝图中将视图函数的函数名作为endpoint endpoint = options.pop(\"endpoint\", f.__name__) self.add_url_rule(rule, endpoint, f, **options) return f return decorator Blueprint.add_url_rule() 如下 : def add_url_rule(self, rule, endpoint=None, view_func=None, **options): if endpoint: assert '.' not in endpoint, \"Blueprint endpoints should not contain dots\" if view_func and hasattr(view_func, '__name__'): assert '.' not in view_func.__name__, \"Blueprint view function name should not contain dots\" # 传入一个匿名函数,该函数将在蓝图注册时被调用 self.record(lambda s: s.add_url_rule(rule, endpoint, view_func, **options)) Blueprint.record() 如下 : def record(self, func): if self._got_registered_once and self.warn_on_modifications: from warnings import warn warn(Warning('The blueprint was already registered once ' 'but is getting modified now. These changes ' 'will not show up.')) # 将函数追加到deferred_functions中 self.deferred_functions.append(func) 至此 , 我们已经完成了一个蓝图的构建 , 但是此时路由并没有注册 , 它仅仅将注册路由的函数放入 deferred_functions 中 , 等待蓝图注册时被调用 注册蓝图 🍀 现在我们需要把构建好的蓝图注册到我们的应用中 , 如下 : from flask import Flask from myapplication.simple_page import simple_page app = Flask(__name__) app.register_blueprint(simple_page) Flask.register_blueprint() 如下 : def register_blueprint(self, blueprint, **options): first_registration = False if blueprint.name in self.blueprints: assert self.blueprints[blueprint.name] is blueprint, ( 'A name collision occurred between blueprints %r and %r. Both' ' share the same name \"%s\". Blueprints that are created on the' ' fly need unique names.' % ( blueprint, self.blueprints[blueprint.name], blueprint.name ) ) else: # 注册蓝图到Flask应用对象 self.blueprints[blueprint.name] = blueprint self._blueprint_order.append(blueprint) first_registration = True # 将回调在构建蓝图实例时的延迟函数,即deferred_functions中的函数 # 函数为:lambda s: s.add_url_rule(rule, endpoint, view_func, **options) # s为flask.blueprints.BlueprintSetupState实例 blueprint.register(self, options, first_registration) Blueprint.register() 具体如下 : def register(self, app, options, first_registration=False): self._got_registered_once = True # 创建一个flask.blueprints.BlueprintSetupState实例 state = self.make_setup_state(app, options, first_registration) if self.has_static_folder: state.add_url_rule( self.static_url_path + '/', view_func=self.send_static_file, endpoint='static' ) for deferred in self.deferred_functions: # 调用flask.blueprints.BlueprintSetupState实例的add_url_rule方法 deferred(state) flask.blueprints.BlueprintSetupState.add_url_rule() 如下 : def add_url_rule(self, rule, endpoint=None, view_func=None, **options): if self.url_prefix is not None: if rule: rule = '/'.join(( self.url_prefix.rstrip('/'), rule.lstrip('/'))) else: rule = self.url_prefix options.setdefault('subdomain', self.subdomain) if endpoint is None: endpoint = _endpoint_from_view_func(view_func) defaults = self.url_defaults if 'defaults' in options: defaults = dict(defaults, **options.pop('defaults')) # 使用flask.Flask.add_url_rule方法 # 第二个参数为endpoint参数,为蓝图名称.视图名称,simple_page.show self.app.add_url_rule(rule, '%s.%s' % (self.blueprint.name, endpoint), view_func, defaults=defaults, **options) 如果是使用蓝图注册路由 , 那么第一个路由同样是设置静态文件路由 , 在 flask.blueprints.Blueprint.register() 中可见如下内容 : if self.has_static_folder: state.add_url_rule( self.static_url_path + '/', view_func=self.send_static_file, endpoint='static' ) 该蓝图注册到应用时 , 路由注册规则如下 : # 可以在flask.Flask.add_url_rule中 # line 1215:self.url_map.add(rule)下添加一行输出代码: # print(rule.__repr()) # 随后启动应用,就能得到如下信息 # 我们也可以输出flas.Flask.view_functions属性查看绑定的视图函数,这里就不说明了 ' (HEAD, OPTIONS, GET) -> static>, ' (HEAD, OPTIONS, GET) -> simple_page.show>, simple_page.show> 蓝图还可以在不同的位置挂载 : app.register_blueprint(simple_page, url_prefix='/pages') 生成规则如下 : ' (HEAD, OPTIONS, GET) -> static>, ' (HEAD, OPTIONS, GET) -> simple_page.show>, simple_page.show> 蓝图资源 🍀 蓝图也可以提供资源 , 有时候你会只为他提供的资源而引入一个蓝图 蓝图资源文件夹 🍀 我们可以通过访问 Blueprint 对象的 root_path 属性来访问蓝图资源文件夹 : >>> simple_page.root_path '/Users/username/TestProject/myapplication' 并且你可以使用 open_response() 函数快速获取文件资源 : with simple_page.open_resource('static/style.css') as f: code = f.read() 静态文件 🍀 与 Flask 一样 , Blueprint 可以通过 static_folder 关键字参数提供一个指向文件系统上文件夹的路径 , 这可以是一个绝对路径 , 也可以是相对于蓝图文件夹的相对路径 : admin = Blueprint('admin', __name__, static_folder='static') 模板 🍀 同样的 , 蓝图也提供模板 : admin = Blueprint('admin', __name__, template_folder='templates') 总而言之 , 蓝图相当于 Flask 应用实例下的 \"Flask\" 应用实例 (\"子应用\") , 它能将你的项目理想化 对于 Blueprint 对象中的方法 , 你不妨看看 , 也许有你想要的功能 "},"05-Web框架/Flask/07-Flask - 源码之本地线程.html":{"url":"05-Web框架/Flask/07-Flask - 源码之本地线程.html","title":"Flask - 源码之本地线程","keywords":"","body":"Flask - 源码之本地线程 介绍 🍀 Flask 中的一条设计原则是保持任务的简单 , 任务的实现不需要花费太多的代码 , 也不会限制到你 ; 例如 , 为了保持线程安全 , Flask 使用了本地线程 (thread-local) , 所以在一个请求中你不需要在函数之间传递对象 本地线程 (thread-local) : 希望不同的线程对于内容的修改只在线程内发挥作用 , 线程之间互不影响 Threading的Local 🍀 我们可以通过一个例子来看看 , 本地线程是如何实现的 , 示例如下 : import threading data = threading.local() data.number = 1 print(data.number) log = [] def func(): data.number = 2 log.append(data.number) thread = threading.Thread(target=func) thread.start() thread.join() print(log) print(data.number) \"\"\" 执行结果: 1 [2] # 在线程内data.number变成了其他的值 1 # 但是没有影响到开始设置的值 \"\"\" 之所以会有这样的结果 , 都是因为 threading.local() 在作祟 , 以上面的代码为例 , 我们来分析一下这个 local , 其源码如下 : class local: # 仅允许_local__iml和__dict__进行绑定 # 其他属性绑定将会触发AttributeError __slots__ = '_local__impl', '__dict__' def __new__(cls, *args, **kw): if (args or kw) and (cls.__init__ is object.__init__): raise TypeError(\"Initialization arguments are not supported\") self = object.__new__(cls) # _localimpl为管理本地线程dicts属性的类 impl = _localimpl() impl.localargs = (args, kw) impl.locallock = RLock() object.__setattr__(self, '_local__impl', impl) # 初始化_localimpl对象的dicts属性 impl.create_dict() return self def __getattribute__(self, name): \"\"\" 此处内容省略 \"\"\" def __setattr__(self, name, value): \"\"\" 此处内容省略 \"\"\" def __delattr__(self, name): \"\"\" 此处内容省略 \"\"\" 我们需要先弄明白 impl.create_dict() 做了什么操作 , 因为这里是本地线程的一个关键点 , 还有一个关键点就是 __setattr__ 方法 , 先看看 impl.create_dict() : def create_dict(self): \"\"\" 为当前线程创建一个新字典,并返回它 \"\"\" localdict = {} # self.key:{ id(Thread) -> (ref(Thread), thread-local dict) } key = self.key # 获取当前线程对象 thread = current_thread() # 获取当前线程对象id idt = id(thread) def local_deleted(_, key=key): thread = wrthread() if thread is not None: del thread.__dict__[key] def thread_deleted(_, idt=idt): local = wrlocal() if local is not None: dct = local.dicts.pop(idt) # 封装成ReferenceType对象 wrlocal = ref(self, local_deleted) wrthread = ref(thread, thread_deleted) # 在当前线程对象的__dict__属性中,以线程id为key,wrlocal为value设置值 # 以保存不同线程的状态 thread.__dict__[key] = wrlocal # 以线程对象id为key,(ReferenceType,ReferenceType)对象为value # self为_localimpl对象 # self.dicts数据形式如下: # {2552096368904: (, {'number': 1})} self.dicts[idt] = wrthread, localdict # 未设置值前localdict为空 return localdict 此时 impl.dicts 属性已经有了 , 接下来回到我们的示例代码 , 当本地线程对象实例化完成之后 , 下一步就是设置属性 data.number = 1 , 也就是会执行 local 对象的 __setattr__ 方法 , 在上面我们把它给省略了 , 现在列出来 : def __setattr__(self, name, value): if name == '__dict__': raise AttributeError( \"%r object attribute '__dict__' is read-only\" % self.__class__.__name__) with _patch(self): return object.__setattr__(self, name, value) 可以看到 , 它走了一个 _patch 方法 , 继续看看 _patch 的详细内容 : # 该装饰器用于将_patch转换为上下文对象 @contextmanager def _patch(self): impl = object.__getattribute__(self, '_local__impl') try: # 返回当前进程对象中的字典 # 如:{'number': 1} dct = impl.get_dict() except KeyError: dct = impl.create_dict() args, kw = impl.localargs self.__init__(*args, **kw) # impl.locallock = RLock() # RLock是一个上下文对象 with impl.locallock: # 设置属性到local对象 object.__setattr__(self, '__dict__', dct) yield 到这里我们可以看出 , 如果在 _localimpl 对象的 dicts 中不存在以线程 id 为 key 的键值对 , 那么必定会调用 create_dict() 来为其创建一个 , 创建形式如下 : # key为当前线程对象id,wrlocal为ReferenceType对象 threading.current_thread().__dict__[key] = wrlocal # 随后以当前线程对象id为key,wrthread,localdict为value存入_localimpl.dicts中 # whthread中存入了当前线程对象,localdict为设置属性字典 # self.dicts[idt] = wrthread, localdict # 当然local对象的__dict__中也存在属性,因为最后调用了object的setattr方法 object.__setattr__(self, name, value) 本地线程的实现原理就是 , 数据的改变是在线程内部进行的 , 在每一个线程内部都有一个独立的字典 , 存放着那些数据 , 并且通过线程 id 和 dicts 属性 , 保存了不同线程的状态 Werkzeug的Local 🍀 总而言之 , 本地线程的实现 , 相当于在线程内部建立了一个数据副本 , 只不过我们需要一些手段来保存好这些线程的状态 上面分析的是 threading 中的本地线程 , 而 Flask 基于的 Werkzeug , 它自己实现了本地线程 , 也就是 werkzeug.local.Local 对象 : class Local(object): __slots__ = ('__storage__', '__ident_func__') def __init__(self): # 此处不能使用self.__storage__ = {}来初始化,原因: # 1. 首先会调用self.__setattr__ # 2. 随后执行self.__ident_func__(),于是会调用self.__getattr__ # 3. self.__storage__[self.__ident_func__()][name]会再次调用__getattr__ # 4. 于是,这里将永远递归下去 object.__setattr__(self, '__storage__', {}) object.__setattr__(self, '__ident_func__', get_ident) def __iter__(self): return iter(self.__storage__.items()) def __call__(self, proxy): \"\"\"Create a proxy for a name.\"\"\" return LocalProxy(self, proxy) def __release_local__(self): self.__storage__.pop(self.__ident_func__(), None) def __getattr__(self, name): try: return self.__storage__[self.__ident_func__()][name] except KeyError: raise AttributeError(name) def __setattr__(self, name, value): # 获取线程/协程标识符 ident = self.__ident_func__() storage = self.__storage__ try: storage[ident][name] = value except KeyError: # 以线程/协程标识符为key,属性键值对为value storage[ident] = {name: value} def __delattr__(self, name): try: del self.__storage__[self.__ident_func__()][name] except KeyError: raise AttributeError(name) 相对来讲 , Werkzeug 自己实现的本地线程 , 可能比 threading 提供的本地线程更加简单明了 , 两者区别如下 : Werkzeug 使用了自定义的 __storage__ 保存不同线程下的状态 Werkzeug 提供了释放本地线程的 release_local 方法 Werkzeug 使用 get_ident 函数来获取线程/协程标识符 在 werkzeug.local 中 , gent_ident 的导入如下 : try: from greenlet import getcurrent as get_ident except ImportError: try: from thread import get_ident except ImportError: from _thread import get_ident 如果已经安装了 Greenlet , 会优先使用 Greenlet , 否则将使用系统线程 ; Greenlet 是以 C 扩展模块形式接入 Python 的轻量级协程 , 它运行在操作系统进程的内部 , 但是会被协作式地调度 小结 🍀 Werkzeug 基于自己实现的 Local 还实现了两种数据结果 : LocalStack : 基于 werkzeug.local.Local 实现的栈结果 , 可以将对象推入 , 弹出 , 也可以快速拿到栈顶对象 LocalProxy : 作用和名字一样 , 最标准的代理模式 , 构造此结构时接收一个可以调用的参数 (一般为函数) , 这个函数执行后就是通过 LocalStack 实例化的栈的栈顶对象 ; 对于 LocalProxy 对象的操作实际上都会转发到这个栈顶对象 (也就是一个 thread-local 对象) 上面 本地线程是 Flask 中非常重要的一部分 , 因为在请求处理时 , 为了解决请求对象在每一个视图函数传递 (意味着每个视图函数需要像 Django 那样添加一个 request 参数) 的问题 , Flask 巧妙地使用上下文把某些对象变为全局可访问 (实际上是特定环境的局部对象的代理) , 再配合本地线程 , 这样每个线程看到的上下文对象都是不同的 本地线程 与 上下文 的结合 , 解决了 Flask 请求的问题 "},"05-Web框架/Flask/08-Flask - 源码之上下文.html":{"url":"05-Web框架/Flask/08-Flask - 源码之上下文.html","title":"Flask - 源码之上下文","keywords":"","body":"Flask - 源码之上下文 介绍 🍀 阅读本文时 , 请先了解 Flask 本地线程 相关内容 在 Flask 中实现了两种上下文对象 : 应用上下文与请求上下文 , 它们两者都是本地线程的 应用上下文 应用上下文存在的主要原因是 , 在过去 , 没有更好的方式来在请求上下文中附加一堆函数 , 因为 Flask 设计的支柱之一是你可以在一个 Python 进程中拥有多个应用 那么代码如何找到 \"正确的\" 应用呢 ? 解决这个问题常用的方法是使用 current_app 代理 (基于 werkzeug.local.Local 实现的 LocalProxy 对象) , 它被限制在当前请求的应用引用 应用上下文的典型应用场景是缓存一些在发生请求之前要使用到的资源 , 比如生成数据库连接和缓存一些对象 请求上下文 本地线程解决了请求对象在函数之间传递的问题 , 但是为了依赖注入或者尝试重用与请求相关的值的代码 , 我们需要一个有效的请求上下文 请求上下文发生在 HTTP 请求开始 , WSGIServer 调用 Flask.__call__() 之后 开始示例 🍀 先看一个简单的例子 from flask import Flask, request app = Flask(__name__) @app.route('/people/') def people(): name = request.args.get('name') return \"People Page!\" if __name__ == '__main__': app.run() 我们先细想一下 , 这里先引用了 flask.request , 但是直到用户访问 /people/ 时才通过 request.args.get('name') 获得请求的 name 字段 , 而在引用时这个请求还没有发生 , 那么请求上下文是怎么获得的呢 ? 其流程是这样的 : 用户访问产生请求 在发生请求的过程中向 _request_ctx_stack 推入这个请求上下文对象 , 它会变成栈顶 , request 就会成为这个请求上下文 , 也就包含了本次请求相关的信息和数据 在视图函数中使用 request 就可以使用 request.args.get('name') 了 flask.request 就是获取一个名为 _request_ctx_stack 的栈顶对象的 LocalProxy 实例 : # partial函数的作用是返回一个给定参数的函数 from functools import partial from werkzeug.local import LocalStack, LocalProxy # _lookup_req_object的name参数将固定为'request' request = LocalProxy(partial(_lookup_req_object, 'request')) def _lookup_req_object(name): # 获取_request_ctx_stack栈顶对象,也就是RequestContext对象 top = _request_ctx_stack.top if top is None: raise RuntimeError(_request_ctx_err_msg) # 获取RequestContext.request属性值 return getattr(top, name) 上面已经说过 , 请求上下文发生在 HTTP 请求开始 , 而请求的开始则是 Flask.__call__() 开始 , WSGIServer 将会调用 Flask 应用对象作为 WSGI 应用 , 也就是会调用 Flask.wsgi_app() : def wsgi_app(self, environ, start_response): # 实例化请求上下文对象 ctx = self.request_context(environ) error = None try: try: # 将请求上下文对象压入栈中,在这之前会先将应用上下文压入栈中 ctx.push() # 返回response对象 response = self.full_dispatch_request() except Exception as e: error = e response = self.handle_exception(e) except: error = sys.exc_info()[1] raise # 调用BaseResponse的__call__方法 # 交给WSGI服务器处理 return response(environ, start_response) finally: if self.should_ignore_error(error): error = None ctx.auto_pop(error) RequestContext.push() 如下 : def push(self): \"\"\"Binds the request context to the current context.\"\"\" # 获取栈顶 top = _request_ctx_stack.top if top is not None and top.preserved: top.pop(top._preserved_exc) # Before we push the request context we have to ensure that there # is an application context. app_ctx = _app_ctx_stack.top if app_ctx is None or app_ctx.app != self.app: # 生成应用上下文AppContext app_ctx = self.app.app_context() # 将应用上下文推入栈中 app_ctx.push() self._implicit_app_ctx_stack.append(app_ctx) else: self._implicit_app_ctx_stack.append(None) if hasattr(sys, 'exc_clear'): sys.exc_clear() # 将请求上下文推入栈中 _request_ctx_stack.push(self) if self.session is None: session_interface = self.app.session_interface self.session = session_interface.open_session( self.app, self.request ) if self.session is None: self.session = session_interface.make_null_session(self.app) 可以看到在 RequestContext.push() 中 , 并不是仅仅将请求上下文压入了栈中 , 同时它还生成了应用上下文并压入了栈中 也就是说 , 事实上在 Web 应用环境中 , 请求上下文和应用上下文是一一对应的 , 请求上下文和应用上下文都是本地线程的 全局变量 🍀 Flask 中有 6 个全局变量 : 2 个本地线程变量和 4 个上下文变量 它们都储存在 flask.globals.py : # context locals # 请求上下文栈,存储请求上下文,基于werkzeug的本地线程实现的栈结构 _request_ctx_stack = LocalStack() # 应用上下文栈,存储应用上下文,基于werkzeug的本地线程实现的栈结构 _app_ctx_stack = LocalStack() # 应用上下文,它是当前app的实例对象 current_app = LocalProxy(_find_app) # 请求上下文,它封装了客户端发出的HTTP请求中的内容 request = LocalProxy(partial(_lookup_req_object, 'request')) # 请求上下文,它存储了用户会话 session = LocalProxy(partial(_lookup_req_object, 'session')) # 应用上下文,它是处理请求时用作临时存储的对象 g = LocalProxy(partial(_lookup_app_object, 'g')) LocalStack() 的内是在 werkzeug.local.Local() 的基础上实现栈的一个结果 , 而 werkzeug.local.Local() 在上一篇中已经分析过了 LocalProxy() 是一个代理对象 , 如通过它来获取请求上下文对象中的 request 属性 关于 LocalProxy 的一些说明 : LocalProxy 传入一个函数为参数 , 其构造函数如下 def __init__(self, local, name=None): # _类名__属性名为私有属性的另一表现形式,此处等价于如下: # self.__local = local object.__setattr__(self, '_LocalProxy__local', local) object.__setattr__(self, '__name__', name) if callable(local) and not hasattr(local, '__release_local__'): # \"local\" is a callable that is not an instance of Local or # LocalManager: mark it as a wrapped function. object.__setattr__(self, '__wrapped__', local) LocalProxy 不会进行额外的操作 , 它会将对其本身的操作转接到上下文对象 我们也可以利用 LocalStack 与 LocalProxy 自己来实现一个全局可访问的 current_user : from werkzeug.local import LocalStack, LocalProxy from flask import Flask import random app = Flask(__name__) _user_err_msg = '''\\ Working outside of login user.\\ ''' _user_stack = LocalStack() def get_current_user(): top = _user_stack.top if top is None: raise RuntimeError(_user_err_msg) return top current_user = LocalProxy(get_current_user) @app.before_request def before_request(): users = ['Lyon', 'Kenneth'] user = random.choice(users) _user_stack.push(user) @app.teardown_appcontext def teardown(exc=None): _user_stack.pop() @app.route('/user') def user_view(): return current_user.__str__() if __name__ == '__main__': app.run() 服务启动后 , 我们多次访问 http://127.0.0.1:5000/user 可观察响应 请求上下文 🍀 在我们使用 flask.request 之前 , 我们必须保证在 _request_ctx_stack 中有 RequestContext 对象 , 因为在 Flask 中 , 请求的处理是从创建 RequestContext 对象 , 并将该对象压入 _request_ctx_stack 栈开始的 # ctx = self.request_context(environ) # environ是由WSGIRequestHandler.make_enviro()制造而来 class RequestContext(object): \"\"\" 请求上下文中包含了请求相关的所有信息 \"\"\" def __init__(self, app, environ, request=None): # Flask应用实例 self.app = app if request is None: # 实例化Request对象 request = app.request_class(environ) self.request = request # 为请求创建一个URL适配器 self.url_adapter = app.create_url_adapter(self.request) self.flashes = None self.session = None # 一个隐式的应用上下文栈 self._implicit_app_ctx_stack = [] # 显示上下文是否被保留 self.preserved = False # remembers the exception for pop if there is one in case the context # preservation kicks in. self._preserved_exc = None # 请求后执行函数 self._after_request_functions = [] # 将Request对象与URL连接 self.match_request() 既然是上下文对象 , 也就以为着在 RequestContext 中必然定义了 __enter__ 与 __exit__ : def __enter__(self): # 将RequestContext对象压入栈中并返回 self.push() return self def __exit__(self, exc_type, exc_value, tb): # 关闭上下文环境时从栈中弹出 self.auto_pop(exc_value) if BROKEN_PYPY_CTXMGR_EXIT and exc_type is not None: reraise(exc_type, exc_value, tb) 所以我们可以使用 with 来开启上下文环境 from flask import Flask from flask.globals import _request_ctx_stack app = Flask(__name__) # 如果你在请求开始前或者请求结束后查看请求上下文栈中的stack # 很不幸,请求开始前还没有这一属性 # 请求结束后,这一属性也被销毁,因为请求上下文对象销毁了 with app.test_request_context('/?next=http://example.com/') as rqc: print(rqc.request) print(_request_ctx_stack._local.stack) \"\"\" 执行结果: [] \"\"\" 回调与错误 🍀 在 Flask 中 , 请求处理时如果发生了一个错误将会发生什么事 ? 这个特殊的行为如下: 在每个请求之前 , 执行 before_request() 上绑定的函数 , 如果这些函数中的某个返回了一个响应 , 其它的函数将不再被调用 , 任何情况下 , 这个返回值都将替换视图函数的返回值 (这一步就像 Django 中的中间件一样) 如果 before_request() 上绑定的函数没有返回一个响应 , 常规的请求处理将会生效 , 匹配的视图函数有机会返回一个响应 视图的返回值之后会被转换成一个实际的响应对象 , 并交给 after_request() 上绑定的函数适当地替换或修改它 在请求的最后 , 会执行 teardown_request() 上绑定的函数 , 这总会发生 , 即使在一个未处理的异常抛出后或是没有请求前处理器执行过 (例如在测试环境中你有时会想不执行请求前回调) 在生产模式中 , 如果一个异常没有被捕获 , 将调用 500 internal server 的处理 , 在生产模式中 , 即便异常没有被处理过 , 也会冒泡给 WSGI 服务器 , 如此 , 像交互式调试器这样的东西可以提供丰富的调试信息 应用上下文 🍀 应用上下文会按需自动创建和销毁 , 如在将请求上下文对象压入栈中时 , 如果应用上下文栈中没有 , 则会先创建应用上下文 , 它不会在线程间移动 , 并且也不会在请求间共享 应用上下文通常是用来缓存那些用于请求之前创建或者请求使用情况下的资源 , 例如数据库连接是注定要使用应用上下文 . 存储的东西时应该为应用程序上下文选择唯一的名称 , 因为这是一个 Flask 应用和扩展之间共享的地方 class AppContext(object): \"\"\"The application context binds an application object implicitly to the current thread or greenlet, similar to how the :class:`RequestContext` binds request information. The application context is also implicitly created if a request context is created but the application is not on top of the individual application context. \"\"\" def __init__(self, app): self.app = app self.url_adapter = app.create_url_adapter(None) #: The class that is used for the :data:`~flask.g` instance. #: #: Example use cases for a custom class: #: #: 1. Store arbitrary attributes on flask.g. #: 2. Add a property for lazy per-request database connectors. #: 3. Return None instead of AttributeError on unexpected attributes. #: 4. Raise exception if an unexpected attr is set, a \"controlled\" flask.g. #: #: In Flask 0.9 this property was called `request_globals_class` but it #: was changed in 0.10 to :attr:`app_ctx_globals_class` because the #: flask.g object is now application context scoped. # app_ctx_globals_class = _AppCtxGlobals self.g = app.app_ctx_globals_class() # 引用计数,以追踪被压入栈的次数 self._refcnt = 0 "},"05-Web框架/Flask/09-Flask - 源码之信号.html":{"url":"05-Web框架/Flask/09-Flask - 源码之信号.html","title":"Flask - 源码之信号","keywords":"","body":"Flask - 源码之信号 介绍 🍀 项目功能越复杂 , 代码量越大 , 就越需要在其之上做开发和维护是很痛苦的 , 尤其是对于团队的新人 ; 而信号就是在框架核心功能或者一些 Flask 扩展发生动作时发送的通知 , 利用信号可以实现一部分的业务解藕 在 Flask 中 , 信号功能由 Blinker 库提供 , 如果没有安装该库就无法使用信号功能 , 但是不会影响其他功能 , 因为如果没有该库 , Flask 将提供一个假的信号 , flask.signals.py 中 : signals_available = False try: from blinker import Namespace signals_available = True except ImportError: class Namespace(object): def signal(self, name, doc=None): return _FakeSignal(name, doc) class _FakeSignal(object): \"\"\"If blinker is unavailable, create a fake class with the same interface that allows sending of signals but will fail with an error on anything else. Instead of doing anything on send, it will just ignore the arguments and do nothing instead. \"\"\" def __init__(self, name, doc=None): self.name = name self.__doc__ = doc def _fail(self, *args, **kwargs): raise RuntimeError('signalling support is unavailable ' 'because the blinker library is ' 'not installed.') send = lambda *a, **kw: None connect = disconnect = has_receivers_for = receivers_for = \\ temporarily_connected_to = connected_to = _fail del _fail # The namespace for code signals. If you are not Flask code, do # not put signals in here. Create your own namespace instead. _signals = Namespace() 所以我们应该先安装 Blinker : $ pip install blinker 下面是一个 Blinker 的示例 : from blinker import signal # 创建信号 started = signal('test-started') def each(round): print(\"Round {}!\".format(round)) def round_two(round): print(\"Only {}\".format(round)) # 订阅信号,each为接收者 started.connect(each) # round_two为接收者,sender为发送者 # 表示只有发送者为2时才接收 started.connect(round_two, sender=2) for round in range(1,4): # 发送信号 started.send(round) Flask 中有一些钩子 , 如 before_request 和 after_request , 这些钩子不需要 Blinker 库并且允许你改变请求对象 (request) 或者响应对象 (response) , 而信号和钩子做的事情很像 , 只不过信号并不对请求对象和响应对象做改变 , 仅承担记录和通知的工作 内置信号 🍀 在 flask.signals.py 中我们可以看到 , Flask 内置了 10 个信号 : # 模板渲染成功时发送 template_rendered = _signals.signal('template-rendered') # 模板渲染前发送 before_render_template = _signals.signal('before-render-template') # 建立请求上下文后,在请求处理开始前发送 request_started = _signals.signal('request-started') # 在响应发送给客户端之前发送 request_finished = _signals.signal('request-finished') # 请求销毁时发送,无论请求成败都会发送 request_tearing_down = _signals.signal('request-tearing-down') # 请求处理抛出异常时发送 got_request_exception = _signals.signal('got-request-exception') # 应用上下文销毁时发送 appcontext_tearing_down = _signals.signal('appcontext-tearing-down') # 应用上下文进栈中时发送 appcontext_pushed = _signals.signal('appcontext-pushed') # 应用上下文出栈时发送 appcontext_popped = _signals.signal('appcontext-popped') # 调用flask在其中添加数据时发送 message_flashed = _signals.signal('message-flashed') 创建信号 🍀 我们以 request_started 为例来看看其内部实现 : from blinker import Namespace _signals = Namespace() # 调用Namespace对象的signal方法 # 完成信号对象的创建,并使其成为全局引用 request_started = _signals.signal('request-started') Namespace.signal() 如下 : class Namespace(dict): \"\"\"A mapping of signal names to signals.\"\"\" def signal(self, name, doc=None): \"\"\" 返回NamedSignal对象 \"\"\" try: return self[name] except KeyError: # Namespace为内置对象dict的派生类, # 设置并返回值, # self.request-started = NameSignal('request-started') return self.setdefault(name, NamedSignal(name, doc)) 订阅信号 🍀 如果我们要使用内置信号 , 那么首先我们需要订阅信号 , 也就是使用 Signal.connect() 方法 from flask import Flask, request_started app = Flask(__name__) # log_reqeust函数为接收方,app为发送方 # 对于接收函数的参数,第一个位置不可缺省, # 因为在send调用该函数时,内部传入了一个sender实参 def log_request(sender, **extra): print('Before the request comes ...') # 订阅信号 request_started.connect(log_request, app) @app.route('/index') def index(): return 'index page' if __name__ == '__main__': app.run() connect() 源码如下 : def connect(self, receiver, sender=ANY, weak=True): \"\"\" Connect *receiver* to signal events sent by *sender*. receiver:为一个可调用对象 \"\"\" receiver_id = hashable_identity(receiver) if weak: # # receiver将在send时被调用,self._cleanup_receiver receiver_ref = reference(receiver, self._cleanup_receiver) receiver_ref.receiver_id = receiver_id else: receiver_ref = receiver if sender is ANY: sender_id = ANY_ID else: sender_id = hashable_identity(sender) self.receivers.setdefault(receiver_id, receiver_ref) # self._by_sender与self._by_receiver为两个默认字典,其value默认为set # {sender_id:{receiver_id,}} self._by_sender[sender_id].add(receiver_id) # {receiver_id:{sender_id,}} self._by_receiver[receiver_id].add(sender_id) del receiver_ref # 此时self._weak_senders为空,所以以下不会执行 if sender is not ANY and sender_id not in self._weak_senders: # wire together a cleanup for weakref-able senders try: sender_ref = reference(sender, self._cleanup_sender) sender_ref.sender_id = sender_id except TypeError: pass else: self._weak_senders.setdefault(sender_id, sender_ref) del sender_ref # 此处条件不成立,也不会执行 if ('receiver_connected' in self.__dict__ and self.receiver_connected.receivers): try: self.receiver_connected.send(self, receiver=receiver, sender=sender, weak=weak) except: self.disconnect(receiver, sender) raise # receiver_connected为空 if receiver_connected.receivers and self is not receiver_connected: try: receiver_connected.send(self, receiver_arg=receiver, sender_arg=sender, weak_arg=weak) except: self.disconnect(receiver, sender) raise return receiver 发送信号 🍀 信号的发送是通过 Signal.send() 来完成的 , 而这一步早已经被定义在 Flask 对象中了 , 如下 : def full_dispatch_request(self): self.try_trigger_before_first_request_functions() try: # 请求处理前发送信号 request_started.send(self) rv = self.preprocess_request() if rv is None: # 分派请求 rv = self.dispatch_request() except Exception as e: rv = self.handle_user_exception(e) return self.finalize_request(rv) Signal.send() 如下 : def send(self, *sender, **kwargs): \"\"\"Emit this signal on behalf of *sender*, passing on \\*\\*kwargs. Returns a list of 2-tuples, pairing receivers with their return value. The ordering of receiver notification is undefined. :param \\*sender: Any object or ``None``. If omitted, synonymous with ``None``. Only accepts one positional argument. :param \\*\\*kwargs: Data to be sent to receivers. \"\"\" # Using '*sender' rather than 'sender=None' allows 'sender' to be # used as a keyword argument- i.e. it's an invisible name in the # function signature. if len(sender) == 0: sender = None elif len(sender) > 1: raise TypeError('send() accepts only one positional argument, ' '%s given' % len(sender)) else: # 取*sender元组中的第一个元素,即self (app) sender = sender[0] if not self.receivers: return [] else: # 返回并完成调用 return [(receiver, receiver(sender, **kwargs)) for receiver in self.receivers_for(sender)] Signal.receivers_for() 如下 : def receivers_for(self, sender): \"\"\"Iterate all live receivers listening for *sender*.\"\"\" # TODO: test receivers_for(ANY) # self.receivers在信号订阅时被设置 if self.receivers: sender_id = hashable_identity(sender) if sender_id in self._by_sender: # 按照上面的例子我们使用的sender不是ANY, # 所以self._by_sender[ANY_ID]为一个空集合, # {sender_id:{receiver_id,}} # self._by_sender[sender_id]为本例ids ids = (self._by_sender[ANY_ID] | self._by_sender[sender_id]) else: ids = self._by_sender[ANY_ID].copy() for receiver_id in ids: # 根据receiver_id获取weakref对象 receiver = self.receivers.get(receiver_id) if receiver is None: continue if isinstance(receiver, WeakTypes): # strong为订阅函数,即本例的log_reqeust # 这里你可能会疑惑,见下 strong = receiver() if strong is None: # 释放信号 self._disconnect(receiver_id, ANY_ID) continue receiver = strong # 返回函数对象 yield receiver 在上面这段代码中 , 对于 strong = receiver() 我们知道 , WeakTypes = (ref, BoundMethodWeakref) , 而在这两个类型中 , ref 才是正主 ; 不用想我们也知道 , ref 也就是 ReferenceType 中必然有 __call__ 方法 , 但是该方法仅仅一个 pass 摆在那里 , 而调用的返回值却返回了我们的订阅函数 , 这不正常 于是 , 在 ReferenceType 的上方我找到了说明 , Weak-reference support module 这个类型是一个弱引用类型 , 它是一个特殊的存在 , 当你对弱引用对象进行引用时 , 并不能保持该类对象的活动 , 只有通过调用引用判断 ; 如果该引用还存活着 , 那么将返回其引用对象 , 否则将会进行回调 大致过程如下 : # 依次调用代码 receiver_ref = reference(receiver, self._cleanup_receiver) weak = callable_reference(object, callback) return annotatable_weakref(object, callback) class annotatable_weakref(ref): 弱引用对象没有属性或方法 , 如下有一个示例 : import weakref class Foo: pass # 实例化Foo o = Foo() # 包装成弱引用对象 r = weakref.ref(o) # 调用弱引用对象 r_result = r() print(o is r_result) \"\"\" 执行结果: True \"\"\" 弱引用详见 : weakref 最后 , 对于其它信号的发送相关代码位置 , 我们可以通过导入信息来查看 , 导入信息如下 : # app.py (5个) from .signals import appcontext_tearing_down, got_request_exception, \\ request_finished, request_started, request_tearing_down # ctx.py (2个) from .signals import appcontext_pushed, appcontext_popped # templating.py (2个) from .signals import template_rendered, before_render_template # helpers.py (1个) from .signals import message_flashed 这里就不再分析其它信号了 自定义信号 🍀 在我们的应用中 , 我们可以直接使用 Blinker 创建信号 , 如下 , 定义一中对于上传大文件的信号 : from blinker import Namespace web_signals = Namespace() large_file_saved = web_signals.signal('large-file-saved') 简直不要太简单 装饰器方式 🍀 在 Signal 对象中还有一个 connect_via() 装饰器订阅信号 , 如下 : def connect_via(self, sender, weak=False): def decorator(fn): self.connect(fn, sender, weak) return fn return decorator 这个就没必要分析了 , 看看用法吧 , 以 flask.appcontext_tearing_down 为例 : from flask import Flask, appcontext_tearing_down, session app = Flask(__name__) @appcontext_tearing_down.connect_via(app) def close_db_connection(sender, **extra): print('Database connection closed ...') @app.route('/index') def index(): return 'index page' if __name__ == '__main__': app.run() 另外在 Flask-Login 插件中还带了 6 种信号 , 可以基于其中的信号做一些额外工作 , 待后续添加 "},"05-Web框架/Flask/10-Flask - 扩展.html":{"url":"05-Web框架/Flask/10-Flask - 扩展.html","title":"Flask - 扩展","keywords":"","body":"Flask - 扩展 介绍 🍀 Flask 扩展多方面地扩充了 Flask 功能 , 例如它们添加了数据库支持以及其他常见任务 Flask 扩展的生态非常繁荣 , Flask 扩展被列出在 Flask Extension Registry 上并且我们可以直接用 easy_install 或者 pip 进行下载安装 下面介绍一些常用的扩展以及它们的使用方式 Flask-Script 🍀 Django 提供了如下管理命令 : $ python manage.py startapp $ python manage.py runserver Flask 也可以通过 Flask-Script 添加运行服务器 , 设置数据库 , 定制 shell 等功能的命令 安装 $ pip install flask-script 创建 py 文件 manage.py , 内容如下 : from flask_script import Manager app = Flask(__name__) # configure your app # Manager将跟踪从命令行调用的所有命令和句柄 manager = Manager(app) if __name__ == \"__main__\": manager.run() 随后我们在当前目录下 : >python manage.py --help # 看来默认有shell和runserver usage: manage.py [-?] {shell,runserver} ... positional arguments: {shell,runserver} shell Runs a Python shell inside Flask application context. runserver Runs the Flask development server i.e. app.run() optional arguments: -?, --help show this help message and exit 创建命令 🍀 接下来我们需要创建我们自己的命令 , 创建命令有三种方式 : 通过实现 Command 类 使用 @command 装饰器 使用 @option 装饰器 通过 Command 类 : from flask_script import Command class Hello(Command): \"prints hello world\" def run(self): print(\"hello world\") # 将创建的命令添加到Manager实例 manager.add_command('hello', Hello()) 通过 @command 装饰器 : @manager.command def lyon(): \"Just say lyon\" print(\"lyon\") 通过 @option 装饰器 : # @option用于读复杂命令进行控制 @manager.option('-n', '--name', help='Your name') def send(name): \"send name of you\" print(\"hello\", name) 我们再次执行 python manage.py --help >python manage.py --help usage: manage.py [-?] {hello,lyon,send,shell,runserver} ... positional arguments: {hello,lyon,send,shell,runserver} hello prints hello world lyon Just say lyon send send name of you shell Runs a Python shell inside Flask application context. runserver Runs the Flask development server i.e. app.run() optional arguments: -?, --help show this help message and exit 添加参数 🍀 大多数命令都采用了在命令行中传递许多命名参数或者位置参数 , 为了方便这一点 , 我们可以使用 Command 类中的 option_list 属性 : from flask_script import Command, Manager, Option class Hello(Command): option_list = ( Option('--name', '-n', dest='name'), ) def run(self, name): print \"hello %s\" % name 或者定义 get_options 方法 : class Hello(Command): def __init__(self, default_name='Joe'): self.default_name=default_name def get_options(self): return [ Option('-n', '--name', dest='name', default=self.default_name), ] def run(self, name): print \"hello\", name 如果使用的是 @command 装饰器 , 那么我们直接加在被装饰函数中就可以了 @manager.command def verify(verified=False): \"\"\" Checks if verified \"\"\" print \"VERIFIED?\", \"YES\" if verified else \"NO\" 结果如下 : > python manage.py verify VERIFIED? NO > python manage.py verify -v VERIFIED? YES > python manage.py verify --verified VERIFIED? YES 其次就是创建命令时使用的 @option 了 , 也可添加任意多个选项 : @manager.option('-n', '--name', dest='name', default='joe') @manager.option('-u', '--url', dest='url', default=None) def hello(name, url): if url is None: print \"hello\", name else: print \"hello\", name, \"from\", url Documentation : Read docs @ pythonhosted.org Flask-DebugToolbar 🍀 Django 有非常知名的 Django-DebugToolbar , 而 Flask 也有对应的替代工具 Flask-DebugToolbar 它会在浏览器上添加右边栏 , 可以快速查看环境变量 , 上下文内容 , 方便调试 安装 $ pip install flask-debugtoolbar 使用前提 : debug 必须药设置为 True 示例 from flask import Flask from flask_debugtoolbar import DebugToolbarExtension app = Flask(__name__) app.debug = True app.config['SECRET_KEY'] = 'a secret key' toolbar = DebugToolbarExtension(app) @app.route('/') def hello(): return '' if __name__ == '__main__': app.run(host='127.0.0.1', port=9000, debug=app.debug) 接下来使用浏览器访问 http://127.0.0.1:9000/ 就可以看到有右边栏了 Flask-DebugToolbar 内置了很多面板 , 如下 : 面板 功能 Versions 列出安装的包的版本 Time 显示处理当前请求花费的时间的信息 HTTP Headers 显示当前请求的 HTTP 头信息 Request Vars 显示当前请求带的变量 , 包含请求参数 , cookie 信息等 Config 显示 app.config 的变量值 Templates 显示模板请求参数信息 SQLAlchemy 显示当前请求下的 SQL , 需要设置 SQLALCHEMY_RECORD_QUERIES 为 True Logging 显示请求过程中的日志信息 Route List 列出 Flask 的路由规则 Profiler 对当前请求添加性能分析 , 默认是关闭的 , 需要点击红色的钩 , 让它变成绿色 Documentation : Read docs @ github.com Flask-Migrate 🍀 使用关系型数据库时 , 修改数据库模型和更新数据库这样的工作时有发生 , 而且很重要 SQLAlchemy 作者为此开发了迁移框架 Alembic , Flask-Migrate 就是基于 Alembic 做了轻量级封装 , 并集成到 Flask-Script 中 , 所有操作都通过 Flask-Script 命令完成 , 它能跟踪数据库结构的变化 , 把变化的部分应用到数据库中 安装 $ pip install Flask-Migrate 示例 manage.py from flask import Flask from flask-script import Manager from flask_sqlalchemy import SQLAlchemy from flask_migrate import Migrate, MigrateCommand app = Flask(__name__) app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///app.db' db = SQLAlchemy(app) migrate = Migrate(app, db) class User(db.Model): id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String(128)) manager = Manager(app) manager.add_command(\"db\",MigrateCommand) if __name__ == '__main__': manager.run() 执行命令 # 初始化 >python manage.py db init Python35\\site-packages\\flask_sqlalchemy\\__init__.py:794: FSADeprecationWarning: SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and will be disabled by default in the future. Set it to True or False to suppress this warning. 'SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and ' Creating directory demo\\migrations ... done Creating directory demo\\migrations\\versions ... done Generating demo\\migrations\\alembic.ini ... done Generating demo\\migrations\\env.py ... done Generating demo\\migrations\\README ... done Generating demo\\migrations\\script.py.mako ... done Please edit configuration/connection/logging settings in 'demo\\\\migrations\\\\alembic.ini' before proceeding. # 创建迁移脚本 >python manage.py db migrate Python35\\site-packages\\flask_sqlalchemy\\__init__.py:794: FSADeprecationWarning: SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and will be disabled by default in the future. Set it to True or False to suppress this warning. 'SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and ' INFO [alembic.runtime.migration] Context impl SQLiteImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.autogenerate.compare] Detected added table 'user' Generating demo\\migrations\\versions\\d121144e719e_.py ... done 更新数据库 >python manage.py db upgrade Python35\\site-packages\\flask_sqlalchemy\\__init__.py:794: FSADeprecationWarning: SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and will be disabled by default in the future. Set it to True or False to suppress this warning. 'SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and ' INFO [alembic.runtime.migration] Context impl SQLiteImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Running upgrade -> d121144e719e, empty message Documentation : Read docs @ pythonhosted.org Flask-RESTful 🍀 Flask-RESTful 帮助你快速创建 REST API 服务 安装 $ pip install flask-restful 示例 config.py DEBUG = True SQLALCHEMY_DATABASE_URI = 'sqlite:///app.db' UPLOAD_FOLDER = '/tmp/permdir' SQLALCHEMY_TRACK_MODIFICATIONS = False SECRET_KEY = 'a secret key' manage.py from flask import Flask, request from flask_restful import Resource, Api, reqparse, fields, marshal_with from flask_sqlalchemy import SQLAlchemy app = Flask(__name__) app.config.from_object('config') api = Api(app) db = SQLAlchemy(app) parser = reqparse.RequestParser() parser.add_argument('admin', type=bool, help='Use super manager mode', default=False) resource_fields = { 'id': fields.Integer, 'name': fields.String, 'address': fields.String } class User(db.Model): __tablename__ = 'restful_user' id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String(128), nullable=False) address = db.Column(db.String(128), nullable=True) db.create_all() class UserResource(Resource): @marshal_with(resource_fields) def get(self, name): user = User.query.filter_by(name=name).first() return user def put(self, name): address = request.form.get('address', '') user = User(name=name, address=address) db.session.add(user) db.session.commit() return {'ok': 0}, 201 def delete(self, name): args = parser.parse_args() is_admin = args['admin'] if not is_admin: return {'error': 'You do not have permissions'} user = User.query.filter_by(name=name).first() db.session.delete(user) db.session.commit() return {'ok': 0} api.add_resource(UserResource, '/users/') if __name__ == '__main__': app.run(host='127.0.0.1', port=9000, debug=True) 利用 http 工具在命令行访问 : # 添加数据 >http -f put http://localhost:9000/users/Lyon address='Beijing' HTTP/1.0 201 CREATED Content-Length: 16 Content-Type: application/json Date: Thu, 28 Jun 2018 12:00:18 GMT Server: Werkzeug/0.12.2 Python/3.5.2 { \"ok\": 0 } # 删除数据 >http -f delete http://localhost:9000/users/Lyon --print b { \"error\": \"You do not have permissions\" } # 以管理员身份删除数据 >http -f delete http://localhost:9000/users/Lyon admin=1 --print b { \"ok\": 0 } # 查询数据 >http -f get http://localhost:9000/users/Lyon --print b { \"address\": null, \"id\": 0, \"name\": null } Documentation : Read docs @ flask-restful.readthedocs.org Flask-Admin 🍀 有了 Flask-Admin 的帮助 , 我们用很少的代码就能像 Django 那样实现一个管理后台 , 它支持 Pymongo , Peewee , Mongoengine , SQLAlchemy 等数据库使用方法 , 自带了基于模型的数据管理 , 文件管理 , Redis 的页面命令行等类型后台 , 尤其是模型的管理后台 , 甚至可以细粒度定制字段级别的权限 安装 $ pip install Flask-Admin 示例 Flask 示例 Documentation : Read docs @ flask-admin.readthedocs.org "},"05-Web框架/Flask/DBUtils.html":{"url":"05-Web框架/Flask/DBUtils.html","title":"DBUtils","keywords":"","body":"DBUtils 介绍 🍀 DBUtils 是 Python 的一个用于实现数据库连接池的模块 , 此连接池有两种连接模式 模式一 🍀 为每个线程创建一个连接 , 线程即使调用了 close 方法 , 也不会关闭 , 只是把连接重新放到连接池 , 供自己线程再次使用 , 当线程终止时 , 连接自动关闭 import time import pymysql import threading from DBUtils.PooledDB import PooledDB, SharedDBConnection POOL = PersistentDB( # 使用链接数据库的模块 creator=pymysql, # 一个链接最多被重复使用的次数,None表示无限制 maxusage=None, # 开始会话前执行的命令列表,如:[\"set datestyle to ...\", \"set time zone ...\"] setsession=[], # ping MySQL服务端,检查是否服务可用 # 0 = None = never, # 1 = default = whenever it is requested, # 2 = when a cursor is created, # 4 = when a query is executed, # 7 = always ping=0, # 如果为False,conn.close()实际上被忽略,供下次使用,再线程关闭时,才会自动关闭链接 # 如果为True,conn.close()则关闭链接,那么再次调用pool.connection时就会报错,因为已经真的关闭了连接(pool.steady_connection()可以获取一个新的链接) closeable=False, # 本线程独享值得对象,用于保存链接对象,如果链接对象被重置 threadlocal=None, host='127.0.0.1', port=3306, user='root', password='123', database='pooldb', charset='utf8' ) def func(): conn = POOL.connection(shareable=False) cursor = conn.cursor() cursor.execute('select * from tb1') result = cursor.fetchall() cursor.close() conn.close() func() 模式二 🍀 创建一批连接到连接池 , 供所有线程共享使用 (由于 pymysql , MySQLdb 等 threadsafety 值为 1 , 所以该模式连接池中的线程会被所有线程共享) import time import pymysql import threading from DBUtils.PooledDB import PooledDB, SharedDBConnection POOL = PooledDB( # 使用链接数据库的模块 creator=pymysql, # 连接池允许的最大连接数,0和None表示不限制连接数 maxconnections=6, # 初始化时,链接池中至少创建的空闲的链接,0表示不创建 mincached=2, # 链接池中最多闲置的链接,0和None不限制 maxcached=5, # 链接池中最多共享的链接数量,0和None表示全部共享 # PS: 无用,因为pymysql和MySQLdb等模块的 threadsafety都为1,所有值无论设置为多少,_maxcached永远为0,所以永远是所有链接都共享 maxshared=3, # 连接池中如果没有可用连接后,是否阻塞等待;True,等待;False,不等待然后报错 blocking=True, # 一个链接最多被重复使用的次数,None表示无限制 maxusage=None, # 开始会话前执行的命令列表,如:[\"set datestyle to ...\", \"set time zone ...\"] setsession=[], # ping MySQL服务端,检查是否服务可用 # 0 = None = never, # 1 = default = whenever it is requested, # 2 = when a cursor is created, # 4 = when a query is executed, # 7 = always ping=0, host='127.0.0.1', port=3306, user='root', password='123', database='pooldb', charset='utf8' ) def func(): # 检测当前正在运行连接数的是否小于最大链接数,如果不小于则等待或报raise TooManyConnections异常 # 否则则优先去初始化时创建的链接中获取链接 SteadyDBConnection, # 然后将SteadyDBConnection对象封装到PooledDedicatedDBConnection中并返回, # 如果最开始创建的链接没有链接,则去创建一个SteadyDBConnection对象,再封装到PooledDedicatedDBConnection中并返回, # 一旦关闭链接后,连接就返回到连接池让后续线程继续使用 conn = POOL.connection() # print(th, '链接被拿走了', conn1._con) # print(th, '池子里目前有', pool._idle_cache, '\\r\\n') cursor = conn.cursor() cursor.execute('select * from tb1') result = cursor.fetchall() conn.close() func() "},"05-Web框架/Flask/virtualenv基本使用.html":{"url":"05-Web框架/Flask/virtualenv基本使用.html","title":"virtualenv基本使用","keywords":"","body":"virtualenv基本使用 介绍 🍀 随着我们所开发或者管理的项目越来越多 , 这就导致了有可能存在不同版本的Python , 又或者是不同版本的Python库 , 于是问题就出现了 , 库的版本问题颇快了向后兼容性的情况相当常见 , 而且零依赖的正式应用也不大可能存在 , 所以项目中的两个或者更多出现依赖性冲突就会频繁出现 所以 , 为了解决这些冲突 , virtualenv出现了 , virtualenv能够允许多个不同版本的Python安装 , 每一个服务与各自的项目 , 但是它并不是分别独立安装一个Python的副本 , 而是提供了一种方式使得环境保持独立 安装virtualenv 🍀 实际上 , virtualenv就是一个Python库 , 所以我们可以使用pip等命令进行安装 Mac OS X 或者 Linux 下 $ sudo pip install virtualenv 如果使用Ubuntu , 请尝试 $ sudo apt-get install python-virtualenv 在 Windows 下 $ pip install virtualenv 创建虚拟环境 🍀 通常我们会先创建一个项目文件夹 , 在其下创建venv虚拟环境 : $ mkdir myproject $ cd myproject $ virtualenv venv New python executable in venv/bin/python Installing distribute............done. 指定Python解释器 $ virtualenv -p /usr/bin/python2.7 venv 激活虚拟环境 🍀 在OS X 和Linux 下 $ . venv/bin/activate 在 Windows 下 $ venv\\scripts\\activate 在 Mac 下 $ source venv/bin/activate 退出虚拟环境 🍀 在OS X 和Linux 下 $ . venv/bin/deactivate 在 Windows 下 $ venv\\scripts\\deactivate.bat 在 Mac 下 $ deactivate "},"05-Web框架/Tornado/":{"url":"05-Web框架/Tornado/","title":"Tornado","keywords":"","body":"Tornado初识 "},"06-Redis/":{"url":"06-Redis/","title":"Redis","keywords":"","body":"Redis REmote DIctionary Server(Redis) 是一个由Salvatore Sanfilippo写的key-value存储系统 Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API 它通常被称为数据结构服务器 , 因为值(value)可以是 字符串(String), 哈希(Map) , 列表(list) , 集合(sets) 和有序集合(sorted sets)等类型 "},"06-Redis/01-Redis - 简介.html":{"url":"06-Redis/01-Redis - 简介.html","title":"Redis - 简介","keywords":"","body":"Redis - 简介 介绍 🍀 REmote DIctionary Server(Redis) 是一个由 Salvatore Sanfilippo 写的 key-value 存储系统 Redis是一个开源的使用 ANSI C 语言编写 , 遵守BSD协议 , 支持网络 , 可基于内存亦可持久化的日志型 , Key-Value 数据库 , 并提供多种语言的API 特点 Redis 与其他 key - value 缓存产品有以下三个特点 : Redis支持数据的持久化 , 可以将内存中的数据保持在磁盘中 , 重启的时候可以再次加载进行使用 Redis不仅仅支持简单的 key-value 类型的数据 , 同时还提供 list , set , zset , hash等数据结构的存储 Redis支持数据的备份 , 即 master-slave 模式的数据备份 优势 性能极高 – Redis能读的速度是 110000次/s , 写的速度是 81000次/s 丰富的数据类型 – Redis支持二进制案例的 Strings , Lists , Hashes , Sets 及 Ordered Sets 数据类型操作 原子 – Redis的所有操作都是原子性的 , 同时Redis还支持对几个操作全并后的原子性执行 丰富的特性 – Redis还支持 publish/subscribe , 通知 key 过期等等特性 安装 🍀 方式一 $ yum isntall redis 运行 $ redis-server /etc/redis.conf 方式二 : $ wget http://download.redis.io/releases/redis-4.0.10.tar.gz $ tar xzf redis-4.0.10.tar.gz $ cd redis-4.0.10 $ make 运行 $ src/redis-server 与内置客户端进行交互 : $ src/redis-cli redis> set foo bar OK redis> get foo \"bar\" 配置 🍀 Redis 的配置文件位于 Redis 安装目录下 , 文件名为 redis.conf 我们可以通过 CONFIG 命令查看或者设置配置项 查看配置 🍀 语法 redis 127.0.0.1:6379> CONFIG GET CONFIG_SETTING_NAME 实例 redis 127.0.0.1:6379> CONFIG GET loglevel 1) \"loglevel\" 2) \"notice\" 使用 * 号获取所有配置项 : redis 127.0.0.1:6379> CONFIG GET * 修改配置 🍀 你可以通过修改 redis.conf 文件或使用 CONFIG SET 命令来修改配置 语法 redis 127.0.0.1:6379> CONFIG SET CONFIG_SETTING_NAME NEW_CONFIG_VALUE 实例 redis 127.0.0.1:6379> CONFIG SET loglevel \"notice\" OK redis 127.0.0.1:6379> CONFIG GET loglevel 1) \"loglevel\" 2) \"notice\" 数据类型 🍀 Redis支持五种数据类型 : string (字符串) , hash (哈希) , list (列表) , set (集合) 及 zset(sorted set : 有序集合) String 🍀 string 是 Redis 最基本的类型 , 你可以理解成与 Memcached 一模一样的类型 , 一个 key 对应一个 value string 类型是二进制安全的 , 意思是 Redis 的 string 可以包含任何数据 , 比如 jpg 图片或者序列化的对象 string 类型是 Redis 最基本的数据类型 , 一个键最大能存储 512 MB 实例 redis 127.0.0.1:6379> SET name \"redis.net.cn\" OK redis 127.0.0.1:6379> GET name \"redis.net.cn\" 在以上实例中我们使用了 Redis 的 SET 和 GET 命令 , 键为 name , 对应的值为redis.net.cn 注意 : 一个键最大能存储 512 MB Hash 🍀 Redis hash 是一个键值对集合 Redis hash 是一个 string 类型的 field 和 value 的映射表 , hash 特别适合用于存储对象 实例 redis 127.0.0.1:6379> HMSET user:1 username redis.net.cn password redis.net.cn points 200 OK redis 127.0.0.1:6379> HGETALL user:1 1) \"username\" 2) \"redis.net.cn\" 3) \"password\" 4) \"redis.net.cn\" 5) \"points\" 6) \"200\" redis 127.0.0.1:6379> 以上实例中 hash 数据类型存储了包含用户脚本信息的用户对象 , 实例中我们使用了 Redis HMSET , HEGTALL 命令 , user:1 为键值 每个 hash 可以存储 2^(32-1) 键值对 , 相当于 40 多亿 List 🍀 Redis 列表是简单的字符串列表 , 按照插入顺序排序 , 你可以添加一个元素导列表的头部 (左边) 或者尾部 (右边) 实例 redis 127.0.0.1:6379> lpush redis.net.cn redis (integer) 1 redis 127.0.0.1:6379> lpush redis.net.cn mongodb (integer) 2 redis 127.0.0.1:6379> lpush redis.net.cn rabitmq (integer) 3 redis 127.0.0.1:6379> lrange redis.net.cn 0 10 1) \"rabitmq\" 2) \"mongodb\" 3) \"redis\" redis 127.0.0.1:6379> 列表最多可存储 2^(32-1) 元素 (4294967295 , 每个列表可存储40多亿) Set 🍀 Redis 的 Set 是 string 类型的无序集合 集合是通过哈希表实现的 , 所以添加 , 删除 , 查找的复杂度都是 O(1) sadd 命令 添加一个 string 元素到 , key 对应的 set 集合中 , 成功返回 1 ,如果元素以及在集合中返回 0 , key 对应的 set 不存在返回错误 sadd key member 实例 redis 127.0.0.1:6379> sadd redis.net.cn redis (integer) 1 redis 127.0.0.1:6379> sadd redis.net.cn mongodb (integer) 1 redis 127.0.0.1:6379> sadd redis.net.cn rabitmq (integer) 1 redis 127.0.0.1:6379> sadd redis.net.cn rabitmq (integer) 0 redis 127.0.0.1:6379> smembers redis.net.cn 1) \"rabitmq\" 2) \"mongodb\" 3) \"redis\" 注意 : 以上实例中 rabitmq 添加了两次 , 但根据集合内元素的唯一性 , 第二次插入的元素将被忽略 集合中最大的成员数为 2^(32-1) (4294967295, 每个集合可存储40多亿个成员) zset 🍀 Redis zset 和 Set 一样也是string类型元素的集合 , 且不允许重复的成员 不同的是每个元素都会关联一个 double 类型的分数 , redis 正是通过分数来为集合中的成员进行从小到大的排序。 zset 的成员是唯一的 , 但分数 (score) 却可以重复 zadd 命令 添加元素到集合 , 元素在集合中存在则更新对应 score zadd key score member 实例 redis 127.0.0.1:6379> zadd redis.net.cn 0 redis (integer) 1 redis 127.0.0.1:6379> zadd redis.net.cn 0 mongodb (integer) 1 redis 127.0.0.1:6379> zadd redis.net.cn 0 rabitmq (integer) 1 redis 127.0.0.1:6379> zadd redis.net.cn 0 rabitmq (integer) 0 redis 127.0.0.1:6379> ZRANGEBYSCORE redis.net.cn 0 1000 1) \"redis\" 2) \"mongodb\" 3) \"rabitmq\" "},"06-Redis/02-Redis - 配置参数说明.html":{"url":"06-Redis/02-Redis - 配置参数说明.html","title":"Redis - 配置参数说明","keywords":"","body":"Redis - 配置参数说明 参数说明 🍀 redis.conf 配置项说明如下 : Redis 默认不是以守护进程的方式运行 , 可以通过该配置项修改 , 使用yes启用守护进程 daemonize no 当 Redis 以守护进程方式运行时 , Redis 默认会把 pid 写入 /var/run/redis.pid 文件 , 可以通过 pidfile 指定 pidfile /var/run/redis.pid 指定 Redis 监听端口 , 默认端口为 6379 , 作者在自己的一篇博文中解释了为什么选用 6379 作为默认端口 , 因为 6379 在手机按键上 MERZ 对应的号码 , 而 MERZ 取自意大利歌女 Alessia Merz 的名字 port 6379 绑定的主机地址 bind 127.0.0.1 当客户端闲置多长时间后关闭连接 , 如果指定为0 , 表示关闭该功能 timeout 300 指定日志记录级别 , Redis总共支持四个级别 : debug , verbose , notice , warning , 默认为 verbose loglevel verbose 日志记录方式 , 默认为标准输出 , 如果配置 Redis 为守护进程方式运行 , 而这里又配置为日志记录方式为标准输出 , 则日志将会发送给 /dev/null logfile stdout 设置数据库的数量 , 默认数据库为0 , 可以使用SELECT 命令在连接上指定数据库id databases 16 指定在多长时间内 , 有多少次更新操作 , 就将数据同步到数据文件 , 可以多个条件配合 save Redis 默认配置文件中提供了三个条件 : save 900 1 save 300 10 save 60 10000 分别表示900秒 ( 15分钟 ) 内有1个更改 , 300秒 ( 5分钟)内有10个更改以及60秒内有10000个更改 指定存储至本地数据库时是否压缩数据 , 默认为yes , Redis 采用 LZF 压缩 , 如果为了节省 CPU 时间 , 可以关闭该选项 , 但会导致数据库文件变的巨大 rdbcompression yes 指定本地数据库文件名 , 默认值为 dump.rdb dbfilename dump.rdb 指定本地数据库存放目录 dir ./ 设置当本机为 slav 服务时 , 设置 master 服务的IP地址及端口 , 在 Redis 启动时 , 它会自动从 master 进行数据同步 slaveof 当 master 服务设置了密码保护时 , slav 服务连接 master 的密码 masterauth 设置 Redis 连接密码 , 如果配置了连接密码 , 客户端在连接Redis时需要通过 AUTH 命令提供密码 , 默认关闭 requirepass foobared 设置同一时间最大客户端连接数 , 默认无限制 , Redis 可以同时打开的客户端连接数为 Redis 进程可以打开的最大文件描述符数 , 如果设置 maxclients 0 , 表示不作限制 , 当客户端连接数到达限制时 , Redis 会关闭新的连接并向客户端返回 max number of clients reached 错误信息 maxclients 128 指定 Redis 最大内存限制 , Redis 在启动时会把数据加载到内存中 , 达到最大内存后 , Redis 会先尝试清除已到期或即将到期的 Key , 当此方法处理 后 , 仍然到达最大内存设置 , 将无法再进行写入操作 , 但仍然可以进行读取操作 , Redis 新的 vm 机制 , 会把 Key 存放内存 , Value 会存放在 swap 区 maxmemory 指定是否在每次更新操作后进行日志记录 , Redis 在默认情况下是异步的把数据写入磁盘 , 如果不开启 , 可能会在断电时导致一段时间内的数据丢失 , 因为 redis本身同步数据文件是按上面 save 条件来同步的 , 所以有的数据会在一段时间内只存在于内存中 , 默认为no appendonly no 指定更新日志文件名 , 默认为 appendonly.aof appendfilename appendonly.aof 指定更新日志条件 , 共有3个可选值 : no : 表示等操作系统进行数据缓存同步到磁盘 ( 快 ) always : 表示每次更新操作后手动调用 fsync() 将数据写到磁盘 ( 慢 , 安全 ) everysec : 表示每秒同步一次 ( 折衷 , 默认值 ) appendfsync everysec 指定是否启用虚拟内存机制 , 默认值为no , 简单的介绍一下 , VM 机制将数据分页存放 , 由 Redis 将访问量较少的页即冷数据 swap 到磁盘上 , 访问多的页面由磁盘自动换出到内存中 ( 在后面的文章我会仔细分析Redis的VM机制 ) vm-enabled no 虚拟内存文件路径 , 默认值为 /tmp/redis.swap , 不可多个Redis实例共享 vm-swap-file /tmp/redis.swap 将所有大于 vm-max-memory 的数据存入虚拟内存,无论 vm-max-memory 设置多小,所有索引数据都是内存存储的(Redis的索引数据 就是keys) , 也就是说,当 vm-max-memory 设置为 0 的时候,其实是所有value 都存在于磁盘 , 默认值为 0 vm-max-memory 0 Redis swap文件分成了很多的 page , 一个对象可以保存在多个 page 上面 , 但一个 page 上不能被多个对象共享 , vm-page-size 是要根据存储的 数据大小来设定的 , 作者建议如果存储很多小对象 , page 大小最好设置为 32 或者 `4bytes ; 如果存储很大大对象 , 则可以使用更大的 page , 如果不 确定 , 就使用默认值 vm-page-size 32 设置 swap 文件中的 page 数量 , 由于页表 ( 一种表示页面空闲或使用的bitmap ) 是在放在内存中的 , 在磁盘上每 8 个 pages 将消耗 1byte 的内存 , vm-pages 134217728 设置访问 swap 文件的线程数,最好不要超过机器的核数,如果设置为 0 , 那么所有对 swap 文件的操作都是串行的 , 可能会造成比较长时间的延迟 , 默认值为 4 vm-max-threads 4 设置在向客户端应答时 , 是否把较小的包合并为一个包发送 , 默认为开启 glueoutputbuf yes 指定在超过一定的数量或者最大的元素超过某一临界值时 , 采用一种特殊的哈希算法 hash-max-zipmap-entries 64 hash-max-zipmap-value 512 指定是否激活重置哈希 , 默认为开启 ( 后面在介绍Redis的哈希算法时具体介绍 ) activerehashing yes 指定包含其它的配置文件 , 可以在同一主机上多个 Redis 实例之间使用同一份配置文件 , 而同时各个实例又拥有自己的特定配置文件 include /path/to/local.conf "},"06-Redis/03-Redis - 基础命令.html":{"url":"06-Redis/03-Redis - 基础命令.html","title":"Redis - 基础命令","keywords":"","body":"Redis - 基础命令 介绍 🍀 Redis 命令用于在 Redis 服务上执行操作 要在 Redis 服务上执行命令需要一个 Redis 客户端 , Redis 客户端在我们之前下载的 Redis 的安装包中 我们只需在 Redis 服务启动后 , 执行如下命令连接 Redis 服务 : $ redis-cli 启动 Redis 客户端后 , 我们可以使用 PING 命令检测 Redis 服务是否启动 : $ redis-cli 127.0.0.1:6379> 127.0.0.1:6379> PING PONG 127.0.0.1:6379> 如果需要在远程 Redis 服务上执行命令 , 我们只需添加一些参数 : $ redis-cli -h host -p port -a password 实例 $ redis-cli -h 127.0.0.1 -p 3769 键命令 🍀 Redis 键命令用于管理 Redis 的键 语法 : redis 127.0.0.1:6379> COMMAND KEY_NAME 实例 redis 127.0.0.1:6379> SET w3ckey redis OK # 删除键w3ckey,成功删除返回(integer) 1 redis 127.0.0.1:6379> DEL w3ckey (integer) 1 Redis 键相关基本命令 : 序号 命令及描述 1 DEL key , 该命令用于在 key 存在是删除 key 2 DUMP key , 序列化给定 key , 并返回被序列化的值 3 EXISTS key , 检查给定 key 是否存在 4 EXPIRE key , seconds 为给定 key 设置过期时间 5 EXPIREAT key timestamp , EXPIREAT 的作用和 EXPIRE 类似 , 都用于为 key 设置过期时间 , 不同在于 EXPIREAT 命令接受的时间参数是 UNIX 时间戳(unix timestamp) 6 PEXPIRE key milliseconds , 设置 key 的过期时间亿以毫秒计 7 PEXPIREAT key milliseconds-timestamp 设置 key 过期时间的时间戳 (unix timestamp) 以毫秒计 8 KEYS pattern , 查找所有符合给定模式( pattern)的 key 9 MOVE key db , 将当前数据库的 key 移动到给定的数据库 db 当中 10 PERSIST key , 移除 key 的过期时间 , key 将持久保持 11 PTTL key , 以毫秒为单位返回 key 的剩余的过期时间 12 TTL key , 以秒为单位 , 返回给定 key 的剩余生存时间(TTL, time to live) 13 RANDOMKEY , 从当前数据库中随机返回一个 key 14 RENAME key newkey , 修改 key 的名称 15 RENAMENX key newkey , 仅当 newkey 不存在时 , 将 key 改名为 newkey 16 TYPE key , 返回 key 所储存的值的类型 字符串命令 🍀 Redis 字符串数据类型的相关命令用于管理 Redis 字符串值 语法 redis 127.0.0.1:6379> COMMAND KEY_NAME 实例 redis 127.0.0.1:6379> SET w3ckey redis OK redis 127.0.0.1:6379> GET w3ckey \"redis\" 常用 Redis 字符串命令如下 : 序号 命令及描述 1 SET key value , 设置指定 key 的值 2 GET key , 获取指定 key 的值 3 GETRANGE key start end , 返回 key 中字符串值的子字符 4 GETSET key value , 将给定 key 的值设为 value , 并返回 key 的旧值(old value) 5 GETBIT key offset , 对 key 所储存的字符串值 , 获取指定偏移量上的位(bit) 6 MGET key[key2..] , 获取所有(一个或多个)给定 key 的值 7 SETBIT key offset value , 对 key 所储存的字符串值 , 设置或清除指定偏移量上的位(bit) 8 SETEX key seconds value , 将值 value 关联到 key , 并将 key 的过期时间设为 seconds (以秒为单位) 9 SETNX key value , 只有在 key 不存在时设置 key 的值 10 SETRANGE key offset value , 用 value 参数覆写给定 key 所储存的字符串值 , 从偏移量 offset 开始 11 STRLEN key , 返回 key 所储存的字符串值的长度 12 MSET key value [key value ...] , 同时设置一个或多个 key-value 对 13 MSETNX key value [key value ...] , 同时设置一个或多个 key-value 对 , 当且仅当所有给定 key 都不存在 14 PSETEX key milliseconds value , 这个命令和 SETEX 命令相似 , 但它以毫秒为单位设置 key 的生存时间 , 而不是像 SETEX 命令那样 , 以秒为单位 15 INCR key , 将 key 中储存的数字值增一 16 INCRBY key increment , 将 key 所储存的值加上给定的增量值（increment） 17 INCRBYFLOAT key increment , 将 key 所储存的值加上给定的浮点增量值（increment） 18 DECR key , 将 key 中储存的数字值减一 19 DECRBY key decrement , key 所储存的值减去给定的减量值(decrement) 20 APPEND key value , 如果 key 已经存在并且是一个字符串 , APPEND 命令将 value 追加到 key 原来的值的末尾 哈希命令 🍀 语法 redis 127.0.0.1:6379> COMMAND KEY_NAME 实例 redis 127.0.0.1:6379> HMSET w3ckey name \"redis tutorial\" description \"redis basic commands for caching\" likes 20 visitors 23000 OK redis 127.0.0.1:6379> HGETALL w3ckey 1) \"name\" 2) \"redis tutorial\" 3) \"description\" 4) \"redis basic commands for caching\" 5) \"likes\" 6) \"20\" 7) \"visitors\" 8) \"23000\" Redis Hash 相关基本命令 : 序号 命令及描述 1 HDEL key field2 [field2] , 删除一个或多个哈希表字段 2 HEXISTS key field , 查看哈希表 key 中 , 指定的字段是否存在 3 HGET key field , 获取存储在哈希表中指定字段的值 4 HGETALL key , 获取在哈希表中指定 key 的所有字段和值 5 HINCRBY key field increment , 为哈希表 key 中的指定字段的整数值加上增量 increment 6 HINCRBYFLOAT key field increment , 为哈希表 key 中的指定字段的浮点数值加上增量 increment 7 HKEYS key , 获取所有哈希表中的字段 8 HLEN key , 获取哈希表中字段的数量 9 HMGET key field1 [field2] , 获取所有给定字段的值 10 HMSET key field1 value1 [field2 value2 ] , 同时将多个 field-value (域-值)对设置到哈希表 key 中 11 HSET key field value , 将哈希表 key 中的字段 field 的值设为 value 12 HSETNX key field value , 只有在字段 field 不存在时 , 设置哈希表字段的值 13 HVALS key , 获取哈希表中所有值 14 HSCAN key cursor [MATCH pattern] [COUNT count] 迭代哈希表中的键值对 列表命令 🍀 语法 redis 127.0.0.1:6379> COMMAND KEY_NAME 实例 redis 127.0.0.1:6379> LPUSH w3ckey redis (integer) 1 redis 127.0.0.1:6379> LPUSH w3ckey mongodb (integer) 2 redis 127.0.0.1:6379> LPUSH w3ckey mysql (integer) 3 redis 127.0.0.1:6379> LRANGE w3ckey 0 10 1) \"mysql\" 2) \"mongodb\" 3) \"redis\" Redis List 相关基本命令如下 : 序号 命令及描述 1 BLPOP key1 [key2 ] timeout , 移出并获取列表的第一个元素 , 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 2 BRPOP key1 [key2 ] timeout 移出并获取列表的最后一个元素 , 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 3 BRPOPLPUSH source destination timeout , 从列表中弹出一个值 , 将弹出的元素插入到另外一个列表中并返回它 ; 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 4 LINDEX key index , 通过索引获取列表中的元素 5 `LINSERT key BEFORE AFTER pivot value` , 在列表的元素前或者后插入元素 6 LLEN key , 获取列表长度 7 LPOP key , 移出并获取列表的第一个元素 8 LPUSH key value1 [value2] , 将一个或多个值插入到列表头部 9 LPUSHX key value , 将一个或多个值插入到已存在的列表头部 10 LRANGE key start stop , 获取列表指定范围内的元素 11 LREM key count value , 移除列表元素 12 LSET key index value , 通过索引设置列表元素的值 13 LTRIM key start stop , 对一个列表进行修剪(trim) , 就是说 , 让列表只保留指定区间内的元素 , 不在指定区间之内的元素都将被删除 14 RPOP key , 移除并获取列表最后一个元素 15 RPOPLPUSH source destination , 移除列表的最后一个元素 , 并将该元素添加到另一个列表并返回 16 RPUSH key value1 [value2] , 在列表中添加一个或多个值 17 RPUSHX key value , 为已存在的列表添加值 集合命令 🍀 Redis 中 集合是通过哈希表实现的 , 所以添加 , 删除 , 查找的复杂度都是 O(1) 语法 redis 127.0.0.1:6379> COMMAND KEY_NAME 实例 redis 127.0.0.1:6379> SADD w3ckey redis (integer) 1 redis 127.0.0.1:6379> SADD w3ckey mongodb (integer) 1 redis 127.0.0.1:6379> SADD w3ckey mysql (integer) 1 redis 127.0.0.1:6379> SADD w3ckey mysql (integer) 0 redis 127.0.0.1:6379> SMEMBERS w3ckey 1) \"mysql\" 2) \"mongodb\" 3) \"redis\" Redis Set 相关基本命令如下 : 序号 命令及描述 1 SADD key member1 [member2] , 向集合添加一个或多个成员 2 SCARD key , 获取集合的成员数 3 SDIFF key1 [key2] , 返回给定所有集合的差集 4 SDIFFSTORE destination key1 [key2] , 返回给定所有集合的差集并存储在 destination 中 5 SINTER key1 [key2] , 返回给定所有集合的交集 6 SINTERSTORE destination key1 [key2] , 返回给定所有集合的交集并存储在 destination 中 7 SISMEMBER key member , 判断 member 元素是否是集合 key 的成员 8 SMEMBERS key , 返回集合中的所有成员 9 SMOVE source destination member , 将 member 元素从 source 集合移动到 destination 集合 10 SPOP key , 移除并返回集合中的一个随机元素 11 SRANDMEMBER key [count] , 返回集合中一个或多个随机数 12 SREM key member1 [member2] , 移除集合中一个或多个成员 13 SUNION key1 [key2] , 返回所有给定集合的并集 14 SUNIONSTORE destination key1 [key2] , 所有给定集合的并集存储在 destination 集合中 15 SSCAN key cursor [MATCH pattern] [COUNT count] ,迭代集合中的元素 有序集合命令 🍀 有序集合的成员是唯一的,但分数(score)却可以重复 语法 redis 127.0.0.1:6379> COMMAND KEY_NAME 实例 redis 127.0.0.1:6379> ZADD w3ckey 1 redis (integer) 1 redis 127.0.0.1:6379> ZADD w3ckey 2 mongodb (integer) 1 redis 127.0.0.1:6379> ZADD w3ckey 3 mysql (integer) 1 redis 127.0.0.1:6379> ZADD w3ckey 3 mysql (integer) 0 redis 127.0.0.1:6379> ZADD w3ckey 4 mysql (integer) 0 redis 127.0.0.1:6379> ZRANGE w3ckey 0 10 WITHSCORES 1) \"redis\" 2) \"1\" 3) \"mongodb\" 4) \"2\" 5) \"mysql\" 6) \"4\" Redis Sorted Set 相关基本命令如下 : 序号 命令及描述 1 ZADD key score1 member1 [score2 member2] , 向有序集合添加一个或多个成员 , 或者更新已存在成员的分数 2 ZCARD key , 获取有序集合的成员数 3 ZCOUNT key min max , 计算在有序集合中指定区间分数的成员数 4 ZINCRBY key increment member , 有序集合中对指定成员的分数加上增量 increment 5 ZINTERSTORE destination numkeys key [key ...] , 计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中 6 ZLEXCOUNT key min max , 在有序集合中计算指定字典区间内成员数量 7 ZRANGE key start stop [WITHSCORES] , 通过索引区间返回有序集合成指定区间内的成员 8 ZRANGEBYLEX key min max [LIMIT offset count] , 通过字典区间返回有序集合的成员 9 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT] , 通过分数返回有序集合指定区间内的成员 10 ZRANK key member , 返回有序集合中指定成员的索引 11 ZREM key member [member ...] , 移除有序集合中的一个或多个成员 12 ZREMRANGEBYLEX key min max , 移除有序集合中给定的字典区间的所有成员 13 ZREMRANGEBYRANK key start stop , 移除有序集合中给定的排名区间的所有成员 14 ZREMRANGEBYSCORE key min max , 移除有序集合中给定的分数区间的所有成员 15 ZREVRANGE key start stop [WITHSCORES] , 返回有序集中指定区间内的成员 , 通过索引 , 分数从高到底 16 ZREVRANGEBYSCORE key max min [WITHSCORES] , 返回有序集中指定分数区间内的成员 , 分数从高到低排序 17 ZREVRANK key member , 返回有序集合中指定成员的排名 , 有序集成员按分数值递减(从大到小)排序 18 ZSCORE key member , 返回有序集中 , 成员的分数值 19 ZUNIONSTORE destination numkeys key [key ...] , 计算给定的一个或多个有序集的并集 , 并存储在新的 key 中 20 ZSCAN key cursor [MATCH pattern] [COUNT count] , 迭代有序集合中的元素（包括元素成员和元素分值） HyperLogLog命令 🍀 Redis HyperLogLog 是用来做基数统计的算法 , HyperLogLog 的优点是 , 在输入元素的数量或者体积非常非常大时 , 计算基数所需的空间总是固定的 , 并且是很小的 在 Redis 里面 , 每个 HyperLogLog 键只需要花费 12 KB 内存 , 就可以计算接近 2^64 个不同元素的基 数 这和计算基数时 , 元素越多耗费内存就越多的集合形成鲜明对比 但是 , 因为 HyperLogLog 只会根据输入元素来计算基数 , 而不会储存输入元素本身 , 所以 HyperLogLog 不能像集合那样 , 返回输入的各个元素 下表列出了 redis HyperLogLog 的基本命令 : 序号 命令及描述 1 PFADD key element [element ...] , 添加指定元素到 HyperLogLog 中 2 PFCOUNT key [key ...] , 返回给定 HyperLogLog 的基数估算值 3 PFMERGE destkey sourcekey [sourcekey ...] , 将多个 HyperLogLog 合并为一个 HyperLogLog "},"06-Redis/04-Redis - 数据库.html":{"url":"06-Redis/04-Redis - 数据库.html","title":"Redis - 数据库","keywords":"","body":"Redis - 数据库 介绍 🍀 Redis 服务器默认会创建 16 个数据库 , 该值由服务器配置的 databases 选项决定 , 默认为16 , 查看方式如下 : 127.0.0.1:6379> config get databases 1) \"databases\" 2) \"16\" 切换数据库 127.0.0.1:6379> SELECT 1 OK 127.0.0.1:6379[1]> SELECT 0 OK 127.0.0.1:6379> 生存时间 🍀 通过 EXPIRE 命令或者 PEXPIRE 命令 , 客户端可以以秒或者毫秒精度为数据库中的某个键设置生存时间 (Time To Live , TTL) , 在经过指定的秒数或者毫秒数之后 , 服务器就会自动删除生存时间为 0 的键 实例 127.0.0.1:6379> SET key value OK 127.0.0.1:6379> EXPIRE key 5 (integer) 1 127.0.0.1:6379> GET key # 5秒之内 \"value\" 127.0.0.1:6379> GET key # 5秒之后 (nil) 过期时间 🍀 与 EXPIRE 命令和 REXPIRE 命令类似 , 客户端可以通过 EXPIREAT 命令或 PEXPIREAT 命令 , 以秒或者毫秒精度给数据库中的某个键设置过期时间 (Expire Time) 过期时间是 UNIX 时间戳 , 当键的过期时间来临时 , 服务器就会自动从数据库中删除这个键 实例 127.0.0.1:6379> SET key value OK 127.0.0.1:6379> EXPIREAT key 1377257300 (integer) 1 127.0.0.1:6379> Time 1) \"1377257296\" 2) \"296543\" 127.0.0.1:6379> GET key # 1377257300之前 \"value\" 127.0.0.1:6379> Time 1) \"1529772303\" 2) \"230656\" 127.0.0.1:6379> GET key # 1377257300之后 (nil) TTL 命令和 PTTL 命令接受一个带有生存时间或者过期时间的键 , 返回这个键的剩余生存时间 , 也就是 , 返回距离这个键被服务器自动删除还有多长时间 实例 127.0.0.1:6379> SET key value OK 127.0.0.1:6379> EXPIRE key 1000 (integer) 1 127.0.0.1:6379> TTL key (integer) 997 127.0.0.1:6379> PTTL key (integer) 93633 我们可以发现 Redis 有四个不同的命令可以用于设置键的生存时间或过期时间 : EXPIRE # 命令用于将键key的生存时间设置为ttl秒 PEXPIRE # 命令用于将键key的生存时间设置为ttl毫秒 EXPIREAT # 命令用于将键key的生存时间设置为timestamp毫秒 PEXPIREAT # 命令用于将键key的过期时间设置为timestamp毫秒 虽然有多种不同单位和不同形式的设置命令 , 但是实际上 EXPIRE , PEXPIRE , EXPIREAT 三个命令都是使用 PEXPIREAT 命令来实现的 : 无论客户端执行的是以上四个命令中的哪一个 , 经过转换之后 , 最终执行的效果都和执行 PEXPIREAT 命令一样 保存过期时间 当客户端执行 PEXPIREAT 命令 (或者其他三个命令) 为一个数据库键设置过期时间时 , 服务器会在数据库的过期字典中关联给定的数据库键和过期时间 移除过期时间 PERSIST 命令是 PEXPIREAT 命令的反操作 : PERSIST 命令在过期字典中查找给定的键 , 并解除和值 (过期时间) 在过期字典中的关联 Redis 过期键删除策略 : 通过配合使用惰性删除和定期删除两种策略 , 服务器可以很好地在合理使用 CPU 时间和避免浪费内存空间之间取得平衡 持久化 🍀 Redis 是一个键值对数据库服务器 , 服务器中通常包含着任意个非空数据库 , 而每个非空数据库中又可以包含任意个键值对 , 为了方便起见 , 我们将服务器中的非空数据库以及他们的键值对统称为数据库状态 因为 Redis 是内存数据库 , 它将自己的数据库状态储存在内存里面 , 所以如果不想办法将储存在内存中的数据库状态保存到磁盘里面 , 那么一旦服务器进程退出 , 服务器中的数据库状态也会消失不见 , 因此为了解决这个问题 , Redis 提供了持久化功能 RDB持久化 🍀 RDB 持久化功能可以将 Redis 在内存中的数据库状态保存到磁盘里面 , 避免数据意外丢失 RDB 持久化既可以手动执行 , 也可以根据服务器配置选项定期执行 , 该功能可以将某个时间点上的数据库状态保存到一个 RDB 文件中 RDB 持久化功能所生成的 RDB 文件是一个经过压缩的二进制文件 , 通过该文件可以还原生成 RDB 文件时的数据库状态 RDB 文件的创建和载入 有两个命令可以用于生成 RDB 文件 : SAVE , 会阻塞 Redis 服务器进程 , 知道 RDB 文件创建完毕为止 , 在服务器进程阻塞期间 , 服务器不能处理任何请求 redis> SAVE OK BGSAVE , 会派生出一个子进程 , 然后由子进程负责创建 RDB 文件 , 服务器进程 (父进程) 继续处理命令请求 redis> BGSAVE Background saving started 和使用 SAVE 命令或者 BGSAVE 命令创建 RDB 文件不同 , RDB 文件的载入工作是在服务器启动时自动执行的 , 所以 Redis 并没有专门用于载入 RDB 文件的命令 , 只要 Redis 服务器在启动时检测到 RDB 文件存在 , 就会自动载入 RDB 文件 并且服务器在载入 RDB 文件期间 , 会一直处于阻塞状态 , 直到载入工作完成为止 注意 : 在载入或生成 RDB 文件时 , 只会载入未过期的键 , 而过期的键会被直接忽略 AOF持久化 🍀 除了 RDB 持久化功能之外 , Redis 还提供了 AOF (Append Only File) 持久化功能 , 与 RDB 持久化通过保存数据库中的键值对来记录数据库状态不同 , AOF 持久化是通过保存 Redis 服务器所执行的写命令来记录数据库状态的 AOF 持久化功能的实现可以分为命令追加 (Append) , 文件写入 , 文件同步 (Sync) 三个步骤 AOF 文件载入与数据还原 因为 AOF 文件里面包含了重建数据库状态所需的所有写命令 , 所以服务器只要读入并重新执行一遍 AOF 文件里面保存的写命令 , 就可以还原服务器关闭之前的数据库状态 Redis 读取 AOF 文件并还原数据库状态的详细步骤如下 : 创建一个不带网络连接的伪客户端 (fake client) : 因为 Redis 的命令只能在客户端上下文中执行 , 而载入 AOF 文件时所使用的命令直接来源于 AOF 文件而不是网络连接 , 所以服务器使用了一个没有网络连接的伪客户端来执行 AOF 文件保存的写命令 , 伪客户端执行命令的效果和带网络连接的客户端执行命令的效果完全一样 从 AOF 文件中分析并读取出一条写命令 使用伪客户端执行被读出的命令 一直执行步骤 2 和步骤 3 , 知道 AOF 文件中的所有写命令都被处理完毕为止 AOF 重写 由于随着服务器运行时间的流逝 , AOF 文件中的内容会越来越多 , 文件的体积也会越来越大 , 为了解决这 AOF 文件体积膨胀的问题 , Redis 提供了 AOF 文件重写 (rewrite) 功能 , 通过该功能 , Redis 服务器可以创建一个新的 AOF 文件来替代现有的 AOF 文件 和生成 RDB 文件时类似 , 在执行 AOF 重写的过程中 , 程序会对数据库中的键进行检查 , 已过期的键不会被保存到重写后的 AOF 文件中 由于 Redis 是单线程的 , 那么在重写 AOF 文件期间 , 服务器将无法处理客户端发来的命令请求 , 所以 Redis 决定将 AOF 重写程序放到子进程里执行 , 以达到如下目的 : 子进程进行 AOF 重写期间 , 服务器进程 (父进程) 可以继续处理命令请求 子进程带有服务器进程的数据副本 , 使用子进程而不是线程 , 可以在避免使用锁的情况下 , 保证数据的安全性 但是由于子进程进行 AOF 重写期间 , 服务器进程还需要继续处理命令请求 , 而新的命令可能会对现有的数据库状态进程修改 , 从而使得服务器当前的数据库状态和重写后的 AOF 文件所保存的数据库状态不一致 ; 为此 , Redis 服务器设置了一个 AOF 重写缓冲区 , 这个缓冲区在服务器创建子进程之后开始使用 , 当 Redis 服务器执行完一个写命令之后 , 它会同时将这个写命令发送给 AOF 缓冲区和 AOF 重写缓冲区 也就是说 AOF 重写期间 , 服务器进程需要执行以下三个工作 : 执行客户端发来的命令 将执行后的写命令追加到 AOF 缓冲区 将执行后的写命令追加到 AOF 重写缓冲区 当子进程完成 AOF 重写工作后 , 它会向父进程发送一个信号 , 会调用一个信号处理函数 , 并执行以下工作 : 将 AOF 重写缓冲区中的所有内容写入到新的 AOF 文件中 , 这时新 AOF 文件所保存的数据库状态将和服务器当前的数据库状态一致 对新的 AOF 文件进行改名 , 原子地覆盖现有的 AOF 文件 , 完成新旧两个 AOF 文件的替换 注意 : 默认如果 AOF 持久化功能开启 , 那么将优先使用 AOF 发布订阅 🍀 Redis 发布订阅(pub/sub)是一种消息通信模式 : 发送者(pub)发送消息 , 订阅者(sub)接收消息 Redis 客户端可以订阅任意数量的频道 下图展示了频道 channel1 , 以及订阅这个频道的三个客户端 : client2 , client5 和 client1 之间的关系 : 当有新消息通过 PUBLISH 命令发送给频道 channel1 时 , 这个消息就会被发送给订阅它的三个客户端 : 实例 以下实例演示了发布订阅是如何工作的 , 在我们实例中我们创建了订阅频道名为 redisChat : redis 127.0.0.1:6379> SUBSCRIBE redisChat Reading messages... (press Ctrl-C to quit) 1) \"subscribe\" 2) \"redisChat\" 3) (integer) 1 现在 , 我们先重新开启个 redis 客户端 , 然后在同一个频道 redisChat 发布两次消息 , 订阅者就能接收到消息 redis 127.0.0.1:6379> PUBLISH redisChat \"Redis is a great caching technique\" (integer) 1 redis 127.0.0.1:6379> PUBLISH redisChat \"Learn redis by w3cschool.cc\" (integer) 1 # 订阅者的客户端会显示如下消息 1) \"message\" 2) \"redisChat\" 3) \"Redis is a great caching technique\" 1) \"message\" 2) \"redisChat\" 3) \"Learn redis by w3cschool.cc\" Redis 发布订阅常用命令 : 序号 命令及描述 1 PSUBSCRIBE pattern [pattern ...] , 订阅一个或多个符合给定模式的频道 2 PUBSUB subcommand [argument [argument ...]] , 查看订阅与发布系统状态 3 PUBLISH channel message , 将信息发送到指定的频道 4 PUNSUBSCRIBE [pattern [pattern ...]] , 退订所有给定模式的频道 5 SUBSCRIBE channel [channel ...] , 订阅给定的一个或多个频道的信息 6 UNSUBSCRIBE [channel [channel ...]] , 指退订给定的频道 "},"06-Redis/05-Redis - 事务.html":{"url":"06-Redis/05-Redis - 事务.html","title":"Redis - 事务","keywords":"","body":"Redis - 事务 介绍 🍀 Redis 事务可以一次执行多个命令 , 并且带有以下两个重要的保证 : 事务是一个单独的隔离操作 : 事务中的所有命令都会序列化 , 按顺序地执行 , 事务在执行的过程中 , 不会被其他客户端发送来的命令请求所打断 事务是一个原子操作 : 事务中的命令要么全部被执行 , 要么全部都不执行 一个事务从开始到执行会经历以下三个阶段 : 开始事务 命令入队 执行事务 实例 🍀 以下是一个事务的例子 , 它先以 MULTI 开始一个事务 , 然后将多个命令入队到事务中 , 最后由 EXEC 命令触发事务 , 一并执行事务中的所有命令 : redis 127.0.0.1:6379> MULTI OK redis 127.0.0.1:6379> SET book-name \"Mastering C++ in 21 days\" QUEUED redis 127.0.0.1:6379> GET book-name QUEUED redis 127.0.0.1:6379> SADD tag \"C++\" \"Programming\" \"Mastering Series\" QUEUED redis 127.0.0.1:6379> SMEMBERS tag QUEUED redis 127.0.0.1:6379> EXEC 1) OK 2) \"Mastering C++ in 21 days\" 3) (integer) 3 4) 1) \"Mastering Series\" 2) \"C++\" 3) \"Programming\" 单个 Redis 命令的执行是原子性的 , 但 Redis 没有在事务上增加任何维持原子性的机制 , 所以 Redis 事务的执行并不是原子性的 事务可以理解为一个打包的批量执行脚本 , 但批量指令并非原子化的操作 , 中间某条指令的失败不会导致前面已做指令的回滚 , 也不会造成后续的指令不做 比如 : redis 127.0.0.1:7000> multi OK redis 127.0.0.1:7000> set a aaa QUEUED redis 127.0.0.1:7000> set b bbb QUEUED redis 127.0.0.1:7000> set c ccc QUEUED redis 127.0.0.1:7000> exec 1) OK 2) OK 3) OK 如果在 set b bbb 处失败 , set a 已成功不会回滚 , set c 还会继续执行 Redis 事务命令 🍀 下表列出了 redis 事务的相关命令 : 序号 命令及描述 1 DISCARD , 取消事务 , 放弃执行事务块内的所有命令 2 EXEC , 执行所有事务块内的命令 3 MULTI , 标记一个事务块的开始 4 UNWATCH , 取消 WATCH 命令对所有 key 的监视 5 WATCH key [key ...] , 监视一个(或多个) key , 如果在事务执行之前这个(或这些) key 被其他命令所改动 , 那么事务将被打断 "},"06-Redis/06-Redis - 主从复制.html":{"url":"06-Redis/06-Redis - 主从复制.html","title":"Redis - 主从复制","keywords":"","body":"Redis - 主从复制 介绍 🍀 在 Redis 中 , 用户可以通过执行 SLAVEOF 命令或者设置 slaveof 选项 , 让一个服务器去复制 (replicate) 另一个服务器 , 我们称呼被复制的服务器为主服务器 (master) , 而对主服务器进行复制的服务器则被称为从服务器 (slave) 假设现在有两个 Redis 服务器 , 地址分别为 127.0.0.1:6379 和 127.0.0.1:12345 , 如果服务器 127.0.0.1:12345 发送以下命令 : 127.0.0.1:12345> SLAVEOF 127.0.0.1 6379 那么服务器 127.0.0.1:12345 将成为 127.0.0.1:6379 的从服务器 , 而服务器 127.0.0.1:6379 则会成为 127.0.0.1:12345 的主服务器 Redis 的复制功能分为同步 (sync) 和命令传播 (command propagate) 同步 🍀 当客户端向从服务器发送 SLAVEOF 命令 , 要求从服务器复制主服务器时 , 从服务器首先需要执行同步操作 , 也就是 , 将从服务器的数据库状态更新至主服务器当前所处的数据库状态 从服务器对主服务器的同步操作需要通过向主服务器发送 SYNC 命令来完成 , 其执行步骤如下 : 从服务器向主服务器发送 SYNC 命令 收到 SYNC 命令的主服务器执行 BGSAVE 命令 , 在后台生成一个 RDB 文件 , 并使用一个缓冲区记录从现在开始执行的所有写命令 当主服务器的 BGSAVE 命令执行完毕时 , 主服务器会将 BGSAVE 命令生成的 RDB 文件发送给从服务器 , 从服务器接收并载入这个 RDB 文件 , 将自己的数据库状态更新至主服务器执行 BGSAVE 命令时的数据库状态 主服务器将记录在缓冲区里面的所有写命令发送给从服务器 , 从服务器执行这些写命令 , 将自己的数据库状态更新至主服务器数据库当前所处的状态 命令传播 🍀 在同步操作完毕之后 , 主从服务器两者的数据库将达到一致状态 , 但这种一致并不是一成不变的 , 每当主服务器执行客户端发送的写命令时 , 主服务器的数据库就有可能会被修改 , 并导致主从服务器状态不一致 为了让主从服务器再次回到一致状态 , 主服务器需要对从服务器执行命令传播操作 : 主服务器会将自己执行的写命令 , 也即造成主服务器不一致的那条写命令 , 发送给从服务器执行 , 当从服务器执行了相同的写命令之后 , 主从服务器将再次回到一致状态 PSYNC 🍀 由于 SYNC 命令在处理断线重复制情况下时效率低下 , Redis 从 2.8 版本开始 , 使用 PSYNC 命令代替 SYNC 命令来执行复制时的同步操作 PSYNC 命令具有完整重同步 (full resynchronization) 和部分重同步 (partial resynchronization) 两种模式 : 完整重同步用于处理初次复制情况 : 完整重同步的执行步骤与 SYNC 命令的执行步骤一样 , 它们都是通过让主服务器创建并发送 RDB 文件 , 以及向从服务器发送保存在缓冲区里面的写命令来进行同步 而部分重同步则用于处理断线后重复制情况 : 当从服务器在断线后重新连接主服务器时 , 如果条件允许 , 主服务器可以将主从服务器连接断开期间执行的写命令发送给从服务器 , 从服务器只要接收并执行这些写命令 , 就可以将数据库更新至主服务器当前所处的状态 "},"06-Redis/07-Redis - 集群.html":{"url":"06-Redis/07-Redis - 集群.html","title":"Redis - 集群","keywords":"","body":"Redis - 集群 介绍 🍀 Redis 集群是 Redis 提供的分布式数据库方案 , 集群通过分片 (sharding) 来进行数据共享 , 并提供复制和故障转移功能 节点 🍀 一个 Redis 集群通常由多个节点 (node) 组成 , 在刚开始的时候 , 每个节点都是相互独立的 , 它们都处于一个只包含自己的集群当中 , 要组建一个真正可工作的集群 , 我们必须将各个独立的节点连接起来 , 构成一个包含多个节点的集群 连接各个节点可以使用 CLUSTER MEET 命令来完成 , 命令格式如下 : CLUSTER MEET 查看集群节点 CLUSTER NODES 一个节点就是一个运行在集群模式下的 Redis 服务器 , Redis 服务器在启动时会根据 cluster-enabled 配置选项是否为 yes 来决定是否开启服务器的集群模式 槽指派 🍀 Redis 集群通过分片的方式来保存数据库中的键值对 : 集群的整个数据被分为 16384 个槽 (slot) , 数据库中的每个键都属于这 16384 个槽的其中一个 , 集群中的每个节点可以处理 0 个或最多 16384 个槽 当数据库中的 16384 个槽都有节点在处理时 , 集群处于上线状态 , 相反的 , 如果数据库中公有任何一个槽没有得到处理 , 那么集群处于下线状态 通过向节点发送 CLUSTER ADDSLOTS 命令 , 我们可以将一个或多个槽指派给节点负责 CLUSTER ADDSLOTS [slot ...] 当客户端向节点发送与数据库键有关的命令时 , 接收命令的节点会计算出命令要处理的数据库键属于哪个槽 , 并检查这个槽是否指派给了自己 : 如果键所在的槽正好就指派给了当前节点 , 那么节点直接执行这个命令 如果键所在的槽并没有指派给当前节点 , 那么节点会向客户端返回一个 MOVED 错误 , 指引客户端转向至正确的节点 , 并再次发送之前想要执行的命令 如果需要将已经指派给某个节点的槽改为指派给另一个节点 , 可以执行重新分片操作 复制与故障转移 🍀 Redis 集群中的节点分为主节点 (master) 和从节点 (slave) , 其中主节点用于处理槽 , 而从节点则用于复制某个节点 , 并在被复制的主节点下线时 , 代替下线主节点继续处理命令请求 设置从节点 # 可以让接收命令的节点成为node_id所指定节点的从节点,并开始节点主从复制 CLUSTER REPLICATE 故障检测 集群中的每个节点都会定期地向集群中的其他节点发送 PING 消息 , 以此来检测对方是否在线 , 如果接收 PING 消息的节点没有在规定的时间内 , 向发送 PING 消息的节点返回 PONG 消息 , 那么发送 PING 消息的节点就会将接收 PING 消息的节点标记为疑似下线 (probable fail , PFAIL) 集群中的各个节点会通过互相发消息的方式来交换集群中各个节点的状态信息 , 例如某个节点是处于在线状态 , 疑似下线状态 , 还是已下线状态 如果在一个集群里面 , 半数以上负责处理槽的主节点将某个主节点 x 报告为疑似下线 , 那么这个主节点 x 将被标记为已下线 , 将主节点 x 标记为下线的节点会向集群广播一条关于主节点 x 的 FALL 消息 , 所有收到这条 FALL 消息的节点都会立即将主节点 x 标记为已下线 故障转移 当一个从节点发现自己正在复制的主节点进入了已下线状态时 , 从节点将会开始对下线主节点进行故障转移 , 以下是故障转移的执行步骤 : 复制下线主节点的所有从节点里面 , 会有一个从节点被选中 被选中的从节点会执行 SLAVEOF no one 命令 , 成为新的主节点 新的主节点会撤销所有对已下线主节点的槽指派 , 并将这些槽全部指派给自己 新的主节点向集群广播一条 PONG 消息 , 这条 PONG 消息可以让集群中的其他节点立即知道这个节点已经由从节点变成了主节点 , 并且这个主节点已经接管了原本由已下线节点负责处理的槽 新的主节点开始接收和自己负责处理的槽有关的命令请求 , 故障转移完成 新的主节点是通过选举产生的 消息 🍀 集群中的各个节点通过发送和接收消息来进行通信 , 我们称发送消息的节点为发送者 , 接收消息的节点为接收者 节点发送的消息主要有以下五种 : MEET 消息 : 当发送者接到客户端发送的 CLUSTER MEET 命令时 , 发送会向接收者发送 MEET 消息 , 请求接收者加入到发送者当前所处的集群里面 PING 消息 : 集群里的每个节点默认每隔一秒钟就会从已知节点列表中随机选出五个节点 , 然后对五个节点中最长时间没有发送过 PING 消息的节点发送 PING 消息 , 以此来检测被选中的节点是否在线 ; 除此之外 , 如果节点 A 最后一次收到节点 B 发送的 PONG 消息的时间 , 距离当前时间已经超过了节点 A 的 cluster-node-timeout 选项设置时长的一半 , 那么节点 A 也会向节点 B 发送 PING 消息 , 这可以防止节点 A 因为长时间没有随机选中节点 B 作为 PING 消息的发送对象而导致节点 B 的信息更新滞后 PONG 消息 : 当接收者收到发送者发来的 MEET 消息时 , 为了向发送者确认这条 MEET 消息或者 PING 消息已达到 , 接收者会向发送者返回一条 PONG 消息 , 另外 , 一个节点也可以通过向集群广播自己的 PONG 消息来让集群中的其他节点立即刷新关于这个节点的认识 , 例如当一次故障转移操作成功执行之后 , 新的主节点会向集群广播一条 PONG 消息 , 以此来让集群中的其他节点立即知道这个节点已经变成了主节点 , 并且接管了已经下线节点负责的槽 FAIL 消息 : 当一个主节点 A 判断另一个主节点 B 已经进入 FAIL 状态时 , 节点 A 会向集群广播一条关于节点 B 的 FAIL 消息 , 所有收到这条消息的节点都会立即将节点 B 标记为已下线 PUBLISH 消息 : 当节点接收到一个 PUBLISH 命令时 , 节点会执行这个命令 , 并向集群广播一条 PUBLIST 消息 , 所有接收到这条 PUBLISH 消息的节点都会执行相同的 PUBLISH 命令 "},"06-Redis/08-Redis - Sentinel.html":{"url":"06-Redis/08-Redis - Sentinel.html","title":"Redis - Sentinel","keywords":"","body":"Redis - Sentinel 介绍 🍀 Sentinel (哨岗 , 哨兵) 是 Redis 的高可用性 (high availability) 解决方案 : 由一个或多个 Sentinel 实例组成的 Sentinel 系统可以监视任意多个主服务器 , 并在被监视的主服务器进入下线状态时 , 自动将下线主服务器属下的某个从服务器升级为新的主服务器 , 然后由新的主服务器代替已下线的主服务器继续处理命令请求 启动并初始化Sentinel 🍀 启动一个 Sentinel 可以使用命令 : $ redis-sentinel /path/to/your/sentinel.conf 或者命令 : $ redis-sentinel /path/to/your/sentinel.conf --sentinel 当一个 Sentinel 启动时 , 它需要执行以下步骤 : 初始化服务器 将普通 Redis 服务器使用的代码替换成 Sentinel 专用代码 初始化 Sentinel 状态 根据给定的配置文件 , 初始化 Sentinel 的监视主服务器列表 创建连向主服务器的网络连接 初始化服务器 🍀 因为 Sentinel 本质上只是一个运行在特殊模式下的 Redis 服务器 , 所以启动 Sentinel 的第一步 , 就是初始化一个普通的 Redis 服务器 ; 不过 , 因为 Sentinel 执行的工作和普通 Redis 服务器执行的工作不同 , 所以 Sentinel 的初始化过程和普通 Redis 服务器的初始化过程并不完全相同 例如 , 普通服务器在初始化时会通过载入 RDB 文件或者 AOF 文件来还原数据库状态 , 但是因为 Sentinel 并不适用数据库 , 所以初始化 Sentinel 时就不会载入 RDB 文件或者 AOF 文件 Sentinel 模式下 Redis 服务器主要功能的使用情况如下 : 功能 使用情况 数据库和键值对方面的命令 , 不如 SET , DEL , FLUSHDB 不使用 事务命令 , 比如 MULTI 和 WATCH 不使用 脚本命令 , 比如 EVAL 不使用 RDB 持久化命令 , 比如 SAVE 和BGSAVE 不使用 AOF 持久化命令 , 比如 BGREWRITEAOF 不使用 复制命令 , 比如 SLAVEOF Sentinel 内部可以使用 , 但客户端不可以使用 发布与订阅命令 , 比如 PUBLISH 和 SUBSCRIBE SUBSCRIBE , PSUBSCRIBE , UNSUBSCRIBE , PUNSUBSCRIBE 四个命令在 Sentinel 内部和客户端都可以使用 , 但 PUBLISH 命令只能在 Sentinel内部使用 文件事件处理器 (负责发送命令请求 , 处理命令回复) Sentinel 内部使用 , 但关联的文件事件处理器和普通 Redis 服务器不同 时间事件处理器 (负责执行 serverCron函数) Sentinel 内部使用 , 时间事件的处理器仍然是 serverCron 函数 , serverCron 函数会调用 sentinel.c/sentinelTimer 函数 , 后者包含了 Sentinel 要执行的所有操作 初始化 Sentinel 的最后一步是创建连向被监视主服务器的网络连接 , Sentinel 将成为主服务器的客户端 , 它可以向主服务器发送命令 , 并从命令回复中获取相关的信息 对于每个被 Sentinel 监视的主服务器来说 , Sentinel 会创建两个连向主服务器的异步网络连接 : 一个是命令连接 , 这个连接专门用于主服务器发送命令 , 并接收命令回复 另一个是订阅连接 , 这个连接专门用于订阅主服务器的 __sentinel__:hello 频道 Sentinel 默认会以每十秒一次的频率 , 通过命令连接向被监视的主服务器发送 INFO 命令 , 并通过分析 INFO 命令的回复来获取主服务器的当前信息 通过分析主服务器返回的 INFO 命令回复 , Sentinel 可以获取以下两方面的信息 : 一方面是关于主服务器本身的信息 , 包括 run_id 域记录的服务器运行 ID , 以及 role 域记录的服务器角色 另一方面是关于主服务器属下所有从服务器的信息 , 每个从服务器都由一个 \"slave\" 字符串开头的行记录 , 每行的 ip= 域记录了从服务器的 IP 地址 , 而 port= 域则记录了从服务器的端口号 , 根据这些 IP 地址和端口号 , Sentinel 无须用户提供从服务器的地址信息 , 就可以自动发现从服务器 检测主观下线状态 🍀 在默认情况下 , Sentinel 会以每秒一次的频率向所有与它创建了命令连接的实例 (包括主服务器 , 从服务器 , 其他 Sentinel 在内) 发送 PING 命令 , 并通过实例返回的 PING 命令回复来判断实例是否在线 Sentinel 配置文件中的 down-after-milliseconds 毫秒内 , 连续向 Sentinel 返回无效回复 , 那么 Sentinel 会修改这个实例所对应的实例结构 , 在结构的 flags 属性中打开 SRI_S_DOWN 标识 , 以此来标识这个实例已经进入主观下线状态 用户设置的 down-after-milliseconds 选项的值 , 不仅会被 Senitinel 用来判断主服务器的主观下线状态 , 还会被用于判断主服务器属下的所有从服务器 , 以及所有同样监视这个主 服务器的其他 Sentinel 的主观下线状态 检测客观下线状态 🍀 当 Sentinel 将一个主服务器判断为主观下线之后 , 为了确认这个主服务器是否真的下线了 , 它会向同样监视这一主服务器的其他 Sentinel 进行询问 , 看它们是否也认为主服务器已经进入了下线状态 (可以是主观下线或者客观下线) , 当 Sentinel 从其他 Sentinel 那里接收到足够数量的已下线判断之后 , Sentinel 就会将从服务器判定为客观下线 , 并对主服务器执行故障转移操作 选举领头 Sentinel 🍀 当一个主服务器被判断为客观下线时 , 监视这个下线主服务器的各个 Sentinel 会进行协商 , 选举出一个领头 Sentinel , 并由领头 Sentinel 对下线主服务器执行故障转移操作 故障转移操作包括以下三个步骤 : 在已下线主服务器属下的所有从服务器里面 , 挑选一个从服务器 , 并将其转换为主服务器 让已下载主服务器属下的所有从服务器改为复制新的主服务器 将已下线主服务器设置为新的主服务器的从服务器 , 当这个旧的主服务器重新上线时 , 它就会成为新的主服务器的从服务器 "},"06-Redis/Python操作MongoDB.html":{"url":"06-Redis/Python操作MongoDB.html","title":"Python操作MongoDB","keywords":"","body":"Python操作MongoDB 介绍 🍀 MongoDB 是一个基于分布式文件存储的数据库 , 由 C++ 编写 MongoDB 是一个介于关系型数据库和非关系型数据库 (NoSQL) 之间的产品 , 是非关系数据库当中功能最丰富 , 最像关系数据库的 它有如下优点 : 文档型存储 使用高效的二进制 BSON 作为数据存储 , BSON 是一个类似 JSON 的格式 , 选择 BSON 可以提供更快的遍历速度 , 提供比 JSON 更多的内置数据类型 自带高可用及分区的解决方案 , 分别为副本集 (Replica Set) 和分片(sharding) 基于文档的富查询语言 , MongoDB 支持动态查询 , 支持非常多的查询方式 , 并且可以对文档中的属性建立索引 内置聚合工具 , 可以通过 MapReduce 等方式进行复杂的统计核并行计算 MongoDB 在 3.0 之后增加了高性能 , 可伸缩 , 支持压缩和文档级锁的数据存储引擎 WiredTiger MongoDB概念 🍀 SQL术语/概念 MongoDB术语/概念 解释/说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins 表连接 , MongoDB 不支持 primary key primary key 主键 , MongoDB 自动将_id字段设置为主键 数据库 🍀 在 MongoDB 中 , 多个文档组成集合 , 多个集合可以组成数据库 数据库也通过名字来标识 , 数据库名可以是满足以下条件的任意UTF-8字符串 : 不能是空字符串 (\"\") 不得含有' ' (空格) , . , $ , / , \\ 和 \\0 (空字符) 应全部小写 最多64字节 有一些数据库名是保留的 , 可以直接访问这些有特殊作用的数据库 admin : 从身份认证的角度讲 , 这是 “root” 数据库 , 如果将一个用户添加到admin数据库 , 这个用户将自动获得所有数据库的权限 , 再者 , 一些特定的服务器端命令也只能从admin数据库运行 , 如列出所有数据库或关闭服务器 local: 这个数据库永远都不可以复制 , 且一台服务器上的所有本地集合都可以存储在这个数据库中 config: MongoDB用于分片设置时 , 分片信息会存储在config数据库中 集合 🍀 集合就是一组文档 , 如果将MongoDB中的一个文档比喻为关系型数据的一行 , 那么一个集合就是相当于一张表 集合存在于数据库中 , 通常情况下为了方便管理 , 不同格式和类型的数据应该插入到不同的集合 , 但其实集合没有固定的结构 , 这意味着我们完全可以把不同格式和类型的数据统统插入一个集合中 组织子集合的方式就是使用 “.” , 分隔不同命名空间的子集合 比如一个具有博客功能的应用可能包含两个集合 , 分别是 blog.posts 和 blog.authors , 这是为了使组织结构更清晰 , 这里的 blog 集合 (这个集合甚至不需要存在）跟它的两个子集合没有任何关系 在MongoDB中 , 使用子集合来组织数据非常高效 , 值得推荐 当第一个文档插入时 , 集合就会被创建 , 合法的集合名 : 集合名不能是空字符串\"\" ; 集合名不能含有 \\0 字符 (空字符) , 这个字符表示集合名的结尾 ; 集合名不能以 \"system.\" 开头 , 这是为系统集合保留的前缀 ; 用户创建的集合名字不能含有保留字符 , 有些驱动程序的确支持在集合名里面包含 , 这是因为某些系统生成的集合中包含该字符 ; 除非你要访问这种系统创建的集合 , 否则千万不要在名字里出现$ ; 文档 🍀 文档是MongoDB的核心概念 , 文档就是键值对的一个有序集 {'msg':'hello','foo':3} , 类似于python中的有序字典 需要注意的是 : 文档中的键/值对是有序的 ; 文档中的值不仅可以是在双引号里面的字符串 , 还可以是其他几种数据类型 (甚至可以是整个嵌入的文档) ; MongoDB区分类型和大小写 ; MongoDB的文档不能有重复的键 ; 文档中的值可以是多种不同的数据类型 , 也可以是一个完整的内嵌文档。文档的键是字符串。除了少数例外情况 , 键可以使用任意UTF-8字符 ; 文档键命名规范 : 键不能含有 \\0 (空字符) , 这个字符用来表示键的结尾 ; . 和 $ 有特别的意义 , 只有在特定环境下才能使用 ; 以下划线 \"_\" 开头的键是保留的(不是严格要求的) ; PS : 把数据库名添加到集合名前 , 得到集合的完全限定名 , 即命名空间 , 如 : 如果要使用 test 数据库中的 coll.posts 集合 , 这个集合的命名空间就是 test.coll.ports , 命名空间的长度不得超过121个字节 , 且在实际使用中应该小于100个字节 连接MongoDB 🍀 >>> from pymongo import MongoClient # 默认主机与端口 >>> client = MongoClient() # 指定主机与端口 >>> client = MongoClient('localhost', 27017) # MongoDBURI格式 >>> client = MongoClient('mongodb://localhost:27017/') 获取数据库 🍀 >>> db = client.test_database # 如果你的数据库是这样的test-database,可以使用字典点方式 >>> db = client['test-database'] 获取集合 🍀 >>> collection = db.test_collection >>> collection = db['test-collection'] 关于 MongoDB 中的集合和数据库一个重要注意事项是 , 它们是延迟创建的 , 上面的命令实际上都没有在MongoDB 服务器上执行任何操作 , 而是当第一个文档插入到集合和数据库中时 , 才创建集合和数据库 文档 🍀 MongoDB中的数据使用JSON样式的文档表示(并存储)。在Pymono中 , 我们使用字典来表示文档 , 如下 : >>> import datetime >>> post = {\"author\": \"Mike\", ... \"text\": \"My first blog post!\", ... \"tags\": [\"mongodb\", \"python\", \"pymongo\"], ... \"date\": datetime.datetime.utcnow()} 插入文档 🍀 单条插入 >>> posts = db.posts >>> post_id = posts.insert_one(post).inserted_id >>> post_id ObjectId('...') 批量插入 >>> new_posts = [{\"author\": \"Mike\", ... \"text\": \"Another post!\", ... \"tags\": [\"bulk\", \"insert\"], ... \"date\": datetime.datetime(2009, 11, 12, 11, 14)}, ... {\"author\": \"Eliot\", ... \"title\": \"MongoDB is fun\", ... \"text\": \"and pretty easy too!\", ... \"date\": datetime.datetime(2009, 11, 10, 10, 45)}] >>> result = posts.insert_many(new_posts) >>> result.inserted_ids [ObjectId('...'), ObjectId('...')] 查询文档 🍀 查看数据库中所有集合 >>> db.collection_names(include_system_collections=False) [u'posts'] 单条查询 # pprint用于数据格式化 >>> import pprint >>> pprint.pprint(posts.find_one()) {u'_id': ObjectId('...'), u'author': u'Mike', u'date': datetime.datetime(...), u'tags': [u'mongodb', u'python', u'pymongo'], u'text': u'My first blog post!'} 指定条件查询 >>> pprint.pprint(posts.find_one({\"author\": \"Mike\"})) {u'_id': ObjectId('...'), u'author': u'Mike', u'date': datetime.datetime(...), u'tags': [u'mongodb', u'python', u'pymongo'], u'text': u'My first blog post!'} 按对象查询 >>> post_id ObjectId(...) >>> pprint.pprint(posts.find_one({\"_id\": post_id})) {u'_id': ObjectId('...'), u'author': u'Mike', u'date': datetime.datetime(...), u'tags': [u'mongodb', u'python', u'pymongo'], u'text': u'My first blog post!'} 由字符串转换成对象 from bson.objectid import ObjectId # The web framework gets post_id from the URL and passes it as a string def get(post_id): # Convert from string to ObjectId: document = client.db.collection.find_one({'_id': ObjectId(post_id)}) 多条查询 >>> for post in posts.find(): ... pprint.pprint(post) ... {u'_id': ObjectId('...'), u'author': u'Mike', u'date': datetime.datetime(...), u'tags': [u'mongodb', u'python', u'pymongo'], u'text': u'My first blog post!'} {u'_id': ObjectId('...'), u'author': u'Mike', u'date': datetime.datetime(...), u'tags': [u'bulk', u'insert'], u'text': u'Another post!'} {u'_id': ObjectId('...'), u'author': u'Eliot', u'date': datetime.datetime(...), u'text': u'and pretty easy too!', u'title': u'MongoDB is fun'} 指定条件查询 >>> for post in posts.find({\"author\": \"Mike\"}): ... pprint.pprint(post) ... {u'_id': ObjectId('...'), u'author': u'Mike', u'date': datetime.datetime(...), u'tags': [u'mongodb', u'python', u'pymongo'], u'text': u'My first blog post!'} {u'_id': ObjectId('...'), u'author': u'Mike', u'date': datetime.datetime(...), u'tags': [u'bulk', u'insert'], u'text': u'Another post!'} 计数查询 🍀 获取查询结果总条数 >>> posts.count() 3 >>> posts.find({\"author\": \"Mike\"}).count() 2 范围查询 🍀 >>> d = datetime.datetime(2009, 11, 12, 12) >>> for post in posts.find({\"date\": {\"$lt\": d}}).sort(\"author\"): ... pprint.pprint(post) ... {u'_id': ObjectId('...'), u'author': u'Eliot', u'date': datetime.datetime(...), u'text': u'and pretty easy too!', u'title': u'MongoDB is fun'} {u'_id': ObjectId('...'), u'author': u'Mike', u'date': datetime.datetime(...), u'tags': [u'bulk', u'insert'], u'text': u'Another post!'} 索引 🍀 创建索引 >>> result = db.profiles.create_index([('user_id', pymongo.ASCENDING)], ... unique=True) >>> sorted(list(db.profiles.index_information())) [u'_id_', u'user_id_1'] 更多 pymongo "},"06-Redis/Python操作Redis.html":{"url":"06-Redis/Python操作Redis.html","title":"Python操作Redis","keywords":"","body":"Python使用Redis流程 安装redis-py 🍀 $ pip install redis 创建Redis接口对象 🍀 创建 Redis 接口对象实例 , 将通过实例对 Redis 进行操作 有两种创建方式 : Redis : 继承 StrictRedis 类 , 用于向后兼容旧版本的 redis-py StrictRedis : 实现大部分官方的命令 , 并使用官方的语法和命令 import redis r = redis.Redis(host='127.0.0.1', port=6379) r = redis.StrictRedis(host='127.0.0.1', port=6379) 使用连接池 🍀 通过连接池管理 Redis 对象 默认每个 Redis 实例都会维护一个自己的连接池 , 可以建立一个连接池 , 然后作为参数创建 Redis 实例 , 以此实现 Redis 实例共享连接池 import redis pool = redis.ConnectionPool(host='127.0.0.1', port=6379) r = redis.Redis(connection_pool=pool) String操作 🍀 def set(name, value, ex=None, px=None, nx=False, xx=False): \"\"\" 设置值 , 默认不存在则创建 , 存在则修改 ex , 过期时间 (秒) px , 过期时间 (毫秒) nx , 如果设置为True , 则只有name不存在时 , 当前set操作才执行 xx , 如果设置为True , 则只有name存在时 , 岗前set操作才执行 \"\"\" def setnx(name, value): \"\"\" 设置值 , 只有name不存在时 , 执行设置操作 \"\"\" def setex(name, value, time): \"\"\" 设置值 time , 过期时间 (数字秒 或 timedelta对象) \"\"\" def psetex(name, time_ms, value): \"\"\" 设置值 time_ms , 过期时间 (数字毫秒 或 timedelta对象) \"\"\" def mset(*args, **kwargs): \"\"\" 批量设置值,参数为关键字或字典 \"\"\" def get(name): \"\"\" 获取值 \"\"\" def mget(keys, *args): \"\"\" 批量获取,如: mget('1', '2') mget(['1', '2']) \"\"\" def getset(name, value): \"\"\" 设置新值并获取原来的值 \"\"\" def getrange(key, start, end): \"\"\" 获取子序列 (根据字节获取 , 非字符) name , Redis 的 name start , 起始位置 (字节) end , 结束位置 (字节) \"\"\" def setrange(name, offset, value): \"\"\" 修改字符串内容 , 从指定字符串索引开始向后替换 (新值太长时 , 则向后添加) offset , 字符串的索引 , 字节 (一个汉字三个字节) value , 要设置的值 \"\"\" def setbit(name, offset, value): \"\"\" 对name对应值的二进制表示的位进行操作 \"\"\" def getbit(name, offset): \"\"\" 获取name对应的值的二进制表示中的某位的值 (0或1) \"\"\" def bitcount(key, start=None, end=None): \"\"\" 获取name对应的值的二进制表示中 1 的个数 key , Redis的name start , 位起始位置 end , 位结束位置 \"\"\" def bitop(operation, dest, *keys): \"\"\" 获取多个值 , 并将值做位运算 , 将最后的结果保存至新的name对应的值 operation,AND (并) 、 OR (或) 、 NOT (非) 、 XOR (异或) dest, 新的Redis的name *keys,要查找的Redis的name \"\"\" def strlen(name): \"\"\" 返回name对应值的字节长度 (一个汉字3个字节) \"\"\" def incr(self, name, amount=1): \"\"\" 自增 name对应的值 , 当name不存在时 , 则创建name＝amount , 否则 , 则自增。 amount,自增数 (必须是整数) \"\"\" def incrbyfloat(self, name, amount=1.0): \"\"\" 自增 name对应的值 , 当name不存在时 , 则创建name＝amount , 否则 , 则自增。 amount,自增数 (浮点型) \"\"\" def decr(self, name, amount=1): \"\"\" 自减 name对应的值 , 当name不存在时 , 则创建name＝amount , 否则 , 则自减 amount,自减数 (整数) \"\"\" def append(key, value): \"\"\" 在redis name对应的值后面追加内容 \"\"\" Hash操作 🍀 def hset(name, key, value): \"\"\" name对应的hash中设置一个键值对 (不存在 , 则创建；否则 , 修改) \"\"\" def hsetnx(name, key, value): \"\"\" 当name对应的hash中不存在当前key时则创建 (相当于添加) \"\"\" def hmset(name, mapping): \"\"\" 在name对应的hash中批量设置键值对 name , redis的name mapping , 字典 , 如 : hmset('xx', {'k1':'v1', 'k2': 'v2'}) \"\"\" def hget(name,key): \"\"\" 在name对应的hash中获取根据key获取value \"\"\" def hmget(name, keys, *args): \"\"\" 在name对应的hash中获取多个key的值 name , reids对应的name keys , 要获取key集合 , 如 : ['k1', 'k2', 'k3'] *args , 要获取的key , 如 : k1,k2,k3 r.mget('xx', ['k1', 'k2']) 或 r.hmget('xx', 'k1', 'k2') \"\"\" def hgetall(name): \"\"\" 获取name对应hash的所有键值 \"\"\" def hlen(name): \"\"\" 获取name对应的hash中键值对的个数 \"\"\" def hkeys(name): \"\"\" 获取name对应的hash中所有的key的值 \"\"\" def hvals(name): \"\"\" 获取name对应的hash中所有的key的值 \"\"\" def hexists(name, key): \"\"\" 检查name对应的hash是否存在当前传入的key \"\"\" def hdel(name,*keys): \"\"\" 将name对应的hash中指定key的键值对删除 \"\"\" def hincrby(name, key, amount=1): \"\"\" 自增name对应的hash中的指定key的值 , 不存在则创建key=amount name , redis中的name key , hash对应的key amount , 自增数 (整数) \"\"\" def hincrbyfloat(name, key, amount=1.0): \"\"\" 自增name对应的hash中的指定key的值 , 不存在则创建key=amount name , redis中的name key , hash对应的key amount , 自增数 (浮点数) \"\"\" def hscan(name, cursor=0, match=None, count=None): \"\"\" 增量式迭代获取 , 对于数据大的数据非常有用 , hscan可以实现分片的获取数据 , 并非一次性将数据全部获取完 , 从而放置内存被撑爆 name , redis的name cursor , 游标 (基于游标分批取获取数据) match , 匹配指定key , 默认None 表示所有的key count , 每次分片最少获取个数 , 默认None表示采用Redis的默认分片个数 如 : 第一次 : cursor1, data1 = r.hscan('xx', cursor=0, match=None, count=None) 第二次 : cursor2, data1 = r.hscan('xx', cursor=cursor1, match=None, count=None) ... 直到返回值cursor的值为0时 , 表示数据已经通过分片获取完毕 \"\"\" def hscan_iter(name, match=None, count=None): \"\"\" 利用yield封装hscan创建生成器 , 实现分批去redis中获取数据 match , 匹配指定key , 默认None 表示所有的key count , 每次分片最少获取个数 , 默认None表示采用Redis的默认分片个数 \"\"\" List操作 🍀 def lpush(name,values): \"\"\" 在name对应的list中添加元素 , 每个新的元素都添加到列表的最左边 \"\"\" def rpush(name, values): \"\"\" 表示从右向左操作 \"\"\" def lpushx(name,value): \"\"\" 在name对应的list中添加元素 , 只有name已经存在时 , 值添加到列表的最左边 \"\"\" def rpushx(name, value): \"\"\" 表示从右向左操作 \"\"\" def llen(name): \"\"\" 返回list的长度 \"\"\" def linsert(name, where, refvalue, value): \"\"\" 在name对应的列表的某一个值前或后插入一个新值 name , redis的name where , BEFORE或AFTER refvalue , 标杆值 , 即 : 在它前后插入数据 value , 要插入的数据 \"\"\" def lset(name, index, value): \"\"\" 对name对应的list中的某一个索引位置重新赋值 name , redis的name index , list的索引位置 value , 要设置的值 \"\"\" def lrem(name, value, num): \"\"\" 在name对应的list中删除指定的值 name , redis的name value , 要删除的值 num , num=0删除列表中所有的指定值 num=2,从前到后 , 删除2个 num=-2,从后向前 , 删除2个 \"\"\" def lpop(name): \"\"\" 在name对应的列表的左侧获取第一个元素并在列表中移除 , 返回值则是第一个元素 \"\"\" def rpop(name): \"\"\" 表示从右向左操作 \"\"\" def lindex(name, index): \"\"\" 在name对应的列表中根据索引获取列表元素 \"\"\" def lrange(name, start, end): \"\"\" 在name对应的列表分片获取数据 name , redis的name start , 索引的起始位置 end , 索引结束位置 \"\"\" def ltrim(name, start, end): \"\"\" 在name对应的列表中移除没有在start-end索引之间的值 name , redis的name start , 索引的起始位置 end , 索引结束位置 \"\"\" def rpoplpush(src, dst): \"\"\" 从一个列表取出最右边的元素 , 同时将其添加至另一个列表的最左边 src , 要取数据的列表的name dst , 要添加数据的列表的name \"\"\" def blpop(keys, timeout): \"\"\" 将多个列表排列 , 按照从左到右去pop对应列表的元素 keys , redis的name的集合 timeout , 超时时间 , 当元素所有列表的元素获取完之后 , 阻塞等待列表内有数据的时间 (秒), 0 表示永远阻塞 \"\"\" def brpop(keys, timeout): \"\"\" 从右向左获取数据 \"\"\" def brpoplpush(src, dst, timeout=0): \"\"\" 从一个列表的右侧移除一个元素并将其添加到另一个列表的左侧 src , 取出并要移除元素的列表对应的name dst , 要插入元素的列表对应的name timeout , 当src对应的列表中没有数据时 , 阻塞等待其有数据的超时时间 (秒) , 0 表示永远阻塞 \"\"\" 自定义增量迭代 # 由于redis类库中没有提供对列表元素的增量迭代 , 如果想要循环name对应的列表的所有元素 , 那么就需要 : # 1、获取name对应的所有列表 # 2、循环列表 # 但是 , 如果列表非常大 , 那么就有可能在第一步时就将程序的内容撑爆 , 所有有必要自定义一个增量迭代的功能 : def list_iter(name): \"\"\" 自定义redis列表增量迭代 :param name: redis中的name , 即 : 迭代name对应的列表 :return: yield 返回列表元素 \"\"\" list_count = r.llen(name) for index in xrange(list_count): yield r.lindex(name, index) # 使用 for item in list_iter('pp'): print(item) Set操作 🍀 def sadd(name,values): \"\"\" name对应的集合中添加元素 \"\"\" def scard(name): \"\"\" 获取name对应的集合中元素个数 \"\"\" def sdiff(keys, *args): \"\"\" 在第一个name对应的集合中且不在其他name对应的集合的元素集合 \"\"\" def sdiffstore(dest, keys, *args): \"\"\" 获取第一个name对应的集合中且不在其他name对应的集合 , 再将其新加入到dest对应的集合中 \"\"\" def sinter(keys, *args): \"\"\" 获取多一个name对应集合的并集 \"\"\" def sinterstore(dest, keys, *args): \"\"\" 获取多一个name对应集合的并集 , 再讲其加入到dest对应的集合中 \"\"\" def sismember(name, value): \"\"\" 检查value是否是name对应的集合的成员 \"\"\" def smembers(name): \"\"\" 获取name对应的集合的所有成员 \"\"\" def smove(src, dst, value): \"\"\" 将某个成员从一个集合中移动到另外一个集合 \"\"\" def spop(name): \"\"\" 从集合的右侧 (尾部)移除一个成员 , 并将其返回 \"\"\" def srandmember(name, numbers): \"\"\" 从name对应的集合中随机获取 numbers 个元素 \"\"\" def srem(name, values): \"\"\" 在name对应的集合中删除某些值 \"\"\" def sunion(keys, *args): \"\"\" 获取多一个name对应的集合的并集 \"\"\" def sunionstore(dest,keys, *args): \"\"\" 获取多一个name对应的集合的并集 , 并将结果保存到dest对应的集合中 \"\"\" def sscan(name, cursor=0, match=None, count=None): sscan_iter(name, match=None, count=None): \"\"\" 同字符串的操作 , 用于增量迭代分批获取元素 , 避免内存消耗太大 \"\"\" Zset操作 🍀 def zadd(name, *args, **kwargs): \"\"\" 在name对应的有序集合中添加元素 zadd('zz', 'n1', 1, 'n2', 2) 或 zadd('zz', n1=11, n2=22) \"\"\" def zcard(name): \"\"\" 获取name对应的有序集合元素的数量 \"\"\" def zcount(name, min, max): \"\"\" 获取name对应的有序集合中分数 在 [min,max] 之间的个数 \"\"\" def zincrby(name, value, amount): \"\"\" 自增name对应的有序集合的 name 对应的分数 \"\"\" def zrange( name, start, end, desc=False, withscores=False, score_cast_func=float): \"\"\" 按照索引范围获取name对应的有序集合的元素 name , redis的name start , 有序集合索引起始位置 (非分数) end , 有序集合索引结束位置 (非分数) desc , 排序规则 , 默认按照分数从小到大排序 withscores , 是否获取元素的分数 , 默认只获取元素的值 score_cast_func , 对分数进行数据转换的函数 \"\"\" def zrevrange(name, start, end, withscores=False, score_cast_func=float): \"\"\" 按照索引范围从大到小排序 \"\"\" def zrangebyscore(name, min, max, start=None, num=None, withscores=False, score_cast_func=float): \"\"\" 按照分数范围获取name对应的有序集合的元素 \"\"\" zrevrangebyscore(name, max, min, start=None, num=None, withscores=False, score_cast_func=float) \"\"\" 按照分数范围从大到小排列 \"\"\" def zrank(name, value): \"\"\" 获取某个值在 name对应的有序集合中的排行 (从 0 开始) \"\"\" def zrevrank(name, value): \"\"\" zrank从大到小排序 \"\"\" def zrangebylex(name, min, max, start=None, num=None): \"\"\" 当有序集合的所有成员都具有相同的分值时 , 有序集合的元素会根据成员的值 (lexicographical ordering)来进行排序 , 而这个命令则可以返回给定的有序集合键 key 中 , 元素的值介于 min 和 max 之间的成员, 对集合中的每个成员进行逐个字节的对比 (byte-by-byte compare) , 并按照从低到高的顺序 , 返回排序后的集合成员, 如果两个字符串有一部分内容是相同的话 , 那么命令会认为较长的字符串比较短的字符串要大 name , redis的name min , 左区间 (值)。 + 表示正无限； - 表示负无限； ( 表示开区间； [ 则表示闭区间 min , 右区间 (值) start , 对结果进行分片处理 , 索引位置 num , 对结果进行分片处理 , 索引后面的num个元素 如 : ZADD myzset 0 aa 0 ba 0 ca 0 da 0 ea 0 fa 0 ga zrangebylex('myzset', \"-\", \"[ca\") 结果为 : ['aa', 'ba', 'ca'] \"\"\" def zrevrangebylex(name, max, min, start=None, num=None): \"\"\" zrangebylex从大到小排序 \"\"\" def zrem(name, values): \"\"\" 删除name对应的有序集合中值是values的成员 如 : zrem('zz', ['s1', 's2']) \"\"\" def zremrangebyrank(name, min, max): \"\"\" 根据排行范围删除 \"\"\" def zremrangebyscore(name, min, max): \"\"\" 根据排行范围删除 \"\"\" def zremrangebylex(name, min, max): \"\"\" 根据值返回删除 \"\"\" def zscore(name, value): \"\"\" 获取name对应有序集合中 value 对应的分数 \"\"\" def zinterstore(dest, keys, aggregate=None): \"\"\" 获取两个有序集合的交集 , 如果遇到相同值不同分数 , 则按照aggregate进行操作 aggregate的值为: SUM MIN MAX \"\"\" def zunionstore(dest, keys, aggregate=None): \"\"\" 获取两个有序集合的并集 , 如果遇到相同值不同分数 , 则按照aggregate进行操作 aggregate的值为: SUM MIN MAX \"\"\" def zscan(name, cursor=0, match=None, count=None, score_cast_func=float): zscan_iter(name, match=None, count=None,score_cast_func=float): \"\"\" 同字符串相似 , 相较于字符串新增score_cast_func , 用来对分数进行操作 \"\"\" 其他操作 🍀 def delete(*names): \"\"\" 根据删除redis中的任意数据类型 \"\"\" def exists(name): \"\"\" 检测redis的name是否存在 \"\"\" def keys(pattern='*'): \"\"\" 根据模型获取redis的name KEYS * 匹配数据库中所有 key 。 KEYS h?llo 匹配 hello , hallo 和 hxllo 等。 KEYS h*llo 匹配 hllo 和 heeeeello 等。 KEYS h[ae]llo 匹配 hello 和 hallo , 但不匹配 hillo \"\"\" def expire(name ,time): \"\"\" 为某个redis的某个name设置超时时间 \"\"\" def rename(src, dst): \"\"\" 对redis的name重命名为 \"\"\" def move(name, db): \"\"\" 将redis的某个值移动到指定的db下 \"\"\" def randomkey(): \"\"\" 随机获取一个redis的name (不删除) \"\"\" def type(name): \"\"\" 获取name对应值的类型 \"\"\" def scan(cursor=0, match=None, count=None): scan_iter(match=None, count=None): \"\"\" 同字符串操作 , 用于增量迭代获取key \"\"\" 管道 🍀 Pipelines 是基 Redis 类的子类 , 它支持在单个请求中缓冲多个命令到服务器 , 它们可以通过减少客户端和服务器之间来回 TCP 数据包的数量来显着地提高命令组的性能 管道的使用非常简单 : >>> r = redis.Redis(...) >>> r.set('bing', 'baz') >>> # Use the pipeline() method to create a pipeline instance >>> pipe = r.pipeline() >>> # The following SET commands are buffered >>> pipe.set('foo', 'bar') >>> pipe.get('bing') >>> # the EXECUTE call sends all buffered commands to the server, returning >>> # a list of responses, one for each command. >>> pipe.execute() [True, 'baz'] 为了便于使用 , 所有被缓冲到管道中的命令都返回管道对象本身 , 因此 , 调用可以如下 : >>> pipe.set('foo', 'bar').sadd('faz', 'baz').incr('auto_number').execute() [True, True, 6] 此外 , 管道还可以确保缓冲命令作为一个组以原子形式执行 , 默认情况下会发生这种情况 , 如果希望禁用管道的原子性质 , 但仍然希望缓冲命令 , 则可以关闭事务 >>> pipe = r.pipeline(transaction=False) 当需要原子事务时 , 需要在 Redis 中检索值以便在事务中使用时 , 就会出现一个常见的问题 , 例如 , 让我们假设 incr 命令不存在 , 我们需要在 Python 中构建 incr 的原子版本 完全天真的实现可以获得值 , 在 Python 中增加值 , 并将新值设置回原来的值 . 但是 , 这不是原子性的 , 因为多个客户端可以同时执行此操作 , 每个客户端从 get 获得相同的值 输入监视命令 , WATCH 命令提供了在启动事务之前监视一个或多个键的能力 , 如果这些键中的任何一个在该事务执行之前发生更改 , 则整个事务将被取消并引发 WatchError , 为了实现我们自己的客户端 incr 命令 , 我们可以这样做 : >>> with r.pipeline() as pipe: ... while 1: ... try: ... # put a WATCH on the key that holds our sequence value ... pipe.watch('OUR-SEQUENCE-KEY') ... # after WATCHing, the pipeline is put into immediate execution ... # mode until we tell it to start buffering commands again. ... # this allows us to get the current value of our sequence ... current_value = pipe.get('OUR-SEQUENCE-KEY') ... next_value = int(current_value) + 1 ... # now we can put the pipeline back into buffered mode with MULTI ... pipe.multi() ... pipe.set('OUR-SEQUENCE-KEY', next_value) ... # and finally, execute the pipeline (the set command) ... pipe.execute() ... # if a WatchError wasn't raised during execution, everything ... # we just did happened atomically. ... break ... except WatchError: ... # another client must have changed 'OUR-SEQUENCE-KEY' between ... # the time we started WATCHing it and the pipeline's execution. ... # our best bet is to just retry. ... continue 注意 , 由于管道必须在监视期间绑定到单个连接 , 因此必须注意通过调用 Reset() 方法确保连接返回到连接池 . 如果管道被用作上下文管理器(如上面的示例所示) , 则将自动调用 Reset() , 当然 , 您可以通过显式调用 Reset() 来手动完成此操作 : >>> pipe = r.pipeline() >>> while 1: ... try: ... pipe.watch('OUR-SEQUENCE-KEY') ... ... ... pipe.execute() ... break ... except WatchError: ... continue ... finally: ... pipe.reset() 存在一个名为transaction 的方便方法 , 用于处理所有处理和重试手表错误的样板 , 它需要一个可调用的 , 应该期望有一个参数 , 一个管道对象和任何数量的键来监视 , 上面的客户端 incr 命令可以这样编写 , 这更容易阅读 : >>> def client_side_incr(pipe): ... current_value = pipe.get('OUR-SEQUENCE-KEY') ... next_value = int(current_value) + 1 ... pipe.multi() ... pipe.set('OUR-SEQUENCE-KEY', next_value) >>> >>> r.transaction(client_side_incr, 'OUR-SEQUENCE-KEY') [True] 利用管道实现计数器 🍀 import redis conn = redis.Redis(host='192.168.1.41',port=6379) conn.set('count',1000) with conn.pipeline() as pipe: # 先监视 , 自己的值没有被修改过 conn.watch('count') # 事务开始 pipe.multi() old_count = conn.get('count') count = int(old_count) if count > 0: # 有库存 pipe.set('count', count - 1) # 执行 , 把所有命令一次性推送过去 pipe.execute() 发布订阅 🍀 发布者 : 服务器 订阅者 : Dashboard和数据处理 示例如下 : helprs.py import redis class RedisHelper: def __init__(self): self.__conn = redis.Redis(host='10.211.55.4') self.chan_sub = 'fm104.5' self.chan_pub = 'fm104.5' def public(self, msg): self.__conn.publish(self.chan_pub, msg) return True def subscribe(self): pub = self.__conn.pubsub() pub.subscribe(self.chan_sub) pub.parse_response() return pub 订阅者 from helpers import RedisHelper obj = RedisHelper() redis_sub = obj.subscribe() while True: msg= redis_sub.parse_response() print(msg) 发布者 from helpers import RedisHelper obj = RedisHelper() obj.public('hello') Sentinel 🍀 redis重的sentinel主要用于在redis主从复制中 , 如果master顾上 , 则自动将slave替换成master from redis.sentinel import Sentinel # 连接哨兵服务器(主机名也可以用域名) sentinel = Sentinel([('10.211.55.20', 26379), ('10.211.55.20', 26380), ], socket_timeout=0.5) # 获取主服务器地址 # master = sentinel.discover_master('mymaster') # print(master) # 获取从服务器地址 # slave = sentinel.discover_slaves('mymaster') # print(slave) # 获取主服务器进行写入 # master = sentinel.master_for('mymaster') # master.set('foo', 'bar') # 获取从服务器进行读取 (默认是round-roubin) # slave = sentinel.slave_for('mymaster', password='redis_auth_pass') # r_ret = slave.get('foo') # print(r_ret) 更多参见 : Redis for GitHub , Doc "},"07-设计模式/":{"url":"07-设计模式/","title":"设计模式","keywords":"","body":"设计模式 "},"07-设计模式/01-六大原则.html":{"url":"07-设计模式/01-六大原则.html","title":"六大原则","keywords":"","body":"六大原则 前言 🍀 设计模式是一个我们编写程序的标准 , 也就是一种规范 , 有时候不能够盲目的追求规范而不理会真实情景 , 因为它可能会适得其反 单一职责原则 🍀 单一职责原则 (Single Responsibility Principle , 简称 SRP ) 定义 : 应该有且仅有一个原因引起类的变更 (原话 : There should never be more than one reason for a class to change. ) 作用 : 降低类的复杂性 , 职责具有清晰明确的定义 提高可读性 提供可维护性 降低变更引起的风险 实例 : 以通话为例 , 我们需要两个操作来实现通话 建立连接 信息传递 伪代码示例 class Phone(object): \"\"\" 未区分职责, converse既负责连接, 也负责信息传递 \"\"\" def converse(self, phone, message): 连接 开始通话 挂断电话 class Phone(object): \"\"\" 区分职责, dial负责连接, chat负责信息传递, hangup负责关闭连接 \"\"\" def dial(self, phone): pass def chat(self, message): pass def hangup(): pass 变更的出现会使你的项目工作量增大 , 所以单一职责原则可以有效的提高你的工作效率 , 但是通常你可能感觉不到它的存在 , 第一是因为你可能会不自觉的这么去做 , 即使它不完全符合这一原则 ; 第二是因为你的项目不允许去深究它 , 总之 , 单一职责原则可以在一定程度上使你的代码更加从容面对项目遇到的变更风险 扩展 : 在面向对象中, 除了继承, 还有另一种方式来实现抽象, 那就是组合, 但是组合是一种强耦合关系, 所以不到迫不得已还是不要使用组合的好 里氏替换原则 🍀 里氏替换原则 (Liskov substitution Principle , 简称 LSP ) 定义 : 如果对每一个类型为 S 的对象 o1 , 都有类型为 T 的对象 o2 , 使得以 T 定义的所有程序 P 在所有的对象 o1 都代换成 o2 时, 程序 P 的行为没有发生变化, 那么类型 S 是类型 T 的子类型 (原话 : If for each object o1 of type S there is an object o2 of type T such that for all programs P defined in terms of T, the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T.) 大致意思就是 , 父类能出现的地方子类也可以出现 , 而且替换将父类替换成子类也不会产生任何错误或异常 , 使用者可能根本就不需要知道是父类还是子类 , 但是 , 反过来子类能出现的地方父类却不一定能出现 , 它的含义如下 : 子类必须完全实现父类的方法 (但是如果子类中的某些方法发生了畸变 , 建议将方法进行独立) 子类可以有自己的个性 覆盖或实现父类的方法时输入参数可以被放大 覆盖或实现父类的方法时输出结果可以被缩小 关于第一点和第二点 , 这几乎就是基本的规则 , 不过在 Python 中并没有接口的概念 , 当然而第三点和第四点则是需要注意的 , 因为一旦参数被缩小 , 或者结果被放大 , 那你在写代码时就得小心了 , 因为你要顾及你的子类是否能正常完成你的任务 作用 : 代码共享 , 减少创建类的工作量 , 每个子类都拥有父类的方法和属性 提高代码的重用性 子类可以形似父类 , 但又异于父类 提高代码的可扩展性 提高产品或项目的开放性 实例 : class Foo(object): def say_hello(self, name): print(name, 'hello~') return self class SubOne(Foo): # 重载, 放大参数 def say_hello(self, name, desc=''): print(name, 'hello', '') return self # 异于父类 def say_hi(self, name): print(name, 'hi') class SubTwo(Foo): # 覆盖 def say_hello(self, name): print('hello', name) return self 这个例子可以说是用来 \"凑数\" 的 , 因为在 Python 中 , 来表达里氏替换原则所有的含义可能会不太像 Python 了 , 因为Python 天生就是多态 , 所以也不需要接口 ; 但是覆盖和重载我们应该遵循里氏替换原则 , 为了保证更好的兼容性 , 扩展性 扩展 : 覆盖(Override) 和重载 (Overload) : 覆盖就意味着它的外观是没有任何变化的 , 使用起来也没有变化 , 但是它其中的内容却已经被改变 ; 重载则是它的名字还是一样的 , 但是也仅仅是名字 , 它的其他都已经被重新塑造 ; 就比如 , 我们写了一个方法( Python 注解形式) a(n: int, m: str) -> str , 子类覆盖你所看到的还是 a(n: int, m: str) -> str , 而重载 a(n: tuple) -> str , 它只是名字还叫 a , 但是它的参数等等已经发生了改变 依赖倒置原则 🍀 依赖倒置原则 (Dependence Inversion Principle , 简称 DIP ) 定义 : 高层模块不应该依赖底层模块 , 两者应该依赖其抽象 ; 抽象不应该依赖于细节 , 细节应该依赖抽象 ( 原话 : High level modules should not depend upon low level modules.Both should depend upon abstractions.Abstractions should not depend upon details.Details should depend upon abstractions. ) 依赖倒置原则是 \"面向接口编程\" OOD (Object-Oriented Design , 面向对象设计) 的精髓之一 作用 : 减少类之间的耦合性 提高系统的稳定性 降低并行开发引起的风险 提高代码的可读性和可维护性 实例 : # 不符合依赖倒置原则 class Benz(object): def run(self): print('奔驰启动...滴滴滴') class BMW(object): def run(self): print('宝马启动...滴滴滴') class Driver(object): \"\"\"驾驶者\"\"\" def drive(self, benz_ojbect): \"\"\"开奔驰\"\"\" benz_object.run() def drive_BMW(self, bmw_object): \"\"\"为了保证兼容性, \"\"\" bmw_object.run() # 符合依赖倒置原则 class Car(object): \"\"\"抽象类\"\"\" def run(self): \"\"\"细节实现应该依赖于抽象\"\"\" raise NotImplementedError('车必须实现run方法') class Benz(Car): def run(self): print('奔驰启动...滴滴滴') class BMW(Car): def run(self): print('宝马启动...滴滴滴') class Driver(object): \"\"\"根据实际需要, 司机也可以抽象出来\"\"\" def drive(self, car): car.run() \"\"\" 抽象不依赖于细节, 即 Car 不应该依赖于 Benz 和 BMW \"\"\" 依赖有三种写法 , 分别为 : 构造函数传递依赖对象 , 在创建对象时进行限制 方法传递依赖对象 , 在使用过程中进行限制 接口传递依赖对象 , 直接在定义上限制 上述例子所使用的是接口传递依赖对象 依赖倒置原则的本职就是通过抽象 , 使各个类或模块的实现彼此独立 , 不互相影响 , 实现模块间的松耦合 接口隔离原则 🍀 接口隔离原则 (Interface Segregation Principle , 简称 ISP ) 定义 : 客户端不应该依赖它不需要的接口 ; 类之间的依赖关系应该建立在最小的接口上 (原话 : “Clients should not be forced to depend upon interfaces that they don't use. The dependency of one class to another one should depend on the smallest possible interface. ) 注意 , 接口隔离原则和单一职责原则审视的角度是不一样的 , 单一职责是要求类或接口职责单一 , 注重的是职责 , 而接口隔离则要求接口的方法尽量少 , 如 : 一个接口的职责可能包含了 10 个方法 , 这10个方法都放在一个接口中 , 并提供给多个模块访问 , 各个模块按照规定的权限来访问 作用 : 提高代码的灵活性和可维护性 提高系统的内聚性 , 降低系统的耦合性 提高代码重用性 , 减少代码冗余 接口隔离原则是对接口进行规范约束 , 其包含以下四层含义 : 接口要尽量小 接口要高内聚 (高内聚: 提高接口、类、模块的处理能力 , 减少对外的交互 ; 高内聚的标准是既符合接口隔离原则又符合单一职责原则) 定制服务 (只提供访问者需要的方法) 接口设计是有限度的 在进行接口隔离时 , 首先必须要满足单一职责原则 实例 : # 未进行接口隔离 class Worker(object): \"\"\"工人\"\"\" def work(self): \"\"\"需要按顺序完成四道工序\"\"\" step1 step2 step3 step4 # 进行了接口隔离 class Worker(object): \"\"\"工人\"\"\" def work(self): self.finish_step1() self.finish_step2() self.finish_step3() self.finish_step4() def finish_step1(self): pass def finish_step2(self): pass def finish_step3(self): pass def finish_step4(self): pass \"\"\" 场景一: 由于材料更加精细, 已经不需要进行第1道工序了, 只需完成2,3,4道即算完成 场景二: 为了能够让员工更加专注, 每个工人将只负责一道工序 工人A负责第一道工序 工人B负责第二道工序 工人C负责第三道工序 工人D负责第四道工序 场景三: 公司研发出了新的产品P1和P2, 工作流程如下: - P1: 工序数量不变, 但是工作顺序需要倒序, 即 4, 3, 2, 1 - P2: 只需要经过第3道工序, 1,2,4皆不需要 隔离与未隔离哪一种更加能适应以上所有场景? \"\"\" 如果大量的重复代码出现在你的程序中 , 那么你就应该反思了 , 因为你没有进行接口隔离 , 或者隔离得还不够细 , 导致代码无法重用 在进行接口隔离时 , 粒度大小不能过大 , 也不能过小 ; 定义太大 , 会降低灵活性 , 无法提供定制服务 , 给整体项目带来无法预料的风险 ; 定义太小 , 则会造成接口数量过多 , 使设计复杂化 迪米特法则 🍀 迪米特法则 (Law of Demeter , 简称 LoD ) 也称最少知识原则 (Least Knowledge Principle , 简称 LKP) 定义 : 一个对象应该对其他对象有最少的了解 , 即 一个类应该对自己需要耦合或调用的类知道得最少 (原话 : Only talk to your immediate friends ) 他强调以下两点 : 从依赖者的角度来说 , 只依赖应该依赖的对象 从被依赖者的角度来说 , 只暴露应该暴露的方法 迪米特法则的核心观念就是类间解耦 , 弱耦合 , 只有弱耦合了以后 , 类的复用率才可以提高 ; 但是其要求的结果就是产生了大量的中转或跳转类 , 导致系统的复杂性提高 , 同时也为维护带来了难度 , 所以在采用迪米特法则时需要反复权衡 , 既做到让结果清晰 , 又做到高内聚低耦合 作用 : 降低类之间的耦合度 , 提高模块的相对独立性 提高类的可复用率和系统的扩展性 实例 : # 对于明星来说, 经纪人是明星的朋友, 而粉丝是陌生人 class Agent(object): \"\"\"经纪人\"\"\" def set_star(self, star_obj): self.star = star_obj def set_fans(self, fans_obj): self.fans = fans_obj def meeting(self): out_string = ''.join(['粉丝', self.fans.get_name(), '与明星', self.star.get_name(), '见面了.']) print(out_string) class Star(object): \"\"\"明星\"\"\" def __init__(self, name): self.name = name def get_name(self): return self.name class Fans(object): \"\"\"粉丝\"\"\" def __init__(self, name): self.name = name def get_name(self): return self.name 总之 , 迪米特法则的作用在就是解耦 , 但是解耦的程度需要我们格外小心 开闭原则 🍀 开闭原则 定义 : 软件实体应该对扩展开放 , 对修改关闭 . 其含义是说一个软件实体应该通过扩展来实现变化 , 而不是通过修改已有的代码来实现变化 (原话 : Software entities like classes,modules and functions should be open for extension but closed for modifications. ) 开闭原则是面向对象程序设计的终极目标 所有已经投产的代码都是有意义的 , 并且都受系统规则的约束 , 这样的代码都要经过 “千锤百炼” 的测试过程，不仅保证逻辑是正确的，还要保证苛刻条件（高压力、异常、错误）下不产生 “有毒代码” , 因此有变化提出时 , 我们就需要考虑一下 , 原有的健壮代码是否可以不修改 , 仅仅通过扩展实现变化呢 ? 否则 , 就需要把原有的测试过程回笼一遍 , 需要进行单元测试 , 功能测试 , 集成测试甚至是验收测试 作用 : 提高代码的可复用性 提高软件的可维护性 前面5个原则是对开闭原则的具体解释 , 但是开闭原则并不局限于这么多 , 它没有边界 , 我们要把它应用到实际工作中需要注意以下几点 : 抽象约束 , 抽象层尽量保持稳定 元数据控制模块行为 (元数据 : 用来描述环境和数据的数据 , 通常说的就是配置参数) 指定项目章程 , 约定优于配置 封装变化 , 将相同的变化封装到一个接口或抽象类中 , 将不同的变化封装到不同的接口或抽象中 "},"07-设计模式/02-单例模式.html":{"url":"07-设计模式/02-单例模式.html","title":"单例模式","keywords":"","body":"单例模式 🍀 定义 🍀 单例模式 ( Singleton Pattern ) 确保某一个类只有一个实例 , 而且自行实例化并向整个系统提供这个实例 ( Ensure a class has only one instance, and provide a global point of access to it. ) 场景 🍀 在一个系统中 , 要求一个类有且仅有一个对象 , 如果出现多个对象就会出现 \"不良反应\" , 可以采用单例模式 如 : 在整个项目中需要一个共享访问点或共享数据 要生成唯一数据 创建一个对象需要消耗的资源过多 , 如要访问 IO 和数据库等资源 需要定义大量的静态常量和静态方法 (如工具类) 的环境 我们可能最多见的就是需要共享数据 , 比如在项目中的配置数据 , 又比如 Web 框架中的路由 实例 🍀 要实现单例模式 , 即保证一个类仅有一个实例 , 并提供一个访问它的全局访问点 Module 🍀 Python 中的 module 天生就是单例 , 至于为什么 , 你应该去看看 compiled-python-file singleton.py class Singleton(object): pass singleton = Singleton() 使用 from singleton import singleton __new__ 🍀 Python 中的对象将有 __new__ 来开辟空间创建实例 import threading class Singleton(object): _threading_lock = threading.RLock() def __new__(cls, *args, **kwargs): if not hasattr(cls, \"_instance\"): with cls._threading_lock: if not hasattr(cls, \"_instance\"): cls._instance = object.__new__(cls) return cls._instance Python 是支持多线程的 , 所以为了线程安全 , 加上锁 Metaclass 🍀 使用元类来实现单例模式 , 实际上就是控制 class() 的行为 , 也就是 __call__ 魔术方法 如果对于 metaclass 不懂 , 你可以看我的另一篇博客 《Python之路 - 元类》 class SingletonMeta(type): _instances = {} def __call__(self, *args, **kwargs): if self not in self._instances: self._instances[self] = super(SingletonMeta, self).__call__(*args, **kwargs) return self._instances[self] class Singleton(metaclass=SingletonMeta): pass singleton = Singleton() 元类创建类本身就是线程安全的 , 所以你并不需要担心抢占资源的问题 类装饰器 🍀 def singleton(cls): _instance = {} def _singleton(*args, **kargs): if cls not in _instance: _instance[cls] = cls(*args, **kargs) return _instance[cls] return _singleton @singleton class Singleton: pass 使用类装饰器 , 实际上就是把类转换成一个函数对象 , 因此跟实例的创建关系不大 , 因为从始至终也就实例化了一次 , 而且它是线程安全的 类方法 🍀 通过我们自定义的方法来获取对象 , 而不通过实例化的途径 class Singleton(object): @classmethod def instance(cls, *args, **kwargs): if not hasattr(cls, \"_instance\"): cls._instance = cls(*args, **kwargs) return cls._instance singleton = Singleton.instance() 虽然这种方式也能实现单例模式 , 但是它不是线程安全的 , 就算在方法里加了锁 , 也不是线程安全的 , 这里可能跟 Python 的类的加载机制有关 , 不深究了 注意 : 在测试过程中 , 千万要把 Python 的垃圾回收这一问题隔离 , 也就是说实例不要一实例化之后就丢弃 , 否则可能会出现无效的结果 "},"08-算法/":{"url":"08-算法/","title":"算法","keywords":"","body":"Algorithms 更多内容待后期推送... "},"08-算法/01-算法基础.html":{"url":"08-算法/01-算法基础.html","title":"算法基础","keywords":"","body":"算法基础 介绍 🍀 定义 算法是解决特定问题求解步骤的描述 , 在计算机中表现为指令的有限序列 , 并且每条指令表示一个或多个操作 算法的特性 输入输出 算法具有零个或多个输入 , 算法至少有一个或多个输出 有穷性 算法在执行有限的步骤之后 , 自动结束而不会出现无限循环 , 并且每一个步骤在可接受的时间内完成 确定性 算法的每一步骤都具有确定的含义 可行性 算法的每一步都必须是可行的 , 也就是说 , 每一步都能够通过执行有限次数完成 设计要求 🍀 解决一个问题的途径可以有非常多种 , 掌握好的算法 , 对我们解决问题很有帮助 , 而一个好的算法应该具备以下要求 正确性 算法的正确性是指算法至少应该具有输入 , 输出和加工处理无歧义性 , 能正确反应问题的需求 , 能够得到问题的正确答案 正确性应该符合以下四点 : 算法程序没有语法错误 算法程序对于合法的输入数据能够产生满足要求的输出结果 算法程序对于非法的输入数据能够得出满足规格说明的结果 算法程序对于精心选择的 , 甚至刁难的测试数据都有满足要求的输出结果 健壮性 一个好的算法还应该能对输入数据不合法的情况做合适的处理 , 比如输入的时间或者距离不应该是负数等 健壮性就是当输入数据不合法时 , 算法也能作出相关处理 , 而不是产生异常或者莫名奇妙的结果 时间效率高和存储量低 时间效率是指算法的执行时间 , 对于同一个问题 , 如果有多个算法能够解决 , 执行时间短的算法效率高 , 执行时间长的效率低 存储量需求值的是在执行过程中需要的最大存储空间 , 主要指算法程序运行时所占用的内存或外部硬盘空间 设计算法应该尽量满足时间效率和存储量低的要求 效率的度量方法 🍀 事后统计方法 这种方法主要是通过设计好的测试程序和数据 , 利用计算机计时器对不同算法编程的程序的运行时间进行比较 , 从而确定算法效率的高低 但是这种方法是有很大的缺陷的 : 必须依据算法事先编制好程序 , 这通常需要花费大量的时间和精力 , 并且一旦编制出来发现它根本是很糟糕的算法 , 那就白忙活了 时间的比较依赖计算机硬件和软件等环境因素 , 有时会掩盖算法本身的优劣 算法的测试数据设计困难 , 并且程序的运行时间往往还与测试数据的规模有很大的关系 , 效率高的算法在小的测试数据面前往往得不到体现 ; 比如10个数据的排序 , 不管用什么算法 , 差异几乎是零 , 而如果有一百万个随机数据排序 , 那不同的算法的差异就非常大了 , 所以用多少数据来测试我们的算法 , 这是一个很难判断的问题 事前分析估算方法 事前分析估算方法就是在计算机程序编制前 , 依据统计方法对算法进行估算 一个用高级程序语言编写的程序在计算机上运行时所消耗的时间取决于下列因素 : 算法采用的策略 , 方法 编译产生的代码质量 问题的输入规模 机器执行指令的速度 抛开与计算机硬件 , 软件有关的因素 , 一个程序的运行时间 , 依赖于算法的好坏和问题的输入规模 时间复杂度 🍀 在进行算法分析时 , 语句总的执行次数T(n)是关于问题规模n的函数 , 进而分析T(n)随n的变化情况并确定T(n)的数量级 算法的时间复杂度 , 也就是算法的时间量度 , 记作 : T(n)=O(f(n)) ; 它表示岁问题规模n 的增大 , 算法执行时间的增长率和f(n)的增长率相同 , 称作算法的渐进时间复杂度 , 简称为时间复杂度 , 其中f(n)是问题规模n的某个函数 我们用大写O() 来体现算法时间复杂度的记法 , 称之为大O记法 推导大O阶 🍀 分析一个算法的时间复杂度 , 推导大O阶时有以下方法 : 用常数1取代运行时间的所有加法常数 在修改后的运行次数函数中 , 只保留最高阶项 如果最高阶项存在且不是1 , 则去除与这个项相乘的常数 由此得到的结果就是大O阶 常数阶 🍀 首先介绍顺序结构的时间复杂度 , 现有如下算法 n = 100 # 执行一次 sum = (1+n)*n/2 # 执行一次 print(sum) # 执行一次 这个算法的运行次数函数是f(n)=3 , 根据推导大O阶的方法 , 直接把常数项改为1 , 在保留高阶项时发现 , 它根本没有最高阶项 , 所以这个算法的时间复杂度为O(1) 注意 : 不管这个常数是多少 , 我们都记作O(1) , 而不是O(3)等其他任意数字 对于分支结构而言 , 无论是真还是假 , 执行的次数都是恒定的 , 不会随着n的变大而发生变化 , 所以单纯的分支结构 (不包含在循环结构中) , 其时间复杂度也是O(1) 线性阶 🍀 线性阶的循环结构会复杂很多 , 要确定某个算法的阶次 , 我们常常需要确定某个特定语句或某个语句集运行的次数 ; 因此 , 我们要分析算法的复杂度 , 关键就是要分析循环结构的运行情况 , 如下 : for i in range(n): print(i) # 时间复杂度为O(1) 上面代码中 , print语句会执行n次 , 所以它的算法复杂度为O(n) 对数阶 🍀 count = 1 while count 上面代码中 , 每次count乘以2之后 , 就距离n更近了一分 , 也就是说 , 有多少个2想乘后大于n , 则会退出循环 , 由2^x = n 可以得到x = log2n , 所以这个循环的时间复杂度为O(logn) 平方阶 🍀 下面例子是一个循环嵌套 for i in range(n): for j in range(n): print(i,j) 对于外层的循环 , 时间复杂度为O(n) , 而内部循环时间复杂度也为O(n) , 所以这段代码的时间复杂度为O(n²) , 如果将外层循环改成m , 那么时间复杂度就会变成O(m×n) 常见时间复杂度 执行次数函数 阶 非正式术语 12 O(1) 常数阶 2n+3 O(n) 线性阶 3n²+2n+1 O(n²) 平方阶 5log2n + 20 O(logn) 对数阶 2n + 3nlog2n + 19 O(nlogn) nlogn阶 6n³ + 2n² + 3n +4 O(n³) 立方阶 2\" O(2\") 指数阶 常用时间复杂度所耗费的时间从小到大一次是 : O(1) 空间复杂度 🍀 算法的空间复杂度通过计算算法所需的存储空间实现 , 算法空间复杂度的计算公式记作 : S(n) = O(f(n)) , 其中n为问题的规模 , f(n)为语句关于n所占存储空间的函数 一般情况下 , 一个程序在机器上执行时 , 除了需要存储程序本身的指令 , 常数 , 变量和输入数据外 , 还需要存储对数据操作的存储单元 , 若输入数据所占空间值取决于问题本身 , 和算法无关 , 这样只需要分析该算法在实现时所需的辅助单元即可 若算法执行时所需的辅助空间相对于输入数据量而言是个常数 , 则称此算法为原地工作 , 空间复杂度为O(1) "},"08-算法/02-数组.html":{"url":"08-算法/02-数组.html","title":"数组","keywords":"","body":""},"08-算法/03-栈.html":{"url":"08-算法/03-栈.html","title":"栈","keywords":"","body":"栈 "},"08-算法/03-链表.html":{"url":"08-算法/03-链表.html","title":"链表","keywords":"","body":"本文为 《91 天学算法》 中的讲义, 因为是非开源项目, 所以这里无法给出链接, 作者 lucifer 后期本人会重写, 暂时引用, 特此声明 链表 链表定义 各种数据结构，不管是队列，栈等线性数据结构还是树，图的等非线性数据结构，从根本上底层都是数组和链表。两者在物理上存储是非常不一样的，如图： （图1. 数组和链表的物理存储图） 物理内存是一个个大小相同的内存单元构成的，如图： （图2. 物理内存） 不难看出，数组和链表只是使用物理内存的两种方式。 数组是连续的内存空间，通常每一个单位的大小也是固定的，因此可以按下标随机访问。而链表则不一定连续，因此其查找只能依靠别的方式，一般我们是通过一个叫 next 指针来遍历查找。 链表是一种物理存储单元上非连续、非顺序的存储结构，数据元素的逻辑顺序是通过链表中的指针链接次序实现的。链表由一系列结点（链表中每一个元素称为结点）组成，结点可以在运行时动态生成。 （图3. 一个典型的链表逻辑表示图） 后面所有的图都是基于逻辑结构，而不是物理结构 基本概念 头节点 尾节点 静态链表 链表分类 以下分类是两种分类标准，也就是一个链表可以既属于循环链表，也属于单链表，这是毋庸置疑的。 按照是否循环分为：循环链表和非循环链表 当我们需要在遍历到尾部之后重新开始遍历的时候，可以考虑使用循环链表。 需要注意的是，如果链表长度始终不变，那么使用循环链表很容易造成死循环，因此循环链表经常会伴随着节点的删除操作，比如约瑟夫环问题。 按照指针个数分为：单链表和双链表 单链表。 每个节点包括两部分：一个是存储数据的数据域，另一个是存储下一个节点指针的指针域。 双向链表。 每个节点包括三部分：一个是存储数据的数据域，一个是存储下一个节点指针的指针域，一个是存储上一个节点指针的指针域。 Java 中的 LinkedHashMap 以及 Python 中的 OrderedDict 底层都是双向链表。 其好处在于删除和插入的时候，可以更快地找到前驱指针。如果用单链表的话， 那么时间复杂度最坏的情况是 $O(N)$。双向链表的本质就是空间换时间，因此如果题目对时间有要求，可以考虑使用双向链表，比如力扣的 145. LRU缓存机制 。 链表的基本操作 插入 插入只需要考虑要插入位置前驱节点和后继节点（双向链表的情况下需要更新后继节点）即可，其他节点不受影响，因此在给定指针的情况下插入的操作时间复杂度为$O(1)$。这里给定指针中的指针指的是插入位置的前驱节点。 伪代码： temp = 待插入位置的前驱节点.next 待插入位置的前驱节点.next = 待插入指针 待插入指针.next = temp 如果没有给定指针，我们需要先遍历找到节点，因此最坏情况下时间复杂度为 $O(N)$。 提示1: 考虑头尾指针的情况。 提示2: 新手推荐先画图，再写代码。等熟练之后，自然就不需要画图了。 删除 只需要将需要删除的节点的前驱指针的next指针修正为其下下个节点即可，注意考虑边界条件。 伪代码： 待删除位置的前驱节点.next = 待删除位置的前驱节点.next.next 提示1: 考虑头尾指针的情况。 提示2: 新手推荐先画图，再写代码。等熟练之后，自然就不需要画图了。 遍历 遍历比较简单，直接上伪代码。 伪代码： 当前指针 = 头指针 while 当前指针不为空 { print(当前节点) 当前指针 = 当前指针.next } 常见题型 反转链表 （图2） 合并链表 （图3） 相交或环形链表 （图4） 设计题 常见套路 反转链表 1.1 将某个链表进行反转 1.2 在 O(n) 时间, O(1) 空间复杂度下逆序读取链表的某个值 1.3 将某个链表按 K 个一组进行反转 合并链表 2.1. 将两条有序或无序的链表合并成一条 有序链表 2.2. 将 k 条有序链表合并成一条有序链表 相交或环形链表 3.1. 判断某条链表是否存在环 3.2. 获取某条链表环的大小 3.3. 获取某两条链表的相交节点 设计题。需要你充分掌握链表特点才能写出来。 模板 反转链表 伪代码: let cur = head; let pre = null; while(cur) { const next = cur.next; cur.next = pre; pre = cur; cur = next; } return pre; 合并链表 伪代码: ans = new Node(-1) // ans 为需要返回的头节点 cur = ans // l1和l2分别为需要合并的两个链表的头节点 while l1 和 l2 都不为空 cur.next = min(l1.val, l2.val) 更新较小的指针，往后移动一位 if l1 == null cur.next = l2 if l2 == null cur.next = l1 return ans.next JS代码参考: ans = now = new ListNode(0); while(l1 !== null && l2 !== null){ if(l1.val 相交或环形链表 3.1 链表相交求交点 哈希法： 有A, B这两条链表, 先遍历其中一个，比如A链表, 并将A中的所有节点存入哈希表。 遍历B链表,检查节点是否在哈希表中, 第一个存在的就是相交节点 时间复杂度O(N), 空间复杂度O(N) 伪代码: data = new Set() // 存放A链表的所有节点的地址 while A不为空{ 哈希表中添加A链表当前节点 A指针向后移动 } while B不为空{ if 如果哈希表中含有B链表当前节点 return B B指针向后移动 } return null // 两条链表没有相交点 JS代码参考: let data = new Set() while(A !== null){ data.add(A) A = A.next } while(B !== null){ if(data.has(B)) return B B = B.next } return null 双指针: 例如使用a, b两个指针分别指向A, B这两条链表, 两个指针相同的速度向后移动, 当 a 到达链表的尾部时,重定位到链表 B 的头结点 当 b到达链表的尾部时,重定位到链表 A 的头结点。 a, b 指针相遇的点为相交的起始节点，否则没有相交点 （图5） PS: 为什么a, b指针相遇的点一定是相交的起始节点? 我们证明一下： 如果我们将两条链表按相交的起始节点继续截断，A链表为: a + c，B链表为: b + c， 当 a 指针将 A 链表遍历完后,重定位到链表 B 的头结点,然后继续遍历至相交点，a指针遍历的距离为 a + c + b，同理b指针遍历的距离为 b + c + a。 时间复杂度O(N), 空间复杂度O(1) 伪代码: a = headA b = headB while a,b指针不相等时 { a, b指针都向后移动 if a, b指针都为空 return null //没有相交点 if a指针为空时 a指针重定位到链表 B的头结点 if b指针为空时 b指针重定位到链表 A的头结点 } return a JS代码参考: let a = headA, b = headB while(a != b){ a = a ? a.next : null b = b ? b.next : null if(a == null && b == null) return null if(a == null) a = headB if(b == null) b = headA } return a 3.2 环形链表求环的起点 哈希法： 遍历整个链表,同时将每个节点都插入哈希表, 如果当前节点在哈希表中不存在,继续遍历, 如果存在,那么当前节点就是环的入口节点 时间复杂度O(n),空间复杂度O(n) 伪代码: data = new Set() // 声明哈希表 while head不为空{ if 当前节点在哈希表中存在{ return head // 当前节点就是环的入口节点 } else { 将当前节点插入哈希表 } head指针后移 } return null // 环不存在 JS代码参考: let data = new Set() while(head){ if(data.has(head)){ return head } else { data.add(head) } head = head.next } return null 快慢指针法： 定义一个fast指针,每次前进两步,一个slow指针,每次前进一步 当两个指针相遇时 2.1. 将fast指针指向链表头部,同时fast指针每次只前进一步 2.2. slow指针继续前进,每次前进一步 当两个指针再次相遇时,当前节点就是环的入口 （图6） 为什么第二次相遇的点为环的入口? 原因如下： 第一次相遇时 慢指针移动的距离为 s1 = A + B + n1 * L 快指针移动的距离为 s2 = A + B + n2 * L 快指针是慢指针速度的两倍,所以 s2 = 2* s1 A + B + n2 L = 2A + 2B + n1 L A = -B + (n2 - n1) * L 因为圆的性质 A = -B + (n2 - n1) * L ===> A = -B 即在第一次相遇点, 向前走A步 ===> 向后走B步 第一次相遇后 快指针从头节点走A步会到达环的入口 慢指针从第一次相遇点走A步,相当于向后走B步,也会到达环的入口 时间复杂度O(n),空间复杂度O(1) 参考：【每日一题】- 2020-01-14 - 142. 环形链表 II 伪代码： fast = head slow = head //快慢指针都指向头部 do { 快指针向后两步 慢指针向后一步 } while 快慢指针不相等时 if 指针都为空时{ return null // 没有环 } while 快慢指针不相等时{ 快指针向后一步 慢指针向后一步 } return fast JS代码参考： if(head == null || head.next == null) return null let fast = slow = head do{ if(fast != null && fast.next != null){ fast = fast.next.next } else { fast = null } slow = slow.next } while(fast != slow) if(fast == null) return null fast = head while(fast != slow){ fast = fast.next slow = slow.next } return fast 设计题 这个直接直接结合一个例子来给大家讲解一下。 题目描述： 设计一个算法支持以下操作: 获取数据 get 和 写入数据 put 。 获取数据 get(key) - 如果关键字 (key) 存在于缓存中，则获取关键字的值（总是正数），否则返回 -1。 写入数据 put(key, value) - 如果关键字已经存在，则变更其数据值；如果关键字不存在，则插入该组「关键字/值」。当缓存容量达到上限时，它应该在写入新数据之前删除最久未使用的数据值，从而为新的数据值留出空间。 在 O(1) 时间复杂度内完成这两种操作 思路: 1.确定需要使用的数据结构 根据题目要求,存储的数据需要保证顺序关系(逻辑层面) ==⇒ 可以使用数组,链表等 同时需要对数据进行频繁的增删, 时间复杂度O(1) == >只能使用链表存储数据 对数据进行读取时, 时间复杂度O(1) =⇒ 使用哈希表 最终采取双向链表 + 哈希表 双向链表按最后一次访问的时间的顺序进行排列, 链表头部为最近访问的节点 哈希表,以关键字为键,以链表节点的地址为值 put操作 通过哈希表, 查看put操作传入的关键字对应的链表节点, 是否已经存在 如果已经存在,将该节点的值进行覆盖,同时将该节点位置调整至链表头部 如果不存在,查看当前链表容量是否已满 如果链表容量未满, 新生成节点, 同时将该节点位置调整至链表头部 如果链表容量已满, 删除尾部节点,新生成节点, 同时将该节点位置调整至链表头部 get操作 通过哈希表, 查看get操作传入的关键字对应的链表节点, 是否已经存在 存在, 返回该节点的值, 同时将该节点位置调整至链表头部 不存在, 返回null 伪代码: var LRUCache = function(capacity) { 保存一个该数据结构的最大容量 生成一个双向链表,同时保存该链表的头结点与尾节点 生成一个哈希表 }; function get (key) { if 哈希表中存在该关键字 { 根据哈希表获取该链表节点 将该节点放置于链表头部 return 链表节点的值 } else { return -1 } }; function put (key, value) { if 哈希表中存在该关键字 { 根据哈希表获取该链表节点 将该链表节点的值更新 将该节点放置于链表头部 } else { if 容量已满 { 删除链表尾部的节点 新生成一个节点 将该节点放置于链表头部 } else { 新生成一个节点 将该节点放置于链表头部 } } }; JS代码参考: function ListNode(key, val) { this.key = key this.val = val; this.pre = this.next = null; } var LRUCache = function(capacity) { this.capacity = capacity this.size = 0 this.data = {} this.head = new ListNode() this.tail = new ListNode() this.head.next = this.tail this.tail.pre = this.head }; function get (key) { if(this.data[key] !== undefined){ let node = this.data[key] this.removeNode(node) this.appendHead(node) return node.val } else { return -1 } }; function put (key, value) { let node if(this.data[key] !== undefined){ node = this.data[key] this.removeNode(node) node.val = value } else { node = new ListNode(key, value) this.data[key] = node if(this.size "},"08-算法/04-树.html":{"url":"08-算法/04-树.html","title":"树","keywords":"","body":"本文为 《91 天学算法》 中的讲义, 因为是非开源项目, 所以这里无法给出链接, 作者 lucifer 后期本人会重写, 暂时引用, 特此声明 树 计算机的数据结构是现实世界物体间关系的一种抽象。家族的族谱，公司架构中的人员组织关系，电脑中的文件夹结构，html渲染的dom结构等等，这些有层次关系的结构在计算机领域都叫做树。 树是一种非线性数据结构。 树结构的基本单位是节点。节点之间的链接，称为分支（branch）。节点与分支形成树状，结构的开端，称为根（root），或根结点。根节点之外的节点，称为子节点（child）。没有链接到其他子节点的节点，称为叶节点（Leaf）。 每个节点用以下结构表示 Node { value: any; // 当前节点的值 children: Array; // 指向其儿子 } 基本概念 树的高度 深度 层 二叉树，三叉树，。。。 N 叉树 二叉树 二叉树是树结构的一种，两个叉就是说每个节点最多只有两个子节点，我们习惯称之为左节点和右节点。 注意这个只是名字而已，并不是实际位置上的左右 二叉树也是我们做算法题最常见的一种树(最简单 🤓)。可以用以下结构表示 Node { value: any; // 当前节点的值 left: Node | null; // 左儿子 right: Node | null; // 右儿子 } 二叉树分类： 完全二叉树 满二叉树 二叉搜索树 平横二叉树 红黑树 二叉树的表示： 链表存储 数组存储。非常适合完全二叉树 二叉树遍历 二叉树的大部分题都围绕二叉树遍历展开，二叉树以下遍历方式 前序遍历 中序遍历 后序遍历 层序遍历(BFS) 前序遍历 访问当前节点 遍历左子树 遍历右子树 preorder(root) { if root doSomething(root) preOrder(root.left) preOrder(root.right) } 中序遍历 遍历左子树 访问当前节点 遍历右子树 inorder(root) { if root inorder(root.left) doSomething(root) inorder(root.right) } 后序遍历 遍历左子树 遍历右子树 访问当前节点 postorder(root) { if root postorder(root.left) postorder(root.right) dosomething(root) 层序遍历(BFS) bfs(root) { queue = [] queue.push(root) while queue.length { curLevel = queue queue = [] for i = 0 to curLevel.length { doSomething(curLevel[i]) if (curLevel[i].left) { queue.push(curLevel[i].left) } if (curLevel[i].right) { queue.push(curLevel[i].right) } } } } 二叉树是最能体现递归美感的结构，看到二叉树的题第一反应应该是递归。 二叉树构建 中序序列和前、后，层次序列任一组合唯一确定一颗二叉树。前、中，层次序列都是提供根结点的信息，中序序列用来区分左右子树； 注意单前/中/后序遍历是无法确定一棵树，比如以下所有二叉树的前序遍历都为123 构造一棵树的本质是 确定根节点 确定其左子树 确定其右子树 比如拿到前序遍历结果preorder和中序遍历inorder，在preorder我们可以能确定树根root，拿到root可以将中序遍历切割中左右子树。这样就可以确定并构造一棵树，整个过程我们可以用递归完成。详情见 105. 从前序与中序遍历序列构造二叉树) 二叉搜索树 二叉搜索树是二叉树的一种，具有以下性质 左子树的所有节点值小于根的节点值 右子树的所有节点值大于根的节点值 二叉搜索树的中序遍历结果是一个有序列表，这个性质很有用。比如 leetcode 1008), 根据先序遍历构建对应的二叉搜索树。由于二叉树的中序遍历是一个有序列表，我们可以有以下思路 对先序遍历结果排序，排序结果是中序遍历结果 根据先序遍历和中序遍历确定一棵树 堆 在这里讲堆是因为堆可以被看作近似的完全二叉树。堆通常以数组形式的存储，而非上述的链式存储。 表示堆的数组A中，如果A[1]为根节点，那么给定任意节点i，其父子节点分别为 父亲节点：Math.floor(i / 2) 左子节点：2 * i 右子节点: 2 * i + 1 如果 A[parent(i)] ≥ A[i]，则称该堆为最大堆，如果A[parent(i)] ≤ A[i]，称该堆为最小堆。 堆这个数据结构有很多应用，比如堆排序，TopK问题，共享计算机系统的作业调度(优先队列)等。下面看下给定一个数据如何构建一个最大堆。 // 自底向上建堆 BUILD-MAX-HEAP(A) A.heap-size = A.length for i = Math.floor(A.length / 2) downto 1 MAX-HEAPIFY(A, i) // 维护最大堆的性质 MAX-HEAPIFY(A, i) l = LEFT(i) r = RIGHT(i) // 找到当前节点和左右儿子节点中最大的一个，并交换 if l A[i] largest = l else largest = i if r A[largest] largest = r if largest != i exchange A[i] with A[largest] // 递归维护交换后的节点堆性质 MAX-HEAPIFY(A, largest) ps: 伪代码参考自算法导论 递归 方法或者函数调用自身的方式成为递归调用。在这个过程中，调用称之为递，返回成为归。 推荐题目 汉诺塔问题 fibonacci 数列 二叉树的前中后序遍历 归并排序 求阶乘 递归求和 相关题目 二叉树的最大路径和 给出所有路径和等于给定值的路径 最近公共祖先 各种遍历。前中后，层次，拉链式等。 参考 图片参考自 https://wylu.me/posts/e85d694a/ 算法导论 更多内容 二叉树的遍历 前缀树专题 "},"08-算法/04-队列.html":{"url":"08-算法/04-队列.html","title":"队列","keywords":"","body":"队列 "},"08-算法/05-哈希表.html":{"url":"08-算法/05-哈希表.html","title":"哈希表","keywords":"","body":"本文为 《91 天学算法》 中的讲义, 因为是非开源项目, 所以这里无法给出链接, 作者 lucifer 后期本人会重写, 暂时引用, 特此声明 哈希表 介绍 定义：散列表（Hash table，也叫哈希表），是根据关键码值(Key)而直接进行访问的数据结构。散列表可以使用数组 + 链表的方式来实现。 哈希函数：哈希表查找过程是根据Key来算出hashcode（通常是一个数字），根据这个数字来随机访问数组，而理论上两个不同的key是可以算出同样的hashcode的。这个就叫做哈希冲突。 哈希冲突：hash函数不是万能的，这样不可避免的可能会造成两个不同的Key算出来的hash值相同，比如hash函数是x % 3，这样key=2和key=5算出的hash值都为2，这是要怎么办呢？一般我们有两种方法来处理，开放地址法和拉链法，具体大家可以查阅相关资料。这里简单的画了个图给大家直观看一下大概意思： 最后偷偷说一下，用Java的同学有兴趣可以看看HashSet的源码，底层也是用的HashMap😄 哈希表的性能：构造一个冲突小，稳定性高的hash函数是很重要的，我们在刷题的时候大部分时间都不会去考虑这个问题，但是实际工程中有时不可避免需要我们自己构造hash函数，这时就要根据实际情况进行分析测试啦。 刷题技巧 常用操作的时间复杂度 插入：O(1) 删除：O(1) 查找：O(1) 是的，常用的操作在非极其特殊情况下，平均的时间复杂度都为O(1) 常见的题类型如下： 统计xx出现次数/频率/ （见下方多人运动） 该种题比较直观，若已知数据范围较小且比较连续，可以考虑用数组来实现 需要查找/增加/删除操作为O(1)时间复杂度 （一些设计题） 见到这种要求的题可以考虑一下是否需要hash表来做，比如LRU，LFU之类的题，题目中要求了时间复杂度，就是用hash表+双向链表解决的。 题目类型为图数据结构相关 （比如并查集） 这样可能需要构建有向图/无向图，这时可以用hash表来表示图并进行后续操作。 需要存储之前的状态以减少计算开销（比如经典的两数和） 相信大家做过dp的一些题目就知道，记忆化搜索，该方法就利用hash表来存储历史状态，这样可以大大减少重复计算。 等等，大家多做类似的题目，相信可以总结出一套自己的思路。 模板（伪代码） 判断目标值是否出现过（例题如：两数之和、是否存在重复元素、合法数独等等） for num in nums: if num(该处为目标值target) in hashtable: return true return false 统计频率 数据比较离散 for num in nums: if num in hashtable: hashtable[num] += 1 else hashtable[num] = 1 # 后续操作 ------------------- 数据范围较小且连续则可以用数组代替 // 假设数据范围是0-n且n较小 int[] hashtable = new int[n + 1]; for num in nums: hashtable[num] += 1; // 后续操作 ------------------ 题目推荐 - 👥多人运动 题目描述 已知小猪每晚都要约好几个女生到酒店房间。每个女生 i 与小猪约好的时间由 [si , ei］表示，其中 si 表示女生进入房间的时间， ei 表示女生离开房间的时间。由于小猪心胸开阔，思想开明，不同女生可以同时存在于小猪的房间。请计算出小猪最多同时在做几人的「多人运动」。 例子： Input ： [[ 0 , 30] ,[ 5 , 10 ] ， [15 , 20 ] ] OutPut ：最多同时有两个女生的「三人运动」 思路 这个题 解法不止一种，但是我们这里因为在讲hash表，统计频率。下面我只写一下大致思路的伪代码，具体细节大家不妨可以尝试自己实现一下。 // 上面刚刚说了关于频率统计的方法，这里读完题，是不是就立刻想到了： // 用hash表来统计每个时刻房间内的人数并维护一个最大值就是我们所求的结果啦 res = -1 for everyGirl in girls: for curTime in [everyGirl.start, everyGirl.end]: // 套上面板子 if curTime in hashtable: hashtable[curTime] += 1 else hashtable[curTime] = 1 // 维护最大值 res = max(res, hashtable[curTime]) ---------------------------- 线下验证通过可以贴到这里哦 【每日一题】- 2020-04-27 - 多人运动 这里还有各种解题方法，大家都可以学习下思路并试着自己做一做！ "},"08-算法/06-双指针.html":{"url":"08-算法/06-双指针.html","title":"双指针","keywords":"","body":"本文为 《91 天学算法》 中的讲义, 因为是非开源项目, 所以这里无法给出链接, 作者 lucifer 后期本人会重写, 暂时引用, 特此声明 双指针 什么是双指针 顾名思议，双指针就是两个指针，但是不同于 C，C++中的指针， 其是一种算法思想。 如果说，我们迭代一个数组，并输出数组每一项，我们需要一个指针来记录当前遍历的项，这个过程我们叫单指针（index）的话。 for(int i = 0;i （图 1） 那么双指针实际上就是有两个这样的指针，最为经典的就是二分法中的左右双指针啦。 int l = 0; int r = nums.size() - 1; while (l （图 2） 读到这里，你发现双指针是一个很宽泛的概念，就好像数组，链表一样，其类型会有很多很多， 比如二分法经常用到左右端点双指针。滑动窗口会用到快慢指针和固定间距指针。 因此双指针其实是一种综合性很强的类型，类似于数组，栈等。 但是我们这里所讲述的双指针，往往指的是某几种类型的双指针，而不是“只要有两个指针就是双指针了”。 有了这样一个算法框架，或者算法思维，有很大的好处。它能帮助你理清思路，当你碰到新的问题，在脑海里进行搜索的时候，双指针这个词就会在你脑海里闪过，闪过的同时你可以根据双指针的所有套路和这道题进行穷举匹配，这个思考解题过程本来就像是算法，我会在进阶篇《搜索算法》中详细阐述。 那么究竟我们算法中提到的双指针指的是什么呢？我们一起来看下算法中双指针的常见题型吧。 常见题型有哪些？ 这里我将其分为三种类类型，分别是： 快慢指针（两个指针步长不同） 左右端点指针（两个指针分别指向头尾，并往中间移动，步长不确定） 固定间距指针（两个指针间距相同，步长相同） 上面是我自己的分类，没有参考别人。可以发现我的分类标准已经覆盖了几乎所有常见的情况。 大家在平时做题的时候一定要养成这样的习惯，将题目类型进行总结，当然这个总结可以是别人总结好的，也可以是自己独立总结的。不管是哪一种，都要进行一定的消化吸收，把它们变成真正属于自己的知识。 不管是哪一种双指针，只考虑双指针部分的话 ，由于最多还是会遍历整个数组一次，因此时间复杂度取决于步长，如果步长是 1，2 这种常数的话，那么时间复杂度就是 O(N)，如果步长是和数据规模有关（比如二分法），其时间复杂度就是 O(logN)。并且由于不管规模多大，我们都只需要最多两个指针，因此空间复杂度是 O(1)。下面我们就来看看双指针的常见套路有哪些。 常见套路 快慢指针 判断链表是否有环 这里给大家推荐两个非常经典的题目，一个是力扣 287 题，一个是 142 题。其中 142 题我在我的 LeetCode 题解仓库中的每日一题板块出过，并且给了很详细的证明和解答。而 287 题相对不直观，比较难以想到，这道题曾被官方选定为每日一题，也是相当经典的。 287. 寻找重复数 【每日一题】- 2020-01-14 - 142. 环形链表 II · Issue #274 · azl397985856/leetcode 读写指针。典型的是删除重复元素 这里推荐我仓库中的一道题， 我这里写了一个题解，横向对比了几个相似题目，并剖析了这种题目的本质是什么，让你看透题目本质，推荐阅读。 80.删除排序数组中的重复项 II 左右端点指针 二分查找。 二分查找会在专题篇展开，这里不多说，大家先知道就行了。 暴力枚举中“从大到小枚举”（剪枝） 一个典型的题目是我之前参加官方每日一题的时候给的一个解法，大家可以看下。这种解法是可以 AC 的。同样地，这道题我也给出了三种方法，帮助大家从多个纬度看清这个题目。强烈推荐大家做到一题多解。这对于你做题很多帮助。除了一题多解，还有一个大招是多题同解，这部分我们放在专题篇介绍。 find-the-longest-substring-containing-vowels-in-even 有序数组。 区别于上面的二分查找，这种算法指针移动是连续的，而不是跳跃性的，典型的是 LeetCode 的两数和，以及N数和系列问题。 固定间距指针 一次遍历（One Pass）求链表的中点 一次遍历（One Pass）求链表的倒数第 k 个元素 固定窗口大小的滑动窗口 模板(伪代码) 我们来看下上面三种题目的算法框架是什么样的。这个时候我们没必要纠结具体的语言，这里我直接使用了伪代码，就是防止你掉进细节。 当你掌握了这种算法的细节，就应该找几个题目试试。一方面是检测自己是否真的掌握了，另一方面是“细节”，”细节“是人类，尤其是软件工程师最大的敌人，毕竟我们都是差不多先生。 快慢指针 l = 0 r = 0 while 没有遍历完 if 一定条件 l += 1 r += 1 return 合适的值 左右端点指针 l = 0 r = n - 1 while l 固定间距指针 l = 0 r = k while 没有遍历完 自定义逻辑 l += 1 r += 1 return 合适的值 题目推荐 如果你差不多理解了上面的东西，那么可以拿下面的题练练手。Let's Go! 左右端点指针 16.3Sum Closest (Medium) 713.Subarray Product Less Than K (Medium) 977.Squares of a Sorted Array (Easy) Dutch National Flag Problem 下面是二分类型 33.Search in Rotated Sorted Array (Medium) 875.Koko Eating Bananas（Medium） 881.Boats to Save People（Medium） 更多二分推荐： search-for-range search-insert-position search-a-2d-matrix first-bad-version find-minimum-in-rotated-sorted-array find-minimum-in-rotated-sorted-array-ii search-in-rotated-sorted-array search-in-rotated-sorted-array-ii 快慢指针 26.Remove Duplicates from Sorted Array（Easy） 141.Linked List Cycle (Easy) 142.Linked List Cycle II（Medium） 287.Find the Duplicate Number（Medium） 202.Happy Number (Easy) 固定间距指针 1456.Maximum Number of Vowels in a Substring of Given Length（Medium） 固定窗口大小的滑动窗口见专题篇的滑动窗口专题（暂未发布） 其他 有时候也不能太思维定式，比如 https://leetcode-cn.com/problems/consecutive-characters/ 这道题根本就没必要双指针什么的。 再比如：https://lucifer.ren/blog/2020/05/31/101.symmetric-tree/ "},"08-算法/07-并查集.html":{"url":"08-算法/07-并查集.html","title":"并查集","keywords":"","body":"本文为 《91 天学算法》 中的讲义, 因为是非开源项目, 所以这里无法给出链接, 作者 lucifer 后期本人会重写, 暂时引用, 特此声明 并查集 并查集是一种树型的数据结构，用于处理一些不交集（Disjoint Sets）的合并及查询问题。 并查集维护了一个不相交动态集合$S = {S1, S2, ..., Sn}$。我们用集合中的某个元素来代表这个集合，该元素称为集合的代表元。 并查集的操作 MAKE-SET(x): 构建并查集 UNION(x, y): 合并两个子集 FIND(x): 确定 x 属于哪个子集 并查集的数据表示 并查集元素一般用树来表示，树中的每个节点代表一个成员，每棵树表表示一个集合，多棵树构成一个并查集森林。 每个集合中，树根即其代表元。 interface Node { parent: Node; } 并查集实现 MAKE-SET(x) MAKE-SET, 初始化时，节点的 parent 指向其自身，每个元素都是一棵数，并以自己为代表元。 function MAKE-SET(x) x.parent = x FIND-SET(x) 查找 x 元素的代表元，以下是递归查询 FIND-SET(x) if x != FIND-SET(x) return FIND-SET(x.parent) return x.parent UNION(x, y) 集合合并，分别找到 x, y 的代表元，将其合并，具体表现形式为将 FIND(x).parent 指向 FIND(y).parent UNION(x, y) xRoot = FIND-SET(x) yRoot = FIND-SET(y) xRoot.parent = yRoot 上述并集是最基础的一种表示，n-1 次的 UNION 操作，可能会构造初一棵恰好含有 n 个节点的线性链的树，考虑以下情景 以下两种启发式的时间优化 按秩合并，这里的秩是指节点的高度，具体操作为将较小秩序的根指向较大秩的根。 路径压缩，在 FIND-SET 的时候将节点的 parent 指向代表元也就是树根。 带秩的 UNION 实现 带秩的数据节点表示为 Node { rank: 0, parent: Node } UNION(x, y) if x.rank > y.rank y.parent = x else x.parent = y // 将低秩树根指向高秩树根 if (x.rank === y.rank) y.rank = y.rank + 1 // 如果两个树秩相同 路径压缩的 FIND-SET 实现 FIND-SET(x) if x !== x.parent x.parent = FIND-SET(x.parent) // 递归时找到树根代表元，回溯时将当前节点的 parent 指向树根 return x.parent 完整代码 python 不带优化的实现 class UF: parent = {} cnt = 0 def __init__(self, M): # 初始化 parent 和 cnt def find(self, x): while x != self.parent[x]: x = self.parent[x] return x def union(self, p, q): if self.connected(p, q): return self.parent[self.find(p)] = self.find(q) self.cnt -= 1 def connected(self, p, q): return self.find(p) == self.find(q) 带路径压缩的代码模板 class UF: parent = {} size = {} cnt = 0 def __init__(self, M): # 初始化 parent，size 和 cnt def find(self, x): while x != self.parent[x]: x = self.parent[x] # 路径压缩 self.parent[x] = self.parent[self.parent[x]]; return x def union(self, p, q): if self.connected(p, q): return # 小的树挂到大的树上， 使树尽量平衡 leader_p = self.find(p) leader_q = self.find(q) if self.size[leader_p] typescript 实现 class UF { private parents: Map = new Map(); private ranks: Map = new Map(); constructor(values: T[]) { this.makeSet(values); } makeSet(values: T[]) { values.forEach((value) => { this.parents.set(value, value); this.ranks.set(value, 0); }); } find(value: T) { const parent = this.parents.get(value); if (parent === value) { return parent; } this.parents.set(value, this.find(parent)); return this.parents.get(value); } union(x: T, y: T) { const xRank = this.ranks.get(x); const yRank = this.ranks.get(y); const xRoot = this.parents.get(x); const yRoot = this.parents.get(y); if (xRank > yRank) { this.parents.set(yRoot, xRoot); } else { this.parents.set(xRoot, yRoot); if (xRank === yRank) { this.ranks.set(y, yRank + 1); } } } } 并查集的应用 确定无向图的连通分量 亲戚问题，是否同个祖先 推荐下面的题练练手 547. 朋友圈 1319. 连通网络的操作次数 924. 尽量减少恶意软件的传播 928. 尽量减少恶意软件的传播 II 参考 算法导论 维基百科 并查集详解 ——图文解说,简单易懂(转) "},"08-算法/08-前缀树.html":{"url":"08-算法/08-前缀树.html","title":"前缀树","keywords":"","body":"本文为 《91 天学算法》 中的讲义, 因为是非开源项目, 所以这里无法给出链接, 作者 lucifer 后期本人会重写, 暂时引用, 特此声明 Trie 简介 字典树也叫前缀树、Trie。它本身就是一个树型结构，也就是一颗多叉树，学过树的朋友应该非常容易理解，它的核心操作是插入，查找。删除很少使用，因此这个讲义不包含删除操作。 应用场景及分析（个人理解）： 它的核心思想是用空间换时间，利用字符串的公共前缀来降低查询的时间开销。 比如给你一个字符串query，问你这个字符串是否在字符串集合中出现过，这样我们就可以将字符串集合建树，建好之后来匹配query是否出现，那有的朋友肯定会问，之前讲过的hashmap岂不是更好？ 我们想一下用百度搜索时候，打个“一语”，搜索栏中会给出“一语道破”，“一语成谶(四声的chen)”等推荐文本，这种叫模糊匹配，也就是给出一个模糊的query，希望给出一个相关推荐列表，很明显，hashmap并不能做到模糊匹配，而Trie可以完美实现。 因此，这里我的理解是：上述精确查找只是模糊查找一个特例，模糊查找hashmap显然做不到，并且如果在精确查找问题中，hashmap出现过多冲突，效率还不一定比Trie高，有兴趣的朋友可以做一下测试，看看哪个快。 给你一个长句和一堆敏感词，找出长句中所有敏感词出现的所有位置（想下，有时候我们口吐芬芳，结果发送出去却变成了**，懂了吧） 还有些其他场景，这里不过多讨论，有兴趣的可以google一下。 Trie的节点： 根结点无实际意义 每一个节点代表一个字符 每个节点中的数据结构可以自定义，如isWord(是否是单词)，count(该前缀出现的次数)等，需实际问题实际分析需要什么。 Trie的插入 假定给出几个单词如[she,he,her,good,god]构造出一个Trie如下图： 也就是说从根结点出发到某一粉色节点所经过的字符组成的单词，在单词列表中出现过，当然我们也可以给树的每个节点加个count属性，代表根结点到该节点所构成的字符串前缀出现的次数 可以看出树的构造非常简单，插入新单词的时候就从根结点出发一个字符一个字符插入，有对应的字符节点就更新对应的属性，没有就创建一个！ Trie的查询 查询更简单了，给定一个Trie和一个单词，和插入的过程类似，一个字符一个字符找 若中途有个字符没有对应节点→Trie不含该单词 若字符串遍历完了，都有对应节点，但最后一个字符对应的节点并不是粉色的，也就不是一个单词→Trie不含该单词 Trie的复杂度 插入和查询的时间复杂度自然是$O(len(key))$，key是待插入(查找)的字串。 建树的最坏空间复杂度是$O(m^{n})$, m是字符集中字符个数，n是字符串长度。 "},"08-算法/09-KMP&RK.html":{"url":"08-算法/09-KMP&RK.html","title":"KMP&RK","keywords":"","body":"本文为 《91 天学算法》 中的讲义, 因为是非开源项目, 所以这里无法给出链接, 作者 lucifer 后期本人会重写, 暂时引用, 特此声明 字符串匹配 字符串搜索算法（String searching algorithms）又称字符串比对算法（string matching algorithms）是一种搜索算法，是字符串算法中的一类，用以试图在一长字符串或文章中，找出其是否包含某一个或多个字符串，以及其位置。 用数学语言描述如下: 假设文本串 T 是一个长度为 n 的数组 $T[1..n]$, P 是长度为 m 的数组 $P[1..m]$, 其中 T 被称为文本串，P 被称为模式串。如果有 $0 字符串匹配问题就是找到模式串在文本串中的位移 s 暴力 遍历文本串，对遍历到的每个位置 i, 判断 $T[i..i+m]$ 是否和模式串$P[1..m]$相等 伪代码 n = length[T] m = length[P] for s = 0 to n - m do if T[s...s+m] == P[1...m] find s 时间复杂度 $O((n - m + 1) * n)$, 空间复杂度$O(1)$ 这种暴力算法效率并不高，因为对于 s 的每个值，我们获得文本串和模式串的信息在考虑 s 的其它值时被丢弃了。比如 t = 'aaaabbbb', p = 'aabb' 当 s = 1 时，我们判断了 t[1] == p[1], t[2] == p[2], t[3] != p[3], s 递增判断 s=2 的情况，注意到在判断 s = 1 的时候，我们已经知道了 t[2] == p[2], 但是这个时候我们还要计算 t[2] 和 p[2]的比对情况，导致了计算重复。如果我们充分利用之前计算过的匹配信息，算法效率会有什么样的提升呢，接下来介绍另外几种算法。 RK RK 算法主要思想主要是对 T 中每个长度为 m 的子字符串 T[s..s+m] 做哈希，生成哈希值 h1, 对 P 做哈希，生成哈希值 h2, 比对 h1 和 h2，如果两个哈希值(不考虑冲突)相等，则判断 P 在 T 中出现，且位移为 s 伪代码 hp = hash(P[0..m]) for s = 0 to n - m hs = hash(T[s..s+m]) if hp == hs then print 'find s' 我们这里选取的哈希函数为 $f(P[1..m]) = P[1..m]表示的10进制值$ 假设 P 和 T 全由 d 个字符组成的，则我们可以选择 d 进制表示 P 和 T，再将 d 进制转为 10 进制便于计算。 为了简化说明，我们更特殊的假设 P 和 T 全由[0-9]10 个数字组成， 那么 P 的 10 进制为 f(P) = P[0] * 10 ^ (m - 1) + P[1] * 10 ^ (m - 2) ... + P[1] T[s...s + m]的 10 进制为 f(T[s...s + m]) = T[s] * 10 ^ (m - 1) + T[s + 1] * 10 ^ (m - 2) ... + T[s + m] T[s+1...s + m + 1] 可以根据 T[s...s + m]推导 f(T[s+1...s + m + 1]) = T[s+1] * 10 ^ (m - 1) + T[s + 2] * 10 ^ (m - 2) ... + T[s + m + 1] = (f(T[s...s + m]) - T[s] * 10 ^ (m - 1)) * 10 + T[s + m + 1] 这样就把以上 hp === hs 的哈希比较转化为正常的 10 进制比较。 到目前为止，以上假设我们回避的一个问题是如果 f(P) 或者 f(T)计算的 10 进制过大，导致运算溢出怎么办？ 这里我们通过选择一个比较大的素数 q, 计算后的 10 进制数对 q 取模后再进行比较。但是这种方案并不完美，f(P) % q === f(T[s]) % q 并不能代表 f(P) === f(T[s])。任何的 f(P) % q === f(T[s]) % q 都需要额外的测试来保证 P === T[s], 这里我们通过检测 P[1...m] === T[s...s+m]来完成。 KMP 下面介绍下 KMP 算法，KMP 算法由 Knuth Morris Pratt 三个大佬联合发明，KMP 算法名字由三个大佬名字首位字符组成。 首先我们定义模式串的前缀函数 f(i) 为 模式串 P 中 P[1...i]相同前缀后缀的最大长度。对 P[1...m]中的每个 i，(i > 0 && i 之前提到暴力算法的时间复杂度很高是因为每次字符失配后，我们只是简单的将 s 位移加一，将之前的匹配信息舍去，造成计算重复。 KMP 算法在每次失配后，会根据上一次的比对信息跳转到相应的 s 处，借助的就是上述的 next 数组。 推导过程可以参考 从头到尾彻底理解KMP，个人觉得这篇讲的非常透彻，这里就不班门弄斧了。 以下是计算 next 数组的伪代码 get_next(P): m = P.length 使得 next 为长度为m的数组 next[1] = 0 k = 0 for i = 2 to m while(k > 0 并且 P[k+1] != P[i]) k = next[k] if P[k+1] == P[i] k = k + 1 next[i] = k return next 以下是 KMP 的伪代码 KMP(T, P) n = T.length m = P.length next = getNext(P) q = 0 for let i = 1 to n: while(q > 0 并且 P[q + 1] !== T[i]) q = next[q] if P[q + 1] == T[i] q = q + 1 if (q == m) 找到匹配位移 s = i 基于有限自动机的字符串匹配 这个算法仅限了解即可，这里不做展开，感兴趣可以参考 Finite Automata algorithm for Pattern Searching 参考 维基百科 从头到尾彻底理解KMP Finite Automata algorithm for Pattern Searching "},"08-算法/10-跳表.html":{"url":"08-算法/10-跳表.html","title":"跳表","keywords":"","body":"本文为 《91 天学算法》 中的讲义, 因为是非开源项目, 所以这里无法给出链接, 作者 lucifer 后期本人会重写, 暂时引用, 特此声明 跳表 虽然在面试中出现的频率不大，但是在工业中，跳表会经常被用到。力扣中关于跳表的题目只有一个。但是跳表的设计思路值得我们去学习和思考。 其中有很多算法和数据结构技巧值得我们学习。比如空间环时间的思想，比如效率的取舍问题等。 解决的问题 只有知道跳表试图解决的问题， 后面学习才会有针对性。实际上，跳表解决的问题非常简单，一句话就可以说清楚，那就是为了减少链表长度增加，查找链表节点时带来的额外比较次数。 不借助额外空间的情况下，在链表中查找一个值，需要按照顺序一个个查找，时间复杂度为 $O(N)$，其中 N 为链表长度。 （单链表） 当链表长度很大的时候， 这种时间是很难接受的。 一种常见的的优化方式是建立哈希表，将所有节点都放到哈希表中，以空间换时间的方式减少时间复杂度，这种做法时间复杂度为 $O(1)$，但是空间复杂度为 $O(N)$。 （单链表 + 哈希表） 为了防止链表中出现重复节点带来的问题，我们需要序列化节点，再建立哈希表，这种空间占用会更高，虽然只是系数级别的增加，但是这种开销也是不小的 。 为了解决上面的问题，跳表应运而生。 如下图所示，我们从链表中每两个元素抽出来，加一级索引，一级索引指向了原始链表，即：通过一级索引 7 的 down 指针可以找到原始链表的 7 。那怎么查找 10 呢？ 注意这个算法要求链表是有序的。 （建立一级索引） 我们可以： 通过现在一级跳表中搜索到 7，发现下一个 18 大于 10 ，也就是说我们要找的 10 在这两者之间。 通过 down 指针回到原始链表，通过原始链表的 next 指针我们找到了 10。 这个例子看不出性能提升。但是如果元素继续增大， 继续增加索引的层数，建立二级，三级。。。索引，使得链表能够实现二分查找，从而获得更好的效率。但是相应地，我们需要付出额外空间的代价。 （增加索引层数） 理解了上面的点，你可以形象地将跳表想象为玩游戏的存档。 一个游戏有 10 关。如果我想要玩第 5 关的某一个地方，那么我可以直接从第五关开始，这样要比从第一关开始快。我们甚至可以在每一关同时设置很多的存档。这样我如果想玩第 5 关的某一个地方，也可以不用从第 5 关的开头开始，而是直接选择离你想玩的地方更近的存档，这就相当于跳表的二级索引。 跳表的时间复杂度和空间复杂度不是很好分析。由于时间复杂度 = 索引的高度 * 平均每层索引遍历元素的个数，而高度大概为 $logn$，并且每层遍历的元素是常数，因此时间复杂度为 $logn$，和二分查找的空间复杂度是一样的。 空间复杂度就等同于索引节点的个数，以每两个节点建立一个索引为例，大概是 n/2 + n/4 + n/8 + … + 8 + 4 + 2 ，因此空间复杂度是 $O(n)$。当然你如果没三个建立一个索引节点的话，空间会更省，但是复杂度不变。 总结 跳表是可以实现二分查找的有序链表； 跳表由多层构成，最底层是包含所有的元素原始链表，往上是索引链表； 实际的设计中，需要做好取舍，设定合理数量的索引。 跳表查询、插入、删除的时间复杂度为 $O(log N)$，空间复杂度为 $O(N)$； "},"08-算法/11-剪枝.html":{"url":"08-算法/11-剪枝.html","title":"剪枝","keywords":"","body":"本文为 《91 天学算法》 中的讲义, 因为是非开源项目, 所以这里无法给出链接, 作者 lucifer 后期本人会重写, 暂时引用, 特此声明 剪枝 简介 关于剪枝这个概念，有的同学对机器学习有一定了解的肯定能脱口而出： 剪枝是为了解决决策树过拟合，为了降低模型复杂度的一种手段。 而我们这次要介绍的剪枝又何尝不是为了降低我们所写程序的时空复杂度呢？我们在日常编程中或多或少都用到了剪枝，只不过大家没有系统去了解过这块的概念而已，相信大家都会用，所以这次的讲义希望将大家对剪枝中模糊不清对概念有一个比较清晰的认识。 剪枝的目的： 上面也说了，我们剪枝就是为了降低我们算法的复杂度，剪枝最常出现在搜索相关的问题上，我们常用的搜索算法，其实描绘出的搜索空间就是一个树形结构，日常生活中剪枝剪的就是树的枝桠，在搜索空间中剪枝剪掉的就是必得不到解的部分来减小搜索空间。 剪枝遵循的三原则： 正确性：这个很好理解，我们把这个树杈剪掉的前提是剪掉的这块一定不存在我们所要搜寻的解，不然我们把正确结果都剪没了，那还搜索个啥呢。 准确性：我们在保证正确性的前提下，尽可能多的剪掉不包含所搜寻解的枝叶，也就是咱们剪，就要努力剪到最好。 高效性：这个就是一个衡量我们剪枝是否必要的一个标准了，比如我们设计出了一个非常优秀的剪枝策略，可以把搜索规模控制在非常小范围，很棒！但是我们去实现这个剪枝策略的时候，又耗费了大量的时间和空间，是不是有点得不偿失呢？也就是我们需要在算法的整体效率和剪枝策略之间trade-off。 常用的剪枝策略： 可行性剪枝：如果我们当前的状态已经不合法了，我们也没有必要继续搜索了，直接把这块搜索空间剪掉，也就是return。 记忆化：常做dp题的同学应该也知道，我们把已经计算出来的问题答案保存下来，下次遇到该问题就可以直接取答案而不用重复计算。 搜索顺序剪枝：在我们已知一些有用的先验信息的前提下，定义我们的搜索顺序。举个最简单例子，有时候我们正序遍历数组遇到答案返回，这种解法会TLE，但是，我们倒着遍历却过了，这就是对搜索顺序进行剪枝。 最优性剪枝：也叫上下边界剪枝，Alpha-Beta剪枝，常用于对抗类游戏。当算法评估出某策略的后续走法比之前策略的还差时，就会剪掉该策略的后续发展。 等等。 用好剪枝，会让我们的算法事半功倍，所以大家一定要掌握剪枝这一强力的思想。 "},"08-算法/Python时间复杂度.html":{"url":"08-算法/Python时间复杂度.html","title":"Python时间复杂度","keywords":"","body":"Python内置操作时间复杂度 列表 - list 操作 平均情况 最坏情况 copy O(n) O(n) append() O(1) O(1) pop() O(1) O(1) pop(k) O(k) O(k) insert(n, x) O(n) O(n) get (即: list[index]) O(1) O(1) set (即: list[index] = x) O(1) O(1) delete (即: list.remove(x)) O(n) O(n) 遍历 O(n) O(n) 获取切片 O(k) O(k) 删除切片 O(n) O(n) 设置切片 O(k+n) O(k+n) extend(list) O(k) O(k) sort O(n log n) O(n log n) [n] * k O(nk) O(nk) x in s O(n) min(s), max(s) O(n) len() O(1) O(1) 双向队列 - collections.deque 操作 平均情况 最坏情况 copy O(n) O(n) append O(1) O(1) appendleft O(1) O(1) pop O(1) O(1) popleft O(1) O(1) extend O(k) O(k) extendleft O(k) O(k) rotate O(k) O(k) remove O(n) O(n) 集合 - set 操作 平均情况 最坏情况 备注 x in s O(1) O(n) Union s\\ t O(len(s)+len(t)) Intersection s&t O(min(len(s), len(t)) O(len(s) * len(t)) replace \"min\" with \"max\" if t is not a set Multiple intersection s1&s2&..&sn (n-1)*O(l) where l is max(len(s1),..,len(sn)) Difference s-t O(len(s)) s.difference_update(t) O(len(t)) Symmetric Difference s^t O(len(s)) O(len(s) * len(t)) s.symmetric_difference_update(t) O(len(t)) O(len(t) * len(s)) 字典 - dict 操作 平均情况 最坏情况 k in d O(1) O(n) copy O(n) O(n) dict[key] O(1) O(n) dict[key] = value O(1) O(n) dict.pop() popitem() O(1) O(n) 遍历 O(n) O(n) "},"08-算法/优先队列.html":{"url":"08-算法/优先队列.html","title":"优先队列","keywords":"","body":"优先队列 "},"08-算法/力扣题解/":{"url":"08-算法/力扣题解/","title":"力扣题解","keywords":"","body":"力扣题解 记录 Python 力扣题解 "},"08-算法/双端队列.html":{"url":"08-算法/双端队列.html","title":"双端队列","keywords":"","body":"双端队列 "},"08-算法/排序合集.html":{"url":"08-算法/排序合集.html","title":"排序合集","keywords":"","body":"排序算法 前言 🍀 排序问题是我们学习编程过程中最常见的 , 以Python中的列表为例 , 进行排序算法分析 , 在进行分析之前 , 我们先自己写一个时间测试装饰器 timewrap.py import time def cal_time(func): def wrapper(*args, **kwargs): t1 = time.time() result = func(*args, **kwargs) t2 = time.time() print(\"%s running time: %s secs.\" % (func.__name__, t2-t1)) return result return wrapper 这个装饰器只是为了进行简单的时间测试 , 因为影响一个算法的执行时间实在是太多 , 但对于我们学习算法确是够了 冒泡排序 🍀 工作流程 : 比较相邻的元素 , 如果第一个比第二个大 , 就交换它们 对每一对相邻元素作同样的工作 , 从列表的开始到结尾依次进行 , 如果列表长度为n , 那么总共走(n-1)躺 图解 : 代码实现 : import random from timewrap import * # 第一版 @cal_time def bubble_sort_1(li): # 总共走n-1躺 for n in range(len(li) - 1): # 每一躺比较(总长度-n-1次) for j in range(0, len(li) - n - 1): if li[j] > li[j+1]: li[j], li[j+1] = li[j+1], li[j] # 在第一版的基础进行优化 @cal_time def bubble_sort_2(li): # 总共走(n-1)躺 for n in range(len(li) - 1): # 没有改变说明元素位置正确,为了防止更多不必要的比较 change = False # 每一躺比较(总长度-n-1次) for j in range(0, len(li) - n - 1): if li[j] > li[j+1]: li[j], li[j+1] = li[j+1], li[j] change = True if not change: return li = list(range(10000)) random.shuffle(li) bubble_sort_1(li) bubble_sort_2(li) ''' 执行结果: bubble_sort_1 running time: 29.570897817611694 secs. bubble_sort_2 running time: 0.002001523971557617 secs. ''' 选择排序 🍀 工作流程 : 每次从待排序的数据元素中选出最小 (或最大) 的一个元素 , 存放在序列的起始位置 , 直到全部待排序的数据元素排完 代码实现 : import random from timewrap import * @cal_time def select_sort(li): # 总共走(n-1)躺 for n in range(len(li) - 1): # 最小数的位置 min_loc = n for j in range(n + 1, len(li) - 1): if li[j] 插入排序 🍀 工作流程 : 存在一个已经有序的数据序列 将后续数据按照要求依次一个个插入有序序列中 图解 : 代码实现 : import random from timewrap import * @cal_time def insert_sort(li): # 循环无序区进行排序 for n in range(1, len(li)): tmp = li[n] # 指向有序区最后位置 j = n - 1 while li[j] > tmp and j >= 0: li[j+1] = li[j] j -= 1 li[j+1] = tmp li = list(range(10000)) random.shuffle(li) insert_sort(li) ''' 执行结果: insert_sort running time: 18.230905055999756 secs. ''' 快速排序 🍀 工作流程 : 取一个元素P(第一个元素) , 使元素P归位 列表被P分成两部分 , 左边都小于P , 右边都大于P 递归完成排序 图解 : 代码实现 : import random from timewrap import * import copy import sys # 修改递归最大层数 sys.setrecursionlimit(100000) def partition(li, left, right): \"\"\" 进行分区 \"\"\" # 防止出现最坏情况 # ri = random.randint(left, right) # li[left], li[ri] = li[ri], li[left] tmp = li[left] while left = tmp: right -= 1 li[left] = li[right] while left 堆排序 🍀 堆分类 大根堆 : 一颗完全二叉树 , 满足任一节点都比其孩子节点大 小根堆 : 一颗完全二叉树 , 满足任一节点都比其孩子节点小 工作流程 : 建立堆 , 已完成调整 (以构建大根堆为例) 得到堆顶元素 , 为最大元素 去掉堆顶 , 将堆最后一个元素放到堆顶 , 随后重新调整 (挨个出数) 一直重复3 , 直到堆为空 挨个出数规则 : 找最后一个数作为棋子 , 然后取堆顶的值 , 存放最后 , 依次执行取出 图解 : 1.构建大根堆 , 索引按照从上倒下 , 从左到右依次递增 2.挨个出数 代码实现 : from timewrap import * import random def sift(li, low, high): \"\"\" :param li: :param low: 堆根节点的位置 :param high: 堆最有一个节点的位置 :return: \"\"\" # 父亲的位置 i = low # 孩子的位置 j = 2 * i + 1 # 原父亲 tmp = li[low] while j li[j]: j += 1 # 如果原父亲比孩子小 if tmp Python中有一个内置模块heapq可以帮助我们快速实现对排序 import heapq import random from timewrap import * @cal_time def heap_sort(li): heapq.heapify(li) n = len(li) new_li = [] for i in range(n): new_li.append(heapq.heappop(li)) return new_li li = list(range(10000)) random.shuffle(li) # 小根堆 heap_sort(li) # 大根堆,直接利用方法nlargest heapq.nlargest(100, li) 归并排序 🍀 假设现在的列表分成两段有序 , 如何将其合成为一个有序的列表 工作流程 : 分解列表 , 直至分解为一个个只有一个元素的列表 比较两段序列中索引相同的值的大小 , 符合条件就进行交换 , 如小的在左 , 大的在右 进行合并 , 重复2操作 , 直至合并为一个列表得出结果 图解 : 代码实现 : import random def merge(li, low, mid, high): i = low j = mid + 1 ltmp = [] while i 希尔排序 🍀 希尔排序是一种分组插入排序算法 工作流程 : 首先取一个整数d1=n/2 , 将元素分为d1个组 , 每组相邻两元素距离为d1 , 在各组内进行直接插入排序 取第二个整数d2=d1/2 , 重复上述分组排序过程 , 直到d1=1 , 即所有元素在同一组内进行直接插入排序 希尔排序每躺并不使某些元素有序 , 而是使整体数据越来越进阶有序 ; 最后一趟排序使得所有数据有序 代码实现 : import random def shell_sort(li): gap = int(len(li) // 2) while gap > 0: for i in range(gap, len(li)): tmp = li[i] print(i-gap) j = i - gap while j >= 0 and tmp 计数排序 🍀 现有一个列表 , 列表中的数范围都在0到100之间 , 列表长度大约为100万 , 设计算法在O(n)时间复杂度内将列表进行排序 工作流程 : 查找列表中最大和最小的元素 开辟一个新的空间存放统计的每个元素出现次数 反向填充目标列表 代码实现 : import random import copy from timewrap import * @cal_time def count_sort(li, max_num = 100): count = [0 for i in range(max_num+1)] for num in li: count[num]+=1 li.clear() for i, val in enumerate(count): for _ in range(val): li.append(i) @cal_time def sys_sort(li): li.sort() li = [random.randint(0,100) for i in range(100000)] li1 = copy.deepcopy(li) count_sort(li) ''' 执行结果: count_sort running time: 0.024517059326171875 secs. ''' 桶排序 🍀 桶排序的基本思想是将一个数据表分割成许多桶 , 然后每个桶各自排序 , 有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排序 工作流程 : 建立一堆buckets 遍历原始数组 , 并将数据放入到各自的buckets当中 对非空的buckets进行排序 按照顺序遍历这些buckets并放回到原始数组中即可构成排序后的数组 代码实现 : import random from timewrap import * def list_to_buckets(li, iteration): \"\"\" :param li: 列表 :param iteration: 装桶是第几次迭代 :return: \"\"\" buckets = [[] for _ in range(10)] for num in li: digit = (num // (10 ** iteration)) % 10 buckets[digit].append(num) return buckets def buckets_to_list(buckets): return [num for bucket in buckets for num in bucket] @cal_time def radix_sort(li): maxval = max(li) it = 0 while 10 ** it 小结 🍀 排序方法 时间复杂度(平均) 时间复杂度(最坏) 时间复杂度(最好) 空间复杂度 稳定性 复杂性 冒泡排序 O(n²) O(n²) O(n) O(1) 稳定 简单 选择排序 O(n²) O(n²) O(n²) O(1) 不稳定 简单 插入排序 O(n²) O(n²) O(n²) O(1) 稳定 简单 快速排序 O(n²) O(nlogn) O(nlogn) 平均情况O(nlogn)最坏情况O(n) 不稳定 较复杂 堆排序 O(nlogn) O(nlogn) O(nlogn) O(1) 不稳定 复杂 归并排序 O(nlogn) O(nlogn) O(nlogn) O(n) 稳定 较复杂 希尔排序 O(nlogn) O(n²) O(n) O(1) 不稳定 较复杂 计数排序 O(n+k) O(n+k) O(n+k) O(n+k) 稳定 简单 桶排序 O(n+k) O(n²) O(n+k) O(n*k) 不稳定 简单 "},"08-算法/树.html":{"url":"08-算法/树.html","title":"树","keywords":"","body":"树 "},"08-算法/链表.html":{"url":"08-算法/链表.html","title":"链表","keywords":"","body":"链表 "},"09-Linux/":{"url":"09-Linux/","title":"Linux","keywords":"","body":""},"09-Linux/Command/":{"url":"09-Linux/Command/","title":"Command","keywords":"","body":""},"09-Linux/Command/01-常用命令.html":{"url":"09-Linux/Command/01-常用命令.html","title":"常用命令","keywords":"","body":"Linux基础命令 ls 🍀 [root@lyonyang ~]# ls [aAdfFhilnrRSt] 目录名称 [root@lyonyang ~]# ls [--colo={never,auto,always}] 目录名称 [root@lyonyang ~]# ls [--full-times] 目录名称 # 参数 : -a : 全部的文件,连同隐藏文件(开头为 . 的文件)一起列出来 -A : 仅列出全部的文件(连同隐藏文件,但不包括 . 与 .. 这两个目录) -d : 仅列出目录本身,而不是列出目录内的文件数据 -f : 直接列出结果,而不进行排序(ls默认会以文件名排序) -F : 根据文件,目录等信息给予附加数据结构,例如: *:代表可执行文件; /:代表目录; =:代表socket文件; |:代表FIFO文件 -h : 将文件容量以较易读的方式(例如GB,KB等)列出来 -i : 列出inode号码 -l : 列出长数据串,包含文件的属性与权限等数据 -n : 列出UID与GID,而非用户与用户组的名称 -r : 将排序结果反向输出 -R : 连同子目录内容一起列出来,等于该目录下的所有文件都会显示出来 -S : 以文件容量大小排序,而不是用文件名排序 -t : 依时间排序,而不是用文件名 --color=never : 不要依据文件特性给予颜色显示 --color=always : 显示颜色 --color=auto : 让系统自行依据设置来判断是否给予颜色 --full-time : 以完整时间模式(包含年,月,日,时,分)输出 --time=(atime,ctime) : 输出访问时间或改变权限属性时间(ctime)而非内容梗概时间(modification time) cp 🍀 [root@lyonyang ~]# cp [-adfilprsu] 源文件(source) 目标文件(destination) [root@lyonyang ~]# cp [options] source1 source2 source3 ... directory # 参数 : -a : 相当于-pdr的意思 -d : 若源文件为连接文件的属性(link file),则复制连接文件属性而非文件本身 -f : 为强制(force)的意思,若目标文件已经存在且无法开启,则删除后再尝试一次 -i : 若目标文件(destination)已经存在时,在覆盖时会先查询问操作的进行 -l : 进行硬连接(hard link)的连接文件创建,而非复制文件本身 -p : 连同文件的属性一起复制过去,而非使用默认属性 -r : 递归持续复制,用于目录的复制行为 -s : 复制成功符号链接文件(symbolic link),即\"快捷方式\"文件 -u : 若destination比source旧才更新destination # 注意 : 如果源文件有两个以上,则最后一个目的文件一定要是\"目录\"才行 rm 🍀 [root@lyonyang ~]# rm [-fir] 文件或目录 # 参数 : -f : 就是force的意思,忽略不存在的文件,不会出现警告信息 -i : 互动模式,在删除前会询问用户是否操作 -r : 递归删除,最常用在目录的删除,此参数异常危险 mv 🍀 [root@lyonyang ~]# mv [-fiu] source destination [root@lyonyang ~]# mv [options] source1 source2 source3 ... directory # 参数 : -f : fource强制的意思,如果目标文件已经存在,不会询问而直接覆盖 -i : 若目标文件(destination)已经存在时,就会询问是否覆盖 -u : 若目标文件已经存在,且source比较新,才会更新 cat 🍀 [root@lyonyang ~]# cat [-AbenTv] # 参数 : -A : 相当于-vET的整合参数,可列出一些特殊字符,而不是空白而已 -b : 列出行号,仅针对非空白行做行号显示,空白行不标行号 -E : 将结尾的断行字符$显示出来 -n : 打印出行号,连同空白行也会有行号,与-b的参数不同 -T : 将[Tab]按键以^I显示出来 -v : 列出一些看不出来的特殊字符 tac 命令与 cat 命令恰好相反 nl 🍀 [root@lyonyang ~]# nl [-bnw] 文件 # 参数 : -b : 指定行号指定的方式,主要有两种: -b a : 表示不论是否为空行,也同样列出行号(类似cat -n) -b t : 如果有空行,空的那一行不要列出行号(默认值) -n : 列出行号表示的方法,主要有三种: -n ln : 行号在屏幕的最左方显示 -n rn : 行号在自己字段的最右方显示,且不加0 -n rz : 行号在自己字段的最右方显示,且加0 -w : 行号字段占用的位数 touch 🍀 [root@lyonyang ~]# touch [-acdmt] 文件 # 参数 : -a : 仅修改访问时间 -c : 仅修改文件的时间,若该文件不存在则不创建新文件 -d : 后面可以接欲修改的日期而不用目前的日期,也可以使用--date=\"日期或时间\" -m : 仅修改mtime -t : 后面可以接欲修改的时间而不用目前的时间,格式为[YYMMDDhhmm] which 🍀 [root@lyonyang ~]# which [-a] command # 参数 : -a : 将所有由 PATH 目录中可以找到的命令均列出 , 而不知第一个被找到的命令名称 whereis 🍀 [root@lyonyang ~]# whereis [-bmsu] 文件或目录名 # 参数 : -b : 只找二进制格式的文件 -m : 只找在说明文件manual路径下的文件 -s : 只找source源文件 -u : 查找不在上述三个选项当中的其他特殊文件 locate 🍀 该命令如果查找新文件 , 需要更新数据库 手动更新 updatedb [root@lyonyang ~]# locate [-ir] keyword # 参数 : -i : 忽略大小写的差异 -r : 后面可接正则表达式的显示方式 find 🍀 [root@lyonyang ~]# find [PATH] [option] [action] # 参数 : 1. 与时间有关的参数:共有-atime,-ctime与-mtime,下面以-mtime说明: -mtime n : n为数字,意义为在n天之前的\"一天之内\"被更改过的文件 -mtime +n : 列出在n天之前(不含n天本身)被更改过的文件名 -mtime -n : 列出在n天之内(含n天本身)被更改过的文件名 -newer file : file为一个存在的文件,列出比file还要新的文件 2. 与用户或用户组有关的参数: -uid n : n为数字,这个数字是用户的账号ID,即UID,这个UID是记录在 /etc/passwd里面与账号名称对应的数字 -gid n : n为数字,这个数字是用户组名的ID,即GID,这个GID记录在 /etc/group中 -user name : name为用户账号名称 -group name : name为用户组名 -nouser : 寻找文件的所有者不存在 /etc/passwd 的人 -nogroup : 寻找文件的所有用户组不存在于 /etc/group 中的文件 3. 与文件权限及名称有关的参数: -name filename : 查找文件名为filename的文件 -size [+-]SIZE : 查找比SIZE还要大(+)或小(-)的文件,SIZE规格有: c:代表byte;k:代表1024bytes; -type TYPE : 查找文件的类型为TYPE的,类型主要有: 一般正规文件(f),设备文件(b,c),目录(d),连接文件(l),socket(s),FIFO(p) -perm mode : 查找文件权限等于mode的文件,mode类似chmod的属性值,如-rwsr-xr-x属性值为4755 -perm -mode : 查找文件权限包括mode的文件 -perm +mode : 查找文件权限包含任一mode的权限的文件 4. 其他可进行的操作: -exec command : command为其他命令,-exec后面可再接其他的命令来处理查找到的结果 -print : 将结果打印到屏幕上,这个操作是默认的 压缩与打包 gzip,zcat 🍀 [root@lyonyang ~]# gzip [-cdtv#] 文件名 # 参数 : -c : 将压缩的数据输出到屏幕上,可通过数据流重定向来处理 -d : 解压缩的参数 -t : 可以用来检测一个压缩文件的一致性,看看文件有无错误 -v : 可以显示出源文件/压缩文件的压缩比等信息 -# : 压缩等级,-1最快,但是压缩比最差,-9最慢,但是压缩比最好,默认是-6 bzip2,bzcat 🍀 [root@lyonyang ~]# bzip2 [-cdkzv#] 文件名 # 参数 : -c : 将压缩的数据输出到屏幕上 -d : 解压缩的参数 -k : 保留原文件,而不会删除原始的文件 -z : 压缩的参数 -v : 可以显示出原文件/压缩文件的压缩比等信息 -# : 与gzip一样 dump 🍀 [root@lyonyang ~]# dump [-Suvj] [-level] [-f 备份文件] 待备份数据 [root@lyonyang ~]# dump -W # 参数 : -S : 仅列出后面的备份数据需要多少磁盘空间才能够备份完毕 -u : 将这次dump的时间记录到/etc/dumpdateS文件中 -v : 将dump的文件过程显示出来 -j : 加入bzip2的支持,将数据进行压缩,默认bzip2压缩等级为2 -level : 等级从0~9共10个等级 -f : 有点类似tar,后面接产生的文件,可接例如/dev/st0设备文件名 -w : 列出在/etc/fstab里面的具有dump设置的分区是否有备份过 restore 🍀 [root@lyonyang ~]# restore -t [-f dumpfile] [-h] # 查看dump文件 [root@lyonyang ~]# restore -C [-f dumpfile] [-D 挂载点] # 比较dump与实际文件 [root@lyonyang ~]# restore -i [-f dumpfile] # 进入互动模式 [root@lyonyang ~]# restore -r [-f dumpfile] # 还原整个文件系统 # 参数 : -t : 此模式用在查看dump起来的备份文件中含有什么重要数据 -C : 此模式可以将dump内的数据拿出来跟实际的文件系统做比较 最终会列出\"在dump文件内有记录的,且目前文件系统不一样\"的文件 -i : 进入互动模式,可以仅还原部分文件,用在dump目录时的还原 -r : 将整个文件系统还原的一种模式,用在还原针对文件系统的dump备份 -h : 查看完整备份数据中的inode与文件系统label等信息 -f : 后面就接你要处理的那个dump文件 -D : 与-C进行搭配,可以查出后面接的挂载点与dump内有不同的文件 tar 🍀 [root@lyonyang ~]# tar [-j|-z] [cv] [-f 新建的文件名] filename # 打包与压缩 [root@lyonyang ~]# tar [-j|-z] [tv] [-f 新建的文件名] # 查看文件名 [root@lyonyang ~]# tar [-j|-z] [xv] [-f 新建的文件名] # 解压缩 # 参数 : -c : 新建打包文件,可搭配-v来查看过程中被打包的文件名 -t : 查看打包文件的内容含有哪些文件名,重点在查看文件名 -x : 解打包或解压缩的功能,可以搭配-C在特定目录解开 -c,-t,-x不可同时出现在一串命令中 -j : 通过bzip2的支持进行压缩/解压缩,此时文件名最好为 *.tar.bz2 -z : 通过gzip的支持进行压缩/解压缩,此时文件名最好为 *.tar.gz -v : 在压缩/解压缩的过程中,将正在处理的文件名显示出来 -f filename : -f后面要接被处理的文件名,建议-f单独写一个参数 -C 目录 : 这个参数用在解压缩时,若要在特定目录解压缩,可以使用这个参数 -P : 保留备份数据的原本权限与属性,常用于备份(-c)重要的配置文件 -p : 保留绝对路径,即允许不备份数据中含有根目录存在之意 --exclude=FILE : 在压缩的过程中,不要将FILE打包 简单使用记忆 : 压缩 : tar -jcv -f filename.tar.bz2 要被压缩的文件或目录名称 查询 : tar -jtv -f filename.tar.bz2 解压缩 : tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录 dd 🍀 [root@lyonyang ~]# dd if=\"input file\" of=\"output file\" bs=\"block size\" count=\"number\" # 参数 : if : 就是input file ,也可以是设备 of : 就是output file ,也可以是设备 bs : 规划的一个block的大小,若未指定则默认为512bytes(一个扇区的大小) count : 多少个bs的意思 ps 🍀 [root@lyonyang ~]# ps aux # 查看系统所有的进程数据 [root@lyonyang ~]# ps -lA # 同上 [root@lyonyang ~]# ps axjf # 连同部分进程树的状态 # 参数 : -A : 所有的进程均显示出来,与-e具有同样的作用 -a : 不与terminal有关的所有进程 -u : 有效用户相关的进程 x : 通常与a这个参数一起使用,可列出较完整信息 l : 较长,较详细地将该PID的信息列出 j : 工作的格式 -f : 做一个更为完整的输出 free 🍀 [root@lyonyang ~]# free [-b|-k|-m|-g] [-t] # 参数 : -b : 直接输入free时,显示的单位是 -t : 在输出的最终结果中显示物理内存与swap的总量 uname 🍀 [root@lyonyang ~]# uname [-asrmpi] # 参数 : -a : 所有系统相关的信息,包括下面的数据都会被列出来 -s : 系统内核名称 -r : 内核的版本 -m : 本系统的硬件名称 -p : CPU的类型 -i : 硬件的平台 netstat 🍀 [root@lyonyang ~]# netstat -[atunlp] # 参数 : -a : 将目前系统上所有的连接,监听,Socket数据都列出来 -t : 列出tcp网络数据包的数据 -u : 列出udp网络数据包的数据 -n : 不列出进程的服务名称 -l : 列出目前正在网络监听的服务 -p : 列出该网络服务的进程PID rpm 🍀 [root@lyonyang ~]# rpm -ivh package_name # 参数 : -i : install 的意思 -v : 查看更详细的安装信息画面 -h : 以安装信息栏显示安装进度 [root@lyonyang ~]# rpm -qa [root@lyonyang ~]# rpm -q[licdR] 已安装的软件名称 [root@lyonyang ~]# rpm -qf 存在于系统上面的某个文件名 [root@lyonyang ~]# rpm -qp[licdR] 未安装的某个文件名称 # 参数 : -q : 仅查询,后面接的软件名称是否有安装 -qa : 列出所有的已经安装在本机Linux系统上面的所有软件名称 -qi : 列出该软件的详细信息,包含开发商,版本与说明 -ql : 列出该软件所有的文件与目录所在完整文件名 -qc : 列出该软件的所有设置文件 -qd : 列出该软件的所有帮助文件 -qR : 列出与该软件有关的依赖软件所含的文件 -qf : 由后面接的文件名称找出该文件属于哪一个已安装的软件 more 🍀 功能描述: 分页显示文件内容 命令所在路径: /usr/bin/more 执行权限: 所有用户 语法: more [文件名] (空格)或 f 翻页 (Enter) 换行 q或Q 退出 less 🍀 功能描述: 分页显示文件内容(可向上翻页) 命令所在路径: /usr/bin/less 执行权限: 所有用户 语法: less [文件名] "},"09-Linux/Command/vim.html":{"url":"09-Linux/Command/vim.html","title":"vim","keywords":"","body":"vim 介绍 🍀 基本上 vi 共分为 3 种模式 , 分别是一般模式 , 编辑模式与命令行模式 一般模式 🍀 以 vi 代开一个文件就直接进入一般模式了 (这是默认的模式) , 在这个模式中 , 你可以使用上下左右按键来移动光标 , 你可以删除字符或删除整行 , 也可以复制 , 粘贴你的文件数据 编辑模式 🍀 在一般模式中可以进行删除 , 复制 , 粘贴等的操作 , 但是却无法编辑文件内容 , 要等待你按下 \"i,I,o,O,a,A,r,R\" 等任何一个字母之后才进入编辑模式 , 通常在 Linux 中 , 按下这些按键时 , 在界面的左下方会出现 INSERT 或 REPLACE 的字样 , 此时才可以进行编辑 , 而如果要回到一般模式时 , 则必须要按下 [Esc] 键即可退出编辑模式 命令行模式 🍀 在一般模式中 , 输入 \":,/,?\" 3 个中的任何一个按钮 , 就可以将光标移动到最下面那一行 , 在这个模式当中 , 可以提供你查找数据的操作 , 而读取 , 保存 , 大量替换字符 , 离开 vi , 显示行号等的操作则是在此模式下完成的 简单使用 🍀 使用 vi 进入一般模式 [root@lyonyang ~]# vi test.txt 按 i 进入编辑模式 , 开始编辑文字 按 [Esc] 回到一般模式 在一般模式中输入 \":wq\" 保存后离开 vi 按键说明 🍀 第一部分 🍀 一般模式可用的按键说明 , 光标移动 , 复制粘贴 , 查找替换等 按键 说明 h 或 向左箭头键 (←) 光标向左移动一个字符 j 或 向下箭头键 (↓) 光标向下移动一个字符 k 或 向上箭头键 (↑) 光标向上移动一个字符 l 或 向右箭头键 (→) 光标向右移动一个字符 如果你将右手放在键盘上的话 , 你会发现 hjkl 是排列在一起的 , 因此可以使用这四个按钮来移动光标 . 如果想要进行多次移动的话 , 例如向下移动 30 行 , 可以使用 \"30j\" 或 \"30↓\" 的组合按键 , 亦即加上想要进行的次数 (数字) 后 , 按下动作即可 按键 说明 [Ctrl] + [f] 屏幕 向下 移动一页 , 相当于 [Page Down]按键 (常用) [Ctrl] + [b] 屏幕 向上 移动一页 , 相当于 [Page Up] 按键 (常用) [Ctrl] + [d] 屏幕 向下 移动半页 [Ctrl] + [u] 屏幕 向上 移动半页 + 光标移动到非空格符的下一列 - 光标移动到非空格符的上一列 n 那个 n 表示 数字 , 例如 20 ; 按下数字后再按空格键 , 光标会向右移动这一行的 n 个字符 . 例如 20 则光标会向后面移动 20 个字符距离 0 或功能键[Home] 这是数字 0 : 移动到这一行的最前面字符处 (常用) $ 或功能键[End] 移动到这一行的最后面字符处 (常用) H 光标移动到这个屏幕的最上方那一行的第一个字符 M 光标移动到这个屏幕的中央那一行的第一个字符 L 光标移动到这个屏幕的最下方那一行的第一个字符 G 移动到这个档案的最后一行(常用) nG n 为数字 . 移动到这个档案的第 n 行 . 例如 20G 则会移动到这个档案的第 20 行(可配合 :set nu) gg 移动到这个档案的第一行 , 相当于 1G (常用) n n 为数字 . 光标向下移动 n 行 (常用) 查找与替换 按键 说明 /word 向光标之下寻找一个名称为 word 的字符串 . 例如要在档案内搜寻 lyon 这个字符串 , 就输入 /lyon 即可 (常用) ?word 向光标之上寻找一个字符串名称为 word 的字符串 n 这个 n 是英文按键 . 代表 重复前一个搜寻的动作 . 举例来说 , 如果刚刚我们执行 /lyon 去向下搜寻 lyon 这个字符串 , 则按下 n 后 , 会向下继续搜寻下一个名称为 lyon 的字符串 . 如果是执行 ?lyon 的话 , 那么按下 n 则会向上继续搜寻名称为 lyon 的字符串 N 这个 N 是英文按键 . 与 n 刚好相反 , 为 反向 进行前一个搜寻动作 . 例如 /lyon 后 , 按下 N 则表示 向上 搜寻 lyon . :n1,n2s/word1/word2/g n1 与 n2 为数字 . 在第 n1 与 n2 行之间寻找 word1 这个字符串 , 并将该字符串取代为 word2 , 举例来说 , 在 100 到 200 行之间搜寻 lyon 并取代为 lyon 则 : :100,200s/lyon/lyon/g (常用) :1,$s/word1/word2/g 从第一行到最后一行寻找 word1 字符串 , 并将该字符串取代为 word2 (常用) :1,$s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串 , 并将该字符串取代为 word2 , 且在取代前显示提示字符给用户确认 (confirm) 是否需要取代 (常用) 删除 , 复制与粘帖 按键 说明 x, X 在一行字当中 , x 为向后删除一个字符 (相当于 [del] 按键) , X 为向前删除一个字符(相当于 [backspace] 亦即是退格键) (常用) nx n 为数字 , 连续向后删除 n 个字符 . 举例来说 , 我要连续删除 10 个字符 , 10x . dd 删除游标所在的那一整列(常用) ndd n 为数字 . 删除光标所在的向下 n 列 , 例如 20dd 则是删除 20 列 (常用) d1G 删除光标所在到第一行的所有数据 dG 删除光标所在到最后一行的所有数据 d$ 删除游标所在处 , 到该行的最后一个字符 d0 那个是数字的 0 , 删除游标所在处 , 到该行的最前面一个字符 yy 复制游标所在的那一行(常用) nyy n 为数字 . 复制光标所在的向下 n 列 , 例如 20yy 则是复制 20 列 (常用) y1G 复制游标所在列到第一列的所有数据 yG 复制游标所在列到最后一列的所有数据 y0 复制光标所在的那个字符到该行行首的所有数据 y$ 复制光标所在的那个字符到该行行尾的所有数据 p, P p 为将已复制的数据在光标下一行贴上 , P 则为贴在游标上一行 , 举例来说 , 我目前光标在第 20 行 , 且已经复制了 10 行数据 . 则按下 p 后 , 那 10 行数据会贴在原本的 20 行之后 , 亦即由 21 行开始贴 . 但如果是按下 P , 那么原本的第 20 行会被推到变成 30 行 (常用) J 将光标所在列与下一列的数据结合成同一列 c 重复删除多个数据 , 例如向下删除 10 行 , [ 10cj ] u 复原前一个动作 (常用) [Ctrl]+r 重做上一个动作 (常用) . 不要怀疑 , 这就是小数点 , 意思是重复前一个动作的意思 . 如果你想要重复删除、重复贴上等等动作 , 按下小数点 . 就好了 (常用) 第二部分 🍀 一般模式切换到编辑模式的可用的按钮说明 按键 说明 i, I 进入插入模式(Insert mode) : i 为 从目前光标所在处插入 , I 为 在目前所在行的第一个非空格符处开始插入 (常用) a, A 进入插入模式(Insert mode) : a 为 从目前光标所在的下一个字符处开始插入 , A 为 从光标所在行的最后一个字符处开始插入 (常用) o, O 进入插入模式(Insert mode) : 这是英文字母 o 的大小写 . o 为 在目前光标所在的下一行处插入新的一行 ； O 为在目前光标所在处的上一行插入新的一行 (常用) r, R 进入取代模式(Replace mode) : r 只会取代光标所在的那一个字符一次；R会一直取代光标所在的文字 , 直到按下 ESC 为止 (常用) [Esc] 退出编辑模式 , 回到一般模式中(常用) 第三部分 🍀 一般模式切换到命令行模式的可用的按钮说明 按键 说明 :w 将编辑的数据写入硬盘档案中 (常用) :w! 若文件属性为 只读 时 , 强制写入该档案 . 不过 , 到底能不能写入 , 还是跟你对该档案的档案权限有关啊 :q 离开 vi (常用) :q! 若曾修改过档案 , 又不想储存 , 使用 ! 为强制离开不储存档案 :wq 储存后离开 , 若为 :wq! 则为强制储存后离开 (常用) ZZ 这是大写的 Z 喔 , 若档案没有更动 , 则不储存离开 , 若档案已经被更动过 , 则储存后离开 :w [filename] 将编辑的数据储存成另一个档案（类似另存新档） :r [filename] 在编辑的数据中 , 读入另一个档案的数据 . 亦即将 filename 这个档案内容加到游标所在行后面 :n1,n2 w [filename] 将 n1 到 n2 的内容储存成 filename 这个档案 :! command 暂时离开 vi 到指令列模式下执行 command 的显示结果 , 例如 :! ls /home 即可在 vi 当中察看 /home 底下以 ls 输出的档案信息 vim 环境的变更 按键 说明 :set nu 显示行号 , 设定之后 , 会在每一行的前缀显示该行的行号 :set nonu 与 set nu 相反 , 为取消行号 "},"09-Linux/Docker/":{"url":"09-Linux/Docker/","title":"Docker","keywords":"","body":"Docker 介绍 Docker 是一个开源的应用容器引擎 , 基于 Go 语言 并遵从 Apache2.0 协议开源 Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中 , 然后发布到任何流行的 Linux 机器上 , 也可以实现虚拟化 功能和组件 Docker 的实现引入了以下核心概念 : Docker 客户端 Docker daemon Docker 容器 Docker 镜像 Registry Docker客户端 Docker 是一个典型的 C/S 架构的应用程序 , 但在发布上 , Docker 将客户端和服务器端统一在同一个二进制文件中 , 不过 , 这只是对于 Linux 系统而言的 , 在其他平台如 Mac 上 , Docker 只提供了客户端 Docker 客户端一般通过 Docker command 来发起清楚 , 另外 , 也可以通过 Docker 提供的一整套 RESTful API 来发起请求 , 这种方式更多地被应用在应用程序的代码中 Docker daemon Docker daemon 也可以被理解为 Docker Server , 另外 , 人们也常常用 Docker Engine 来直接描述它 , 因为这实际上就是驱动整个 Docker 功能的核心引擎 简单地说 , Docker daemon 实现的功能就是接收客户端发来的请求 , 并实现请求所要求的功能 , 同时针对请求返回响应的结果 , 在功能的实现上 , 因为涉及了容器 , 镜像 , 存储等多方便的内容 , daemon 内部的机制会复杂很多 , 涉及了多个模块的实现和交互 Docker容器 在 Docker 的功能和概念中 , 容器是一个核心内容 , 相对于传统虚拟化 , 它作为一项基础技术在性能上给 Docker 带来了极大优势 在概念上 , 容器很好地诠释了 Docker 集装箱的理念 , 集装箱可以存放任何货物 , 可以通过邮轮将货物运输到世界各地 , 运输集装箱的邮轮和装载卸载集装箱的码头都不用关心集装箱里的货物 , 这是一种标准的集装和运输方式 ; 类似的 , Docker 的容器就是 \"软件界的集装箱\" , 它可以安装任意的软件和库文件 , 做任意的运行环境配置 , 开发及运维人员在转义和部署应用的时候 , 不用关心容器里装了什么软件 , 也不用了解它们是如何配置的 , 而管理容器的 Docker 引擎同样不关心容器里的内容 , 它只要像码头工人一样让这个容器运行起来就可以了 , 就像所有其他容器那样 Docker镜像 与容器相对应 , 如果说容器提供了一个完整的 , 隔离的运行环境 , 那么镜像则是这个运行环境的静态体现 , 是一个还没有运行起来的 \"运行环境\" 相对传统虚拟化中的 ISO 镜像 , Docker 镜像要轻量化很多 , 它只是一个可定制的 rootfs , Docker 镜像的另一个创新是它是层级的并且是可服用的 Docker 镜像通常是通过 Dockerfile 来创建的 , Dockerfile 提供了镜像内容的定制 , 同时也体现了层级关系的建立 , 另外 Docker 镜像也可以通过使用 docker commit 这样的命令来手动将修改后的内容生成镜像 Registry Registry 是一个存放镜像的仓库 , 它通常被部署在互联网服务或者云端 , 通常 , 集装箱是需要通过邮轮经行海洋运输到世界各地的 , 而互联网时代的传输则要方便很多 , 在镜像的传输过程中 , Registry 就是这个传输的重要中转站 ; 假如我们在公司将一个软件的运行环境作成镜像 , 并上传到 Registry 中 , 这时就可以很方便地在家里的笔记本上 , 或者在客户的生产环境上直接从 Registry 上下载并运行了 比较 跟传统VM比较 , Docker 具有如下优点 : 操作启动快 , 运行时的性能可以获取极大提升 , 管理操作(启动 , 停止 , 开始 , 重启等等) 都是以秒或毫秒为单位的 轻量级虚拟化 , 你会拥有足够的“操作系统” , 仅需添加或减小镜像即可 开源免费 , 开源的 , 免费的 , 低成本的 , 由现代Linux内核支持并驱动 前景及云支持 跟传统VM比较 , Docker 具有如下缺点 : 它是一项新型的技术 , 可能还不够稳定 Go 语言还没有完全成熟 "},"09-Linux/Git/":{"url":"09-Linux/Git/","title":"Git","keywords":"","body":"Git "},"09-Linux/Git/01-Git&GitHub.html":{"url":"09-Linux/Git/01-Git&GitHub.html","title":"Git&GitHub","keywords":"","body":"Git&Github 1. Git简介 🍀 1.1. Git 🍀 Git是一个免费并且开源的分布式版本控制系统 , 被设计用来快速 , 高效的管理一切从小到大的项目 1.2. 版本控制 🍀 我们所开发的软件在其整个生命周期 , 都需要进行不断的改进 , 比如 , 日常写bug改bug , 对业务的增删改查等 , 如果没有一个工具来帮助我们控制 , 那么日常开发将会是多么的坑 ... 一不小心删了个文件 , 一不小心提交了错误代码 , 一不小心整个项目就没了 ... 所以为了方便我们在开发软件中对各个软件版本的管理与控制 , 于是就有了如下版本控制系统 : CVS , (Concurrent Version System) 诞生于1985年 , 是由荷兰阿姆斯特丹VU大学的Dick Grune教授实现的 , 是有史以来第一个被大规模使用的版本控制工具 —— 开启版本控制大爆发 SVN , (Subversion) 诞生于2000年 , 由CollabNet公司资助并开始开发 , 由于其命令行工具名为svn , 因此通常被简称为SVN , 目的是取代CVS —— 集中式版本控制集大成者 Git 诞生于2005年 , 又Linux系统的创始人Linus开发而成 , 主要是因为在2005年Andrew Tridgell (大名鼎鼎的Samba的作者) 社团对BitKeeper进行反向工程 , 于是激怒了BitKeeper软件的所有者BitMover公司 , 收回了对Linux社区免费使用BitKeeper的授权 , 导致Linus第二个伟大作品 —— 分布式版本控制系统Git 1.3. 集中式与分布式 🍀 集中式 集中式版本控制系统 , 将所有数据集中存放在服务器中 , 有便于管理的优点 , 如SVN 集中式只存在一个仓库 , 如果开发者开发所处的环境不能连接到服务器 , 就无法获取最新的源代码 , 开发也就无法进行 ; 并且一旦服务器宕机 , 万一服务器故障导致数据消失 , 那么开发者就再也见不到最新的源代码了 分布式 分布式版本控制系统 , 每个人都有完整的版本库 , 开发者不必连接远程仓库 , 如Git 分布式中每个开发者都具有独立的完整的版本库 , 安全性高 ; 在分布式版本控制系统中通常会有一台服务器充当 \"中央服务器\" , 这个服务器的作用仅仅是用来方便 \"交换\" 开发者们之间的修改 , 当然这仅仅是为了使开发者们之间 \"交换\" 更加方便 所以集中式与分布式都具有各自的优缺点 , 在使用时 , 我们应该根据具体情况选择 不过随着Git与Github的普及 , 使用分布式的开发者会占绝大多数 ; 只要规则指定得当 , 分布式同样能够像集中式那样进行管理 2. GitHub简介 🍀 Github 与 Git 是两回事 GitHub 是为开发者提供Git仓库的托管服务 , 但是注意 Github并不只是 Git仓库的托管服务 在Git中 , 开发者将源代码存入名叫 \"Git仓库\" 的资料库中并加以使用 , 而Github则是在网络上提供Git仓库的一项服务 也就是说 , Github上公开的软件源代码全都是由Git进行管理 2.1. GitHub提供的主要功能 🍀 Git 仓库 我们可以免费建立任意个 GitHub 提供的 Git 仓库 , 但如果需要建立只对特定人物或只对自己公开的私有仓库 , 则需要依照套餐类型B支付每月最低 7 美元的使用费 Organization 企业导入 GitHub 时建议使用 Organization 账户 , 利用这一功能 , 可以让开发者们使用同一控制面板 , 还能够创建团队并统一管理权限 ; 另外这一账户还为企业提供了用户管理和支付等便捷功能 Issue Issue 功能 , 是将一个任务或问题分配给一个 Issue 进行追踪和管理的功能 Wiki 该功能常用在开发文档或手册的编写中 , Wiki 页也是作为 Git 仓库进行管理的 , 改版的历史记录会被切实保存下来 , 使用者可以放心改写 Pull Request 开发者向 GitHub 的仓库推送更改或功能添加后 , 可以通过 Pull Request 功能向别人的仓库提出申请 , 请求对方合并 Pull Request 送出后 , 目标仓库的管理者等人将能够查看 Pull Request 的内容及其中包含的代码更改 3. 安装Git 🍀 在 Linux 上 $ sudo apt-get install git 在 Windows 上 下载Git , 下载完成后之前安装就可以了 安装成功后我们可以在开始菜单里找到 Git Bash , 或者查看鼠标右键菜单中是否有Git Bash Here 3.1. 初始设置 🍀 显示当前配置 $ git config --list 设置姓名和邮箱地址 $ git config --global user.name \"lyon\" $ git config --global user.email \"lyon@xxxxx\" # lyon@xxx为邮箱地址 提高命令输出的可读性 $ git config --global color.ui auto 4. GitHub准备工作 🍀 4.1. 创建账户 🍀 进入注册页面完成注册 , 点我跳转注册页面 4.2. 设置SSH Key 🍀 GitHub上连接已有的仓库时的认证 , 是通过使用SSH的公开密钥认证方式进行的 , 所以我们需要创建一个SSH Key , 并将其添加至GitHub中 $ ssh-keygen -t rsa -C \"lyon@xxx\" # lyon@xxx为邮箱地址 Generating public/private rsa key pair. Enter file in which to save the key (/Users/your_user_directory/.ssh/id_rsa): # 回车 Enter passphrase (empty for no passphrase): # 输入密码 Enter same passphrase again: # 输入密码 创建完成后可以在你的User目录下可以看到一个.ssh的文件夹 : .ssh ├── id_rsa 私有密钥 ├── id_rsa.pub 公有密钥 ├── known_hosts └── qwe.ppk 4.3. 添加公钥 🍀 创建好密钥之后 , 我们需要在GitHub中添加公有密钥 , 这样就可以使用私有密钥进行认证了 进入头像下拉框中的settings , 添加公有密钥 , 即复制id_rsa.pub中的内容 添加完成之后 , 创建账号时所用的邮箱会接到一封邮件 , 提示 \"公有密钥添加完成\" 完成后 , 可以用手中的私有密钥与GitHub进行认证和通信 , 如下 : $ ssh -T git@github.com 出现如下结果即为成功 Hi hirocastest! You've successfully authenticated, but GitHub does not provide shell access. 5. 创建仓库 🍀 点击头像旁边的 \"+\" 下拉框 , 选择New repository Repository name 仓库名 Descriptiion 描述信息 Public , Private 选择创建公开仓库还是私有仓库 , 私有仓库是收费的 Initialize this repository with a README 选择自动完成初始化仓库 , 并设置README文件 如果想向GitHub添加手中已有的仓库 , 建议不要勾选 Add .gitignore 添加.gitignore文件 Add a license 添加的许可协议 "},"09-Linux/Git/02-Git基础命令.html":{"url":"09-Linux/Git/02-Git基础命令.html","title":"Git基础命令","keywords":"","body":"Git基础命令 1. 前言 🍀 1.1. 工作区 🍀 对于Git来说 , 版本库位于工作区根目录下的.git目录中 , 且仅此一处 , 在工作区的子目录下则没有任何其他跟踪文件或目录 而工作区就是我们进行版本控制的某个文件夹 , 我们初始化之后就可以利用Git来进行管理了 1.2. 暂存区 🍀 在.git 目录中有一个index文件 , 这个文件就是暂存区(stage) , 当我们执行git add命令时 , 我们的修改并不是直接添加到了master分支 , 而是添加到了暂存区 , 我们需要继续执行git commit命令才能将修改从暂存区移到master分支 , 这样才算完成了一次提交 2. 基本操作 🍀 2.1. 初始化仓库 🍀 我们要使用Git进行版本管理 , 必须先初始化仓库 Git 使用git init命令进行初始化 我们可以打开Git Bash后手动切换到仓库 , 或者到仓库目录点击右键选择Git Bash Here $ mkdir git-tutorial $ cd git-tutorial $ git init Initialized empty Git repository in /Users/github-book /git-tutorial/.git/ 初始化成功后 , 会自动生成.get目录 , 这个.git 存储着管理当前目录内容所需的仓库数据 , 在Git中 , 我们将这个目录的内容称为 \"附属于该仓库的工作区\" 2.2. 查看仓库状态 🍀 git status 命令用于显示Git仓库的状态 , 工作区和仓库在被操作的过程中 , 状态会不断发生变化 , 在Git操作过程中经常用git status命令查看 \"当前状态\" $ git status On branch master # 当前处于master分支 Initial commit nothing to commit (create/copy files and use \"git add\" to track) # 没有可提交的内容 2.3. 增删文件 🍀 我们一般创建一个GitHub远程仓库时 , 都会选择不自动初始化 , 而是自己来建立README.md文件作为管理对象 , 为第一次提交做前期准备 我们可以使用touch命令来创建文件 新建README.md到本地仓库 $ touch README.md # 创建一个新文件README.md $ git status # 查看仓库状态 On branch master Initial commit Untracked files: (use \"git add ...\" to include in what will be committed) README.md nothing added to commit but untracked files present (use \"git add\" to track) 我们可以看到在Untracked files中显示了README.md文件 , 类似地 , 只要对Git的工作区或仓库进行操作 , git status命令的显示结果就会发生改变 向暂存区中添加文件 如果只是用Git仓库的工作区创建了文件 , 那么该文件并不会被记入Git仓库的版本管理对象当中 , 因此我们用git status命令查看时 , 新增的README.md文件时 , 它会显示在Untracked files中 所以如果要想让文件成为Git仓库的管理对象 , 就需要用git add命令将其加入暂存区 (Stage或者Index) . 暂存区是提交之前的一个临时区域 $ git add README.md # 添加至暂存区 $ git status # 查看仓库状态 On branch master Initial commit Changes to be committed: (use \"git rm --cached ...\" to unstage) new file: README.md 将README.md文件加入暂存区后 , git status命令的显示结果发生了变化 , Changes to be committed: 中显示new file: README.md 如果想要删除暂存区中的文件 , 可以使用git rm [filename]命令进行删除 保存仓库的历史记录 当我们使用git add命令之后 , 需要使用git commit命令将当前暂存区中的文件实际保存到仓库的历史记录中 , 通过这些记录 , 我们就可以在工作区中复原文件 , 在git commit中有一个-m参数 , 为提交信息 , 是对这个提交的概述 $ git commit -m \"First commit\" [master (root-commit) 4733231] First commit 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 README.md $ git status On branch master nothing to commit, working tree clean 2.4. 查看日志 🍀 git log 命令可以查看以往仓库中提交的日志 , 包括可以查看什么人在什么时候进行了提交或合并 , 以及操作前后有怎样的差别 $ git log commit 4733231b262d9cd1d4449240735ed56edab65ca1 (HEAD -> master) Author: lyonyang Date: Mon May 29 22:56:50 2017 +0800 First commit 显示提交信息的第一行 可以在git log命令后加上--pretty=short来只查看提交信息的第一行 $ git log --pretty=short commit 4733231b262d9cd1d4449240735ed56edab65ca1 (HEAD -> master) Author: lyonyang First commit 显示指定目录或文件日志 只要在git log命令后加上目录名 , 就可以显示改目录下的日志 , 如果是文件名 , 就会只显示与该文件相关的日志 $ git log README.md commit 4733231b262d9cd1d4449240735ed56edab65ca1 (HEAD -> master) Author: lyonyang Date: Mon May 29 22:56:50 2017 +0800 First commit 显示文件的改动 如果想查看提交所带来的改动 , 可以加上-p参数 $ git log -p # 指定文件 $ git log -p README.md 2.5. 查看更改前后区别 🍀 git diff命令可以查看工作区 , 暂存区 , 最新提交至今的差别 通过vim命令修改README.md $ vim README.md +# edit README.md +# +# First 执行git diff命令 , 查看当前工作区与暂存区的差别 $ git diff diff --git a/README.md b/README.md index e69de29..88b52b3 100644 --- a/README.md +++ b/README.md @@ -0,0 +1,5 @@ +# edit README.md + +# First 如果工作区和暂存区的状态并无差别 , 那么我们在执行git commit命令之前先执行git diff HEAD命令 , 查看本次提交与上次提交之间有什么差别 , HEAD是指向当前分支中最新一次提交的指针 , 这是一个好习惯 3. 分支操作 🍀 在进行多个并行作业时 , 我们会用到分支 , 而在我们日常开发中 , 基本都会采用并发作业的方式 ; 在这类并行开发的过程中 , 往往同时存在多个最新代码状态 从master分支创建了feature-A分支和fix-B分支 在不同的分支中 , 可以同时进行完全不同的作业 , 等改分支的作业完成之后 , 再与master分支合并 , 如下 : 通过灵活的使用分支 , 可以让多个人同时高效地进行并行开发 , 并且使开发过程更加的安全 3.1. 显示分支 🍀 git branch命令可以将分支名列表显示 , 同时可以确认当前所在分支 $ git branch * master # 当前只有一个master分支 可以看到master分支左侧标有 * 号 , 这表示我们当前所在分支 , 也就是说 , 我们正在master分支下进行开发 3.2. 创建分支 🍀 git checkout -b命令可以用于我们以当前master分支为基础 , 创建一个新的分支 , 并切换到新建分支 $ git checkout -b feature-A # 创建分支并切换 Switched to a new branch 'feature-A' M README.md $ git branch # 显示分支 * feature-A master 创建并切换我们通过以下两条命令也能收到同样的效果 $ git branch feature-A # 创建分支feature-A $ git checkout feature-A # 切换到分支feature-A 在feature-A分支中修改文件并提交 $ vim README.md # 修改文件 +# +# branch feature-A $ git add README.md # 添加至暂存区 $ git commit -m \"Add feature-A\" # 提交 [feature-A 78c070a] Add feature-A 1 file changed, 5 insertions(+) 3.3. 切换分支 🍀 我们在feature-A分支下对文件进行了修改后 , 可以使用git checkout [branch]切分支 $ git checkout master # 切换到master分支 Switched to branch 'master' $ vim README.md # 进入文件,我们可以发现没有发生改变 因为我们的修改只是在feature-A分支上建立的 , 而各个分支都是相互独立的 , 所以在feature-A分支上做任何事情也不会影响到其他的分支 切换回上一个分支 如果想要切换到上一个分支 , 可以使用 - (连字符)代替分支名 $ git checkout - # 从master分支继续切换回feature-A分支 Switched to branch 'feature-A' 3.4. 合并分支 🍀 通过合并分支 , 可以将一个分支的内容合并到另一个分支中去 , 比如我们上面修改了feature-A分支 , 现在将其与master分支进行合并 git merge命令用来合并分支 , 首先切换到master分支 $ git checkout master Switched to branch 'master' Merge branch 'feature-A' # Please enter a commit message to explain why this merge is necessary, # especially if it merges an updated upstream into a topic branch. # # Lines starting with '#' will be ignored, and an empty message aborts # the commit. # 上述内容保存后关闭即可 $ git merge --no-ff feature-A Merge made by the 'recursive' strategy. README.md | 5 +++++ 1 file changed, 5 insertions(+) 为了在历史记录中明确记录下本次分支合并 , 我们需要创建合并提交 , 因此 , 在合并时加上 --no-ff参数 这样就完成了分支的合并了 用git log --graph命令可以以图表的形式查看分支提交的内容已被合并 $ git log --graph * commit a905c7028b96d2c003970b095a20b22a03ccc3ad (HEAD -> master) |\\ Merge: 4733231 78c070a | | Author: lyonyang | | Date: Mon May 29 23:31:47 2017 +0800 | | | | Merge branch 'feature-A' | | | * commit 78c070aeb464329116f1fc1bf7f84f8201bf7165 (feature-A) |/ Author: lyonyang | Date: Mon May 29 23:12:23 2017 +0800 | | Add feature-A | * commit 4733231b262d9cd1d4449240735ed56edab65ca1 Author: lyonyang Date: Mon May 29 22:56:50 2017 +0800 First commit 4. 版本回溯 🍀 4.1. 回溯 🍀 我们可以使用git rest --hard命令让仓库的HEAD , 暂存区 , 工作区回溯到指定状态 , 只要提供目标时间点的哈希值 , 就可以完全恢复至该时间点的状态 $ git reset --hard 78c070aeb464329116f1fc1bf7f84f8201bf7165 HEAD is now at 78c070a Add feature-A git log命令只能查看以当前状态为终点的历史日志 , 而git reflog命令可以查看当前仓库的操作日志 ; 所以如果我们想恢复到回溯之前的版本 , 可以先执行gitreflog命令, 查看当前仓库执行过的操作的日志 $ git reflog 78c070a (HEAD -> master, feature-A) HEAD@{0}: reset: moving to 78c070aeb464329116f1fc1bf7f84f8201bf7165 a905c70 HEAD@{1}: merge feature-A: Merge made by the 'recursive' strategy. 4733231 HEAD@{2}: checkout: moving from feature-A to master 78c070a (HEAD -> master, feature-A) HEAD@{3}: checkout: moving from master to feature-A 4733231 HEAD@{4}: checkout: moving from feature-A to master 78c070a (HEAD -> master, feature-A) HEAD@{5}: commit: Add feature-A 4733231 HEAD@{6}: checkout: moving from master to feature-A 4733231 HEAD@{7}: commit (initial): First commit 只要我们不进行Git的GC(Garbage Collection, 垃圾回收) , 就可以通过日志随意调去近期的历史状态 , 这样我们就可以在过去未来中自由穿梭 4.2. 消除冲突 🍀 合并分支有时会出现冲突的情况 , 我们创建一个新的分支来进行说明 在feature-B分支上进行修改 创建并切换feature-B分支 $ git checkout -b feature-B Switched to a new branch 'feature-B' 在feature-B分支上修改README.md文件内容 $ vim README.md # 在README.md最后添加内容 # +add this line to B branch 提交本次修改 $ git add README.md # 添加至暂存区 $ git commit -m \"add a line to the featrue-B branch\" # 将暂存区的内容提交到当前分支 [feature-B 89be876] add a line to the featrue-B branch 1 file changed, 1 insertions(+) 在master分支上进行修改 切换到master分支 $ git checkout master Switched to branch 'master' 在master分支上修改README.md文件内容 $ vim README.md # 在README.md最后添加内容 # +add this line to B branch 提交本次修改 $ git add README.md $ git commit -m \"add a line to the master branch\" [master 0abd9eb] add a line to the master branch 1 file changed, 1 insertions(+) 到这里 , 两个分支上的README.md文件中的内容就不一样了 , 接下来我们合并这两个分支 合并分支 # 当前在master分支 $ git merge feature-B Auto-merging README.md CONFLICT (content): Merge conflict in README.md Automatic merge failed; fix conflicts and then commit the result. Git返回的内容告诉我们 , README.md文件存在冲突 , 必须手动解决冲突后再提交 通过git status查看冲突 $ git status On branch master You have unmerged paths. (fix conflicts and run \"git commit\") (use \"git merge --abort\" to abort the merge) Unmerged paths: (use \"git add ...\" to mark resolution) both modified: README.md no changes added to commit (use \"git add\" and/or \"git commit -a\") 查看README.md文件 $ cat README.md # edit README.md # First # branch feature-A ... #>>>>>> feature-B Git用 , ======= , >>>>>>> 标记出不同分支的内容 , 为了解决冲突 , 我们需要修改一方来解决冲突 , 比如修改master分支或者feature-B分支中的README.md文件的内容 , 修改完成后保存 , 然后再提交 $ git add readme.txt $ git commit -m \"conflict fixed\" [master 1f4c296] conflict fixed 再合并分支 $ git merge feature-B Already up-to-date. 查看分支的合并情况 $ git log --graph --pretty=oneline --abbrev-commit * 1f4c296 (HEAD -> master) conflict fixed |\\ | * 89be876 (feature-B) add a line to the featrue-B branch * | 0abd9eb add a line to the master branch |/ * 78c070a (feature-A) Add feature-A * 4733231 First commit 最后 , 删除feature-B分支 $ git branch -d feature-B Deleted branch feature-B (was 89be876). 这样我们解决分支冲突问题 4.3. 分支管理 🍀 通常 , 合并分支时 , 如果课可能 , Git会使用Fast forward模式 , 但是这种模式下 , 一旦我们删除分支后 , 那么分支的信息也随之被丢掉了 , 所以为了保留历史的分支信息 , 我们可以强制禁用Fast forward模式 在合并分支时使用--no-f参数 , 就可以禁用Fast forward $ git merge --no-ff feature-A Already up-to-date. 本篇主要参考以下两本书籍 : GitHub入门与实践 Git权威指南 "},"09-Linux/Git/GitHub Pages&Gitbook&Travis CI持续构建博客.html":{"url":"09-Linux/Git/GitHub Pages&Gitbook&Travis CI持续构建博客.html","title":"GitHub Pages&Gitbook&Travis CI持续构建博客","keywords":"","body":"GitHub Pages&Gitbook&Travis CI持续构建博客 欢迎收藏交流 , 如需转载 , 请注明出处 1. 开始 🍀 如今程序猿没个个人站点或是博客 , 都不好意思出门了 所以在这里教大家如何构建一本书 (博客) : https://lyonyang.github.io/blogs/ 这是我的个人博客 , 样式就是这样的啦 那么 , 开始吧 1.1. 搭建准备 🍀 实际上 , 没什么要准备的 ... GitHub账号就不用说了 , 创建一个新的仓库吧 , 来存放个人博客笔记文件 1.2. 搭建要求 🍀 本博客 (更形象点 : 一本书) , 文件必须为.md文件 , 也就是MarkDown 文件 , 所以如果你不是这种格式 , 那么我建议你开始使用 , Markdown语法说明 , 当然还有.rst , 也就是reStructuredText 文件也是可以的 , 但是本文仅说明关于Markdown的构建 为什么用Markdown? Markdown是一种轻量级的「标记语言」 , 通常为程序员群体所用 , 适用于泡技术论坛、写博客日志、技术文稿、记录代码片段、起草邮件等场景 Markdown语法十分简单 , 常用的标记符号不超过十个 , 用于日常写作记录绰绰有余 纯文本编辑 , 可以转换成各种文档格式 , 如 : html , tex , pd等等 最重要的是 , Markdown能在GitHub上直接展示出来 , 所以你可以看到很多GitHub开源项目 (不开源你也看不到啊) 中的README都是.md文件 本地Markdown编辑可以使用Typora , VSCode , Atom等等 , 本人用的是Typora , 支持功能比较多 1.3. 搭建说明 🍀 本博客使用GitBook进行构建 , 如Hexo一样 , 我们并不是使用GitBook所有 , 而是仅仅使用它的一个功能 : GitBook和Hexo都能帮我们把Markdown格式文件转换成html文件 , 并且是附带了样式的html文件 当然这种构建对于我们只使用一两次还好 , 如果像我们的博客需要长期更新 , 那么手动构建就太麻烦了 , 所以我们使用一个持续集成工具 Travis CI , 它是开源的 , 所以放心 , 不要钱 也就是说 , 所有的准备如下 : GitHub + .md + GitBook + Travis CI 那么就开始了 2. 配置文件 🍀 创建仓库就不说了 , 创建完成之后 , 我们就先开始添加配置文件了 , 这是构建的重中之重 注意 : GitHub 现在默认分支不是 master 而是 main , 这里在关联 Travis CI 之前 , 先改成 master 配置文件目录 . ├── .travis.yml -- 持续集成配置 ├── deploy.sh -- 构建脚本 ├── book.json -- gitbook样式文件 ├── summary_create.sh -- 自动创建SUMMARY.md文件 ├── SUMMARY.md -- 必须要有 └── README.md -- 必须要有 2.1. .travis.yml 🍀 当Travis CI发现你的仓库有更新时 , 就会来你仓库找到这个配置文件 , 并执行它 以node_js语言为例 language: \"node_js\" node_js: - \"8\" install: - \"npm install gitbook\" - \"npm install gitbook-cli\" branches: only: - master env: global: - GH_REF: github.com/用户名/仓库名.git script: - bash summary_create.sh - travis_wait 100 bash deploy.sh 注意GH_REF修改成各自的GitHub用户名和仓库 2.2. deploy.sh 🍀 该文件是.travis.yml中需要执行的脚本 #!/bin/bash git config user.name \"user\" git config user.email \"email@xxx.com\" git checkout -b gitbook git status git add . git commit -m \"[Travis] Update SUMMARY.md\" git push -f \"https://${GH_TOKEN}@${GH_REF}\" gitbook:gitbook gitbook install gitbook build . if [ $? -ne 0 ];then exit 1 fi cd _book sed -i '/a href.*\\.md/s#\\.md#.html#g;/a href.*README\\.html/s#README\\.html##g' SUMMARY.html git init git checkout --orphan gh-pages git status sleep 5 git add . git commit -m \"Update gh-pages\" git remote add origin git@github.com:用户名/仓库.git git push -f \"https://${GH_TOKEN}@${GH_REF}\" gh-pages:gh-pages 2.3. summary_create.sh 🍀 #!/bin/bash # 文件命名增加 [0-9][0-9]- 通过文件名对文章进行排序,生成目录 find `ls|egrep -v \"_book|_other|node_modules\"` -type f -name \"*.md\"|sed 's#README.md#00README.md#g'|sort|awk -F \"/\" '{if($NF!=\"00README.md\") print $0\"/\" ;else print $0}' OFS=\"/\"|sed 's#[^/]##g'|awk '{a=(length-1);while(a>0){printf \" \";a--}print \"* \"}' > /tmp/summary_1 find `ls|egrep -v \"_book|_other|node_modules\"` -type f -name \"*.md\"|sed \"s#README.md#00README.md#g\"|sort|awk -F \"[./]\" '{if($(NF-1) != \"00README\") print $(NF-1)\"](\"$0\")\" ;else print $(NF-2)\"](\"$0\")\"}' > /tmp/summary_2 paste -d \"[\" /tmp/summary_1 /tmp/summary_2 > tmp_SUMMARY.md sed 's#00README.md#README.md#g' tmp_SUMMARY.md|grep -v \"SUMMARY](SUMMARY\"|awk '{if(NR==1)print \"# Summary\\n\\n* [介绍](README.md)\\n* [目录](SUMMARY.md)\";else print $0}' > SUMMARY.md && mv tmp_SUMMARY.md /tmp # 由于Mac下,sed -i参数必须要指定备份文件(虽然可以使用 -i \"\" 传递一个空字符,不备份,但是这种写法在Linux上会报错),所以这里不使用-i参数 # 文件名便于排序的时候会使用类似01-,开头, 在目录显示的时候删除这部分 # 调整目录显示, 在Mac 下使用需要调整参数 sed -ri 's#(\\S+* \\[)[0-9]+-(.*$)#\\1\\2#g' SUMMARY.md 2.4. book.json 🍀 GitBook样式文件 , 也就是生成html附带的样式 { \"title\": \"blog\", \"author\": \"author\", \"description\": \"desc\", \"extension\": null, \"generator\": \"site\", \"links\": { \"sharing\": { \"all\": null, \"facebook\": null, \"google\": null, \"twitter\": null, \"weibo\": null } }, \"pdf\": { \"fontSize\": 18, \"footerTemplate\": null, \"headerTemplate\": null, \"margin\": { \"bottom\": 36, \"left\": 62, \"right\": 62, \"top\": 36 }, \"pageNumbers\": false, \"paperSize\": \"a4\" }, \"plugins\": [ \"advanced-emoji\", \"toggle-chapters\", \"theme-comscore\", \"splitter\", \"github\", \"github-buttons@2.1.0\", \"-sharing\", \"-lunr\", \"-search\", \"search-plus\", \"anchor-navigation-ex-toc@0.0.9\", \"editlink\", \"fontsettings\", \"copy-code-button\", \"lightbox\" ], \"pluginsConfig\": { \"github\": { \"url\": \"https://github.com/lyonyang/blogs\" }, \"github-buttons\": { \"repo\": \"lyonyang/blogs\", \"types\": [ \"star\" ], \"size\": \"small\" }, \"editlink\": { \"base\": \"https://github.com/lyonyang/blogs/blob/master\", \"label\": \"编辑本页\" }, \"anchor-navigation-ex-toc\": { \"showLevel\": false, \"mode\": \"float\", \"float\": { \"showLevelIcon\": false, \"level1Icon\": \"fa fa-hand-o-right\", \"level2Icon\": \"fa fa-hand-o-right\", \"level3Icon\": \"fa fa-hand-o-right\" } } } } 2.5. README.md 🍀 项目根目录下必须要有README.md文件 , 并且所有需要构建的文件下 , 也必须要有README.md文件 如 : . ├── Python │ ├── ... │ ├── ... │ └── README.md -- 必须要有 ├── ... ├── ... └── README.md -- 必须要有 README.md中的内容就是页面上目录显示的内容 2.6. SUMMARY.md 🍀 必须要有 , 这个GitBook构建需要的 , 上面summary_create.sh脚本就是来自动帮我们生成目录的 3. 添加Personal access tokens 🍀 配置文件添加完成之后 , 我们就要将Travis CI 与GitHub建立连接了 点击你的GitHub头像 --> Settings --> Developer settings --> Personal access tokens --> Generate new token 进入如下图页面 : 我们只需给个repo权限就行了 接下来就会自动跳转到如下页面 : 注意 : 一定要先复制这个生成的token , 如果你还没来得及复制 , 那么对不起 , 重新添加一个再复制吧 这个token是用来配置Travis的环境变量的 4. 配置Travis CI 🍀 首先我们进入 : https://travis-ci.org/ 使用GitHub登录 接下来我们可以在Profile下面看到我们的GitHub仓库啦 , 注意https://travis-ci.org/ 搜索的我们的Public仓库 , 对于Private仓库 , 就需要进入https://travis-ci.com/ 勾选需要持续集成的仓库 接下来我们去做一些设置了 , 点击勾选右边的设置按钮 勾选General中的Build only if .travis.yml is present Environment Variables 现在我们需要用到上面生成的token了 , 将其复制到Environment Variables中的Value框中 , 并将Name设置为GH_TOKEN , 点击ADD添加 , 完成后如下 : 现在基本已经完成了 , 我们可以向我们的仓库做一点更新 , 或者直接点击More options选择Trigger build进行构建 , 等待几分钟后 (这是GitBook的一个缺点 , 构建有点慢) 待https://travis-ci.org/左侧 My Repositories 变成绿色 (黄色表示正在构建) , 我们就可以访问我们的博客啦 , 如果构建失败了 , 注意检查一下 book.json 中的插件是否还有效 地址 : https://用户名.github.io/仓库名 这个地址使用的是每个仓库自带的GitHub Pages功能 , 在脚本自动创建了gh-pages分支 , 就是用于显示的 至此 , 搭建完成 ! 5. 注意事项 注意事项一 由于构建是从.md文件转到.html文件 , 所以如果文章中使用了模板语言的语法 , 请嵌入html代码块 , 并顶格写 , 示例 ​```html 内容{{ name }} Django 的模板语言的文章 , 你可能需要这个 , 因为模板语言会导致你的博客构建失败 注意事项二 文章排序默认按英文字母排序 , 如果需要对文章进行排序 , 可以在文件名前添加需要 , 如 : 01- , 序号不会在文章页面显示 , 这一步是通过summary_create.sh中实现的 注意事项三 文件夹名称中不得包含空格 你可能需要的一些gitbook插件:插件 "},"09-Linux/Git/Travis  CI.html":{"url":"09-Linux/Git/Travis  CI.html","title":"Travis  CI","keywords":"","body":"Travis CI 介绍 🍀 Travis CI 是一款免费服务 , 专门托管面向开源开发组织的CI (Continuous Integration , 持续集成) CI是XP (Extreme Programming , 极限编程) 的实践之一 , 近年来人们普遍使用Jenkins等软件来实现这一目的 让CI软件监视仓库 , 可以在开发者发送提交后立刻执行自动测试或构建 ; 通过持续执行这样一个操作 , 可以检测出开发者意外发送的提交或无意的逻辑偏差 , 让代码保持在一定质量以上 所以当我们使用GitHub发布代码时 , 可以使用Travis CI ; Travis CI支持Ruby , PHP , Perl , Python , Java , JavaScript (node.js)等Web相关的语言 更多支持语言 , Support language 编写配置文件 🍀 我们如果想要仓库使用Travis CI , 一般情况下 , 我们只需要在仓库中添加.travis.yml这样一个Travis CI专用的文件 , Travis CI 就与GitHub集成了 以node.js为例 , 编写.travis.yml文件 language: \"node_js\" # 指定默认运行环境,这里是node_js node_js: # 指定node版本 - \"node\" install: # 指定安装脚本 - \"npm install gitbook -g\" - \"npm install -g gitbook-cli\" branches: # 指定分支 only: - master env: # 定义环境变量 global: - GH_REF: github.com/[username]/[repository].git - secure: [ssh_key] script: # 指定要运行的脚本 - bash summary_create.sh - travis_wait 100 bash deploy.sh 将这个文件放到本地仓库中 , 然后push给GitHub端 , 我们基本完成了使用Travis CI的准备工作 关于各种语言的配置参考 , 点击进入语言参考 检测配置文件 🍀 Travis CI专门提供了Travis WebLint提供用户检测.travis.yml文件是否存在问题 , 检测时只需要指定仓库即可 与GitHub集成 首先我们访问Travis CI , 点击右上角的Sign in with GitHub , 进行GitHub认证 登录完毕后 , Sign in with GitHub就会变成用户的GitHub信息了 , 点击头像的下拉框的Profile , 我们在下方就可以看到我们的GitHub仓库了 , 只需要将对应的仓库名旁边的开关设置为ON , 就可以对该仓库应用Travis CI了 至此 , 你再进入你的GitHub → → Settings → → Applications → → Authorized OAuth Apps , 我们就可以看到可以访问我们账户的应用程序信息了 , 这里就是Travis CI了 这样 , 我们只要向GitHub进行push操作 , 就会自动触发Travis CI端的自动测试了 集成的仓库在Travis CI端的URL为https://travis-ci.org/用户名/仓库名 , 用户可以在这个页面查看自动测试的执行情况 , 另外跳转至Travis CI首页直接收缩自己的 将Travis CI的结果添加至README.md 如果我们想要在我们的GitHub的README.md中看到我们的测试结果 , 只需在我们的README.md中添加如下信息 : # Markdown语法 [![Build Status](https://secure.travis-ci.org/用户名/仓库名.png?imageMogr2/blur/1x0/quality/75|watermark/2/text/bHlvbi55YW5nQHFxLmNvbQ==/font/YXBhcmFqaXRh/fontsize/560/fill/Izk0ODI4Mg==/dissolve/100/gravity/SouthEast/dx/10/dy/10)](http://travis-ci.org/用户名/仓库名) 这样 , 在我们查看README.md时 , 就能够通过图片观察测试是否通过了 ; 绿色的图片就表示仓库内代码顺利通过测试 , 灰色的图片表示仓库没有通过测试 , 证明仓库可能存在某种问题 , 这样既可以显示仓库的健全性 , 又可以防止自己遗漏Travis CI的结果 生成Access Token 🍀 与GitHub继承之后 , 此时Travis已经开始监控了 , 但是它却没有访问权限 , 所以我们需要生成一个Personal access tokens 进入我们的GitHub , 点击头像下的Settings → Developer settings → Personal access tokens 随后点击Generate new token , 进入New personal access token界面 , 在Token description中加入Travis , 随后选择该令牌所具有的权限范围 , 这里我们只需将repo勾上就可以了 最后Generate token 生成之后一定不要离开页面 , 我们需要快速复制这个token , 虽然添加到Travis的settings中 , 因为一旦页面刷新或者关掉 , 就会消失 , 那么你又得重新创建了 ..... "},"09-Linux/RabbitMQ.html":{"url":"09-Linux/RabbitMQ.html","title":"RabbitMQ","keywords":"","body":"RabbitMQ 介绍 🍀 RabbitMQ 是一个实现了 AMQP 协议标准的开源消息代理和队列服务器 , 和 Beanstalkd 不同的是 , 它是企业级消息系统 , 自带了集群 , 管理 , 插件系统等特性 , 在高可用性 , 可扩展性性 , 易用性等方面做得很好 , 现在被互联网公司广泛使用 安装服务端 $ sudo apt-get install rabbitmq-server -yq 安装客户端 $ pip install pika AMQP 🍀 AMQP (Advanced Message Queuing Protocol , 高级消息队列协议) 是一个异步消息传递所使用的应用层协议规范 , 它的设计初衷是为了摆脱商业 MQ 高额费用和不同 MQ 供应商的接口不统一的问题 , 所以一开始就设计成开放标准 , 以解决企业复杂的消息队列需求问题 基本概念 : 消息 : 消息实际包含两部分内容 : 有效载荷 , 也就是传输的数据 , 数据类型可以纯文本也可以是 JSON 标签 , 它包含交换机的名字和可选的主题标记等 , AMQP 仅仅描述了标签 , 而RabbitMQ 决定了把这个消息发给哪个消费者 发布者 : 也就是生产者 , 它创建消息并且设置标签 消费者 : 消费者连接到代理服务器上 , 接收消息的有效载荷 (注意 , 消费者并不需要消息中的标签) 在 AMQP 模块中 , 为了保证消息被正确取出并执行 , 消息投递失败后会重发 , 于是有了一个消息确认的概念 : 当一个消息从队列中投递给消费者后 , 消费者会通知消息代理 (Broker) , 这个通知可以是自动完成的 , 也可以由处理消息的应用来执行 , 当消息确认 (Ack) 被启用的时候 , 消息代理不会完全将消息从队列中删除 , 除非收到来自消费者的确认回执 AMQP 工作流程如下 : 消息发布者发送消息 , 交换机拿到消息后会将它路由给队列 , 它使用哪种路由算法是由交换机类型和被称作 \"绑定\" 的规则所决定的 , 目前 RabbitMQ 提供了如下四种交换机 : 直接交换机 : 根据消息携带的路由建将消息投递给对应队列 主题交换机 : 通过对消息的路由建和队列到交换机的绑定模式之间的匹配 , 将消息路由给一个或多个队列 扇形交换机 : 将消息路由给绑定到它身上的所有队列 , 且不理会绑定的路由建 , 扇形交换机用来处理消息的广播路由 头交换机 : 一般用不到 , 允许匹配 AMQP 的头而非路由建 , 和直接交换机差不多 , 但是性能差很多 简单示例 🍀 发布者 import sys import pika # %2F是被转义的/,这里使用了默认的虚拟主机,默认的guest这个账号和密码 parameters = pika.URLParameters('amqp://guest:guest@localhost:5672/%2F') # connection就是所谓的消息代理 connection = pika.BlockingConnection(parameters) # 获得信道 channel = connection.channel() # 声明交换机,指定交换类型为直接交换,最后2个参数表示想要持久化的交换机 channel.exchange_declare(exchange='web_develop', exchange_type='direct', passive=False, durable=True, auto_delete=False) if len(sys.argv) != 1: # 使用命令行参数作为消息体 msg = sys.argv[1] else: msg = 'hah' # 创建一个消息, delivery_mode为2表示让这个消息持久化, 重启RabbitMQ也不会丢失 props = pika.BasicProperties(content_type='text/plain', delivery_mode=2) # basic_publish表示发送路由键为xxx_routing_key,消息体为haha的消息给web_develop这个交换机 channel.basic_publish('web_develop', 'xxx_routing_key', msg, properties=props) # 关闭连接 connection.close() 消费者 import pika # 处理接收到的消息的回调函数 # method_frame携带了投递标记, header_frame表示AMQP信息头的对象 # body为消息实体 def on_message(channel, method_frame, header_frame, body): # 消息确认, 确认之后才会删除消息并给消费者发送新的消息 channel.basic_ack(delivery_tag=method_frame.delivery_tag) print body parameters = pika.URLParameters('amqp://guest:guest@localhost:5672/%2F') connection = pika.BlockingConnection(parameters) channel = connection.channel() channel.exchange_declare(exchange='web_develop', exchange_type='direct', passive=False, durable=True, auto_delete=False) # 声明队列, 如果没有就创建 channel.queue_declare(queue='standard', auto_delete=True) # 通过路由键将队列和交换机绑定 channel.queue_bind(queue='standard', exchange='web_develop', routing_key='xxx_routing_key') # 订阅队列 channel.basic_consume(on_message, 'standard') try: # 开始消费 channel.start_consuming() except KeyboardInterrupt: # 退出消费 channel.stop_consuming() connection.close() 官方教程 "},"Read/":{"url":"Read/","title":"Read","keywords":"","body":"读书 "}}